<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-16T04:20:06.173282492Z">09-16</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust：鱼和熊掌可兼得</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=e3cdfca4-cdaa-4e19-aab3-563872bc4c68">
<div class="article-summary-box-inner">
<span><p>国内外不少知名互联网或科技公司，都在采用 Rust 重构技术栈，比如字节的飞书、Google 的 Fuchsia 等。非凸科技也在原有基础上全面升级到互联网新一代技术架构，采用Rust构建智能算法交易平台，逐步迭代，持续为券商、量化私募等众多大型金融机构提供优质的算法服务。</p>
<p>在可预见的未来，Rust 或在大多数领域代替 C/C++，成为开发者的主力语言。</p>
<p>编程语言设计在两个看似不可调和的愿景之间长期存在矛盾对立：</p>
<ol>
<li>安全</li>
</ol>
<p>想要强类型系统静态地排除大量错误</p>
<p>想要自动内存管理</p>
<p>想要数据封装</p>
<p>......</p>
<p>这样我们就能对私有变量执行不变的对象的表示形式，确保它们将不会被不受信任的代码所破坏。</p>
<ol start="2">
<li>控制</li>
</ol>
<p>想要了解数据的字节级表示</p>
<p>想要用底层语言的编程技术优化程序的时间和空间的使用</p>
<p>想要在需要时使用裸机</p>
<p>......</p>
<p>因为对于Web 浏览器、操作系统、游戏引擎这样的系统编程程序，约束它们的性能或资源是一个重要的问题。</p>
<p>然而，按照传统的看法，“鱼和熊掌不可兼得”。</p>
<p>Java语言给了我们极大的安全保障，但代价是牺牲对底层的控制。于是，很多系统编程应用程序，唯一现实的选择是使用一种像C/C++那样提供细粒度的语言控制资源管理。然而，获得这种控制需要很高的成本。例如，微软近期报告表示，修复的70%安全漏洞归因于内存安全违规行为33，且都能能被强类型系统排除。</p>
<p>为解决“不可兼得”的问题，Rust语言应运而生，既能安全系统编程，又能对底层有控制权。</p>
<p>用Rust编写的程序，运行时速度和内存使用量应该和用C编写的程序相差不大，但这两种语言的总体编程风格不同。</p>
<ol>
<li>
<p>Rust 语言抽象程度比C语言更高，抽象会隐藏一些不那么优化的代码。这意味着，默认实现的 Rust 代码性能不是最好的。所以，你的Rust代码必须经过优化才能达到媲美 C 的性能。Unsafe Rust 就是高性能出口。</p>
</li>
<li>
<p>Rust 默认线程安全，消除数据竞争，让多线程并发编程更具实用价值。</p>
</li>
<li>
<p>Rust 在某些方面比C快。理论上，C 语言什么都可以做，但在实践中，C 抽象能力比较低，开发效率也比较低。</p>
</li>
</ol>
<p>现阶段，非凸科技正在寻找行业内优秀的Rust开发工程师，薪资福利超级优厚。关键是团队有很好的Rust开发氛围，Rust大神手把手辅导，助你从Rust新人不断升级。非凸诚挚邀您加入！</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-15 Rust Conf 线上举行</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7ba8720b-dc3a-4082-9d7f-76c0ab282654">
<div class="article-summary-box-inner">
<span><h3>DHCProto 发布，支持 DHCPv4 和 DHCPv6</h3>
<p>DHCP 用于动态分配 IP 地址。DHCProto 是使用 Rust 实现的基础库。</p>
<p>https://leshow.github.io/post/dhcproto/</p>
<h3>使用 Rust 写 shellcode 代码</h3>
<p>shellcode 是啥？黑客攻击壳代码，一般使用汇编语言写。Rust也可以完成此类工作。尝试在这里：</p>
<p>https://kerkour.com/blog/shellcode-in-rust/</p>
<h3>本年度的全球 Rust 大会网络开幕</h3>
<p>本次网络大会的主题：</p>
<ul>
<li>Project Update: Lang Team</li>
<li>Project Update: Libs Team</li>
<li>Move Constructors: Is it Possible?</li>
<li>The Importance of Not Over-Optimizing in Rust</li>
<li>Identifying Pokémon Cards</li>
<li>Fuzz Driven Development</li>
<li>Writing the Fastest GBDT Library in Rust</li>
<li>Whoops! I Rewrote It in Rust</li>
<li>How I Used Rust to Become Extremely Offline</li>
<li>Supercharging Your Code With Five Little-Known Attributes</li>
<li>Compile-Time Social Coordination</li>
<li>Hacking rustc: Contributing to the Compiler</li>
<li>This Week in Rust: 400 Issues and Counting!</li>
</ul>
<p>干货满满哦~</p>
<p>https://rustconf.com/schedule</p>
<p>https://www.youtube.com/watch?v=pLdCcolQsxA</p>
<h3>使用 Rust 开发 FLTK GUI程序</h3>
<p>示例在这里：</p>
<p>https://www.youtube.com/watch?v=S1NSsHZs6hI&amp;feature=youtu.be</p>
<h3>一个使用 axum 框架开发的二维码生成服务</h3>
<p>axum 的一个实战项目示例，是一个真正在跑的服务：</p>
<p>https://github.com/sayanarijit/qrcode.show</p>
<h3>steveklabnik: 拒绝让 Amazon 来定义 Rust</h3>
<p>鉴于 Amazon 拥有很多 Rust 核心组的领导及成员，其影响力在 Rust 社区越来越大。steveklabnik（The Rust Book的作者）站出来，挑起一个话题：拒绝让 Amazon 来定义 Rust。</p>
<p>可持续关注。关于这个，您怎么看？</p>
<p>https://twitter.com/steveklabnik/status/1437441118745071617</p>
<p>--</p>
<p>From 日报小组 Mike</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li>Rustcc论坛: 支持rss</li>
<li>微信公众号：Rust语言中文社区</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">结构体中数据访问</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=3960a9fe-79b2-43d5-bced-5f0e26f07f43">
<div class="article-summary-box-inner">
<span><blockquote>
<p>根据字符串字段名得到结构体中的字段值</p>
</blockquote>
<pre><code>struct A{
    a:i32,
    b:u64
}

macro_rules! get{
    ($var:expr,$field:expr)=&gt;{
        //如何实现返回指定字段？？
    }
}

fn main() {
    let a=A{a:1,b:2};
    let f=get!(a,"a");//期望得到1

}
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">根据不同条件生成不同类型</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=fcd467e0-e6d0-4854-ae0f-5cc3b08c01cb">
<div class="article-summary-box-inner">
<span><pre><code>let val=if true {
   1
}else {
   "ss"
};
println!("{:?}",val);
</code></pre>
<p>怎么实现val在不同条件下得到不同的类型？</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IntelliJ IDEA debug rust问题咨询</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8a356de1-b838-4615-88f5-91b96ed9f449">
<div class="article-summary-box-inner">
<span><p>操作系统：mac os
IntelliJ IDEA（配置内存2G） + RUST插件 + TOML插件 + native debugging support插件
全为最新版</p>
<p>debug 最简单的hello world</p>
<p>提示
Debugger process finished with exit code 139 (interrupted by signal 11: SIGSEGV).Rerun 'Run xxx'</p>
<p>请问这是什么原因？？直接运行正常的</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-14 Why Rust for offensive security</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7750136c-b31b-4e85-ab94-ae1459b4150e">
<div class="article-summary-box-inner">
<span><h2>Why Rust for offensive security</h2>
<blockquote>
<p>想象一下：你的坦克都是纸板做的。然后你的飞机也都是用纸做的，你的海军也全都是纸船，那也太惨了吧？</p>
<p>虽然很荒唐，但是这就是现在的黑客技术的状态。</p>
<p>Imagine: all the tanks of your army are made of cardboard. Now imagine that not only your tanks but also all your airforce is composed of paper planes and your navy of paper vessels. It would be a pretty bad situation, don’t you think?</p>
<p>While it sounds absurd, this is the sad state of hacking today.</p>
</blockquote>
<h3>TL;DR</h3>
<p>文章指出，过去的编程语言（c, Java， python）等都只能局限在一个领域应用，然而现在我们等来了 Rust 救场——不再有奇怪的包管理器、二级制打包工具或者脆弱的网络代码，这些方面的可靠性一旦被黑客们意识到，就可能带来安全攻防的变革。</p>
<p>为了安利可靠的 Rust，作者还写了本书 <a href="https://academy.kerkour.com/black-hat-rust?coupon=BLOG" rel="noopener noreferrer">Black Hat Rust</a>, 来总结自己通过 Rust 在黑客技术中的实践，以其让读者少踩坑，更好地理解 Rust 的可靠。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-13 Rust 在 linux 内核中的最新进展</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=1c26513e-c4c2-4d52-becf-8c39379474e9">
<div class="article-summary-box-inner">
<span><h1>Rust 在 linux 内核中的最新进展</h1>
<p>虽然Rust编程语言在内核中使用的支持还没有登陆到本周末结束的 <code>Linux 5.15</code> 合并窗口，但这项工作仍在进行中。本周，关于Rust在Linux内核中的使用的最新进展被分享了出来。</p>
<p>作为Rust for Linux项目的主要开发人员之一，Miguel Ojeda在本周的Linaro Connect虚拟会议上介绍了该项目，他目前正在为谷歌的合同工作。</p>
<p>对周五的演讲感兴趣的人可以查看下面的 Presentation。</p>
<p><a href="https://bigthinkbuzz.com/the-latest-progress-on-rust-for-the-linux-kernel/" rel="noopener noreferrer">原文链接</a></p>
<p><a href="https://static.linaro.org/connect/lvc21f/presentations/LVC21F-317.pdf" rel="noopener noreferrer">Presentation地址</a></p>
<h1>Matchbox: Rust wasm 中的 p2p 网络解决方案</h1>
<p>Matchbox 的诞生是因为作者在<code>rust</code> 中制作了一款多人网页游戏，遇到了以下问题:</p>
<p>如何使用不可靠的、无序的 p2p connection 连接 N 个web浏览器?</p>
<p><a href="https://johanhelsing.studio/posts/introducing-matchbox" rel="noopener noreferrer">原文链接</a></p>
<h1>Learn Wgpu 更新了</h1>
<p><code>wgrpu</code> 是 <code>WebGPU API spec</code> 的 Rust 实现, 目前这个教程已经更新到了 0.10 版本, 有大量的原理和代码示例讲解.</p>
<p><a href="https://sotrh.github.io/learn-wgpu/beginner/tutorial2-surface/" rel="noopener noreferrer">原文链接</a></p>
<h1>Sycamore: v0.6.0 版本发布了</h1>
<p>Sycamore是一个用 Rust 和 WebAssembly 构建同构web应用程序的库. 目前发布了 0.6.0 版本了.</p>
<ul>
<li>静态生成</li>
<li>服务端渲染</li>
<li>重验证</li>
<li>增量构建</li>
<li>开放构建矩阵</li>
<li>CLI利用，让您轻松和自信地构建应用程序</li>
<li>充分利用 Fluent 开箱即用的 i18n 支持</li>
</ul>
<p><a href="https://sycamore-rs.netlify.app/news/announcing-v0.6.0" rel="noopener noreferrer">原文链接</a></p>
<p>--</p>
<p>From 日报小组 BobQin，FBI小白</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【成都】招聘Rust开发工程师</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ed86a028-a5b8-41e6-9849-e43a55ff7faf">
<div class="article-summary-box-inner">
<span><h2>Rust开发工程师招聘</h2>
<h3>岗位职责：</h3>
<ul>
<li>1、负责电商产品后端功能接口的开发；</li>
<li>2、负责电商产品业务功能开发、迭代和维护，对业务数据进行处理和分析；</li>
<li>3、配合前端开发完成功能的前后台功能联调；</li>
<li>4、配合完成产品测试，BUG修改。</li>
</ul>
<h3>任职要求：</h3>
<ul>
<li>1、后端开发语言基础扎实，有电商产品后端开发经验；</li>
<li>2、熟练使用使用Mysql关系型数据库；</li>
<li>3、至少了解并使用过RocketMQ、RabbitMQ、Kafka中的一种；</li>
<li>4、有Rust语言的基础，或者愿意转Rust开发；</li>
<li>5、三年以上的互联网开发工作经验；</li>
<li>6、熟习微服务或ServicesMesh架构者优先；</li>
</ul>
<p>工作地点四川成都环球时代中心
有意者请发邮件至：shaipe@sina.com 或直接添加微信号：shaipe</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">read_dir 返回的 io::Result<DirEntry> 会在什么情况下返回 Error 呢？</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c1f3f464-9146-4b43-8467-7eacc8f53bf8">
<div class="article-summary-box-inner">
<span><p><code>read_dir</code> 迭代的时候给到的是一个 <code>io::Result&lt;DirEntry&gt;</code>，文档里面只是简单说了 <strong>New errors may be encountered after an iterator is initially constructed.</strong></p>
<p>但是具体这个 New errors 到底是什么？</p>
<p>我试过了在迭代的时候对文件夹或者里面的文件作删除、重命名、改变权限，都没有返回 error；
（测试平台包括 Mac 和 Linux，没有 windows 暂时没测）。</p>
<p><img src="https://i.loli.net/2021/09/13/AbE9KdTL1sSxWJg.png" alt="screenshot-20210913-174925.png"></p>
<pre><code>/// Iterator over the entries in a directory.
///
/// This iterator is returned from the [`read_dir`] function of this module and
/// will yield instances of [`io::Result`]`&lt;`[`DirEntry`]`&gt;`. Through a [`DirEntry`]
/// information like the entry's path and possibly other metadata can be
/// learned.
///
/// The order in which this iterator returns entries is platform and filesystem
/// dependent.
///
/// # Errors
///
/// This [`io::Result`] will be an [`Err`] if there's some sort of intermittent
/// IO error during iteration.
#[stable(feature = "rust1", since = "1.0.0")]
#[derive(Debug)]
pub struct ReadDir(fs_imp::ReadDir);
</code></pre>
<p>This [<code>io::Result</code>] will be an [<code>Err</code>] if there's some sort of intermittent IO error during iteration.</p>
<p>看起来一定要是比较罕见的 IO 错误？</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">关于我前些天，在 GitHub 上 Rust 的 repo 的那些事</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=111d596f-f53f-44dc-9a59-69ccf2ff7563">
<div class="article-summary-box-inner">
<span><p>上周我在 GitHub 上整理的 <a href="https://github.com/0voice/Understanding_in_Rust" rel="noopener noreferrer">《 Rust 工程师枕边资料》</a> ,涉及了侵权行为。在这里向大家赔礼道歉。并且在第一时间，处理了相关内容。
我在整理的之前的初衷只是单纯为了给大家提供更好、更多、更全、更专业地的 Rust 学习资料。并没有丝毫的商业化手段。
我收集的内容全部来源于互联网，由于我的疏忽没有注明文章出处链接，确实是不应该的。</p>
<p>再一次，给作品的作者道歉。</p>
<p>我将在以后 repo 里将不会出现类似的错误事件，同时也希望广大开发者们监督。如果有任何问题，可以邮箱至：wchao_isvip@163.com ，我会在第一时间处理的。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b">
<div class="article-summary-box-inner">
<span><p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>
<ol>
<li>认识面向基础架构语言</li>
<li>理解 Rust 所有权</li>
<li>通过实战理解 Rust 宏</li>
<li>通过 Datafuse 理解全链路跟踪</li>
<li>Rust 异步编程入门 Future Part 1</li>
<li>Rust 异步编程入门 Future Part 2</li>
</ol>
<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。
本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>
<hr>
<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>
<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>
<p>分享讲师： 董泽润</p>
<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现
讲师介绍：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<hr>
<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>
<p>分享时间： 周日晚上2021-09-12 20:00-21:00</p>
<p>分享讲师： 苏林</p>
<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>
<ol>
<li>利用 Tokio 实现一个 Mini Http server</li>
<li>基于 Http server提供内容动态的 API 网关</li>
<li>利用 Redis 实现对 API 网关加速</li>
<li>学习 Rust RPC 调用，实现微服务调用</li>
</ol>
<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<p>Rust 异步编程入门 Future Part 1 | Vol. 5
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<p>Rust 异步编程入门 Future Part 2 | Vol. 6
https://www.bilibili.com/video/bv1oy4y1G7jC</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust 学习随笔</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8">
<div class="article-summary-box-inner">
<span><h1>切换镜像源</h1>
<p>crm =&gt; https://github.com/wtklbm/crm</p>
<p>常用命令就是 <code>crm best</code></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">pretree 补全文档发布了,再次谢谢大神的指点终于入门了。</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827">
<div class="article-summary-box-inner">
<span><h1>Pretree</h1>
<p>pretree is a package for storing and querying routing rules with prefix tree .</p>
<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>
<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>
<p>Inspired by <a href="https://github.com/obity/pretree" rel="noopener noreferrer">obity/pretree</a> (golang)</p>
<h1>Doc</h1>
<p>See this document at <a href="https://docs.rs/pretree" rel="noopener noreferrer">API documentation</a></p>
<h1>Install</h1>
<p>Add the following line to your Cargo.toml file:</p>
<pre><code>pretree = "1.0.0"
</code></pre>
<h1>Example</h1>
<pre><code>use pretree::Pretree;
let mut p = Pretree::new();
p.store("GET","account/{id}/info/:name");
p.store("GET","account/:id/login");
p.store("GET","account/{id}");
p.store("GET","bacteria/count_number_by_month");
let (ok,rule,vars) = p.query("GET","account/929239");
println!("ok:{} rule:{} vars:{:#?}",ok,rule,vars);

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>
<p><strong>课程时间:</strong> 2021年9月5日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识, Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>
<h3>课程大纲</h3>
<p>1、回顾 Rust 异步编程模型.</p>
<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>
<p>3、Tokio 介绍.</p>
<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>
<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/
Rust 异步编程入门 Future Part 1 回放地址：
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课：《 Rust 异步编程入门 Future 》|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>
<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是 Rust 异步编程的核心基础。</p>
<h3>课程大纲</h3>
<p>1、为什么需要异步.</p>
<p>2、理解异步编程模型.</p>
<p>3、Future 编程模型讲解.</p>
<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>
<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>异步引擎 C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D 游戏引擎</h3>
<ul>
<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>
<li><strong>延迟着色</strong></li>
<li><strong>内置保存/加载</strong></li>
<li><strong>独立场景编辑器</strong></li>
<li><strong>高级物理模型</strong></li>
<li><strong>分层模型资源</strong></li>
<li><strong>几何实例化</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>
<p><strong>课程时间：</strong> 2021年8月22日 20:30-21:30</p>
<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>
<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>
<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>
<h3>课程大纲</h3>
<ol>
<li>
<p>什么是分布式追踪系统OpenTracing及应用场景</p>
</li>
<li>
<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>
</li>
<li>
<p>为什么需要tokio-rs/tracing库</p>
</li>
<li>
<p>演示Datafuse项目中tokio-rs/tracing的使用</p>
</li>
</ol>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">论坛github账户无法登录解决笔记</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>有反映这两天github账户无法登录了。</p>
<p>报这个错：</p>
<pre><code>get github user info err
</code></pre>
<p>查了几个地方：</p>
<ol>
<li>代码是否运行正常：Ok</li>
<li>https代理是否正常：Ok</li>
<li>检查了github返回日志，发现是：</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>进入这个地址一看：<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>
<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>
<p>特此记录。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 的 Future 与 Javascript 的 Promise 功能对照参考</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>
<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>
<blockquote>
<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* 什么都不做 */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被 （1）跨线程传递 （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：《通过实战理解 Rust 宏》| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>
<p><strong>课程时间：</strong> 2021年8月15日 20:30-21:30</p>
<p><strong>课程介绍：</strong></p>
<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>这就是通过宏实现配置的统一行为，代码参考：
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>
<h3>课程大纲</h3>
<ul>
<li>什么是 Rust 宏</li>
<li>什么是宏运行原理</li>
<li>如何创建 Rust 宏过程</li>
<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>
</ul>
<p><strong>讲师介绍</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-16T01:30:00Z">09-16</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Decision-Focused Summarization. (arXiv:2109.06896v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06896">
<div class="article-summary-box-inner">
<span><p>Relevance in summarization is typically defined based on textual information
alone, without incorporating insights about a particular decision. As a result,
to support risk analysis of pancreatic cancer, summaries of medical notes may
include irrelevant information such as a knee injury. We propose a novel
problem, decision-focused summarization, where the goal is to summarize
relevant information for a decision. We leverage a predictive model that makes
the decision based on the full text to provide valuable insights on how a
decision can be inferred from text. To build a summary, we then select
representative sentences that lead to similar model decisions as using the full
text while accounting for textual non-redundancy. To evaluate our method
(DecSum), we build a testbed where the task is to summarize the first ten
reviews of a restaurant in support of predicting its future rating on Yelp.
DecSum substantially outperforms text-only summarization methods and
model-based explanation methods in decision faithfulness and
representativeness. We further demonstrate that DecSum is the only method that
enables humans to outperform random chance in predicting which restaurant will
be better rated in the future.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">fairseq S^2: A Scalable and Integrable Speech Synthesis Toolkit. (arXiv:2109.06912v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06912">
<div class="article-summary-box-inner">
<span><p>This paper presents fairseq S^2, a fairseq extension for speech synthesis. We
implement a number of autoregressive (AR) and non-AR text-to-speech models, and
their multi-speaker variants. To enable training speech synthesis models with
less curated data, a number of preprocessing tools are built and their
importance is shown empirically. To facilitate faster iteration of development
and analysis, a suite of automatic metrics is included. Apart from the features
added specifically for this extension, fairseq S^2 also benefits from the
scalability offered by fairseq and can be easily integrated with other
state-of-the-art systems provided in this framework. The code, documentation,
and pre-trained models are available at
https://github.com/pytorch/fairseq/tree/master/examples/speech_synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Language-specificity of Multilingual BERT and the Impact of Fine-tuning. (arXiv:2109.06935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06935">
<div class="article-summary-box-inner">
<span><p>Recent work has shown evidence that the knowledge acquired by multilingual
BERT (mBERT) has two components: a language-specific and a language-neutral
one. This paper analyses the relationship between them, in the context of
fine-tuning on two tasks -- POS tagging and natural language inference -- which
require the model to bring to bear different degrees of language-specific
knowledge. Visualisations reveal that mBERT loses the ability to cluster
representations by language after fine-tuning, a result that is supported by
evidence from language identification experiments. However, further experiments
on 'unlearning' language-specific representations using gradient reversal and
iterative adversarial learning are shown not to add further improvement to the
language-independent component over and above the effect of fine-tuning. The
results presented here suggest that the process of fine-tuning causes a
reorganisation of the model's limited representational capacity, enhancing
language-independent representations at the expense of language-specific ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Stem Cell Hypothesis: Dilemma behind Multi-Task Learning with Transformer Encoders. (arXiv:2109.06939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06939">
<div class="article-summary-box-inner">
<span><p>Multi-task learning with transformer encoders (MTL) has emerged as a powerful
technique to improve performance on closely-related tasks for both accuracy and
efficiency while a question still remains whether or not it would perform as
well on tasks that are distinct in nature. We first present MTL results on five
NLP tasks, POS, NER, DEP, CON, and SRL, and depict its deficiency over
single-task learning. We then conduct an extensive pruning analysis to show
that a certain set of attention heads get claimed by most tasks during MTL, who
interfere with one another to fine-tune those heads for their own objectives.
Based on this finding, we propose the Stem Cell Hypothesis to reveal the
existence of attention heads naturally talented for many tasks that cannot be
jointly trained to create adequate embeddings for all of those tasks. Finally,
we design novel parameter-free probes to justify our hypothesis and demonstrate
how attention heads are transformed across the five tasks during MTL through
label analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatically Exposing Problems with Neural Dialog Models. (arXiv:2109.06950v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06950">
<div class="article-summary-box-inner">
<span><p>Neural dialog models are known to suffer from problems such as generating
unsafe and inconsistent responses. Even though these problems are crucial and
prevalent, they are mostly manually identified by model designers through
interactions. Recently, some research instructs crowdworkers to goad the bots
into triggering such problems. However, humans leverage superficial clues such
as hate speech, while leaving systematic problems undercover. In this paper, we
propose two methods including reinforcement learning to automatically trigger a
dialog model into generating problematic responses. We show the effect of our
methods in exposing safety and contradiction issues with state-of-the-art
dialog models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Residual Adapters for Parameter-Efficient ASR Adaptation to Atypical and Accented Speech. (arXiv:2109.06952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06952">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) systems are often optimized to work best
for speakers with canonical speech patterns. Unfortunately, these systems
perform poorly when tested on atypical speech and heavily accented speech. It
has previously been shown that personalization through model fine-tuning
substantially improves performance. However, maintaining such large models per
speaker is costly and difficult to scale. We show that by adding a relatively
small number of extra parameters to the encoder layers via so-called residual
adapter, we can achieve similar adaptation gains compared to model fine-tuning,
while only updating a tiny fraction (less than 0.5%) of the model parameters.
We demonstrate this on two speech adaptation tasks (atypical and accented
speech) and for two state-of-the-art ASR architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Searching for More Efficient Dynamic Programs. (arXiv:2109.06966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06966">
<div class="article-summary-box-inner">
<span><p>Computational models of human language often involve combinatorial problems.
For instance, a probabilistic parser may marginalize over exponentially many
trees to make predictions. Algorithms for such problems often employ dynamic
programming and are not always unique. Finding one with optimal asymptotic
runtime can be unintuitive, time-consuming, and error-prone. Our work aims to
automate this laborious process. Given an initial correct declarative program,
we search for a sequence of semantics-preserving transformations to improve its
running time as much as possible. To this end, we describe a set of program
transformations, a simple metric for assessing the efficiency of a transformed
program, and a heuristic search procedure to improve this metric. We show that
in practice, automated search -- like the mental search performed by human
programmers -- can find substantial improvements to the initial program.
Empirically, we show that many common speed-ups described in the NLP literature
could have been discovered automatically by our system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explainable Identification of Dementia from Transcripts using Transformer Networks. (arXiv:2109.06980v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06980">
<div class="article-summary-box-inner">
<span><p>Alzheimer's disease (AD) is the main cause of dementia which is accompanied
by loss of memory and may lead to severe consequences in peoples' everyday life
if not diagnosed on time. Very few works have exploited transformer-based
networks and despite the high accuracy achieved, little work has been done in
terms of model interpretability. In addition, although Mini-Mental State Exam
(MMSE) scores are inextricably linked with the identification of dementia,
research works face the task of dementia identification and the task of the
prediction of MMSE scores as two separate tasks. In order to address these
limitations, we employ several transformer-based models, with BERT achieving
the highest accuracy accounting for 85.56%. Concurrently, we propose an
interpretable method to detect AD patients based on siamese networks reaching
accuracy up to 81.18%. Next, we introduce two multi-task learning models, where
the main task refers to the identification of dementia (binary classification),
while the auxiliary one corresponds to the identification of the severity of
dementia (multiclass classification). Our model obtains accuracy equal to
84.99% on the detection of AD patients in the multi-task learning setting.
Finally, we present some new methods to identify the linguistic patterns used
by AD patients and non-AD ones, including text statistics, vocabulary
uniqueness, word usage, correlations via a detailed linguistic analysis, and
explainability techniques (LIME). Findings indicate significant differences in
language between AD and non-AD patients.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NOPE: A Corpus of Naturally-Occurring Presuppositions in English. (arXiv:2109.06987v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06987">
<div class="article-summary-box-inner">
<span><p>Understanding language requires grasping not only the overtly stated content,
but also making inferences about things that were left unsaid. These inferences
include presuppositions, a phenomenon by which a listener learns about new
information through reasoning about what a speaker takes as given.
Presuppositions require complex understanding of the lexical and syntactic
properties that trigger them as well as the broader conversational context. In
this work, we introduce the Naturally-Occurring Presuppositions in English
(NOPE) Corpus to investigate the context-sensitivity of 10 different types of
presupposition triggers and to evaluate machine learning models' ability to
predict human inferences. We find that most of the triggers we investigate
exhibit moderate variability. We further find that transformer-based models
draw correct inferences in simple cases involving presuppositions, but they
fail to capture the minority of exceptional cases in which human judgments
reveal complex interactions between context and triggers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Three Step Training Approach with Data Augmentation for Morphological Inflection. (arXiv:2109.07006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07006">
<div class="article-summary-box-inner">
<span><p>We present the BME submission for the SIGMORPHON 2021 Task 0 Part 1,
Generalization Across Typologically Diverse Languages shared task. We use an
LSTM encoder-decoder model with three step training that is first trained on
all languages, then fine-tuned on each language families and finally finetuned
on individual languages. We use a different type of data augmentation technique
in the first two steps. Our system outperformed the only other submission.
Although it remains worse than the Transformer baseline released by the
organizers, our model is simpler and our data augmentation techniques are
easily applicable to new languages. We perform ablation studies and show that
the augmentation techniques and the three training steps often help but
sometimes have a negative effect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Will this Question be Answered? Question Filtering via Answer Model Distillation for Efficient Question Answering. (arXiv:2109.07009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07009">
<div class="article-summary-box-inner">
<span><p>In this paper we propose a novel approach towards improving the efficiency of
Question Answering (QA) systems by filtering out questions that will not be
answered by them. This is based on an interesting new finding: the answer
confidence scores of state-of-the-art QA systems can be approximated well by
models solely using the input question text. This enables preemptive filtering
of questions that are not answered by the system due to their answer confidence
scores being lower than the system threshold. Specifically, we learn
Transformer-based question models by distilling Transformer-based answering
models. Our experiments on three popular QA datasets and one industrial QA
benchmark demonstrate the ability of our question models to approximate the
Precision/Recall curves of the target QA system well. These question models,
when used as filters, can effectively trade off lower computation cost of QA
systems for lower Recall, e.g., reducing computation by ~60%, while only losing
~3-4% of Recall.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Written Justifications are Key to Aggregate Crowdsourced Forecasts. (arXiv:2109.07017v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07017">
<div class="article-summary-box-inner">
<span><p>This paper demonstrates that aggregating crowdsourced forecasts benefits from
modeling the written justifications provided by forecasters. Our experiments
show that the majority and weighted vote baselines are competitive, and that
the written justifications are beneficial to call a question throughout its
life except in the last quarter. We also conduct an error analysis shedding
light into the characteristics that make a justification unreliable.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Frequency Effects on Syntactic Rule Learning in Transformers. (arXiv:2109.07020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07020">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models perform well on a variety of linguistic tasks
that require symbolic reasoning, raising the question of whether such models
implicitly represent abstract symbols and rules. We investigate this question
using the case study of BERT's performance on English subject-verb agreement.
Unlike prior work, we train multiple instances of BERT from scratch, allowing
us to perform a series of controlled interventions at pre-training time. We
show that BERT often generalizes well to subject-verb pairs that never occurred
in training, suggesting a degree of rule-governed behavior. We also find,
however, that performance is heavily influenced by word frequency, with
experiments showing that both the absolute frequency of a verb form, as well as
the frequency relative to the alternate inflection, are causally implicated in
the predictions BERT makes at inference time. Closer analysis of these
frequency effects reveals that BERT's behavior is consistent with a system that
correctly applies the SVA rule in general but struggles to overcome strong
training priors and to estimate agreement features (singular vs. plural) on
infrequent lexical items.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attention Is Indeed All You Need: Semantically Attention-Guided Decoding for Data-to-Text NLG. (arXiv:2109.07043v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07043">
<div class="article-summary-box-inner">
<span><p>Ever since neural models were adopted in data-to-text language generation,
they have invariably been reliant on extrinsic components to improve their
semantic accuracy, because the models normally do not exhibit the ability to
generate text that reliably mentions all of the information provided in the
input. In this paper, we propose a novel decoding method that extracts
interpretable information from encoder-decoder models' cross-attention, and
uses it to infer which attributes are mentioned in the generated text, which is
subsequently used to rescore beam hypotheses. Using this decoding method with
T5 and BART, we show on three datasets its ability to dramatically reduce
semantic errors in the generated outputs, while maintaining their
state-of-the-art quality.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Conditional Generative Matching Model for Multi-lingual Reply Suggestion. (arXiv:2109.07046v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07046">
<div class="article-summary-box-inner">
<span><p>We study the problem of multilingual automated reply suggestions (RS) model
serving many languages simultaneously. Multilingual models are often challenged
by model capacity and severe data distribution skew across languages. While
prior works largely focus on monolingual models, we propose Conditional
Generative Matching models (CGM), optimized within a Variational Autoencoder
framework to address challenges arising from multi-lingual RS. CGM does so with
expressive message conditional priors, mixture densities to enhance
multi-lingual data representation, latent alignment for language
discrimination, and effective variational optimization techniques for training
multi-lingual RS. The enhancements result in performance that exceed
competitive baselines in relevance (ROUGE score) by more than 10\% on average,
and 16\% for low resource languages. CGM also shows remarkable improvements in
diversity (80\%) illustrating its expressiveness in representation of
multi-lingual data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARCH: Efficient Adversarial Regularized Training with Caching. (arXiv:2109.07048v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07048">
<div class="article-summary-box-inner">
<span><p>Adversarial regularization can improve model generalization in many natural
language processing tasks. However, conventional approaches are computationally
expensive since they need to generate a perturbation for each sample in each
epoch. We propose a new adversarial regularization method ARCH (adversarial
regularization with caching), where perturbations are generated and cached once
every several epochs. As caching all the perturbations imposes memory usage
concerns, we adopt a K-nearest neighbors-based strategy to tackle this issue.
The strategy only requires caching a small amount of perturbations, without
introducing additional training time. We evaluate our proposed method on a set
of neural machine translation and natural language understanding tasks. We
observe that ARCH significantly eases the computational burden (saves up to
70\% of computational time in comparison with conventional approaches). More
surprisingly, by reducing the variance of stochastic gradients, ARCH produces a
notably better (in most of the tasks) or comparable model generalization. Our
code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Training with Differentiable Teacher. (arXiv:2109.07049v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07049">
<div class="article-summary-box-inner">
<span><p>Self-training achieves enormous success in various semi-supervised and
weakly-supervised learning tasks. The method can be interpreted as a
teacher-student framework, where the teacher generates pseudo-labels, and the
student makes predictions. The two models are updated alternatingly. However,
such a straightforward alternating update rule leads to training instability.
This is because a small change in the teacher may result in a significant
change in the student. To address this issue, we propose {\ours}, short for
differentiable self-training, that treats teacher-student as a Stackelberg
game. In this game, a leader is always in a more advantageous position than a
follower. In self-training, the student contributes to the prediction
performance, and the teacher controls the training process by generating
pseudo-labels. Therefore, we treat the student as the leader and the teacher as
the follower. The leader procures its advantage by acknowledging the follower's
strategy, which involves differentiable pseudo-labels and differentiable sample
weights. Consequently, the leader-follower interaction can be effectively
captured via Stackelberg gradient, obtained by differentiating the follower's
strategy. Experimental results on semi- and weakly-supervised classification
and named entity recognition tasks show that our model outperforms existing
approaches by large margins.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Text Auto-Completion with Next Phrase Prediction. (arXiv:2109.07067v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07067">
<div class="article-summary-box-inner">
<span><p>Language models such as GPT-2 have performed well on constructing
syntactically sound sentences for text auto-completion task. However, such
models often require considerable training effort to adapt to specific writing
domains (e.g., medical). In this paper, we propose an intermediate training
strategy to enhance pre-trained language models' performance in the text
auto-completion task and fastly adapt them to specific domains. Our strategy
includes a novel self-supervised training objective called Next Phrase
Prediction (NPP), which encourages a language model to complete the partial
query with enriched phrases and eventually improve the model's text
auto-completion performance. Preliminary experiments have shown that our
approach is able to outperform the baselines in auto-completion for email and
academic writing domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Lexically Constrained Headline Generation. (arXiv:2109.07080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07080">
<div class="article-summary-box-inner">
<span><p>This paper explores a variant of automatic headline generation methods, where
a generated headline is required to include a given phrase such as a company or
a product name. Previous methods using Transformer-based models generate a
headline including a given phrase by providing the encoder with additional
information corresponding to the given phrase. However, these methods cannot
always include the phrase in the generated headline. Inspired by previous
RNN-based methods generating token sequences in backward and forward directions
from the given phrase, we propose a simple Transformer-based method that
guarantees to include the given phrase in the high-quality generated headline.
We also consider a new headline generation strategy that takes advantage of the
controllable generation order of Transformer. Our experiments with the Japanese
News Corpus demonstrate that our methods, which are guaranteed to include the
phrase in the generated headline, achieve ROUGE scores comparable to previous
Transformer-based methods. We also show that our generation strategy performs
better than previous strategies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast Extraction of Word Embedding from Q-contexts. (arXiv:2109.07084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07084">
<div class="article-summary-box-inner">
<span><p>The notion of word embedding plays a fundamental role in natural language
processing (NLP). However, pre-training word embedding for very large-scale
vocabulary is computationally challenging for most existing methods. In this
work, we show that with merely a small fraction of contexts (Q-contexts)which
are typical in the whole corpus (and their mutual information with words), one
can construct high-quality word embedding with negligible errors. Mutual
information between contexts and words can be encoded canonically as a sampling
state, thus, Q-contexts can be fast constructed. Furthermore, we present an
efficient and effective WEQ method, which is capable of extracting word
embedding directly from these typical contexts. In practical scenarios, our
algorithm runs 11$\sim$13 times faster than well-established methods. By
comparing with well-known methods such as matrix factorization, word2vec,
GloVeand fasttext, we demonstrate that our method achieves comparable
performance on a variety of downstream NLP tasks, and in the meanwhile
maintains run-time and resource advantages over all these baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Document-Level Paraphrase Generation with Sentence Rewriting and Reordering. (arXiv:2109.07095v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07095">
<div class="article-summary-box-inner">
<span><p>Paraphrase generation is an important task in natural language processing.
Previous works focus on sentence-level paraphrase generation, while ignoring
document-level paraphrase generation, which is a more challenging and valuable
task. In this paper, we explore the task of document-level paraphrase
generation for the first time and focus on the inter-sentence diversity by
considering sentence rewriting and reordering. We propose CoRPG (Coherence
Relationship guided Paraphrase Generation), which leverages graph GRU to encode
the coherence relationship graph and get the coherence-aware representation for
each sentence, which can be used for re-arranging the multiple (possibly
modified) input sentences. We create a pseudo document-level paraphrase dataset
for training CoRPG. Automatic evaluation results show CoRPG outperforms several
strong baseline models on the BERTScore and diversity scores. Human evaluation
also shows our model can generate document paraphrase with more diversity and
semantic preservation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Edge Probing Tasks Reveal Linguistic Knowledge in QA Models?. (arXiv:2109.07102v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07102">
<div class="article-summary-box-inner">
<span><p>There have been many efforts to try to understand what grammatical knowledge
(e.g., ability to understand the part of speech of a token) is encoded in large
pre-trained language models (LM). This is done through `Edge Probing' (EP)
tests: simple ML models that predict the grammatical properties of a span
(whether it has a particular part of speech) using \textit{only} the LM's token
representations. However, most NLP applications use \finetuned\ LMs. Here, we
ask: if a LM is \finetuned, does the encoding of linguistic information in it
change, as measured by EP tests? Conducting experiments on multiple
question-answering (QA) datasets, we answer that question negatively: the EP
test results do not change significantly when the fine-tuned QA model performs
well or in adversarial situations where the model is forced to learn wrong
correlations. However, a critical analysis of the EP task datasets reveals that
EP models may rely on spurious correlations to make predictions. This indicates
even if \finetuning\ changes the encoding of such knowledge, the EP tests might
fail to measure it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Named Entity Recognition Based on Multi-hop Dependency Trigger. (arXiv:2109.07118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07118">
<div class="article-summary-box-inner">
<span><p>This paper presents a simple and effective approach in low-resource named
entity recognition (NER) based on multi-hop dependency trigger. Dependency
trigger refer to salient nodes relative to a entity in the dependency graph of
a context sentence. Our main observation is that there often exists trigger
which play an important role to recognize the location and type of entity in
sentence. Previous research has used manual labelling of trigger. Our main
contribution is to propose use a syntactic parser to automatically annotate
trigger. Experiments on two English datasets (CONLL 2003 and BC5CDR) show that
the proposed method is comparable to the previous trigger-based NER model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Does The User Want? Information Gain for Hierarchical Dialogue Policy Optimisation. (arXiv:2109.07129v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07129">
<div class="article-summary-box-inner">
<span><p>The dialogue management component of a task-oriented dialogue system is
typically optimised via reinforcement learning (RL). Optimisation via RL is
highly susceptible to sample inefficiency and instability. The hierarchical
approach called Feudal Dialogue Management takes a step towards more efficient
learning by decomposing the action space. However, it still suffers from
instability due to the reward only being provided at the end of the dialogue.
We propose the usage of an intrinsic reward based on information gain to
address this issue. Our proposed reward favours actions that resolve
uncertainty or query the user whenever necessary. It enables the policy to
learn how to retrieve the users' needs efficiently, which is an integral aspect
in every task-oriented conversation. Our algorithm, which we call FeudalGain,
achieves state-of-the-art results in most environments of the PyDial framework,
outperforming much more complex approaches. We confirm the sample efficiency
and stability of our algorithm through experiments in simulation and a human
trial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Universality of Deep COntextual Language Models. (arXiv:2109.07140v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07140">
<div class="article-summary-box-inner">
<span><p>Deep Contextual Language Models (LMs) like ELMO, BERT, and their successors
dominate the landscape of Natural Language Processing due to their ability to
scale across multiple tasks rapidly by pre-training a single model, followed by
task-specific fine-tuning. Furthermore, multilingual versions of such models
like XLM-R and mBERT have given promising results in zero-shot cross-lingual
transfer, potentially enabling NLP applications in many under-served and
under-resourced languages. Due to this initial success, pre-trained models are
being used as `Universal Language Models' as the starting point across diverse
tasks, domains, and languages. This work explores the notion of `Universality'
by identifying seven dimensions across which a universal model should be able
to scale, that is, perform equally well or reasonably well, to be useful across
diverse settings. We outline the current theoretical and empirical results that
support model performance across these dimensions, along with extensions that
may help address some of their current limitations. Through this survey, we lay
the foundation for understanding the capabilities and limitations of massive
contextual language models and help discern research gaps and directions for
future work to make these LMs inclusive and fair to diverse applications,
users, and linguistic phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Glass-Box Features: Uncertainty Quantification Enhanced Quality Estimation for Neural Machine Translation. (arXiv:2109.07141v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07141">
<div class="article-summary-box-inner">
<span><p>Quality Estimation (QE) plays an essential role in applications of Machine
Translation (MT). Traditionally, a QE system accepts the original source text
and translation from a black-box MT system as input. Recently, a few studies
indicate that as a by-product of translation, QE benefits from the model and
training data's information of the MT system where the translations come from,
and it is called the "glass-box QE". In this paper, we extend the definition of
"glass-box QE" generally to uncertainty quantification with both "black-box"
and "glass-box" approaches and design several features deduced from them to
blaze a new trial in improving QE's performance. We propose a framework to fuse
the feature engineering of uncertainty quantification into a pre-trained
cross-lingual language model to predict the translation quality. Experiment
results show that our method achieves state-of-the-art performances on the
datasets of WMT 2020 QE shared task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantics of European poetry is shaped by conservative forces: The relationship between poetic meter and meaning in accentual-syllabic verse. (arXiv:2109.07148v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07148">
<div class="article-summary-box-inner">
<span><p>Recent advances in cultural analytics and large-scale computational studies
of art, literature and film often show that long-term change in the features of
artistic works happens gradually. These findings suggest that conservative
forces that shape creative domains might be underestimated. To this end, we
provide the first large-scale formal evidence of the persistent association
between poetic meter and semantics in 18-19th European literatures, using
Czech, German and Russian collections with additional data from English poetry
and early modern Dutch songs. Our study traces this association through a
series of clustering experiments using the abstracted semantic features of
150,000 poems. With the aid of topic modeling we infer semantic features for
individual poems. Texts were also lexically simplified across collections to
increase generalizability and decrease the sparseness of word frequency
distributions. Topics alone enable recognition of the meters in each observed
language, as may be seen from highly robust clustering of same-meter samples
(median Adjusted Rand Index between 0.48 and 1). In addition, this study shows
that the strength of the association between form and meaning tends to decrease
over time. This may reflect a shift in aesthetic conventions between the 18th
and 19th centuries as individual innovation was increasingly favored in
literature. Despite this decline, it remains possible to recognize semantics of
the meters from past or future, which suggests the continuity of semantic
traditions while also revealing the historical variability of conditions across
languages. This paper argues that distinct metrical forms, which are often
copied in a language over centuries, also maintain long-term semantic inertia
in poetry. Our findings, thus, highlight the role of the formal features of
cultural items in influencing the pace and shape of cultural evolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Residual and Normalization Layers into Analysis of Masked Language Models. (arXiv:2109.07152v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07152">
<div class="article-summary-box-inner">
<span><p>Transformer architecture has become ubiquitous in the natural language
processing field. To interpret the Transformer-based models, their attention
patterns have been extensively analyzed. However, the Transformer architecture
is not only composed of the multi-head attention; other components can also
contribute to Transformers' progressive performance. In this study, we extended
the scope of the analysis of Transformers from solely the attention patterns to
the whole attention block, i.e., multi-head attention, residual connection, and
layer normalization. Our analysis of Transformer-based masked language models
shows that the token-to-token interaction performed via attention has less
impact on the intermediate representations than previously assumed. These
results provide new intuitive explanations of existing reports; for example,
discarding the learned attention patterns tends not to adversely affect the
performance. The codes of our experiments are publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Models be Biomedical Knowledge Bases?. (arXiv:2109.07154v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07154">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (LMs) have become ubiquitous in solving various
natural language processing (NLP) tasks. There has been increasing interest in
what knowledge these LMs contain and how we can extract that knowledge,
treating LMs as knowledge bases (KBs). While there has been much work on
probing LMs in the general domain, there has been little attention to whether
these powerful LMs can be used as domain-specific KBs. To this end, we create
the BioLAMA benchmark, which is comprised of 49K biomedical factual knowledge
triples for probing biomedical LMs. We find that biomedical LMs with recently
proposed probing methods can achieve up to 18.51% Acc@5 on retrieving
biomedical knowledge. Although this seems promising given the task difficulty,
our detailed analyses reveal that most predictions are highly correlated with
prompt templates without any subjects, hence producing similar results on each
relation and hindering their capabilities to be used as domain-specific KBs. We
hope that BioLAMA can serve as a challenging benchmark for biomedical factual
probing.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Match Job Candidates Using Multilingual Bi-Encoder BERT. (arXiv:2109.07157v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07157">
<div class="article-summary-box-inner">
<span><p>In this talk, we will show how we used Randstad history of candidate
placements to generate labeled CV-vacancy pairs dataset. Afterwards we
fine-tune a multilingual BERT with bi encoder structure over this dataset, by
adding a cosine similarity log loss layer. We will explain how using the
mentioned structure helps us overcome most of the challenges described above,
and how it enables us to build a maintainable and scalable pipeline to match
CVs and vacancies. In addition, we show how we gain a better semantic
understanding, and learn to bridge the vocabulary gap. Finally, we highlight
how multilingual transformers help us handle cross language barrier and might
reduce discrimination.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Disentangling Generative Factors in Natural Language with Discrete Variational Autoencoders. (arXiv:2109.07169v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07169">
<div class="article-summary-box-inner">
<span><p>The ability of learning disentangled representations represents a major step
for interpretable NLP systems as it allows latent linguistic features to be
controlled. Most approaches to disentanglement rely on continuous variables,
both for images and text. We argue that despite being suitable for image
datasets, continuous variables may not be ideal to model features of textual
data, due to the fact that most generative factors in text are discrete. We
propose a Variational Autoencoder based method which models language features
as discrete variables and encourages independence between variables for
learning disentangled representations. The proposed model outperforms
continuous and discrete baselines on several qualitative and quantitative
benchmarks for disentanglement as well as on a text style transfer downstream
application.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Mixing Policy for Relaxing Locally Linear Constraints in Mixup. (arXiv:2109.07177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07177">
<div class="article-summary-box-inner">
<span><p>Mixup is a recent regularizer for current deep classification networks.
Through training a neural network on convex combinations of pairs of examples
and their labels, it imposes locally linear constraints on the model's input
space. However, such strict linear constraints often lead to under-fitting
which degrades the effects of regularization. Noticeably, this issue is getting
more serious when the resource is extremely limited. To address these issues,
we propose the Adversarial Mixing Policy (AMP), organized in a min-max-rand
formulation, to relax the Locally Linear Constraints in Mixup. Specifically,
AMP adds a small adversarial perturbation to the mixing coefficients rather
than the examples. Thus, slight non-linearity is injected in-between the
synthetic examples and synthetic labels. By training on these data, the deep
networks are further regularized, and thus achieve a lower predictive error
rate. Experiments on five text classification benchmarks and five backbone
models have empirically shown that our methods reduce the error rate over Mixup
variants in a significant margin (up to 31.3%), especially in low-resource
conditions (up to 17.5%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformer-based Language Models for Factoid Question Answering at BioASQ9b. (arXiv:2109.07185v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07185">
<div class="article-summary-box-inner">
<span><p>In this work, we describe our experiments and participating systems in the
BioASQ Task 9b Phase B challenge of biomedical question answering. We have
focused on finding the ideal answers and investigated multi-task fine-tuning
and gradual unfreezing techniques on transformer-based language models. For
factoid questions, our ALBERT-based systems ranked first in test batch 1 and
fourth in test batch 2. Our DistilBERT systems outperformed the ALBERT variants
in test batches 4 and 5 despite having 81% fewer parameters than ALBERT.
However, we observed that gradual unfreezing had no significant impact on the
model's accuracy compared to standard fine-tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiagent Multimodal Categorization for Symbol Emergence: Emergent Communication via Interpersonal Cross-modal Inference. (arXiv:2109.07194v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07194">
<div class="article-summary-box-inner">
<span><p>This paper describes a computational model of multiagent multimodal
categorization that realizes emergent communication. We clarify whether the
computational model can reproduce the following functions in a symbol emergence
system, comprising two agents with different sensory modalities playing a
naming game. (1) Function for forming a shared lexical system that comprises
perceptual categories and corresponding signs, formed by agents through
individual learning and semiotic communication between agents. (2) Function to
improve the categorization accuracy in an agent via semiotic communication with
another agent, even when some sensory modalities of each agent are missing. (3)
Function that an agent infers unobserved sensory information based on a sign
sampled from another agent in the same manner as cross-modal inference. We
propose an interpersonal multimodal Dirichlet mixture (Inter-MDM), which is
derived by dividing an integrative probabilistic generative model, which is
obtained by integrating two Dirichlet mixtures (DMs). The Markov chain Monte
Carlo algorithm realizes emergent communication. The experimental results
demonstrated that Inter-MDM enables agents to form multimodal categories and
appropriately share signs between agents. It is shown that emergent
communication improves categorization accuracy, even when some sensory
modalities are missing. Inter-MDM enables an agent to predict unobserved
information based on a shared sign.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis in Poems in Misurata Sub-dialect -- A Sentiment Detection in an Arabic Sub-dialect. (arXiv:2109.07203v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07203">
<div class="article-summary-box-inner">
<span><p>Over the recent decades, there has been a significant increase and
development of resources for Arabic natural language processing. This includes
the task of exploring Arabic Language Sentiment Analysis (ALSA) from Arabic
utterances in both Modern Standard Arabic (MSA) and different Arabic dialects.
This study focuses on detecting sentiment in poems written in Misurata Arabic
sub-dialect spoken in Misurata, Libya. The tools used to detect sentiment from
the dataset are Sklearn as well as Mazajak sentiment tool 1. Logistic
Regression, Random Forest, Naive Bayes (NB), and Support Vector Machines (SVM)
classifiers are used with Sklearn, while the Convolutional Neural Network (CNN)
is implemented with Mazajak. The results show that the traditional classifiers
score a higher level of accuracy as compared to Mazajak which is built on an
algorithm that includes deep learning techniques. More research is suggested to
analyze Arabic sub-dialect poetry in order to investigate the aspects that
contribute to sentiments in these multi-line texts; for example, the use of
figurative language such as metaphors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Relation-Oriented Clustering Method for Open Relation Extraction. (arXiv:2109.07205v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07205">
<div class="article-summary-box-inner">
<span><p>The clustering-based unsupervised relation discovery method has gradually
become one of the important methods of open relation extraction (OpenRE).
However, high-dimensional vectors can encode complex linguistic information
which leads to the problem that the derived clusters cannot explicitly align
with the relational semantic classes. In this work, we propose a
relation-oriented clustering model and use it to identify the novel relations
in the unlabeled data. Specifically, to enable the model to learn to cluster
relational data, our method leverages the readily available labeled data of
pre-defined relations to learn a relation-oriented representation. We minimize
distance between the instance with same relation by gathering the instances
towards their corresponding relation centroids to form a cluster structure, so
that the learned representation is cluster-friendly. To reduce the clustering
bias on predefined classes, we optimize the model by minimizing a joint
objective on both labeled and unlabeled data. Experimental results show that
our method reduces the error rate by 29.2% and 15.7%, on two datasets
respectively, compared with current SOTA methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">{E}fficient{BERT}: Progressively Searching Multilayer Perceptron via Warm-up Knowledge Distillation. (arXiv:2109.07222v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07222">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have shown remarkable results on various NLP
tasks. Nevertheless, due to their bulky size and slow inference speed, it is
hard to deploy them on edge devices. In this paper, we have a critical insight
that improving the feed-forward network (FFN) in BERT has a higher gain than
improving the multi-head attention (MHA) since the computational cost of FFN is
2$\sim$3 times larger than MHA. Hence, to compact BERT, we are devoted to
designing efficient FFN as opposed to previous works that pay attention to MHA.
Since FFN comprises a multilayer perceptron (MLP) that is essential in BERT
optimization, we further design a thorough search space towards an advanced MLP
and perform a coarse-to-fine mechanism to search for an efficient BERT
architecture. Moreover, to accelerate searching and enhance model
transferability, we employ a novel warm-up knowledge distillation strategy at
each search stage. Extensive experiments show our searched EfficientBERT is
6.9$\times$ smaller and 4.4$\times$ faster than BERT$\rm_{BASE}$, and has
competitive performances on GLUE and SQuAD Benchmarks. Concretely,
EfficientBERT attains a 77.7 average score on GLUE \emph{test}, 0.7 higher than
MobileBERT$\rm_{TINY}$, and achieves an 85.3/74.5 F1 score on SQuAD v1.1/v2.0
\emph{dev}, 3.2/2.7 higher than TinyBERT$_4$ even without data augmentation.
The code is released at https://github.com/cheneydon/efficient-bert.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Much do Lyrics Matter? Analysing Lyrical Simplicity Preferences for Individuals At Risk of Depression. (arXiv:2109.07227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07227">
<div class="article-summary-box-inner">
<span><p>Music affects and in some cases reflects one's emotional state. Key to this
influence is lyrics and their meaning in conjunction with the acoustic
properties of the track. Recent work has focused on analysing these acoustic
properties and showing that individuals prone to depression primarily consume
low valence and low energy music. However, no studies yet have explored lyrical
content preferences in relation to online music consumption of such
individuals. In the current study, we examine lyrical simplicity, measured as
the Compressibility and Absolute Information Content of the text, associated
with preferences of individuals at risk for depression. Using the six-month
listening history of 541 Last.fm users, we compare lyrical simplicity trends
for users grouped as being at risk (At-Risk) of depression from those that are
not (No-Risk). Our findings reveal that At-Risk individuals prefer songs with
greater information content (lower Compressibility) on average, especially for
songs characterised as Sad. Furthermore, we found that At-Risk individuals also
have greater variability of Absolute Information Content across their listening
history. We discuss the results in light of existing socio-psychological
lab-based research on music habits associated with depression and their
relevance to naturally occurring online music listening behaviour.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialog speech sentiment classification for imbalanced datasets. (arXiv:2109.07228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07228">
<div class="article-summary-box-inner">
<span><p>Speech is the most common way humans express their feelings, and sentiment
analysis is the use of tools such as natural language processing and
computational algorithms to identify the polarity of these feelings. Even
though this field has seen tremendous advancements in the last two decades, the
task of effectively detecting under represented sentiments in different kinds
of datasets is still a challenging task. In this paper, we use single and
bi-modal analysis of short dialog utterances and gain insights on the main
factors that aid in sentiment detection, particularly in the underrepresented
classes, in datasets with and without inherent sentiment component.
Furthermore, we propose an architecture which uses a learning rate scheduler
and different monitoring criteria and provides state-of-the-art results for the
SWITCHBOARD imbalanced sentiment dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Mathematical Properties of Integers. (arXiv:2109.07230v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07230">
<div class="article-summary-box-inner">
<span><p>Embedding words in high-dimensional vector spaces has proven valuable in many
natural language applications. In this work, we investigate whether
similarly-trained embeddings of integers can capture concepts that are useful
for mathematical applications. We probe the integer embeddings for mathematical
knowledge, apply them to a set of numerical reasoning tasks, and show that by
learning the representations from mathematical sequence data, we can
substantially improve over number embeddings learned from English text corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SWEAT: Scoring Polarization of Topics across Different Corpora. (arXiv:2109.07231v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07231">
<div class="article-summary-box-inner">
<span><p>Understanding differences of viewpoints across corpora is a fundamental task
for computational social sciences. In this paper, we propose the Sliced Word
Embedding Association Test (SWEAT), a novel statistical measure to compute the
relative polarization of a topical wordset across two distributional
representations. To this end, SWEAT uses two additional wordsets, deemed to
have opposite valence, to represent two different poles. We validate our
approach and illustrate a case study to show the usefulness of the introduced
measure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Unreasonable Effectiveness of the Baseline: Discussing SVMs in Legal Text Classification. (arXiv:2109.07234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07234">
<div class="article-summary-box-inner">
<span><p>We aim to highlight an interesting trend to contribute to the ongoing debate
around advances within legal Natural Language Processing. Recently, the focus
for most legal text classification tasks has shifted towards large pre-trained
deep learning models such as BERT. In this paper, we show that a more
traditional approach based on Support Vector Machine classifiers reaches
competitive performance with deep learning models. We also highlight that error
reduction obtained by using specialised BERT-based models over baselines is
noticeably smaller in the legal domain when compared to general language tasks.
We discuss some hypotheses for these results to support future discussions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Regressive Ensemble for Machine Translation Quality Evaluation. (arXiv:2109.07242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07242">
<div class="article-summary-box-inner">
<span><p>This work introduces a simple regressive ensemble for evaluating machine
translation quality based on a set of novel and established metrics. We
evaluate the ensemble using a correlation to expert-based MQM scores of the WMT
2021 Metrics workshop. In both monolingual and zero-shot cross-lingual
settings, we show a significant performance improvement over single metrics. In
the cross-lingual settings, we also demonstrate that an ensemble approach is
well-applicable to unseen languages. Furthermore, we identify a strong
reference-free baseline that consistently outperforms the commonly-used BLEU
and METEOR measures and significantly improves our ensemble's performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhancing Clinical Information Extraction with Transferred Contextual Embeddings. (arXiv:2109.07243v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07243">
<div class="article-summary-box-inner">
<span><p>The Bidirectional Encoder Representations from Transformers (BERT) model has
achieved the state-of-the-art performance for many natural language processing
(NLP) tasks. Yet, limited research has been contributed to studying its
effectiveness when the target domain is shifted from the pre-training corpora,
for example, for biomedical or clinical NLP applications. In this paper, we
applied it to a widely studied a hospital information extraction (IE) task and
analyzed its performance under the transfer learning setting. Our application
became the new state-of-the-art result by a clear margin, compared with a range
of existing IE models. Specifically, on this nursing handover data set, the
macro-average F1 score from our model was 0.438, whilst the previous best deep
learning models had 0.416. In conclusion, we showed that BERT based
pre-training models can be transferred to health-related documents under mild
conditions and with a proper fine-tuning process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Learning of Flowchart Grounded Task-Oriented Dialogs. (arXiv:2109.07263v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07263">
<div class="article-summary-box-inner">
<span><p>We propose a novel problem within end-to-end learning of task-oriented
dialogs (TOD), in which the dialog system mimics a troubleshooting agent who
helps a user by diagnosing their problem (e.g., car not starting). Such dialogs
are grounded in domain-specific flowcharts, which the agent is supposed to
follow during the conversation. Our task exposes novel technical challenges for
neural TOD, such as grounding an utterance to the flowchart without explicit
annotation, referring to additional manual pages when user asks a clarification
question, and ability to follow unseen flowcharts at test time. We release a
dataset (FloDial) consisting of 2,738 dialogs grounded on 12 different
troubleshooting flowcharts. We also design a neural model, FloNet, which uses a
retrieval-augmented generation architecture to train the dialog agent. Our
experiments find that FloNet can do zero-shot transfer to unseen flowcharts,
and sets a strong baseline for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scope resolution of predicted negation cues: A two-step neural network-based approach. (arXiv:2109.07264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07264">
<div class="article-summary-box-inner">
<span><p>Neural network-based methods are the state of the art in negation scope
resolution. However, they often use the unrealistic assumption that cue
information is completely accurate. Even if this assumption holds, there
remains a dependency on engineered features from state-of-the-art machine
learning methods. The current study adopted a two-step negation resolving
apporach to assess whether a Bidirectional Long Short-Term Memory-based method
can be used for cue detection as well, and how inaccurate cue predictions would
affect the scope resolution performance. Results suggest that this method is
not suitable for negation detection. Scope resolution performance is most
robust against inaccurate information for models with a recurrent layer only,
compared to extensions with a Conditional Random Fields layer or a
post-processing algorithm. We advocate for more research into the application
of deep learning on negation detection and the effect of imperfect information
on scope resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequence Length is a Domain: Length-based Overfitting in Transformer Models. (arXiv:2109.07276v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07276">
<div class="article-summary-box-inner">
<span><p>Transformer-based sequence-to-sequence architectures, while achieving
state-of-the-art results on a large number of NLP tasks, can still suffer from
overfitting during training. In practice, this is usually countered either by
applying regularization methods (e.g. dropout, L2-regularization) or by
providing huge amounts of training data. Additionally, Transformer and other
architectures are known to struggle when generating very long sequences. For
example, in machine translation, the neural-based systems perform worse on very
long sequences when compared to the preceding phrase-based translation
approaches (Koehn and Knowles, 2017).
</p>
<p>We present results which suggest that the issue might also be in the mismatch
between the length distributions of the training and validation data combined
with the aforementioned tendency of the neural networks to overfit to the
training data. We demonstrate on a simple string editing task and a machine
translation task that the Transformer model performance drops significantly
when facing sequences of length diverging from the length distribution in the
training data. Additionally, we show that the observed drop in performance is
due to the hypothesis length corresponding to the lengths seen by the model
during training rather than the length of the input sequence.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context. (arXiv:2109.07293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07293">
<div class="article-summary-box-inner">
<span><p>Embedding based methods are widely used for unsupervised keyphrase extraction
(UKE) tasks. Generally, these methods simply calculate similarities between
phrase embeddings and document embedding, which is insufficient to capture
different context for a more effective UKE model. In this paper, we propose a
novel method for UKE, where local and global contexts are jointly modeled. From
a global view, we calculate the similarity between a certain phrase and the
whole document in the vector space as transitional embedding based models do.
In terms of the local view, we first build a graph structure based on the
document where phrases are regarded as vertices and the edges are similarities
between vertices. Then, we proposed a new centrality computation method to
capture local salient information based on the graph structure. Finally, we
further combine the modeling of global and local context for ranking. We
evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010)
and compare with existing state-of-the-art models. The results show that our
model outperforms most models while generalizing better on input documents with
different domains and length. Additional ablation study shows that both the
local and global information is crucial for unsupervised keyphrase extraction
tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What Vision-Language Models `See' when they See Scenes. (arXiv:2109.07301v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07301">
<div class="article-summary-box-inner">
<span><p>Images can be described in terms of the objects they contain, or in terms of
the types of scene or place that they instantiate. In this paper we address to
what extent pretrained Vision and Language models can learn to align
descriptions of both types with images. We compare 3 state-of-the-art models,
VisualBERT, LXMERT and CLIP. We find that (i) V&amp;L models are susceptible to
stylistic biases acquired during pretraining; (ii) only CLIP performs
consistently well on both object- and scene-level descriptions. A follow-up
ablation study shows that CLIP uses object-level information in the visual
modality to align with scene-level textual descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Allocating Large Vocabulary Capacity for Cross-lingual Language Model Pre-training. (arXiv:2109.07306v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07306">
<div class="article-summary-box-inner">
<span><p>Compared to monolingual models, cross-lingual models usually require a more
expressive vocabulary to represent all languages adequately. We find that many
languages are under-represented in recent cross-lingual language models due to
the limited vocabulary capacity. To this end, we propose an algorithm VoCap to
determine the desired vocabulary capacity of each language. However, increasing
the vocabulary size significantly slows down the pre-training speed. In order
to address the issues, we propose k-NN-based target sampling to accelerate the
expensive softmax. Our experiments show that the multilingual vocabulary
learned with VoCap benefits cross-lingual language model pre-training.
Moreover, k-NN-based target sampling mitigates the side-effects of increasing
the vocabulary size while achieving comparable performance and faster
pre-training speed. The code and the pretrained multilingual vocabularies are
available at https://github.com/bozheng-hit/VoCapXLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Embedding Convolutions for Short Text Extreme Classification with Millions of Labels. (arXiv:2109.07319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07319">
<div class="article-summary-box-inner">
<span><p>Automatic annotation of short-text data to a large number of target labels,
referred to as Short Text Extreme Classification, has recently found numerous
applications in prediction of related searches and product recommendation
tasks. The conventional usage of Convolutional Neural Network (CNN) to capture
n-grams in text-classification relies heavily on uniformity in word-ordering
and the presence of long input sequences to convolve over. However, this is
missing in short and unstructured text sequences encountered in search and
recommendation. In order to tackle this, we propose an orthogonal approach by
recasting the convolution operation to capture coupled semantics along the
embedding dimensions, and develop a word-order agnostic embedding enhancement
module to deal with the lack of structure in such queries. Benefitting from the
computational efficiency of the convolution operation, Embedding Convolutions,
when applied on the enriched word embeddings, result in a light-weight and yet
powerful encoder (InceptionXML) that is robust to the inherent lack of
structure in short-text extreme classification.
</p>
<p>Towards scaling our model to problems with millions of labels, we also
propose InceptionXML+, which addresses the shortcomings of the dynamic
hard-negative mining framework in the recently proposed LightXML by improving
the alignment between the label-shortlister and extreme classifier. On popular
benchmark datasets, we empirically demonstrate that the proposed method
outperforms state-of-the-art deep extreme classifiers such as Astec by an
average of 5% and 8% on the P@k and propensity-scored PSP@k metrics
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mi{\dh}eind's WMT 2021 submission. (arXiv:2109.07343v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07343">
<div class="article-summary-box-inner">
<span><p>We present Mi{\dh}eind's submission for the English$\to$Icelandic and
Icelandic$\to$English subsets of the 2021 WMT news translation task.
Transformer-base models are trained for translation on parallel data to
generate backtranslations iteratively. A pretrained mBART-25 model is then
adapted for translation using parallel data as well as the last backtranslation
iteration. This adapted pretrained model is then used to re-generate
backtranslations, and the training of the adapted model is continued.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Introducing an Abusive Language Classification Framework for Telegram to Investigate the German Hater Community. (arXiv:2109.07346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07346">
<div class="article-summary-box-inner">
<span><p>Since traditional social media platforms ban more and more actors that
distribute hate speech or other forms of abusive language (deplatforming),
these actors migrate to alternative platforms that do not moderate the users'
content. One known platform that is relevant for the German hater community is
Telegram, for which there have only been made limited research efforts so far.
</p>
<p>The goal of this study is to develop a broad framework that consists of (i)
an abusive language classification model for German Telegram messages and (ii)
a classification model for the hatefulness of Telegram channels. For the first
part, we employ existing abusive language datasets containing posts from other
platforms to build our classification models. For the channel classification
model, we develop a method that combines channel specific content information
coming from a topic model with a social graph to predict the hatefulness of
channels. Furthermore, we complement these two approaches for hate speech
detection with insightful results on the evolution of the hater community on
Telegram in Germany. Moreover, we propose methods to the hate speech research
community for scalable network analyses for social media platforms. As an
additional output of the study, we release an annotated abusive language
dataset containing 1,149 annotated Telegram messages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Transfer of Monolingual Models. (arXiv:2109.07348v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07348">
<div class="article-summary-box-inner">
<span><p>Recent studies in zero-shot cross-lingual learning using multilingual models
have falsified the previous hypothesis that shared vocabulary and joint
pre-training are the keys to cross-lingual generalization. Inspired by this
advancement, we introduce a cross-lingual transfer method for monolingual
models based on domain adaptation. We study the effects of such transfer from
four different languages to English. Our experimental results on GLUE show that
the transferred models outperform the native English model independently of the
source language. After probing the English linguistic knowledge encoded in the
representations before and after transfer, we find that semantic information is
retained from the source language, while syntactic information is learned
during transfer. Additionally, the results of evaluating the transferred models
in source language tasks reveal that their performance in the source domain
deteriorates after transfer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The ELITR ECA Corpus. (arXiv:2109.07351v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07351">
<div class="article-summary-box-inner">
<span><p>We present the ELITR ECA corpus, a multilingual corpus derived from
publications of the European Court of Auditors. We use automatic translation
together with Bleualign to identify parallel sentence pairs in all 506
translation directions. The result is a corpus comprising 264k document pairs
and 41.9M sentence pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Incremental Transformers: An Empirical Analysis of Transformer Models for Incremental NLU. (arXiv:2109.07364v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07364">
<div class="article-summary-box-inner">
<span><p>Incremental processing allows interactive systems to respond based on partial
inputs, which is a desirable property e.g. in dialogue agents. The currently
popular Transformer architecture inherently processes sequences as a whole,
abstracting away the notion of time. Recent work attempts to apply Transformers
incrementally via restart-incrementality by repeatedly feeding, to an unchanged
model, increasingly longer input prefixes to produce partial outputs. However,
this approach is computationally costly and does not scale efficiently for long
sequences. In parallel, we witness efforts to make Transformers more efficient,
e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we
examine the feasibility of LT for incremental NLU in English. Our results show
that the recurrent LT model has better incremental performance and faster
inference speed compared to the standard Transformer and LT with
restart-incrementality, at the cost of part of the non-incremental (full
sequence) quality. We show that the performance drop can be mitigated by
training the model to wait for right context before committing to an output and
that training with input prefixes is beneficial for delivering correct partial
outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniST: Unified End-to-end Model for Streaming and Non-streaming Speech Translation. (arXiv:2109.07368v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07368">
<div class="article-summary-box-inner">
<span><p>This paper presents a unified end-to-end frame-work for both streaming and
non-streamingspeech translation. While the training recipes for non-streaming
speech translation have been mature, the recipes for streaming
speechtranslation are yet to be built. In this work, wefocus on developing a
unified model (UniST) which supports streaming and non-streaming ST from the
perspective of fundamental components, including training objective, attention
mechanism and decoding policy. Experiments on the most popular speech-to-text
translation benchmark dataset, MuST-C, show that UniST achieves significant
improvement for non-streaming ST, and a better-learned trade-off for BLEU score
and latency metrics for streaming ST, compared with end-to-end baselines and
the cascaded models. We will make our codes and evaluation tools publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Topic Transferable Table Question Answering. (arXiv:2109.07377v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07377">
<div class="article-summary-box-inner">
<span><p>Weakly-supervised table question-answering(TableQA) models have achieved
state-of-art performance by using pre-trained BERT transformer to jointly
encoding a question and a table to produce structured query for the question.
However, in practical settings TableQA systems are deployed over table corpora
having topic and word distributions quite distinct from BERT's pretraining
corpus. In this work we simulate the practical topic shift scenario by
designing novel challenge benchmarks WikiSQL-TS and WikiTQ-TS, consisting of
train-dev-test splits in five distinct topic groups, based on the popular
WikiSQL and WikiTableQuestions datasets. We empirically show that, despite
pre-training on large open-domain text, performance of models degrades
significantly when they are evaluated on unseen topics. In response, we propose
T3QA (Topic Transferable Table Question Answering) a pragmatic adaptation
framework for TableQA comprising of: (1) topic-specific vocabulary injection
into BERT, (2) a novel text-to-text transformer generator (such as T5, GPT2)
based natural language question generation pipeline focused on generating topic
specific training data, and (3) a logical form reranker. We show that T3QA
provides a reasonably good baseline for our topic shift benchmarks. We believe
our topic split benchmarks will lead to robust TableQA solutions that are
better suited for practical deployment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RankNAS: Efficient Neural Architecture Search by Pairwise Ranking. (arXiv:2109.07383v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07383">
<div class="article-summary-box-inner">
<span><p>This paper addresses the efficiency challenge of Neural Architecture Search
(NAS) by formulating the task as a ranking problem. Previous methods require
numerous training examples to estimate the accurate performance of
architectures, although the actual goal is to find the distinction between
"good" and "bad" candidates. Here we do not resort to performance predictors.
Instead, we propose a performance ranking method (RankNAS) via pairwise
ranking. It enables efficient architecture search using much fewer training
examples. Moreover, we develop an architecture selection method to prune the
search space and concentrate on more promising candidates. Extensive
experiments on machine translation and language modeling tasks show that
RankNAS can design high-performance architectures while being orders of
magnitude faster than state-of-the-art NAS systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs. (arXiv:2109.07396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07396">
<div class="article-summary-box-inner">
<span><p>End-to-End task-oriented dialogue systems generate responses based on dialog
history and an accompanying knowledge base (KB). Inferring those KB entities
that are most relevant for an utterance is crucial for response generation.
Existing state of the art scales to large KBs by softly filtering over
irrelevant KB information. In this paper, we propose a novel filtering
technique that consists of (1) a pairwise similarity based filter that
identifies relevant information by respecting the n-ary structure in a KB
record. and, (2) an auxiliary loss that helps in separating contextually
unrelated KB information. We also propose a new metric -- multiset entity F1
which fixes a correctness issue in the existing entity F1 metric. Experimental
results on three publicly available task-oriented dialog datasets show that our
proposed approach outperforms existing state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Matching with Transformers in MELT. (arXiv:2109.07401v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07401">
<div class="article-summary-box-inner">
<span><p>One of the strongest signals for automated matching of ontologies and
knowledge graphs are the textual descriptions of the concepts. The methods that
are typically applied (such as character- or token-based comparisons) are
relatively simple, and therefore do not capture the actual meaning of the
texts. With the rise of transformer-based language models, text comparison
based on meaning (rather than lexical features) is possible. In this paper, we
model the ontology matching task as classification problem and present
approaches based on transformer models. We further provide an easy to use
implementation in the MELT framework which is suited for ontology and knowledge
graph matching. We show that a transformer-based filter helps to choose the
correct correspondences given a high-recall alignment and already achieves a
good result with simple alignment post-processing methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT is Robust! A Case Against Synonym-Based Adversarial Examples in Text Classification. (arXiv:2109.07403v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07403">
<div class="article-summary-box-inner">
<span><p>Deep Neural Networks have taken Natural Language Processing by storm. While
this led to incredible improvements across many tasks, it also initiated a new
research field, questioning the robustness of these neural networks by
attacking them. In this paper, we investigate four word substitution-based
attacks on BERT. We combine a human evaluation of individual word substitutions
and a probabilistic analysis to show that between 96% and 99% of the analyzed
attacks do not preserve semantics, indicating that their success is mainly
based on feeding poor data to the model. To further confirm that, we introduce
an efficient data augmentation procedure and show that many adversarial
examples can be prevented by including data similar to the attacks during
training. An additional post-processing step reduces the success rates of
state-of-the-art attacks below 5%. Finally, by looking at more reasonable
thresholds on constraints for word substitutions, we conclude that BERT is a
lot more robust than research on attacks suggests.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Assisting the Human Fact-Checkers: Detecting All Previously Fact-Checked Claims in a Document. (arXiv:2109.07410v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07410">
<div class="article-summary-box-inner">
<span><p>Given the recent proliferation of false claims online, there has been a lot
of manual fact-checking effort. As this is very time-consuming, human
fact-checkers can benefit from tools that can support them and make them more
efficient. Here, we focus on building a system that could provide such support.
Given an input document, it aims to detect all sentences that contain a claim
that can be verified by some previously fact-checked claims (from a given
database). The output is a re-ranked list of the document sentences, so that
those that can be verified are ranked as high as possible, together with
corresponding evidence. Unlike previous work, which has looked into claim
retrieval, here we take a document-level perspective. We create a new manually
annotated dataset for the task, and we propose suitable evaluation measures. We
further experiment with a learning-to-rank approach, achieving sizable
performance gains over several strong baselines. Our analysis demonstrates the
importance of modeling text similarity and stance, while also taking into
account the veracity of the retrieved previously fact-checked claims. We
believe that this research would be of interest to fact-checkers, journalists,
media, and regulatory authorities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SupCL-Seq: Supervised Contrastive Learning for Downstream Optimized Sequence Representations. (arXiv:2109.07424v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07424">
<div class="article-summary-box-inner">
<span><p>While contrastive learning is proven to be an effective training strategy in
computer vision, Natural Language Processing (NLP) is only recently adopting it
as a self-supervised alternative to Masked Language Modeling (MLM) for
improving sequence representations. This paper introduces SupCL-Seq, which
extends the supervised contrastive learning from computer vision to the
optimization of sequence representations in NLP. By altering the dropout mask
probability in standard Transformer architectures, for every representation
(anchor), we generate augmented altered views. A supervised contrastive loss is
then utilized to maximize the system's capability of pulling together similar
samples (e.g., anchors and their altered views) and pushing apart the samples
belonging to the other classes. Despite its simplicity, SupCLSeq leads to large
gains in many sequence classification tasks on the GLUE benchmark compared to a
standard BERTbase, including 6% absolute improvement on CoLA, 5.4% on MRPC,
4.7% on RTE and 2.6% on STSB. We also show consistent gains over self
supervised contrastively learned representations, especially in non-semantic
tasks. Finally we show that these gains are not solely due to augmentation, but
rather to a downstream optimized sequence representation. Code:
https://github.com/hooman650/SupCL-Seq
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discriminative and Generative Transformer-based Models For Situation Entity Classification. (arXiv:2109.07434v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07434">
<div class="article-summary-box-inner">
<span><p>We re-examine the situation entity (SE) classification task with varying
amounts of available training data. We exploit a Transformer-based variational
autoencoder to encode sentences into a lower dimensional latent space, which is
used to generate the text and learn a SE classifier. Test set and cross-genre
evaluations show that when training data is plentiful, the proposed model can
improve over the previous discriminative state-of-the-art models. Our approach
performs disproportionately better with smaller amounts of training data, but
when faced with extremely small sets (4 instances per label), generative RNN
methods outperform transformers. Our work provides guidance for future efforts
on SE and semantic prediction tasks, and low-label training regimes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative. (arXiv:2109.07437v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07437">
<div class="article-summary-box-inner">
<span><p>Pre-training, where models are trained on an auxiliary objective with
abundant data before being fine-tuned on data from the downstream task, is now
the dominant paradigm in NLP. In general, the pre-training step relies on
little to no direct knowledge of the task on which the model will be
fine-tuned, even when the end-task is known in advance. Our work challenges
this status-quo of end-task agnostic pre-training. First, on three different
low-resource NLP tasks from two domains, we demonstrate that multi-tasking the
end-task and auxiliary objectives results in significantly better downstream
task performance than the widely-used task-agnostic continued pre-training
paradigm of Gururangan et al. (2020). We next introduce an online meta-learning
algorithm that learns a set of multi-task weights to better balance among our
multiple auxiliary objectives, achieving further improvements on end task
performance and data efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Is "moby dick" a Whale or a Bird? Named Entities and Terminology in Speech Translation. (arXiv:2109.07439v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07439">
<div class="article-summary-box-inner">
<span><p>Automatic translation systems are known to struggle with rare words. Among
these, named entities (NEs) and domain-specific terms are crucial, since errors
in their translation can lead to severe meaning distortions. Despite their
importance, previous speech translation (ST) studies have neglected them, also
due to the dearth of publicly available resources tailored to their specific
evaluation. To fill this gap, we i) present the first systematic analysis of
the behavior of state-of-the-art ST systems in translating NEs and terminology,
and ii) release NEuRoparl-ST, a novel benchmark built from European Parliament
speeches annotated with NEs and terminology. Our experiments on the three
language directions covered by our benchmark (en-&gt;es/fr/it) show that ST
systems correctly translate 75-80% of terms and 65-70% of NEs, with very low
performance (37-40%) on person names.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Detoxifying Language Models. (arXiv:2109.07445v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07445">
<div class="article-summary-box-inner">
<span><p>Large language models (LM) generate remarkably fluent text and can be
efficiently adapted across NLP tasks. Measuring and guaranteeing the quality of
generated text in terms of safety is imperative for deploying LMs in the real
world; to this end, prior work often relies on automatic evaluation of LM
toxicity. We critically discuss this approach, evaluate several toxicity
mitigation strategies with respect to both automatic and human evaluation, and
analyze consequences of toxicity mitigation in terms of model bias and LM
quality. We demonstrate that while basic intervention strategies can
effectively optimize previously established automatic metrics on the
RealToxicityPrompts dataset, this comes at the cost of reduced LM coverage for
both texts about, and dialects of, marginalized groups. Additionally, we find
that human raters often disagree with high automatic toxicity scores after
strong toxicity reduction interventions -- highlighting further the nuances
involved in careful evaluation of LM toxicity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Does Translation Require Context? A Data-driven, Multilingual Exploration. (arXiv:2109.07446v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07446">
<div class="article-summary-box-inner">
<span><p>Although proper handling of discourse phenomena significantly contributes to
the quality of machine translation (MT), common translation quality metrics do
not adequately capture them. Recent works in context-aware MT attempt to target
a small set of these phenomena during evaluation. In this paper, we propose a
new metric, P-CXMI, which allows us to identify translations that require
context systematically and confirm the difficulty of previously studied
phenomena as well as uncover new ones that have not been addressed in previous
work. We then develop the Multilingual Discourse-Aware (MuDA) benchmark, a
series of taggers for these phenomena in 14 different language pairs, which we
use to evaluate context-aware MT. We find that state-of-the-art context-aware
MT models find marginal improvements over context-agnostic models on our
benchmark, which suggests current models do not handle these ambiguities
effectively. We release code and data to invite the MT research community to
increase efforts on context-aware translation on discourse phenomena and
languages that are currently overlooked.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WikiGUM: Exhaustive Entity Linking for Wikification in 12 Genres. (arXiv:2109.07449v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07449">
<div class="article-summary-box-inner">
<span><p>Previous work on Entity Linking has focused on resources targeting non-nested
proper named entity mentions, often in data from Wikipedia, i.e. Wikification.
In this paper, we present and evaluate WikiGUM, a fully wikified dataset,
covering all mentions of named entities, including their non-named and
pronominal mentions, as well as mentions nested within other mentions. The
dataset covers a broad range of 12 written and spoken genres, most of which
have not been included in Entity Linking efforts to date, leading to poor
performance by a pretrained SOTA system in our evaluation. The availability of
a variety of other annotations for the same data also enables further research
on entities in context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Machines Read Coding Manuals Yet? -- A Benchmark for Building Better Language Models for Code Understanding. (arXiv:2109.07452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07452">
<div class="article-summary-box-inner">
<span><p>Code understanding is an increasingly important application of Artificial
Intelligence. A fundamental aspect of understanding code is understanding text
about code, e.g., documentation and forum discussions. Pre-trained language
models (e.g., BERT) are a popular approach for various NLP tasks, and there are
now a variety of benchmarks, such as GLUE, to help improve the development of
such models for natural language understanding. However, little is known about
how well such models work on textual artifacts about code, and we are unaware
of any systematic set of downstream tasks for such an evaluation. In this
paper, we derive a set of benchmarks (BLANCA - Benchmarks for LANguage models
on Coding Artifacts) that assess code understanding based on tasks such as
predicting the best answer to a question in a forum post, finding related forum
posts, or predicting classes related in a hierarchy from class documentation.
We evaluate the performance of current state-of-the-art language models on
these tasks and show that there is a significant improvement on each task from
fine tuning. We also show that multi-task training over BLANCA tasks helps
build better language models for code understanding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Comparing Text Representations: A Theory-Driven Approach. (arXiv:2109.07458v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07458">
<div class="article-summary-box-inner">
<span><p>Much of the progress in contemporary NLP has come from learning
representations, such as masked language model (MLM) contextual embeddings,
that turn challenging problems into simple classification tasks. But how do we
quantify and explain this effect? We adapt general tools from computational
learning theory to fit the specific characteristics of text datasets and
present a method to evaluate the compatibility between representations and
tasks. Even though many tasks can be easily solved with simple bag-of-words
(BOW) representations, BOW does poorly on hard natural language inference
tasks. For one such task we find that BOW cannot distinguish between real and
randomized labelings, while pre-trained MLM representations show 72x greater
distinction between real and random labelings than BOW. This method provides a
calibrated, quantitative measure of the difficulty of a classification-based
NLP task, enabling comparisons between representations without requiring
empirical evaluations that may be sensitive to initializations and
hyperparameters. The method provides a fresh perspective on the patterns in a
dataset and the alignment of those patterns with specific labels.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Domain Adaptation of Language Models via Adaptive Tokenization. (arXiv:2109.07460v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07460">
<div class="article-summary-box-inner">
<span><p>Contextual embedding-based language models trained on large data sets, such
as BERT and RoBERTa, provide strong performance across a wide range of tasks
and are ubiquitous in modern NLP. It has been observed that fine-tuning these
models on tasks involving data from domains different from that on which they
were pretrained can lead to suboptimal performance. Recent work has explored
approaches to adapt pretrained language models to new domains by incorporating
additional pretraining using domain-specific corpora and task data. We propose
an alternative approach for transferring pretrained language models to new
domains by adapting their tokenizers. We show that domain-specific subword
sequences can be efficiently determined directly from divergences in the
conditional token distributions of the base and domain-specific corpora. In
datasets from four disparate domains, we find adaptive tokenization on a
pretrained RoBERTa model provides &gt;97% of the performance benefits of domain
specific pretraining. Our approach produces smaller models and less training
and inference time than other approaches using tokenizer augmentation. While
adaptive tokenization incurs a 6% increase in model parameters in our
experimentation, due to the introduction of 10k new domain-specific tokens, our
approach, using 64 vCPUs, is 72x faster than further pretraining the language
model on domain-specific corpora on 8 TPUs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AnnIE: An Annotation Platform for Constructing Complete Open Information Extraction Benchmark. (arXiv:2109.07464v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07464">
<div class="article-summary-box-inner">
<span><p>Open Information Extraction (OIE) is the task of extracting facts from
sentences in the form of relations and their corresponding arguments in
schema-free manner. Intrinsic performance of OIE systems is difficult to
measure due to the incompleteness of existing OIE benchmarks: the ground truth
extractions do not group all acceptable surface realizations of the same fact
that can be extracted from a sentence. To measure performance of OIE systems
more realistically, it is necessary to manually annotate complete facts (i.e.,
clusters of all acceptable surface realizations of the same fact) from input
sentences. We propose AnnIE: an interactive annotation platform that
facilitates such challenging annotation tasks and supports creation of complete
fact-oriented OIE evaluation benchmarks. AnnIE is modular and flexible in order
to support different use case scenarios (i.e., benchmarks covering different
types of facts). We use AnnIE to build two complete OIE benchmarks: one with
verb-mediated facts and another with facts encompassing named entities.
Finally, we evaluate several OIE systems on our complete benchmarks created
with AnnIE. Our results suggest that existing incomplete benchmarks are overly
lenient, and that OIE systems are not as robust as previously reported. We
publicly release AnnIE under non-restrictive license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Limits of Minimal Pairs in Contrastive Evaluation. (arXiv:2109.07465v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.07465">
<div class="article-summary-box-inner">
<span><p>Minimal sentence pairs are frequently used to analyze the behavior of
language models. It is often assumed that model behavior on contrastive pairs
is predictive of model behavior at large. We argue that two conditions are
necessary for this assumption to hold: First, a tested hypothesis should be
well-motivated, since experiments show that contrastive evaluation can lead to
false positives. Secondly, test data should be chosen such as to minimize
distributional discrepancy between evaluation time and deployment time. For a
good approximation of deployment-time decoding, we recommend that minimal pairs
are created based on machine-generated text, as opposed to human-written
references. We present a contrastive evaluation suite for English-German MT
that implements this recommendation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negative Statements Considered Useful. (arXiv:2001.04425v5 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2001.04425">
<div class="article-summary-box-inner">
<span><p>Knowledge bases (KBs) about notable entities and their properties are an
important asset in applications such as search, question answering and
dialogue. All popular KBs capture virtually only positive statements, and
abstain from taking any stance on statements not stored in the KB. This paper
makes the case for explicitly stating salient statements that do not hold.
Negative statements are useful to overcome limitations of question answering
systems that are mainly geared for positive questions; they can also contribute
to informative summaries of entities. Due to the abundance of such invalid
statements, any effort to compile them needs to address ranking by saliency. We
present a statisticalinference method for compiling and ranking negative
statements, based on expectations from positive statements of related entities
in peer groups. Experimental results, with a variety of datasets, show that the
method can effectively discover notable negative statements, and extrinsic
studies underline their usefulness for entity summarization. Datasets and code
are released as resources for further research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Neural Program Synthesis from Multimodal Specifications. (arXiv:2010.01678v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.01678">
<div class="article-summary-box-inner">
<span><p>Multimodal program synthesis, which leverages different types of user input
to synthesize a desired program, is an attractive way to scale program
synthesis to challenging settings; however, it requires integrating noisy
signals from the user, like natural language, with hard constraints on the
program's behavior. This paper proposes an optimal neural synthesis approach
where the goal is to find a program that satisfies user-provided constraints
while also maximizing the program's score with respect to a neural model.
Specifically, we focus on multimodal synthesis tasks in which the user intent
is expressed using a combination of natural language (NL) and input-output
examples. At the core of our method is a top-down recurrent neural model that
places distributions over abstract syntax trees conditioned on the NL input.
This model not only allows for efficient search over the space of syntactically
valid programs, but it allows us to leverage automated program analysis
techniques for pruning the search space based on infeasibility of partial
programs with respect to the user's constraints. The experimental results on a
multimodal synthesis dataset (StructuredRegex) show that our method
substantially outperforms prior state-of-the-art techniques in terms of
accuracy and efficiency, and finds model-optimal programs more frequently.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infusing Multi-Source Knowledge with Heterogeneous Graph Neural Network for Emotional Conversation Generation. (arXiv:2012.04882v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04882">
<div class="article-summary-box-inner">
<span><p>The success of emotional conversation systems depends on sufficient
perception and appropriate expression of emotions. In a real-world
conversation, we firstly instinctively perceive emotions from multi-source
information, including the emotion flow of dialogue history, facial
expressions, and personalities of speakers, and then express suitable emotions
according to our personalities, but these multiple types of information are
insufficiently exploited in emotional conversation fields. To address this
issue, we propose a heterogeneous graph-based model for emotional conversation
generation. Specifically, we design a Heterogeneous Graph-Based Encoder to
represent the conversation content (i.e., the dialogue history, its emotion
flow, facial expressions, and speakers' personalities) with a heterogeneous
graph neural network, and then predict suitable emotions for feedback. After
that, we employ an Emotion-Personality-Aware Decoder to generate a response not
only relevant to the conversation context but also with appropriate emotions,
by taking the encoded graph representations, the predicted emotions from the
encoder and the personality of the current speaker as inputs. Experimental
results show that our model can effectively perceive emotions from multi-source
knowledge and generate a satisfactory response, which significantly outperforms
previous state-of-the-art models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Synthetic Data Improves Neural Machine Translation with Knowledge Distillation. (arXiv:2012.15455v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15455">
<div class="article-summary-box-inner">
<span><p>This paper explores augmenting monolingual data for knowledge distillation in
neural machine translation. Source language monolingual text can be
incorporated as a forward translation. Interestingly, we find the best way to
incorporate target language monolingual text is to translate it to the source
language and round-trip translate it back to the target language, resulting in
a fully synthetic corpus. We find that combining monolingual data from both
source and target languages yields better performance than a corpus twice as
large only in one language. Moreover, experiments reveal that the improvement
depends upon the provenance of the test set. If the test set was originally in
the source language (with the target side written by translators), then forward
translating source monolingual data matters. If the test set was originally in
the target language (with the source written by translators), then
incorporating target monolingual data matters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-to-text Generation by Splicing Together Nearest Neighbors. (arXiv:2101.08248v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08248">
<div class="article-summary-box-inner">
<span><p>We propose to tackle data-to-text generation tasks by directly splicing
together retrieved segments of text from "neighbor" source-target pairs. Unlike
recent work that conditions on retrieved neighbors but generates text
token-by-token, left-to-right, we learn a policy that directly manipulates
segments of neighbor text, by inserting or replacing them in partially
constructed generations. Standard techniques for training such a policy require
an oracle derivation for each generation, and we prove that finding the
shortest such derivation can be reduced to parsing under a particular weighted
context-free grammar. We find that policies learned in this way perform on par
with strong baselines in terms of automatic and human evaluation, but allow for
more interpretable and controllable generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Attention Meets Fast Recurrence: Training Language Models with Reduced Compute. (arXiv:2102.12459v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.12459">
<div class="article-summary-box-inner">
<span><p>Large language models have become increasingly difficult to train because of
the growing computation time and cost. In this work, we present SRU++, a
highly-efficient architecture that combines fast recurrence and attention for
sequence modeling. SRU++ exhibits strong modeling capacity and training
efficiency. On standard language modeling tasks such as Enwik8, Wiki-103 and
Billion Word datasets, our model obtains better bits-per-character and
perplexity while using 3x-10x less training cost compared to top-performing
Transformer models. For instance, our model achieves a state-of-the-art result
on the Enwik8 dataset using 1.6 days of training on an 8-GPU machine. We
further demonstrate that SRU++ requires minimal attention for near
state-of-the-art performance. Our results suggest jointly leveraging fast
recurrence with little attention as a promising direction for accelerating
model training and inference.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Attribute Alignment: Controlling Text Generation from Pre-trained Language Models. (arXiv:2103.11070v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11070">
<div class="article-summary-box-inner">
<span><p>Large language models benefit from training with a large amount of unlabeled
text, which gives them increasingly fluent and diverse generation capabilities.
However, using these models for text generation that takes into account target
attributes, such as sentiment polarity or specific topics, remains a challenge.
We propose a simple and flexible method for controlling text generation by
aligning disentangled attribute representations. In contrast to recent efforts
on training a discriminator to perturb the token level distribution for an
attribute, we use the same data to learn an alignment function to guide the
pre-trained, non-controlled language model to generate texts with the target
attribute without changing the original language model parameters. We evaluate
our method on sentiment- and topic-controlled generation, and show large
performance gains over previous methods while retaining fluency and diversity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Explanations from Empirical Explainers. (arXiv:2103.15429v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.15429">
<div class="article-summary-box-inner">
<span><p>Amid a discussion about Green AI in which we see explainability neglected, we
explore the possibility to efficiently approximate computationally expensive
explainers. To this end, we propose feature attribution modelling with
Empirical Explainers. Empirical Explainers learn from data to predict the
attribution maps of expensive explainers. We train and test Empirical
Explainers in the language domain and find that they model their expensive
counterparts surprisingly well, at a fraction of the cost. They could thus
mitigate the computational burden of neural explanations significantly, in
applications that tolerate an approximation error.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adversarial Regularization as Stackelberg Game: An Unrolled Optimization Approach. (arXiv:2104.04886v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04886">
<div class="article-summary-box-inner">
<span><p>Adversarial regularization has been shown to improve the generalization
performance of deep learning models in various natural language processing
tasks. Existing works usually formulate the method as a zero-sum game, which is
solved by alternating gradient descent/ascent algorithms. Such a formulation
treats the adversarial and the defending players equally, which is undesirable
because only the defending player contributes to the generalization
performance. To address this issue, we propose Stackelberg Adversarial
Regularization (SALT), which formulates adversarial regularization as a
Stackelberg game. This formulation induces a competition between a leader and a
follower, where the follower generates perturbations, and the leader trains the
model subject to the perturbations. Different from conventional approaches, in
SALT, the leader is in an advantageous position. When the leader moves, it
recognizes the strategy of the follower and takes the anticipated follower's
outcomes into consideration. Such a leader's advantage enables us to improve
the model fitting to the unperturbed data. The leader's strategic information
is captured by the Stackelberg gradient, which is obtained using an unrolling
algorithm. Our experimental results on a set of machine translation and natural
language understanding tasks show that SALT outperforms existing adversarial
regularization baselines across all tasks. Our code is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Span Pointer Networks for Non-Autoregressive Task-Oriented Semantic Parsing. (arXiv:2104.07275v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07275">
<div class="article-summary-box-inner">
<span><p>An effective recipe for building seq2seq, non-autoregressive, task-oriented
parsers to map utterances to semantic frames proceeds in three steps: encoding
an utterance $x$, predicting a frame's length |y|, and decoding a |y|-sized
frame with utterance and ontology tokens. Though empirically strong, these
models are typically bottlenecked by length prediction, as even small
inaccuracies change the syntactic and semantic characteristics of resulting
frames. In our work, we propose span pointer networks, non-autoregressive
parsers which shift the decoding task from text generation to span prediction;
that is, when imputing utterance spans into frame slots, our model produces
endpoints (e.g., [i, j]) as opposed to text (e.g., "6pm"). This natural
quantization of the output space reduces the variability of gold frames,
therefore improving length prediction and, ultimately, exact match.
Furthermore, length prediction is now responsible for frame syntax and the
decoder is responsible for frame semantics, resulting in a coarse-to-fine
model. We evaluate our approach on several task-oriented semantic parsing
datasets. Notably, we bridge the quality gap between non-autogressive and
autoregressive parsers, achieving 87 EM on TOPv2 (Chen et al. 2020).
Furthermore, due to our more consistent gold frames, we show strong
improvements in model generalization in both cross-domain and cross-lingual
transfer in low-resource settings. Finally, due to our diminished output
vocabulary, we observe 70% reduction in latency and 83% reduction in memory at
beam size 5 compared to prior non-autoregressive parsers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Open-Vocabulary Translation from Visual Text Representations. (arXiv:2104.08211v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08211">
<div class="article-summary-box-inner">
<span><p>Machine translation models have discrete vocabularies and commonly use
subword segmentation techniques to achieve an 'open vocabulary.' This approach
relies on consistent and correct underlying unicode sequences, and makes models
susceptible to degradation from common types of noise and variation. Motivated
by the robustness of human language processing, we propose the use of visual
text representations, which dispense with a finite set of text embeddings in
favor of continuous vocabularies created by processing visually rendered text
with sliding windows. We show that models using visual text representations
approach or match performance of traditional text models on small and larger
datasets. More importantly, models with visual embeddings demonstrate
significant robustness to varied types of noise, achieving e.g., 25.9 BLEU on a
character permuted German-English task where subword models degrade to 1.9.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does language help generalization in vision models?. (arXiv:2104.08313v3 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08313">
<div class="article-summary-box-inner">
<span><p>Vision models trained on multimodal datasets can benefit from the wide
availability of large image-caption datasets. A recent model (CLIP) was found
to generalize well in zero-shot and transfer learning settings. This could
imply that linguistic or "semantic grounding" confers additional generalization
abilities to the visual feature space. Here, we systematically evaluate various
multimodal architectures and vision-only models in terms of unsupervised
clustering, few-shot learning, transfer learning and adversarial robustness. In
each setting, multimodal training produced no additional generalization
capability compared to standard supervised visual training. We conclude that
work is still required for semantic grounding to help improve vision models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CLIPScore: A Reference-free Evaluation Metric for Image Captioning. (arXiv:2104.08718v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08718">
<div class="article-summary-box-inner">
<span><p>Image captioning has conventionally relied on reference-based automatic
evaluations, where machine captions are compared against captions written by
humans. This is in contrast to the reference-free manner in which humans assess
caption quality.
</p>
<p>In this paper, we report the surprising empirical finding that CLIP (Radford
et al., 2021), a cross-modal model pretrained on 400M image+caption pairs from
the web, can be used for robust automatic evaluation of image captioning
without the need for references. Experiments spanning several corpora
demonstrate that our new reference-free metric, CLIPScore, achieves the highest
correlation with human judgements, outperforming existing reference-based
metrics like CIDEr and SPICE. Information gain experiments demonstrate that
CLIPScore, with its tight focus on image-text compatibility, is complementary
to existing reference-based metrics that emphasize text-text similarities.
Thus, we also present a reference-augmented version, RefCLIPScore, which
achieves even higher correlation. Beyond literal description tasks, several
case studies reveal domains where CLIPScore performs well (clip-art images,
alt-text rating), but also where it is relatively weaker in comparison to
reference-based metrics, e.g., news captions that require richer contextual
knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieval-Free Knowledge-Grounded Dialogue Response Generation with Adapters. (arXiv:2105.06232v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06232">
<div class="article-summary-box-inner">
<span><p>To diversify and enrich generated dialogue responses, knowledge-grounded
dialogue has been investigated in recent years. The existing methods tackle the
knowledge grounding challenge by retrieving the relevant sentences over a large
corpus and augmenting the dialogues with explicit extra information. Despite
their success, however, the existing works have drawbacks on the inference
efficiency. This paper proposes KnowExpert, an end-to-end framework to bypass
the explicit retrieval process and inject knowledge into the pre-trained
language models with lightweight adapters and adapt to the knowledge-grounded
dialogue task. To the best of our knowledge, this is the first attempt to
tackle this challenge without retrieval in this task under an open-domain
chit-chat scenario. The experimental results show that KknowExpert performs
comparably with some retrieval-based baselines while being time-efficient in
inference, demonstrating the potential of our proposed direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Counterfactual Interventions Reveal the Causal Effect of Relative Clause Representations on Agreement Prediction. (arXiv:2105.06965v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.06965">
<div class="article-summary-box-inner">
<span><p>When language models process syntactically complex sentences, do they use
their representations of syntax in a manner that is consistent with the grammar
of the language? We propose AlterRep, an intervention-based method to address
this question. For any linguistic feature of a given sentence, AlterRep
generates counterfactual representations by altering how the feature is
encoded, while leaving intact all other aspects of the original representation.
By measuring the change in a model's word prediction behavior when these
counterfactual representations are substituted for the original ones, we can
draw conclusions about the causal effect of the linguistic feature in question
on the model's behavior. We apply this method to study how BERT models of
different sizes process relative clauses (RCs). We find that BERT variants use
RC boundary information during word prediction in a manner that is consistent
with the rules of English grammar; this RC boundary information generalizes to
a considerable extent across different RC types, suggesting that BERT
represents RCs as an abstract linguistic category.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">See, Hear, Read: Leveraging Multimodality with Guided Attention for Abstractive Text Summarization. (arXiv:2105.09601v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.09601">
<div class="article-summary-box-inner">
<span><p>In recent years, abstractive text summarization with multimodal inputs has
started drawing attention due to its ability to accumulate information from
different source modalities and generate a fluent textual summary. However,
existing methods use short videos as the visual modality and short summary as
the ground-truth, therefore, perform poorly on lengthy videos and long
ground-truth summary. Additionally, there exists no benchmark dataset to
generalize this task on videos of varying lengths. In this paper, we introduce
AVIATE, the first large-scale dataset for abstractive text summarization with
videos of diverse duration, compiled from presentations in well-known academic
conferences like NDSS, ICML, NeurIPS, etc. We use the abstract of corresponding
research papers as the reference summaries, which ensure adequate quality and
uniformity of the ground-truth. We then propose FLORAL, a factorized
multi-modal Transformer based decoder-only language model, which inherently
captures the intra-modal and inter-modal dynamics within various input
modalities for the text summarization task. FLORAL utilizes an increasing
number of self-attentions to capture multimodality and performs significantly
better than traditional encoder-decoder based networks. Extensive experiments
illustrate that FLORAL achieves significant improvement over the baselines in
both qualitative and quantitative evaluations on the existing How2 dataset for
short videos and newly introduced AVIATE dataset for videos with diverse
duration, beating the best baseline on the two datasets by $1.39$ and $2.74$
ROUGE-L points respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PTR: Prompt Tuning with Rules for Text Classification. (arXiv:2105.11259v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.11259">
<div class="article-summary-box-inner">
<span><p>Fine-tuned pre-trained language models (PLMs) have achieved awesome
performance on almost all NLP tasks. By using additional prompts to fine-tune
PLMs, we can further stimulate the rich knowledge distributed in PLMs to better
serve downstream tasks. Prompt tuning has achieved promising results on some
few-class classification tasks such as sentiment classification and natural
language inference. However, manually designing lots of language prompts is
cumbersome and fallible. For those auto-generated prompts, it is also expensive
and time-consuming to verify their effectiveness in non-few-shot scenarios.
Hence, it is still challenging for prompt tuning to address many-class
classification tasks. To this end, we propose prompt tuning with rules (PTR)
for many-class text classification and apply logic rules to construct prompts
with several sub-prompts. In this way, PTR is able to encode prior knowledge of
each class into prompt tuning. We conduct experiments on relation
classification, a typical and complicated many-class classification task, and
the results show that PTR can significantly and consistently outperform
existing state-of-the-art baselines. This indicates that PTR is a promising
approach to take advantage of both human prior knowledge and PLMs for those
complicated classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Speaker Detection as a Multi-Objective Optimization with Uncertainty-based Multimodal Fusion. (arXiv:2106.03821v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.03821">
<div class="article-summary-box-inner">
<span><p>It is now well established from a variety of studies that there is a
significant benefit from combining video and audio data in detecting active
speakers. However, either of the modalities can potentially mislead audiovisual
fusion by inducing unreliable or deceptive information. This paper outlines
active speaker detection as a multi-objective learning problem to leverage best
of each modalities using a novel self-attention, uncertainty-based multimodal
fusion scheme. Results obtained show that the proposed multi-objective learning
architecture outperforms traditional approaches in improving both mAP and AUC
scores. We further demonstrate that our fusion strategy surpasses, in active
speaker detection, other modality fusion methods reported in various
disciplines. We finally show that the proposed method significantly improves
the state-of-the-art on the AVA-ActiveSpeaker dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Grounding with 3D Objects. (arXiv:2107.12514v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12514">
<div class="article-summary-box-inner">
<span><p>Seemingly simple natural language requests to a robot are generally
underspecified, for example "Can you bring me the wireless mouse?" Flat images
of candidate mice may not provide the discriminative information needed for
"wireless." The world, and objects in it, are not flat images but complex 3D
shapes. If a human requests an object based on any of its basic properties,
such as color, shape, or texture, robots should perform the necessary
exploration to accomplish the task. In particular, while substantial effort and
progress has been made on understanding explicitly visual attributes like color
and category, comparatively little progress has been made on understanding
language about shapes and contours. In this work, we introduce a novel
reasoning task that targets both visual and non-visual language about 3D
objects. Our new benchmark, ShapeNet Annotated with Referring Expressions
(SNARE) requires a model to choose which of two objects is being referenced by
a natural language description. We introduce several CLIP-based models for
distinguishing objects and demonstrate that while recent advances in jointly
modeling vision and language are useful for robotic language understanding, it
is still the case that these image-based models are weaker at understanding the
3D nature of objects -- properties which play a key role in manipulation. We
find that adding view estimation to language grounding models improves accuracy
on both SNARE and when identifying objects referred to in language on a robot
platform, but note that a large gap remains between these models and human
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Continual Entity Learning in Language Models for Conversational Agents. (arXiv:2108.00082v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.00082">
<div class="article-summary-box-inner">
<span><p>Neural language models (LM) trained on diverse corpora are known to work well
on previously seen entities, however, updating these models with dynamically
changing entities such as place names, song titles and shopping items requires
re-training from scratch and collecting full sentences containing these
entities. We aim to address this issue, by introducing entity-aware language
models (EALM), where we integrate entity models trained on catalogues of
entities into the pre-trained LMs. Our combined language model adaptively adds
information from the entity models into the pre-trained LM depending on the
sentence context. Our entity models can be updated independently of the
pre-trained LM, enabling us to influence the distribution of entities output by
the final LM, without any further training of the pre-trained LM. We show
significant perplexity improvements on task-oriented dialogue datasets,
especially on long-tailed utterances, with an ability to continually adapt to
new entities (to an extent).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perturbing Inputs for Fragile Interpretations in Deep Natural Language Processing. (arXiv:2108.04990v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04990">
<div class="article-summary-box-inner">
<span><p>Interpretability methods like Integrated Gradient and LIME are popular
choices for explaining natural language model predictions with relative word
importance scores. These interpretations need to be robust for trustworthy NLP
applications in high-stake areas like medicine or finance. Our paper
demonstrates how interpretations can be manipulated by making simple word
perturbations on an input text. Via a small portion of word-level swaps, these
adversarial perturbations aim to make the resulting text semantically and
spatially similar to its seed input (therefore sharing similar
interpretations). Simultaneously, the generated examples achieve the same
prediction label as the seed yet are given a substantially different
explanation by the interpretation methods. Our experiments generate fragile
interpretations to attack two SOTA interpretation methods, across three popular
Transformer models and on two different NLP datasets. We observe that the rank
order correlation drops by over 20% when less than 10% of words are perturbed
on average. Further, rank-order correlation keeps decreasing as more words get
perturbed. Furthermore, we demonstrate that candidates generated from our
method have good quality metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Span Fine-tuning for Pre-trained Language Models. (arXiv:2108.12848v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12848">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models (PrLM) have to carefully manage input units when
training on a very large text with a vocabulary consisting of millions of
words. Previous works have shown that incorporating span-level information over
consecutive words in pre-training could further improve the performance of
PrLMs. However, given that span-level clues are introduced and fixed in
pre-training, previous methods are time-consuming and lack of flexibility. To
alleviate the inconvenience, this paper presents a novel span fine-tuning
method for PrLMs, which facilitates the span setting to be adaptively
determined by specific downstream tasks during the fine-tuning phase. In
detail, any sentences processed by the PrLM will be segmented into multiple
spans according to a pre-sampled dictionary. Then the segmentation information
will be sent through a hierarchical CNN module together with the representation
outputs of the PrLM and ultimately generate a span-enhanced representation.
Experiments on GLUE benchmark show that the proposed span fine-tuning method
significantly enhances the PrLM, and at the same time, offer more flexibility
in an efficient way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$\infty$-former: Infinite Memory Transformer. (arXiv:2109.00301v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00301">
<div class="article-summary-box-inner">
<span><p>Transformers struggle when attending to long contexts, since the amount of
computation grows with the context length, and therefore they cannot model
long-term memories effectively. Several variations have been proposed to
alleviate this problem, but they all have a finite memory capacity, being
forced to drop old information. In this paper, we propose the $\infty$-former,
which extends the vanilla transformer with an unbounded long-term memory. By
making use of a continuous-space attention mechanism to attend over the
long-term memory, the $\infty$-former's attention complexity becomes
independent of the context length. Thus, it is able to model arbitrarily long
contexts and maintain "sticky memories" while keeping a fixed computation
budget. Experiments on a synthetic sorting task demonstrate the ability of the
$\infty$-former to retain information from long sequences. We also perform
experiments on language modeling, by training a model from scratch and by
fine-tuning a pre-trained language model, which show benefits of unbounded
long-term memories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language Model Pre-Training. (arXiv:2109.00993v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00993">
<div class="article-summary-box-inner">
<span><p>Large Transformer-based language models such as BERT have led to broad
performance improvements on many NLP tasks. Domain-specific variants of these
models have demonstrated excellent performance on a variety of specialised
tasks. In legal NLP, BERT-based models have led to new state-of-the-art results
on multiple tasks. The exploration of these models has demonstrated the
importance of capturing the specificity of the legal language and its
vocabulary. However, such approaches suffer from high computational costs,
leading to a higher ecological impact and lower accessibility. Our findings,
focusing on English language legal text, show that lightweight LSTM-based
Language Models are able to capture enough information from a small legal text
pretraining corpus and achieve excellent performance on short legal text
classification tasks. This is achieved with a significantly reduced
computational overhead compared to BERT-based models. However, our method also
shows degraded performance on a more complex task, multi-label classification
of longer documents, highlighting the limitations of this lightweight approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From Alignment to Assignment: Frustratingly Simple Unsupervised Entity Alignment. (arXiv:2109.02363v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02363">
<div class="article-summary-box-inner">
<span><p>Cross-lingual entity alignment (EA) aims to find the equivalent entities
between crosslingual KGs, which is a crucial step for integrating KGs.
Recently, many GNN-based EA methods are proposed and show decent performance
improvements on several public datasets. Meanwhile, existing GNN-based EA
methods inevitably inherit poor interpretability and low efficiency from neural
networks. Motivated by the isomorphic assumption of GNNbased methods, we
successfully transform the cross-lingual EA problem into the assignment
problem. Based on this finding, we propose a frustratingly Simple but Effective
Unsupervised entity alignment method (SEU) without neural networks. Extensive
experiments show that our proposed unsupervised method even beats advanced
supervised methods across all public datasets and has high efficiency,
interpretability, and stability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed Attention Transformer for Leveraging Word-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02789">
<div class="article-summary-box-inner">
<span><p>Pretrained contextualized representations offer great success for many
downstream tasks, including document ranking. The multilingual versions of such
pretrained representations provide a possibility of jointly learning many
languages with the same model. Although it is expected to gain big with such
joint training, in the case of cross lingual information retrieval (CLIR), the
models under a multilingual setting are not achieving the same level of
performance as those under a monolingual setting. We hypothesize that the
performance drop is due to the translation gap between query and documents. In
the monolingual retrieval task, because of the same lexical inputs, it is
easier for model to identify the query terms that occurred in documents.
However, in the multilingual pretrained models that the words in different
languages are projected into the same hyperspace, the model tends to translate
query terms into related terms, i.e., terms that appear in a similar context,
in addition to or sometimes rather than synonyms in the target language. This
property is creating difficulties for the model to connect terms that cooccur
in both query and document. To address this issue, we propose a novel Mixed
Attention Transformer (MAT) that incorporates external word level knowledge,
such as a dictionary or translation table. We design a sandwich like
architecture to embed MAT into the recent transformer based deep neural models.
By encoding the translation knowledge into an attention matrix, the model with
MAT is able to focus on the mutually translated words in the input sequence.
Experimental results demonstrate the effectiveness of the external knowledge
and the significant improvement of MAT embedded neural reranking model on CLIR
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers. (arXiv:2109.05186v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05186">
<div class="article-summary-box-inner">
<span><p>This paper investigates continual learning for semantic parsing. In this
setting, a neural semantic parser learns tasks sequentially without accessing
full training data from previous tasks. Direct application of the SOTA
continual learning algorithms to this problem fails to achieve comparable
performance with re-training models with all seen tasks because they have not
considered the special properties of structured outputs yielded by semantic
parsers. Therefore, we propose TotalRecall, a continual learning method
designed for neural semantic parsers from two aspects: i) a sampling method for
memory replay that diversifies logical form templates and balances
distributions of parse actions in a memory; ii) a two-stage training method
that significantly improves generalization capability of the parsers across
tasks. We conduct extensive experiments to study the research problems involved
in continual semantic parsing and demonstrate that a neural semantic parser
trained with TotalRecall achieves superior performance than the one trained
directly with the SOTA continual learning algorithms and achieve a 3-6 times
speedup compared to re-training from scratch. Code and datasets are available
at: https://github.com/zhuang-li/cl_nsp.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Good-Enough Example Extrapolation. (arXiv:2109.05602v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05602">
<div class="article-summary-box-inner">
<span><p>This paper asks whether extrapolating the hidden space distribution of text
examples from one class onto another is a valid inductive bias for data
augmentation. To operationalize this question, I propose a simple data
augmentation protocol called "good-enough example extrapolation" (GE3). GE3 is
lightweight and has no hyperparameters. Applied to three text classification
datasets for various data imbalance scenarios, GE3 improves performance more
than upsampling and other hidden-space data augmentation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Models Localize Linguistic Knowledge in the Same Place: A Layer-wise Probing on BERToids' Representations. (arXiv:2109.05958v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05958">
<div class="article-summary-box-inner">
<span><p>Most of the recent works on probing representations have focused on BERT,
with the presumption that the findings might be similar to the other models. In
this work, we extend the probing studies to two other models in the family,
namely ELECTRA and XLNet, showing that variations in the pre-training
objectives or architectural choices can result in different behaviors in
encoding linguistic information in the representations. Most notably, we
observe that ELECTRA tends to encode linguistic knowledge in the deeper layers,
whereas XLNet instead concentrates that in the earlier layers. Also, the former
model undergoes a slight change during fine-tuning, whereas the latter
experiences significant adjustments. Moreover, we show that drawing conclusions
based on the weight mixing evaluation strategy -- which is widely used in the
context of layer-wise probing -- can be misleading given the norm disparity of
the representations across different layers. Instead, we adopt an alternative
information-theoretic probing with minimum description length, which has
recently been proven to provide more reliable and informative results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Emergence of the Shape Bias Results from Communicative Efficiency. (arXiv:2109.06232v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06232">
<div class="article-summary-box-inner">
<span><p>By the age of two, children tend to assume that new word categories are based
on objects' shape, rather than their color or texture; this assumption is
called the shape bias. They are thought to learn this bias by observing that
their caregiver's language is biased towards shape based categories. This
presents a chicken and egg problem: if the shape bias must be present in the
language in order for children to learn it, how did it arise in language in the
first place? In this paper, we propose that communicative efficiency explains
both how the shape bias emerged and why it persists across generations. We
model this process with neural emergent language agents that learn to
communicate about raw pixelated images. First, we show that the shape bias
emerges as a result of efficient communication strategies employed by agents.
Second, we show that pressure brought on by communicative need is also
necessary for it to persist across generations; simply having a shape bias in
an agent's input language is insufficient. These results suggest that, over and
above the operation of other learning strategies, the shape bias in human
learners may emerge and be sustained by communicative pressures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-OCR Document Correction with large Ensembles of Character Sequence Models. (arXiv:2109.06264v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06264">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel method based on character
sequence-to-sequence models to correct documents already processed with Optical
Character Recognition (OCR) systems. The main contribution of this paper is a
set of strategies to accurately process strings much longer than the ones used
to train the sequence model while being sample- and resource-efficient,
supported by thorough experimentation. The strategy with the best performance
involves splitting the input document in character n-grams and combining their
individual corrections into the final output using a voting scheme that is
equivalent to an ensemble of a large number of sequence models. We further
investigate how to weigh the contributions from each one of the members of this
ensemble. We test our method on nine languages of the ICDAR 2019 competition on
post-OCR text correction and achieve a new state-of-the-art performance in five
of them. Our code for post-OCR correction is shared at
https://github.com/jarobyte91/post_ocr_correction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expert Knowledge-Guided Length-Variant Hierarchical Label Generation for Proposal Classification. (arXiv:2109.06661v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06661">
<div class="article-summary-box-inner">
<span><p>To advance the development of science and technology, research proposals are
submitted to open-court competitive programs developed by government agencies
(e.g., NSF). Proposal classification is one of the most important tasks to
achieve effective and fair review assignments. Proposal classification aims to
classify a proposal into a length-variant sequence of labels. In this paper, we
formulate the proposal classification problem into a hierarchical multi-label
classification task. Although there are certain prior studies, proposal
classification exhibit unique features: 1) the classification result of a
proposal is in a hierarchical discipline structure with different levels of
granularity; 2) proposals contain multiple types of documents; 3) domain
experts can empirically provide partial labels that can be leveraged to improve
task performances. In this paper, we focus on developing a new deep proposal
classification framework to jointly model the three features. In particular, to
sequentially generate labels, we leverage previously-generated labels to
predict the label of next level; to integrate partial labels from experts, we
use the embedding of these empirical partial labels to initialize the state of
neural networks. Our model can automatically identify the best length of label
sequence to stop next label prediction. Finally, we present extensive results
to demonstrate that our method can jointly model partial labels, textual
information, and semantic dependencies in label sequences, and, thus, achieve
advanced performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Fuzzy Attention for Structured Sentiment Analysis. (arXiv:2109.06719v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06719">
<div class="article-summary-box-inner">
<span><p>Attention scorers have achieved success in parsing tasks like semantic and
syntactic dependency parsing. However, in tasks modeled into parsing, like
structured sentiment analysis, "dependency edges" are very sparse which hinders
parser performance. Thus we propose a sparse and fuzzy attention scorer with
pooling layers which improves parser performance and sets the new
state-of-the-art on structured sentiment analysis. We further explore the
parsing modeling on structured sentiment analysis with second-order parsing and
introduce a novel sparse second-order edge building procedure that leads to
significant improvement in parsing performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding. (arXiv:2109.06838v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06838">
<div class="article-summary-box-inner">
<span><p>While large language models have shown exciting progress on several NLP
benchmarks, evaluating their ability for complex analogical reasoning remains
under-explored. Here, we introduce a high-quality crowdsourced dataset of
narratives for employing proverbs in context as a benchmark for abstract
language understanding. The dataset provides fine-grained annotation of aligned
spans between proverbs and narratives, and contains minimal lexical overlaps
between narratives and proverbs, ensuring that models need to go beyond
surface-level reasoning to succeed. We explore three tasks: (1) proverb
recommendation and alignment prediction, (2) narrative generation for a given
proverb and topic, and (3) identifying narratives with similar motifs. Our
experiments show that neural language models struggle in our tasks compared to
humans, and the tasks pose multiple learning challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Legal Transformer Models May Not Always Help. (arXiv:2109.06862v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06862">
<div class="article-summary-box-inner">
<span><p>Deep learning-based Natural Language Processing methods, especially
transformers, have achieved impressive performance in the last few years.
Applying those state-of-the-art NLP methods to legal activities to automate or
simplify some simple work is of great value. This work investigates the value
of domain adaptive pre-training and language adapters in legal NLP tasks. By
comparing the performance of language models with domain adaptive pre-training
on different tasks and different dataset splits, we show that domain adaptive
pre-training is only helpful with low-resource downstream tasks, thus far from
being a panacea. We also benchmark the performance of adapters in a typical
legal NLP task and show that they can yield similar performance to full model
tuning with much smaller training costs. As an additional result, we release
LegalRoBERTa, a RoBERTa model further pre-trained on legal corpora.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-16 04:20:06.230387579 UTC">2021-09-16 04:20:06 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>