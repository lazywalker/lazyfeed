<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-06T01:56:40.168498942Z">09-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">又见Rust区块链招聘 -_-!, but 这个点进来不后悔^_^</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=640daecb-eec5-4b6d-87a1-02b640bb0433">
<div class="article-summary-box-inner">
<span><p>大家好，我们是Deeper Network, 官网是deeper.network, 一个总部位于硅谷的区块链初创企业,我们跟绝大部分区块链公司不同的是，我们有顶级的产品来支撑我们的业务和发展远景。重要的事情说三遍，有产品，有产品，有产品，而且是业界领先的产品！</p>
<p>我们的旗舰产品Deeper Connect已经迭代到最新的第四代产品，其中Deeper Connect mini在Indiegogo平台首发，仅预售成绩就超过270万美元，在Indiegogo历史上的一百多万个项目中排名前10。(前十中的绝大部分项目都是Sony, 华为这样级别的公司)</p>
<p>我们即将发售的最新产品pico的介绍，可以在下面找到：
http://dev.deepernetwork.com:8088/down/tmp/pico.png
世界上最小，最轻，最薄，功能最强大的网络安全+区块链产品：Deeper Connect Pico</p>
<p>目前我们在全球150多个国家拥有20，000+的用户，30，000+节点。
我们于2020年入选了波卡的builders program并且获得了Web 3.0基金会赞助，是波卡生态重要的一员。
2021年，我们的区块链网络Deeper Chain获得波卡黑客松比赛的社区最佳欢迎奖和亚军。</p>
<p>Deeper Connect + Deeper Chain是目前世界上唯一的全栈WEB3.0解决方案,包括：web3.0网关，去中心化安全网络，去中心化广告，去中心化视频点播平台，去中心化CDN等等。</p>
<p>目前公司已经盈利，现金贮备丰厚，正在对接业界最等级的风险投资机构，处于起飞的前夕。</p>
<p>我们希望您具有以下技能：</p>
<p>基本要求：</p>
<p>1、扎实的计算机科学基础知识</p>
<p>2、动手能力强，有死磕精神</p>
<p>3、有丰富的 Rust 开发经验</p>
<p>4、曾经独立完成或者主导完成过具有挑战性的项目</p>
<p>5、对工作有强大的责任心</p>
<p>加分项：</p>
<p>1、区块链相关数据结构与算法</p>
<p>2、Substrate或其他区块链节点开发经验</p>
<p>3、跨链、Layer 2 开发经验</p>
<p>待遇：</p>
<p>丰厚的薪资待遇：40K~80K/月</p>
<p>灵活的工作方式：您可以在任何时间，任何地点，只要有网络就行</p>
<p>表现合格者提供股票/币权的丰厚激励</p>
<p>表现优异者提供美国/加拿大移民机会（目前闹瘟疫，说实话也没啥好移的）</p>
<p>有意向的选手，请发个人简历到:jobs@deeper.network, 我们在这里等你！</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-05</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=f480ea38-5b5e-423e-9c78-e78bd1344c6e">
<div class="article-summary-box-inner">
<span><h3>rust-tui-template：使用 tui-rs 和 crossterm 引导 Rust TUI 应用程序的模板</h3>
<p>项目结构如下：</p>
<pre><code>src/
├── app.rs     -&gt; holds the states and renders the widgets
├── event.rs   -&gt; handles the terminal events (key press, mouse click, resize, etc.)
├── handler.rs -&gt; handles the key press events and updates the application
├── lib.rs     -&gt; module definitions
├── main.rs    -&gt; entry-point
└── tui.rs     -&gt; initializes/exits the terminal interface
</code></pre>
<p>按 README 下载执行后效果如下：</p>
<p><img src="http://qnimg.lovevivian.cn/rust-daily-20210905-1.jpg" alt></p>
<p>GitHub：<a href="https://github.com/orhun/rust-tui-template" rel="noopener noreferrer">orhun/rust-tui-template: A template for bootstrapping a Rust TUI application with tui-rs &amp; crossterm</a></p>
<h3>perseus：完全支持 SSR 和 SSG 的 Rust 高端前端开发框架</h3>
<p>Perseus 是一个使用 Rust 构建的极快的前端 Web 开发框架，它支持主要的渲染策略、在没有虚拟 DOM 的情况下具有反应性，并且具有极高的可定制性。它封装了 Sycamore 的底层功能，提供了一个类似 NextJS 的 API！</p>
<p>✨ 支持静态生成（只提供静态资源）
✨ 支持服务端渲染（服务动态资源）
✨ 支持一段时间后重新验证和 / 或使用自定义逻辑（更新已渲染页面）
✨ 支持增量重建（按需构建）
✨开放构建矩阵（主要使用任何渲染策略和其他任何东西）
✨ CLI 工具，让您轻松自信地构建应用程序</p>
<p>项目的主要目标是：支持每一个主要的渲染策略，并为开发人员提供使用 Rust 高效创建超快速应用程序的能力和炫酷的的开发体验！</p>
<p>文档：<a href="https://arctic-hen7.github.io/perseus/" rel="noopener noreferrer">Introduction - Perseus Book</a></p>
<p>GitHub：<a href="https://github.com/arctic-hen7/perseus" rel="noopener noreferrer">arctic-hen7/perseus: A high-level frontend development framework for Rust with full support for SSR and SSG.</a></p>
<h3>Rust 构建 LC-3 虚拟机</h3>
<p>Little Computer 3，或 LC-3，是一种计算机教育编程语言，一种汇编语言。它具有相对简单的指令集，但可用于编写中等复杂的汇编程序，是 C 编译器的可行目标。 该语言不如 x86 汇编语言复杂，但具有许多类似于更复杂语言的功能。 这些功能使其对入门教学非常有用，因此它最常用于向计算机科学和计算机工程专业的学生教授编程和计算机体系结构的基础知识。</p>
<p>教程地址：<a href="https://www.rodrigoaraujo.me/posts/lets-build-an-lc-3-virtual-machine/" rel="noopener noreferrer">Let's build an LC-3 Virtual Machine :: Rodrigo Araujo — Computer Scientist and Software Engineer</a></p>
<p>另外附上 2 个之前的一个教程：</p>
<ul>
<li><a href="https://github.com/KuldeepSinh/lc3_vm" rel="noopener noreferrer">KuldeepSinh/lc3_vm: LC-3 (Little Computer 3) VM implemented in Rust</a></li>
<li><a href="https://github.com/justinmeiners/lc3-vm" rel="noopener noreferrer">justinmeiners/lc3-vm: Write your own virtual machine for the LC-3 computer!</a></li>
</ul>
<h3>RustGameJam 中使用的游戏引擎分布</h3>
<p>GameJam 是一个游戏开发者的 hackathon，<a href="https://itch.io/jam/rusty-jam" rel="noopener noreferrer">第一届 Rust Game Jam</a> 是于2021年8月22号到8月29号举办，游戏开发者们使用的游戏引擎最多的是 Bevy，其次是 macroquad，当然还有其他引擎，比如：pixels、 RG3D、minifb。想看GameJam的游戏作品，请点击下面链接。</p>
<ul>
<li>https://itch.io/jam/rusty-jam</li>
</ul>
<h3>memuse 一个分析动态内存使用的库</h3>
<pre><code>use memuse::DynamicUsage;

assert_eq!(7u64.dynamic_usage(), 0);
assert_eq("I'm simple!".dynamic_usage(), 0);
assert_eq(vec![7u64; 2].dynamic_usage(), 16);

let empty: Vec&lt;u32&gt; = Vec::with_capacity(100);
assert_eq!(empty.len(), 0);
assert_eq!(empty.dynamic_usage, 400);
</code></pre>
<ul>
<li>Repo <a href="https://crates.io/crates/memuse" rel="noopener noreferrer">crates.io/crates/memuse</a></li>
</ul>
<hr>
<p>From 日报小组 太子长琴，李冬杰</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rust.cc/" rel="noopener noreferrer">Rustcc 论坛: 支持 rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f620" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust ffi link 错误</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=6497a814-b6d9-4b32-9d9d-35cc2fe53845">
<div class="article-summary-box-inner">
<span><p>Clionerror: linking with <code>link.exe</code> failed: exit code: 1112
|
= note: "C:\Program Files (x86)\Microsoft Visual Studio\2019\Community\VC
\Tools\MSVC\14.25.28610\bin\HostX64\x64\link.exe" "/NOLOGO" "/NXCOMPAT" "
/LIBPATH:C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc-windows
-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib" "D:\rust\test\test11\targe
t\debug\deps\test11.1inhodm0mguvy78p.rcgu.o" "D:\rust\test\test11\target<br>
\debug\deps\test11.1itxz1f879gxqo0r.rcgu.o" "D:\rust\test\test11\target\d
ebug\deps\test11.1nfdhbezxkijr69v.rcgu.o" "D:\rust\test\test11\target\deb
ug\deps\test11.1okn7si93kck3d1v.rcgu.o" "D:\rust\test\test11\target\debug
\deps\test11.1r7eavti91oymq20.rcgu.o" "D:\rust\test\test11\target\debug\
deps\test11.24nm5lpcn3t54lyd.rcgu.o" "D:\rust\test\test11\target\debug\de
ps\test11.2hyws9epzawmgilt.rcgu.o" "D:\rust\test\test11\target\debug\deps
\test11.2old9hh0jdnzckpx.rcgu.o" "D:\rust\test\test11\target\debug\deps\
test11.3bc649uegqq66vdn.rcgu.o" "D:\rust\test\test11\target\debug\deps\te
st11.3nyjusu4bu7ihnwt.rcgu.o" "D:\rust\test\test11\target\debug\deps\test
11.44jxfqx2w1so8gxa.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11
.47a9u7imyimutge1.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.4
9azsbqjuetnb404.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.4bk
fakiuuxmwqfyp.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.4mu1b
nojyqp0utjl.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.4rszv42
zy3ba1jb2.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.4t77rmx77
tcvwir6.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.4tr7ij9mclu
clpsg.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.4xi12smsmm3qs
mbh.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.4yye09rafbxycx8
1.rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.56g0yr2v0k1zblp9.
rcgu.o" "D:\rust\test\test11\target\debug\deps\test11.yv2gzy9b0ix4k8o.rcg
u.o" "D:\rust\test\test11\target\debug\deps\test11.zu1yaj42bq3ogyk.rcgu.o
" "/OUT:D:\rust\test\test11\target\debug\deps\test11.exe" "D:\rust\test
\test11\target\debug\deps\test11.sic3ibelt8jd29e.rcgu.o" "/OPT:REF,NOICF" "
/DEBUG" "/NATVIS:C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc
-windows-msvc\lib\rustlib\etc\intrinsic.natvis" "/NATVIS:C:\Users\Administ
rator\.rustup\toolchains\stable-x86_64-pc-windows-msvc\lib\rustlib\etc\li
balloc.natvis" "/NATVIS:C:\Users\Administrator\.rustup\toolchains\stable-x8
6_64-pc-windows-msvc\lib\rustlib\etc\libcore.natvis" "/NATVIS:C:\Users\Adm
inistrator\.rustup\toolchains\stable-x86_64-pc-windows-msvc\lib\rustlib\et
c\libstd.natvis" "/LIBPATH:D:\rust\test\test11\target\debug\deps" "/LIBPA
TH:C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc-windows-msvc<br>
\lib\rustlib\x86_64-pc-windows-msvc\lib" "./libs/nav-apps.lib" "/WHOLEARCHIVE
:./libs/nav-apps.lib" "D:\rust\test\test11\target\debug\deps\liblibc-bc05
adbb061c4c16.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64
-pc-windows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\libstd-1feb4ba9912f
83e4.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc-wind
ows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\libpanic_unwind-10caf631bf1
7818d.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc-win
dows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\librustc_demangle-5f5b841e
7dcb5069.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc-
windows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\libhashbrown-886e420424
40a542.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc-wi
ndows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\librustc_std_workspace_al
loc-fc3dfd2deda68757.rlib" "C:\Users\Administrator\.rustup\toolchains\stabl
e-x86_64-pc-windows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\libunwind-4
765baa3d9fc6a1b.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86
_64-pc-windows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\libcfg_if-2af04b
7075550e2b.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64-p
c-windows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\liblibc-9f4eae3434a19
b51.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc-windo
ws-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\liballoc-14b08c3097e998dc.rl
ib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc-windows-msv
c\lib\rustlib\x86_64-pc-windows-msvc\lib\librustc_std_workspace_core-9c0450
bb353ef0cc.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64-p
c-windows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\libcore-4856f32e5e48b
ded.rlib" "C:\Users\Administrator\.rustup\toolchains\stable-x86_64-pc-windo
ws-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\libcompiler_builtins-0f66c8d
6b2ebbbc4.rlib" "advapi32.lib" "ws2_32.lib" "userenv.lib" "msvcrt.lib"
= note: Non-UTF-8 output: nav-apps.lib(base64.obj) : \xd5\xd2\xb5\xbd MSIL .ne
tmodule \xbb\xf2\xca\xb9\xd3\xc3 /GL \xb1\xe0\xd2\xeb\xb5\xc4\xc4\xa3\xbf\xe9\xa
3\xbb\xd5\xfd\xd4\xda\xca\xb9\xd3\xc3 /LTCG \xd6\xd8\xd0\xc2\xc6\xf4\xb6\xaf\xc1
\xb4\xbd\xd3\xa3\xbb\xbd\xab /LTCG \xcc\xed\xbc\xd3\xb5\xbd\xc1\xb4\xbd\xd3\xc3<br>
xfc\xc1\xee\xd0\xd0\xd2\xd4\xb8\xc4\xbd\xf8\xc1\xb4\xbd\xd3\xc6\xf7\xd0\xd4\xc4<br>
xdc\r\ntest11.4mu1bnojyqp0utjl.rcgu.o : error LNK2005: main \xd2\xd1\xbe\xad\xd4
\xda test11.4mu1bnojyqp0utjl.rcgu.o \xd6\xd0\xb6\xa8\xd2\xe5\r\nnav-apps.lib(lib
mysql32.dll) : fatal error LNK1112: \xc4\xa3\xbf\xe9\xbc\xc6\xcb\xe3\xbb\xfa\xc0
\xe0\xd0\xcd\xa1\xb0x86\xa1\xb1\xd3\xeb\xc4\xbf\xb1\xea\xbc\xc6\xcb\xe3\xbb\xfa<br>
xc0\xe0\xd0\xcd\xa1\xb0x64\xa1\xb1\xb3\xe5\xcd\xbb\r\n</p>
<p>求大佬给点提示</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">分享一个命令行文本美化工具库，https://crates.io/crates/colorstyle</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=73d264ed-5f2d-43db-b3ec-12965ccaa550">
<div class="article-summary-box-inner">
<span><p>ColorStyle is a library of styles for command-line text.
Used to modify the style of text for standard output to the terminal interface, you can change the foreground colour of the text, the background colour, add underline and bold, etc.</p>
<p>ColorStyle 是一个用于命令行文本的样式库。
用于标准输出到终端界面的文本的样式修改，可以修改文本前景色，背景色，增加下划线和加粗显等。</p>
<p>可以作为一个新手docs.rs 文档编写参考。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">[已解决]使用rocket框架时sqlite出现的问题</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=089f8f06-780e-409a-952e-fa39c2e79dc6">
<div class="article-summary-box-inner">
<span><p>其实应该是diesel的问题
之前sqlite缺少lib我是通过这个博客解决了问题
<a href="https://blog.itdevwu.com/post/915/" rel="noopener noreferrer">解决使用Rust与Sqlite3交互时出现LNK1181错误（Diesel 或 rusqlite）</a>
但是后面的cargo run 阶段又出现了</p>
<pre><code>sqlite3.lib : warning LNK4272:库计算机类型“x86”与目标计算机类型“x64”冲突
          D:\Project\Private\point_plan\target\debug\deps\point_plan.exe : fatal error LNK1120: 60 个无法解析的外部命令
</code></pre>
<p>我明明用的是64位指令编译64位sqlite3.def得到lib的，为啥还会出现这种问题？
麻烦弄过的这方面的朋友指点下</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-04</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=e3c58130-184d-482b-b081-168c54694384">
<div class="article-summary-box-inner">
<span><h3>cURL 中的 Rust</h3>
<p>Allen Wyma 与 cURL 的原作者 Daniel 谈论在 cURL 中使用 Rust。</p>
<ul>
<li>cURL 是一个命令行工具和库，用于通过 URL 传输数据。</li>
<li>cURL 及其数据传输核心 libcurl 都是用 C 编写的，众所周知，这不是内存安全的。</li>
<li>虽然几乎不可能将其重写为另一种语言，但提供一个用 Rust 编写的第三方库可能会更进一步。</li>
</ul>
<p><a href="https://rustacean-station.org/episode/035-daniel-stenberg/" rel="noopener noreferrer">文章链接</a>，https://rustacean-station.org/episode/035-daniel-stenberg/</p>
<h3>NoProto：灵活、快速和紧凑的序列化和rpc</h3>
<ul>
<li>
<p>轻量</p>
<ul>
<li>零依赖</li>
<li>支持no_std，WASM</li>
<li>最紧凑的非编译存储格式</li>
</ul>
</li>
<li>
<p>稳定...</p>
</li>
</ul>
<p><a href="https://github.com/only-cliches/NoProto" rel="noopener noreferrer">Gitlab 链接</a>，https://github.com/only-cliches/NoProto</p>
<h3>gradient介绍</h3>
<p>用于玩颜色渐变的命令行工具</p>
<p>Features:</p>
<ul>
<li>许多预设渐变。</li>
<li>自定义渐变。</li>
<li>从 SVG 和 GIMP 渐变 (ggr) 文件中读取渐变
...</li>
</ul>
<p><a href="https://github.com/mazznoer/gradient-rs" rel="noopener noreferrer">Gitlab 链接</a>，https://github.com/mazznoer/gradient-rs</p>
<hr>
<p>From 日报小组 <a href="https://rustcc.cn/blog_with_author?author_id=dd4a77ca-2042-459e-901a-b8f9bfeb7db0" rel="noopener noreferrer">TOM</a></p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li>[微信公众号：Rust语言中文社区](https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d88</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">argh：基于 derive 宏且对二进制体积进行优化的命令行解析工具</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=01a3db1f-b567-47d4-9c3b-08142a11cd3e">
<div class="article-summary-box-inner">
<span><blockquote>
<p>Derive-based argument parsing optimized for code size and conformance to the Fuchsia commandline tools specification.</p>
<p>基于 derive 宏的参数解析工具，针对代码大小进行了优化，并且遵循 Fuchsia 命令行工具规范。</p>
</blockquote>
<p>repo：<a href="https://github.com/google/argh" rel="noopener noreferrer">https://github.com/google/argh</a></p>
<p>由 Google 开发者编写，但并非 Google 官方支持。</p>
<p>官方给的基本例子：</p>
<pre><code>use argh::FromArgs;

#[derive(FromArgs)]
/// Reach new heights.
struct GoUp {
    /// whether or not to jump
    #[argh(switch, short = 'j')]
    jump: bool,

    /// how high to go
    #[argh(option)]
    height: usize,

    /// an optional nickname for the pilot
    #[argh(option)]
    pilot_nickname: Option&lt;String&gt;,
}

fn main() {
    let up: GoUp = argh::from_env();
}
</code></pre>
<pre><code>Usage: cmdname [-j] --height &lt;height&gt; [--pilot-nickname &lt;pilot-nickname&gt;]

Reach new heights.

Options:
  -j, --jump        whether or not to jump
  --height          how high to go
  --pilot-nickname  an optional nickname for the pilot
  --help            display usage information
</code></pre>
<p>过程宏-参数类型：</p>
<ul>
<li><code>switch</code>：用在 bool 类型的字段上，表明命令行参数是可选的，而且一旦提供该命令行参数，则给该字段的值赋给 true 。</li>
<li><code>option</code>：
<ul>
<li>用在 <code>Option</code> 类型上，表明命令行参数是可选的。</li>
<li>用在 <code>Vec</code> 类型上，表明命令行参数可选，而且可以重复出现，即这个参数及其值可以在命令行中出现 0 次或更多次。</li>
<li>用在非 <code>Option</code> 、非 <code>Vec</code> 类型上，则表示命令行参数必选。</li>
</ul>
</li>
<li><code>positional</code>：位置参数，表明按照结构体声明的字段顺序解析命令行参数，无需 <code>--xx value</code> 的 <code>--xx</code> 。最后一个位置参数可以包含默认值，也可以包装在 Option 或 Vec 中来接收可选（指 0 或 1 个）或重复（指 0 或多个）的位置参数。</li>
<li><code>subcommand</code>：需定义一个顶层结构体、一个表示子命令的枚举体（这个枚举体列举所有子命令，子命令以结构体形式呈现，子命令结构体还需要 name 设置名称）</li>
</ul>
<p>过程宏-其他设置：</p>
<ul>
<li><code>short = 'a'</code>：解析 <code>-a</code> 形式的简短参数，只支持 ascii 的 <code>Char</code> 类型，比如大小写、数字。</li>
<li><code>long = "xx-xx"</code>：重新命名这个字段的参数名称，由此可允许参数名称带连字符 <code>--xx-xx</code>。这个设置的默认值为字段名称，只支持 ascii 小写形式的名称，不支持大写和数字。</li>
<li><code>default = "default_height()")</code>、<code>default = "String::from(\"only up\")")</code>：默认值，引号内可以是函数名（带括号）、表达式</li>
<li><code>from_str_fn(always_five)</code>：针对某个解析的参数进行自定义处理，<code>always_five</code> 的函数签名方式为 <code>fn(&amp;str) -&gt; Result&lt;T, String&gt;</code></li>
<li><code>description = "xxxxx"</code>：给参数添加帮助信息。<code>///</code> 文档注释也可以提供用帮助信息，而 <code>description</code> 的内容在命令行帮助信息里会覆盖掉 <code>///</code> 提供的信息。注意：换行和空换行会在 --help 信息里变成一个空格；描述信息不能过长，否则会出现 <code>error: invalid reference to positional arguments 4 and 5 (there is 1 argument</code> （这个报错信息不准确，我也是排查了很久才发现）。</li>
</ul>
<p>trait：</p>
<ul>
<li><code>FromArgs</code> trait：用于 argh 命令行解析的所有结构体和枚举体，都必须 derive 这个 trait 。</li>
<li><code>FromArgValue</code> trait：用于 argh 命令行解析的结构体字段的类型必须实现这个 trait ，argh 已经给所有实现 <code>FromStr</code> trait 的类型实现了这个 trait 。std 的基础类型都实现了 <code>FromStr</code> trait ，所以可以直接使用 std 的基础类型；自定义类型需要实现 <code>FromStr</code> trait 和 <code>FromArgValue</code> trait 。</li>
</ul>
<p>优点：</p>
<ul>
<li>使用简单而直观，上手快，适用于基础的命令行解析场景</li>
<li>生成的体积比 clap 小</li>
<li>依赖少，编译速度快</li>
<li>支持 unicode</li>
</ul>
<p>缺点：</p>
<ul>
<li>终端输出结果非彩色</li>
<li>默认不支持很长的 help 信息；只支持 <code>--help</code> 不支持 <code>-h</code> （但是也带来优点——可以自定义一个字段，short as <code>-h</code>，从而有一份默认简洁的 help info，又有一份完全自定义的 info，比如 <code>#[argh(option, short = 'h')] description: Vec&lt;String&gt;</code> =&gt; <code>cmd -h arg1 arg2</code> 就可以显示 arg1 和 arg2 的说明）</li>
<li>只支持 <code>--option value</code> 和 <code>-o value</code>，不支持 <code>--option=value</code> 和 <code>-ovalue</code></li>
</ul>
<p>其他 args-parser：</p>
<blockquote>
<ul>
<li><a href="https://github.com/blyxxyz/lexopt" rel="noopener noreferrer">lexopt</a>：零依赖、注重正确性的极简 args-parser 。</li>
<li><a href="https://github.com/clap-rs/clap" rel="noopener noreferrer"><code>clap</code></a>/<a href="https://github.com/TeXitoi/structopt" rel="noopener noreferrer"><code>structopt</code></a>: very fully-featured. The only other argument parser for Rust I know of that truly handles invalid unicode properly, if used right. Large.</li>
<li><a href="https://github.com/google/argh" rel="noopener noreferrer"><code>argh</code></a> and <a href="https://github.com/murarth/gumdrop" rel="noopener noreferrer"><code>gumdrop</code></a>: much leaner, yet still convenient and powerful enough for most purposes. Panic on invalid unicode.
<ul>
<li><code>argh</code> adheres to the <a href="https://fuchsia.dev/fuchsia-src/concepts/api/cli#command_line_arguments" rel="noopener noreferrer">Fuchsia specification</a> and therefore does <em>not</em> support <code>--option=value</code> and <code>-ovalue</code>, only <code>--option value</code> and <code>-o value</code>.</li>
</ul>
</li>
<li><a href="https://github.com/RazrFalcon/pico-args" rel="noopener noreferrer"><code>pico-args</code></a>: slightly smaller than lexopt and easier to use (but less rigorous).</li>
<li><a href="https://docs.rs/ap" rel="noopener noreferrer"><code>ap</code></a>: I have not used this, but it seems to support iterative parsing while being less bare-bones than lexopt.</li>
<li>libc's <a href="https://en.wikipedia.org/wiki/Getopt#Examples" rel="noopener noreferrer"><code>getopt</code></a>.</li>
</ul>
<p>src: <a href="https://github.com/blyxxyz/lexopt#see-also" rel="noopener noreferrer">https://github.com/blyxxyz/lexopt#see-also</a></p>
</blockquote>
<p>P.S. 不得不说，Rust 利用抽象的类型系统和宏，在 args-parser 方面太棒了。写 Rust 是一种享受。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">爱死你了，阿克苏姆</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=628500dc-3232-40ff-98f8-03540eaa5d12">
<div class="article-summary-box-inner">
<span><p>最近花了点时间来学习axum, 并成功将一个用warp写的项目改用axum重写。axum太棒了，充分体现了rust这门语言的表达能力。</p>
<ol>
<li>路由设计非常简洁，演示了Rust不用宏，也可以搞DSL的方法。</li>
<li>Extractor与AddExtension极为灵活，简化了warp通过构建参数获取Request与环境数据的设计。</li>
<li>借用Tower生态提高了代码利用率。</li>
</ol>
<p>axum非常稳定，压力测试中同时开15K并发妥妥的。在axum面世之前，warp是最棒的web框架，现在该是阿克苏姆担当主角了。由于两者都是基于hyper平台，从warp移植到axum也是分分钟的事。
下面贴出实战项目中两段代码main.rs与servce.rs。main.rs中演示了如何通过命令行参数切换，实现http与https两种服务，还演示了如何调用了静态文件服务功能。service.rs是放api的地方，演示了如何处理get与post请求，如何获取数据库中的数据，如何提供动态下载内容等功能。</p>
<pre><code>//main.rs
mod addr;
mod base16;
mod bb8_tiberius;
mod ccb_gwk;
mod ccb_socket;
mod config;
mod context;
mod database;
mod json_helper;
mod json_value;
mod parse_exp;
mod parse_param;
mod service;
mod service_da;

use axum::{http::StatusCode, Router};
use tower_http::services::ServeDir;

use std::env::args;

use chrono::prelude::*;
use context::AppContext;
use json_helper::JsonHelper;
use json_value::JsonValue;

const VERSION: &amp;str = "1.3.0";

#[tokio::main]
async fn main() {
    pretty_env_logger::init_timed();

    let is_https = args().nth(1).unwrap_or("http".into()) == "https";

    let context = AppContext::new().await;
    let ctx = context.clone();
    let config = &amp;ctx.config;
    let server_config = &amp;config["config"];
    let ctx = context.clone();
    let app = Router::new()
        .nest(
            "/",
            axum::service::get(ServeDir::new("D:/Js/OnlyOne/public")).handle_error(
                |error: std::io::Error| {
                    Ok::&lt;_, std::convert::Infallible&gt;((
                        StatusCode::INTERNAL_SERVER_ERROR,
                        format!("Unhandled internal error: {}", error),
                    ))
                },
            ),
        )
        .nest("/api", service::api(ctx));

    let addr = addr::Addr::new(server_config, is_https);
    let now = Local::now().to_string();
    let now = &amp;now[0..19];
    println!(
        "{} HTTP{} Server V{} is starting at {:19}, {}",
        server_config["server_name"].string("W3"),
        if is_https { "S" } else { "" },
        VERSION,
        now,
        addr
    );

    let addr = addr.to_string_full();

    if is_https {
        axum_server::bind_rustls(addr)
            .private_key_file("key.pem")
            .certificate_file("cert.pem")
            .serve(app)
            .await
            .unwrap();
    } else {
        axum_server::bind(addr).serve(app).await.unwrap();
    }
}
</code></pre>
<pre><code>//service.rs
use crate::base16;
use crate::ccb_gwk;
use crate::database;
use crate::parse_param;
use crate::service_da::{da_read_about, da_write_about, download_photos, DA_WEBP_DISABLE};
use crate::AppContext;
use crate::JsonHelper;
use crate::JsonValue;
use anyhow::{anyhow, Result};
use encoding::{all::GB18030, EncoderTrap, Encoding};
use serde_json::{json, Value};
use std::collections::HashMap;
use std::sync::Arc;
use tiberius::ToSql;
use tracing::info;

use axum::{
    extract::{Extension, Form, Query},
    response::Json,
    handler::{get, post},
    http::header::{HeaderMap, HeaderName, HeaderValue},
    routing::BoxRoute,
    AddExtensionLayer, Router,
};

fn string_to_gb18030bytes(string: &amp;str) -&gt; Result&lt;Vec&lt;u8&gt;&gt; {
    GB18030
        .encode(string, EncoderTrap::Strict)
        .map_err(|e| anyhow!("string_to_gb18030bytes failure: {:?}", e))
}

pub(crate) fn api(ctx: Arc&lt;AppContext&gt;) -&gt; Router&lt;BoxRoute&gt; {
    Router::new()
        .route("/ask", get(ask))
        .route("/act", post(act))
        .layer(AddExtensionLayer::new(ctx))
        .boxed()
}


pub(crate) async fn ask(
    Query(qs): Query&lt;HashMap&lt;String, String&gt;&gt;,
    Extension(context): Extension&lt;Arc&lt;AppContext&gt;&gt;,
) -&gt; (HeaderMap, Vec&lt;u8&gt;) {
    let mut headers = HeaderMap::new();
    //let qs=format!("{:?}",qs);
    //let bytes=Vec::from(json!({"ask":qs}).to_string());

    let empty = String::from("");
    let dbs = context.dbs.clone();
    let ask = qs.get("ask").unwrap_or(&amp;empty).clone();
    let params = base16::base16_decode(qs.get("params").unwrap_or(&amp;empty)).unwrap();
    let params: Value = serde_json::from_str(&amp;params).unwrap();
    info!("ask={} params={}", ask, params);
    //let content_type = "application/json";
    let p1: JsonValue;
    let p2: JsonValue;
    let p3: JsonValue;
    let p4: JsonValue;
    let p5: JsonValue;
    let pof = |id| JsonValue::of(&amp;params[id]);
    let static_path = context.config["config"]["static_path"].string("wwwroot");
    let da_webp_active = context.config["config"]["da_webp_active"].bool(false);
    let accept_webp = params["acceptWebp"].bool(false);
    let da_webp_quality = if da_webp_active &amp;&amp; accept_webp {
        context.config["config"]["da_webp_quality"].i64(20) as i8
    } else {
        DA_WEBP_DISABLE
    };
    let mut sql: String = "".into();
    let mut sql_params: Vec&lt;&amp;dyn ToSql&gt; = Vec::new();
    let mut pending = true;
    let mut result: String = "null".into();
    let mut about_file: String = "".into();
    let mut voucher_id: &amp;str = &amp;empty;
    let mut attach: String = "".into();

    if ask == "@login" {
        sql = r"EXEC TM_OnlyOneLogin @P1,@P2".into();
        p1 = pof("userId");
        p2 = pof("password");
        sql_params = vec![&amp;p1, &amp;p2];
    } else if ask == "workload" {
        sql = "EXEC TM_WorkLoad @P1,@P2,@P3".into();
        p1 = pof("userName");
        p2 = pof("year");
        let more_where = if params["limitMonth"].bool(false) {
            let month_from = params["monthFrom"].i64(1);
            let month_to = params["monthTo"].i64(13);
            let month_to = if month_to &lt; month_from {
                month_from
            } else {
                month_to
            };
            format!(
                " AND z.kjqj BETWEEN '{:02}' AND '{:02}'",
                month_from, month_to
            )
        } else {
            "".to_string()
        };
        //println!("moreWhere:{}",more_where);
        p3 = JsonValue::new(json!(more_where));
        sql_params = vec![&amp;p1, &amp;p2, &amp;p3];
    } else if ask == "wujinFH" {
        sql = "EXEC dbo.TM_UpdateOracleWSZZ4WujinFH @P1,@P2".into();
        p1 = pof("p1");
        p2 = pof("p2");
        sql_params = vec![&amp;p1, &amp;p2];
    } else if ask == "salaryVoucher" {
        sql = "EXEC dbo.TM_MakeSalaryVoucher @P1,@P2".into();
        p1 = pof("period");
        p2 = pof("personType");
        sql_params = vec![&amp;p1, &amp;p2];
    } else if ask == "salaryVoucherBank" {
        sql = "EXEC dbo.TM_GetSalaryBankDetail @P1,@P2".into();
        p1 = pof("period");
        p2 = pof("personType");
        sql_params = vec![&amp;p1, &amp;p2];
    } else if ask == "salaryVoucherSheet" {
        sql = "EXEC dbo.TM_GetSalaryVoucher @P1".into();
        p1 = pof("batch");
        sql_params = vec![&amp;p1];
    } else if ask == "checkncye" {
        sql = "EXEC dbo.TM_CheckNCYE @P1,@P2".into();
        p1 = pof("year");
        p2 = pof("tblname");
        sql_params = vec![&amp;p1, &amp;p2];
    } else if ask == "py2code" {
        sql = "EXEC dbo.TM_PY2Code @P1,@P2".into();
        p1 = pof("type");
        p2 = pof("code");
        sql_params = vec![&amp;p1, &amp;p2];
    } else if ask == "@aboutvoucher" {
        sql = "EXEC dbo.TM_AboutVoucher @P1".into();
        p1 = pof("pznm");
        sql_params = vec![&amp;p1];
    } else if ask == "@voucherphotos" {
        voucher_id = params["voucherId"].str("");
        match da_read_about(voucher_id, &amp;static_path, da_webp_quality).await {
            Ok((about_file_exists, read_result, about_file_name)) =&gt; {
                about_file = about_file_name;
                if about_file_exists {
                    result = format!("{{\"msg\":\"ok\", \"data\":{}}}", read_result);
                    pending = false;
                }
            }
            Err(e) =&gt; {
                result = format!("{{\"msg\":\"{:?}\"}}", e);
                pending = false;
            }
        }
        if pending {
            sql = "EXEC dbo.TM_VoucherPhotos @P1".into();
            p1 = pof("voucherId");
            sql_params = vec![&amp;p1];
        }
    } else if ask == "@aboutreceipt" {
        sql = "EXEC dbo.TM_AboutReceipt @P1,@P2,@P3,@P4,@P5".into();
        p1 = pof("id");
        p2 = pof("checkSum");
        p3 = pof("datePaid");
        p4 = pof("amount");
        p5 = pof("checker");
        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4, &amp;p5];
    } else if ask == "payee" {
        sql = "EXEC dbo.TM_QueryPayee @P1,@P2".into();
        p1 = pof("bankName");
        p2 = pof("bankAcct");
        sql_params = vec![&amp;p1, &amp;p2];
    } else if ask == "ledger" || ask == "voucher" {
        // CREATE PROCEDURE dbo.TM_QueryLedgerExt
        // @起始年 INT,@终止年 INT,@查询条件 VARCHAR(4096),@排序 VARCHAR(80)='日期,凭证号,笔号',
        // @借贷对冲 BIT=0,@隐藏负值 BIT=0,@Select VARCHAR(250)='*'
        let params = parse_param::params_convert(&amp;context.config, &amp;ask, &amp;params);
        let pof = |id| JsonValue::of(&amp;params[id]);
        let only_sum_line = if ask == "ledger" { ",1" } else { ",0" };
        sql = "EXEC dbo.TM_QueryLedgerExt @P1,@P2,@P3,@P4,0,0,@P5".to_string() + only_sum_line;
        p1 = pof("yearFrom");
        p2 = pof("yearTo");
        p3 = pof("filter");
        p4 = pof("orderby");
        p5 = pof("select");
        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4, &amp;p5];
    } else if ask == "balance" {
        // CREATE PROCEDURE dbo.TM_QueryBalanceExt
        // @起始年 INT,@终止年 INT,@查询条件 VARCHAR(4096),@期初条件 VARCHAR(4096)=NULL,
        // @年初条件 VARCHAR(4096)=NULL,@余额条件 VARCHAR(250)=NULL,
        // @顶层 VARCHAR(10)='科目1级',@底层 VARCHAR(10)='科目4级',
        // @合并 INT=NULL,@合计 BIT=0,@仅底层 BIT=0,@仅编码 BIT=0,@倍率 INT=1,@查项目余额 BIT=0
        let params = parse_param::params_convert(&amp;context.config, &amp;ask, &amp;params);
        let pof = |id| JsonValue::of(&amp;params[id]);
        sql = format!(
            "EXEC dbo.TM_QueryBalanceExt @P1,@P2,@P3,@P4,@P5{}",
            params["params_in_sql"].str("")
        );
        p1 = pof("yearFrom");
        p2 = pof("yearTo");
        p3 = pof("filter");
        p4 = pof("filter_qc");
        p5 = pof("filter_nc");
        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4, &amp;p5];
    }
    if pending {
        let result_json = if !sql.is_empty() {
            let row_is_obj = ask.starts_with('@');
            let result = database::query(dbs, &amp;sql, &amp;sql_params, row_is_obj).await;
            match result {
                Ok(result) =&gt; {
                    if ask == "ledger" {
                        let params =
                            parse_param::params_convert(&amp;context.config, "voucher", &amp;params);
                        json!({ "msg":"ok","voucherColDefs":params["select"], "data":result})
                    } else if ask == "@voucherphotos" {
                        let row_count = result["rowCount"].u64(0);
                        if row_count &gt; 0 {
                            let result = download_photos(context, result, da_webp_quality).await;
                            match da_write_about(&amp;about_file, &amp;result).await {
                                Ok(_) =&gt; json!({ "msg":"ok", "data":result}),
                                Err(e) =&gt; json!({ "msg": format!("{:?}", e) }),
                            }
                        } else {
                            json!({
                                "msg": format!("没有找到凭证{}的影像资料", voucher_id)
                            })
                        }
                    } else if ask == "salaryVoucherSheet" {
                        let empty_vec: Vec&lt;Value&gt; = Vec::new();
                        let sheet = result["rows"].as_array().unwrap_or(&amp;empty_vec);
                        let sheet = sheet
                            .iter()
                            .map(|x| x.get(0).unwrap_or(&amp;Value::Null).string(""))
                            .fold("".to_string(), |lines, line| lines + &amp;line + "\r\n");
                        attach = sheet;
                        json!("attachment")
                    } else {
                        json!({ "msg":"ok", "data":result})
                    }
                }
                Err(err) =&gt; {
                    json!({ "msg": format!("{:?}", err) })
                }
            }
        } else if ask == "checkgwk" {
            let check_all = params["checkAll"].bool(false);
            ccb_gwk::check_gwk(&amp;context.config, check_all)
                .await
                .unwrap()
        } else {
            json!({ "msg": format!("unknown ask {} params:{}", ask, params.to_string()) })
        };
        result = format!("{}", result_json);
    }
    if ask == "salaryVoucherSheet" {
        let file_name = params["fileName"].str("凭证");
        let value = format!("attachment;filename={}.txt", file_name);
        let bytes: Vec&lt;u8&gt; = string_to_gb18030bytes(&amp;attach).unwrap_or_default();
        //reply::with_header(bytes, "Content-disposition", value)
        headers.insert(
            HeaderName::from_static("content-type"),
            HeaderValue::from_static("text/plain"),
        );
        headers.insert(
            HeaderName::from_static("content-disposition"),
            HeaderValue::from_str(&amp;value).unwrap(),
        );
        (headers, bytes)
    } else {
        let bytes: Vec&lt;u8&gt; = result.into_bytes();
        //reply::with_header(bytes, "content-type", content_type)
        headers.insert(
            HeaderName::from_static("content-type"),
            HeaderValue::from_static("application/json"),
        );
        (headers, bytes)
    }
}

async fn act(
    Form(qs): Form&lt;HashMap&lt;String, String&gt;&gt;,
    Extension(context): Extension&lt;Arc&lt;AppContext&gt;&gt;,
) -&gt; Json&lt;Value&gt; {
    let empty = String::from("");
    let dbs = context.dbs.clone();
    let act = qs.get("act").unwrap_or(&amp;empty).clone();
    let params = base16::base16_decode(qs.get("params").unwrap_or(&amp;empty)).unwrap();
    let params: Value = serde_json::from_str(&amp;params).unwrap_or(Value::Null);
    info!("act={} params={}", act, params);
    //let content_type = "application/json";
    let sql: String;
    let p1: JsonValue;
    let p2: JsonValue;
    let p3: JsonValue;
    let p4: JsonValue;
    let p5: JsonValue;
    let sql_params: Vec&lt;&amp;dyn ToSql&gt;;
    let pof = |id| JsonValue::of(&amp;params[id]);
    if act == "exam" {
        sql = r"EXEC dbo.TM_Exam @P1,@P2,@P3,@P4".into();
        p1 = pof("ids");
        p2 = pof("fhr");
        p3 = pof("fhrId");
        p4 = pof("isUndo");
        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4];
    } else if act == "changepayee" {
        sql = "EXEC dbo.TM_ChangePayee @P1,@P2,@P3,@P4,@P5".into();
        p1 = pof("bankName");
        p2 = pof("bankAcct");
        p3 = pof("unitCode");
        p4 = pof("updateDate");
        p5 = pof("mark");
        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4, &amp;p5];
    } else if act == "execsql" || act == "@execsql" {
        sql = params["sql"].string("");
        sql_params = Vec::new();
    } else {
        sql = "".into();
        sql_params = Vec::new();
    }
    let result = if !sql.is_empty() {
        let row_is_obj = act.starts_with('@');
        let result = database::query(dbs, &amp;sql, &amp;sql_params, row_is_obj).await;
        match result {
            Ok(result) =&gt; {
                json!({ "msg":"ok", "data":result})
            }
            Err(err) =&gt; {
                json!({ "msg": format!("{:?}", err) })
            }
        }
    } else {
        json!({ "msg": format!("unknown act {} params:{}", act, params.to_string()) })
    };
    Json(result)
}

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-03 Bebop v2.3.0：为 Bebop 序列化添加 Rust 支持</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=99b9fdf0-4026-4753-a07f-b7355caced95">
<div class="article-summary-box-inner">
<span><h4>Bebop v2.3.0：为 Bebop 序列化添加 Rust 支持</h4>
<p>Bebop 是一种基于模型的二进制序列化技术，类似于 Protocol Buffers 或 MessagePack。特别是，Bebop 试图非常适合需要比 JSON 或 MessagePack 更快、更简洁和类型安全的客户端-服务器或分布式 Web 应用程序。Matthew Conover 2021年8月30日宣布 Bebop 添加了 Rust 的支持</p>
<ul>
<li>https://rainway.com/blog/2021/08/30/bebop-rust/</li>
</ul>
<h4>将 TensorFlow 模型移植到 Rust 的开发成本</h4>
<p>通过 CrowdStrike 的可扩展性，可以立即将 TensorFlow 模型成功转换为纯 Rust 代码，文章介绍了通过这一方法的时间和精力成本</p>
<ul>
<li>https://www.crowdstrike.com/blog/development-cost-of-porting-tensorflow-models-to-pure-rust/</li>
</ul>
<h4>Rust 中的结构更新语法</h4>
<ul>
<li>https://www.reddit.com/r/rust/comments/pchp8h/media_struct_update_syntax_in_rust/</li>
</ul>
<hr>
<p>From 日报小组 北纬27度 侯盛鑫</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rust.cc 论坛: 支持 rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust 语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">bytes::ByteMut 无法写入数据</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=1ab18776-e162-4fbc-80b3-d23bb7402cf3">
<div class="article-summary-box-inner">
<span><p>想用 <a href="https://docs.rs/bytes/1.1.0/bytes/struct.BytesMut.html" rel="noopener noreferrer">ByteMut</a> 作缓冲区，从同步io数据源中读取数据，可是无法读取，也没报错。。。</p>
<pre><code>fn main() -&gt; io::Result&lt;()&gt; {
    let mut buf = BytesMut::with_capacity(10);

    let mut input: Cursor&lt;Vec&lt;u8&gt;&gt; = Cursor::new({
        (0..100).collect()
    });

    loop {
        match input.read(&amp;mut buf)? {
            0 =&gt; {
                println!("[READ OVER]");
                break;
            }
            n =&gt; {
                println!("{:?}", &amp;buf);
                println!("[READ ONCE]");
            }
        }
    }

    Ok(())
}
</code></pre>
<p>如果只用普通的数组，是可以读取数据</p>
<pre><code>let mut buf = [0; 10];
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>
<p><strong>课程时间:</strong> 2021年9月5日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识, Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>
<h3>课程大纲</h3>
<p>1、回顾 Rust 异步编程模型.</p>
<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>
<p>3、Tokio 介绍.</p>
<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>
<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/
Rust 异步编程入门 Future Part 1 回放地址：
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课：《 Rust 异步编程入门 Future 》|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>
<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是 Rust 异步编程的核心基础。</p>
<h3>课程大纲</h3>
<p>1、为什么需要异步.</p>
<p>2、理解异步编程模型.</p>
<p>3、Future 编程模型讲解.</p>
<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>
<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>异步引擎 C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D 游戏引擎</h3>
<ul>
<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>
<li><strong>延迟着色</strong></li>
<li><strong>内置保存/加载</strong></li>
<li><strong>独立场景编辑器</strong></li>
<li><strong>高级物理模型</strong></li>
<li><strong>分层模型资源</strong></li>
<li><strong>几何实例化</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>
<p><strong>课程时间：</strong> 2021年8月22日 20:30-21:30</p>
<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>
<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>
<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>
<h3>课程大纲</h3>
<ol>
<li>
<p>什么是分布式追踪系统OpenTracing及应用场景</p>
</li>
<li>
<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>
</li>
<li>
<p>为什么需要tokio-rs/tracing库</p>
</li>
<li>
<p>演示Datafuse项目中tokio-rs/tracing的使用</p>
</li>
</ol>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">论坛github账户无法登录解决笔记</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>有反映这两天github账户无法登录了。</p>
<p>报这个错：</p>
<pre><code>get github user info err
</code></pre>
<p>查了几个地方：</p>
<ol>
<li>代码是否运行正常：Ok</li>
<li>https代理是否正常：Ok</li>
<li>检查了github返回日志，发现是：</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>进入这个地址一看：<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>
<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>
<p>特此记录。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 的 Future 与 Javascript 的 Promise 功能对照参考</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>
<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>
<blockquote>
<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* 什么都不做 */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被 （1）跨线程传递 （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：《通过实战理解 Rust 宏》| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>
<p><strong>课程时间：</strong> 2021年8月15日 20:30-21:30</p>
<p><strong>课程介绍：</strong></p>
<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>这就是通过宏实现配置的统一行为，代码参考：
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>
<h3>课程大纲</h3>
<ul>
<li>什么是 Rust 宏</li>
<li>什么是宏运行原理</li>
<li>如何创建 Rust 宏过程</li>
<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>
</ul>
<p><strong>讲师介绍</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：理解Rust的所有权| Vol 2</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c107b830-9fe1-43dd-94a3-9efcd5544205">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：《理解Rust所有权》</strong></p>
<p><strong>课程时间：2021年8月8日 20:30-21:30</strong></p>
<p><strong>嘉宾讲师： 苏林</strong></p>
<p><strong>嘉宾介绍：</strong></p>
<p>Rust中文社区成员，多点Dmall技术Leader，前折800互联网研发团队负责人、10余年一线研发经验。具有多年的软件开发经验, 熟练Ruby、Java、Rust等开发语言, 同时也参与过Rust中文社区日报维护工作。</p>
<p><strong>课程介绍</strong></p>
<p>本次课程通过10个左右的小例子，带大家理解一下Rust的所有权，Rust引用和借用，Rust变量克隆和复制的理念。</p>
<p><strong>参加课程</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/Rust-pbc-1.jpg" alt></p>
<p><strong>课程规划</strong></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">数据表 Timestamp 日期 Serialize</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2ff8a69e-59bb-4502-87c0-c3416ffae8a0">
<div class="article-summary-box-inner">
<span><p>主要参考：<a href="https://github.com/rustcc/forustm" rel="noopener noreferrer">Rustcc网站源码库</a></p>
<p>在处理数据表中日期相关数据时，Seralize序列化相关操作会报错，提示 DateTime 字段不识别，
查了 rustcc 源码才发现依赖中需要开启相应的feature。特此记录。</p>
<h2>1.依赖的库：</h2>
<pre><code>[dependencies]
# 日期时间处理 需要开启 serde 特征 支持序列化
chrono = { version = "0.4.19", features = ["serde"] }

# 数据库ORM
diesel = { version = "1.4.4", features = ["postgres", "chrono", "uuid", "r2d2"] }
dotenv = "0.15.0"
serde = { version = "1.0.127", features = ["derive"] }
serde_json = "1.0.66"
uuid = { version = "0.8.2", features = ["serde", "v4"] }
</code></pre>
<h2>2.创建数据表</h2>
<pre><code>CREATE TABLE characters (
    id SERIAL PRIMARY KEY,
    name VARCHAR(128) UNIQUE NOT NULL,
    age INTEGER NOT NULL DEFAULT 0,
    friends VARCHAR NOT NULL DEFAULT '',
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
)
</code></pre>
<h2>3.数据表对应的 model</h2>
<pre><code>use chrono::{NaiveDateTime};
use serde::{Deserialize, Serialize};

#[derive(Queryable, Serialize, Deserialize, Debug)]
pub struct Characters {
    pub id: i32,
    pub name: String,
    pub age: i32,
    pub friends: String,
    // 这里的 NaiveDateTime 日期格式序列化需要开启相关 features
    pub created_at: NaiveDateTime,
}
</code></pre>
<h2>4.获取数据</h2>
<pre><code>use db::schema::characters;
use db::{get_connection};
use db::models::{Characters, NewCharacter};
use db::schema::characters::dsl::*;
use diesel::QueryDsl;
use diesel::prelude::*;

fn main() {
    let conn = get_connection();

    // 查询年龄大于30的10条数据
    let arr: Vec&lt;Characters&gt; = characters.filter(characters::age.gt(30))
        .limit(10)
        .load::&lt;Characters&gt;(&amp;conn)
        .expect("Loading Error");

    let date_arr = arr.iter()
        .map(|item| {
	    // 数据格式化
            let t = item.created_at.format("%Y-%m-%d %H:%M:%S").to_string();
            println!("{} {}", item.name, t);
            t
        })
        .collect::&lt;Vec&lt;String&gt;&gt;();
}
</code></pre>
<p>输出结果类似：</p>
<pre><code>Box 2021-08-05 09:39:34
Bobe 2021-08-05 09:39:34
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cargo workspace config</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c3dcce30-1fc0-4819-8992-142365c7e21c">
<div class="article-summary-box-inner">
<span><p><a href="https://kaisery.github.io/trpl-zh-cn/ch14-03-cargo-workspaces.html" rel="noopener noreferrer">Workspace 文档链接</a></p>
<h2>目录结构</h2>
<pre><code>workspace-test/
    Cargo.toml
    db/
        src/
            bin/
                init.rs
        Cargo.tml
</code></pre>
<h2>workspace</h2>
<p>workspace-test/Cargo.toml</p>
<pre><code>[workspace]
members = ["db"]
default-member = "db"
</code></pre>
<h2>子项目</h2>
<p>workspace-test/db/Cargo.toml</p>
<pre><code>[package]
name = "db"
version = "0.1.0"
edition = "2018"

[dependencies]

# 可选的可执行文件配置
# [[bin]]
# name = "init"
# path = "src/bin/init.rs"
</code></pre>
<h2>操作</h2>
<pre><code># 运行 init
cargo run --bin init
# -p 指定项目
cargo run -p db --bin init
</code></pre>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-06T01:30:00Z">09-06</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenges in Generalization in Open Domain Question Answering. (arXiv:2109.01156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01156">
<div class="article-summary-box-inner">
<span><p>Recent work on Open Domain Question Answering has shown that there is a large
discrepancy in model performance between novel test questions and those that
largely overlap with training questions. However, it is as of yet unclear which
aspects of novel questions that make them challenging. Drawing upon studies on
systematic generalization, we introduce and annotate questions according to
three categories that measure different levels and kinds of generalization:
training set overlap, compositional generalization (comp-gen), and novel entity
generalization (novel-entity). When evaluating six popular parametric and
non-parametric models, we find that for the established Natural Questions and
TriviaQA datasets, even the strongest model performance for
comp-gen/novel-entity is 13.1/5.4% and 9.6/1.5% lower compared to that for the
full test set -- indicating the challenge posed by these types of questions.
Furthermore, we show that whilst non-parametric models can handle questions
containing novel entities, they struggle with those requiring compositional
generalization. Through thorough analysis we find that key question difficulty
factors are: cascading errors from the retrieval component, frequency of
question pattern, and frequency of the entity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Conformer: Progressive Downsampling and Grouped Attention for Automatic Speech Recognition. (arXiv:2109.01163v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01163">
<div class="article-summary-box-inner">
<span><p>The recently proposed Conformer architecture has shown state-of-the-art
performances in Automatic Speech Recognition by combining convolution with
attention to model both local and global dependencies. In this paper, we study
how to reduce the Conformer architecture complexity with a limited computing
budget, leading to a more efficient architecture design that we call Efficient
Conformer. We introduce progressive downsampling to the Conformer encoder and
propose a novel attention mechanism named grouped attention, allowing us to
reduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence
length $n$, hidden dimension $d$ and group size parameter $g$. We also
experiment the use of strided multi-head self-attention as a global
downsampling operation. Our experiments are performed on the LibriSpeech
dataset with CTC and RNN-Transducer losses. We show that within the same
computing budget, the proposed architecture achieves better performances with
faster training and decoding compared to the Conformer. Our 13M parameters CTC
model achieves competitive WERs of 3.6\%/9.0\% without using a language model
and 2.7\%/6.7\% with an external n-gram language model on the
test-clean/test-other sets while being 29\% faster than our CTC Conformer
baseline at inference and 36\% faster to train.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ranking Scientific Papers Using Preference Learning. (arXiv:2109.01190v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01190">
<div class="article-summary-box-inner">
<span><p>Peer review is the main quality control mechanism in academia. Quality of
scientific work has many dimensions; coupled with the subjective nature of the
reviewing task, this makes final decision making based on the reviews and
scores therein very difficult and time-consuming. To assist with this important
task, we cast it as a paper ranking problem based on peer review texts and
reviewer scores. We introduce a novel, multi-faceted generic evaluation
framework for making final decisions based on peer reviews that takes into
account effectiveness, efficiency and fairness of the evaluated system. We
propose a novel approach to paper ranking based on Gaussian Process Preference
Learning (GPPL) and evaluate it on peer review data from the ACL-2018
conference. Our experiments demonstrate the superiority of our GPPL-based
approach over prior work, while highlighting the importance of using both texts
and review scores for paper ranking during peer review aggregation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Establishing Interlingua in Multilingual Language Models. (arXiv:2109.01207v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01207">
<div class="article-summary-box-inner">
<span><p>Large multilingual language models show remarkable zero-shot cross-lingual
transfer performance on a range of tasks. Follow-up works hypothesized that
these models internally project representations of different languages into a
shared interlingual space. However, they produced contradictory results. In
this paper, we correct %one of the previous works the famous prior work
claiming that "BERT is not an Interlingua" and show that with the proper choice
of sentence representation different languages actually do converge to a shared
space in such language models. Furthermore, we demonstrate that this
convergence pattern is robust across four measures of correlation similarity
and six mBERT-like models. We then extend our analysis to 28 diverse languages
and find that the interlingual space exhibits a particular structure similar to
the linguistic relatedness of languages. We also highlight a few outlier
languages that seem to fail to converge to the shared space. The code for
replicating our results is available at the following URL:
https://github.com/maksym-del/interlingua.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantifying Reproducibility in NLP and ML. (arXiv:2109.01211v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01211">
<div class="article-summary-box-inner">
<span><p>Reproducibility has become an intensely debated topic in NLP and ML over
recent years, but no commonly accepted way of assessing reproducibility, let
alone quantifying it, has so far emerged. The assumption has been that wider
scientific reproducibility terminology and definitions are not applicable to
NLP/ML, with the result that many different terms and definitions have been
proposed, some diametrically opposed. In this paper, we test this assumption,
by taking the standard terminology and definitions from metrology and applying
them directly to NLP/ML. We find that we are able to straightforwardly derive a
practical framework for assessing reproducibility which has the desirable
property of yielding a quantified degree of reproducibility that is comparable
across different reproduction studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">So Cloze yet so Far: N400 Amplitude is Better Predicted by Distributional Information than Human Predictability Judgements. (arXiv:2109.01226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01226">
<div class="article-summary-box-inner">
<span><p>More predictable words are easier to process - they are read faster and
elicit smaller neural signals associated with processing difficulty, most
notably, the N400 component of the event-related brain potential. Thus, it has
been argued that prediction of upcoming words is a key component of language
comprehension, and that studying the amplitude of the N400 is a valuable way to
investigate the predictions that we make. In this study, we investigate whether
the linguistic predictions of computational language models or humans better
reflect the way in which natural language stimuli modulate the amplitude of the
N400. One important difference in the linguistic predictions of humans versus
computational language models is that while language models base their
predictions exclusively on the preceding linguistic context, humans may rely on
other factors. We find that the predictions of three top-of-the-line
contemporary language models - GPT-3, RoBERTa, and ALBERT - match the N400 more
closely than human predictions. This suggests that the predictive processes
underlying the N400 may be more sensitive to the surface-level statistics of
language than previously thought.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multimodal Conditionality for Natural Language Generation. (arXiv:2109.01229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01229">
<div class="article-summary-box-inner">
<span><p>Large scale pretrained language models have demonstrated state-of-the-art
performance in language understanding tasks. Their application has recently
expanded into multimodality learning, leading to improved representations
combining vision and language. However, progress in adapting language models
towards conditional Natural Language Generation (NLG) has been limited to a
single modality, generally text. We propose MAnTiS, Multimodal Adaptation for
Text Synthesis, a general approach for multimodal conditionality in
transformer-based NLG models. In this method, we pass inputs from each modality
through modality-specific encoders, project to textual token space, and finally
join to form a conditionality prefix. We fine-tune the pretrained language
model and encoders with the conditionality prefix guiding the generation. We
apply MAnTiS to the task of product description generation, conditioning a
network on both product images and titles to generate descriptive text. We
demonstrate that MAnTiS outperforms strong baseline approaches on standard NLG
scoring metrics. Furthermore, qualitative assessments demonstrate that MAnTiS
can generate human quality descriptions consistent with given multimodal
inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Empirical Study on Leveraging Position Embeddings for Target-oriented Opinion Words Extraction. (arXiv:2109.01238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01238">
<div class="article-summary-box-inner">
<span><p>Target-oriented opinion words extraction (TOWE) (Fan et al., 2019b) is a new
subtask of target-oriented sentiment analysis that aims to extract opinion
words for a given aspect in text. Current state-of-the-art methods leverage
position embeddings to capture the relative position of a word to the target.
However, the performance of these methods depends on the ability to incorporate
this information into word representations. In this paper, we explore a variety
of text encoders based on pretrained word embeddings or language models that
leverage part-of-speech and position embeddings, aiming to examine the actual
contribution of each component in TOWE. We also adapt a graph convolutional
network (GCN) to enhance word representations by incorporating syntactic
information. Our experimental results demonstrate that BiLSTM-based models can
effectively encode position information into word representations while using a
GCN only achieves marginal gains. Interestingly, our simple methods outperform
several state-of-the-art complex neural structures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Entity Linking and Discovery via Arborescence-based Supervised Clustering. (arXiv:2109.01242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01242">
<div class="article-summary-box-inner">
<span><p>Previous work has shown promising results in performing entity linking by
measuring not only the affinities between mentions and entities but also those
amongst mentions. In this paper, we present novel training and inference
procedures that fully utilize mention-to-mention affinities by building minimum
arborescences (i.e., directed spanning trees) over mentions and entities across
documents in order to make linking decisions. We also show that this method
gracefully extends to entity discovery, enabling the clustering of mentions
that do not have an associated entity in the knowledge base. We evaluate our
approach on the Zero-Shot Entity Linking dataset and MedMentions, the largest
publicly available biomedical dataset, and show significant improvements in
performance for both entity linking and discovery compared to identically
parameterized models. We further show significant efficiency improvements with
only a small loss in accuracy over previous work, which use more
computationally expensive models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Do Prompt-Based Models Really Understand the Meaning of their Prompts?. (arXiv:2109.01247v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01247">
<div class="article-summary-box-inner">
<span><p>Recently, a boom of papers have shown extraordinary progress in few-shot
learning with various prompt-based models. Such success can give the impression
that prompts help models to learn faster in the same way that humans learn
faster when provided with task instructions expressed in natural language. In
this study, we experiment with over 30 prompts manually written for natural
language inference (NLI). We find that models learn just as fast with many
prompts that are intentionally irrelevant or even pathologically misleading as
they do with instructively "good" prompts. Additionally, we find that model
performance is more dependent on the choice of the LM target words (a.k.a. the
"verbalizer" that converts LM vocabulary prediction to class labels) than on
the text of the prompt itself. In sum, we find little evidence that suggests
existing prompt-based models truly understand the meaning of their given
prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Context-Aware Hierarchical BERT Fusion Network for Multi-turn Dialog Act Detection. (arXiv:2109.01267v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01267">
<div class="article-summary-box-inner">
<span><p>The success of interactive dialog systems is usually associated with the
quality of the spoken language understanding (SLU) task, which mainly
identifies the corresponding dialog acts and slot values in each turn. By
treating utterances in isolation, most SLU systems often overlook the semantic
context in which a dialog act is expected. The act dependency between turns is
non-trivial and yet critical to the identification of the correct semantic
representations. Previous works with limited context awareness have exposed the
inadequacy of dealing with complexity in multiproned user intents, which are
subject to spontaneous change during turn transitions. In this work, we propose
to enhance SLU in multi-turn dialogs, employing a context-aware hierarchical
BERT fusion Network (CaBERT-SLU) to not only discern context information within
a dialog but also jointly identify multiple dialog acts and slots in each
utterance. Experimental results show that our approach reaches new
state-of-the-art (SOTA) performances in two complicated multi-turn dialogue
datasets with considerable improvements compared with previous methods, which
only consider single utterances for multiple intents and slot filling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Open-Source Dataset and A Multi-Task Model for Malay Named Entity Recognition. (arXiv:2109.01293v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01293">
<div class="article-summary-box-inner">
<span><p>Named entity recognition (NER) is a fundamental task of natural language
processing (NLP). However, most state-of-the-art research is mainly oriented to
high-resource languages such as English and has not been widely applied to
low-resource languages. In Malay language, relevant NER resources are limited.
In this work, we propose a dataset construction framework, which is based on
labeled datasets of homologous languages and iterative optimization, to build a
Malay NER dataset (MYNER) comprising 28,991 sentences (over 384 thousand
tokens). Additionally, to better integrate boundary information for NER, we
propose a multi-task (MT) model with a bidirectional revision (Bi-revision)
mechanism for Malay NER task. Specifically, an auxiliary task, boundary
detection, is introduced to improve NER training in both explicit and implicit
ways. Furthermore, a gated ignoring mechanism is proposed to conduct
conditional label transfer and alleviate error propagation by the auxiliary
task. Experimental results demonstrate that our model achieves comparable
results over baselines on MYNER. The dataset and the model in this paper would
be publicly released as a benchmark dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Information Symmetry Matters: A Modal-Alternating Propagation Network for Few-Shot Learning. (arXiv:2109.01295v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01295">
<div class="article-summary-box-inner">
<span><p>Semantic information provides intra-class consistency and inter-class
discriminability beyond visual concepts, which has been employed in Few-Shot
Learning (FSL) to achieve further gains. However, semantic information is only
available for labeled samples but absent for unlabeled samples, in which the
embeddings are rectified unilaterally by guiding the few labeled samples with
semantics. Therefore, it is inevitable to bring a cross-modal bias between
semantic-guided samples and nonsemantic-guided samples, which results in an
information asymmetry problem. To address this problem, we propose a
Modal-Alternating Propagation Network (MAP-Net) to supplement the absent
semantic information of unlabeled samples, which builds information symmetry
among all samples in both visual and semantic modalities. Specifically, the
MAP-Net transfers the neighbor information by the graph propagation to generate
the pseudo-semantics for unlabeled samples guided by the completed visual
relationships and rectify the feature embeddings. In addition, due to the large
discrepancy between visual and semantic modalities, we design a Relation
Guidance (RG) strategy to guide the visual relation vectors via semantics so
that the propagated information is more beneficial. Extensive experimental
results on three semantic-labeled datasets, i.e., Caltech-UCSD-Birds 200-2011,
SUN Attribute Database, and Oxford 102 Flower, have demonstrated that our
proposed method achieves promising performance and outperforms the
state-of-the-art approaches, which indicates the necessity of information
symmetry.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Indexing Context-Sensitive Reachability. (arXiv:2109.01321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01321">
<div class="article-summary-box-inner">
<span><p>Many context-sensitive data flow analyses can be formulated as a variant of
the all-pairs Dyck-CFL reachability problem, which, in general, is of sub-cubic
time complexity and quadratic space complexity. Such high complexity
significantly limits the scalability of context-sensitive data flow analysis
and is not affordable for analyzing large-scale software. This paper presents
\textsc{Flare}, a reduction from the CFL reachability problem to the
conventional graph reachability problem for context-sensitive data flow
analysis. This reduction allows us to benefit from recent advances in
reachability indexing schemes, which often consume almost linear space for
answering reachability queries in almost constant time. We have applied our
reduction to a context-sensitive alias analysis and a context-sensitive
information-flow analysis for C/C++ programs. Experimental results on standard
benchmarks and open-source software demonstrate that we can achieve orders of
magnitude speedup at the cost of only moderate space to store the indexes. The
implementation of our approach is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Speaker Personas from Conversational Texts. (arXiv:2109.01330v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01330">
<div class="article-summary-box-inner">
<span><p>Personas are useful for dialogue response prediction. However, the personas
used in current studies are pre-defined and hard to obtain before a
conversation. To tackle this issue, we study a new task, named Speaker Persona
Detection (SPD), which aims to detect speaker personas based on the plain
conversational text. In this task, a best-matched persona is searched out from
candidates given the conversational text. This is a many-to-many semantic
matching task because both contexts and personas in SPD are composed of
multiple sentences. The long-term dependency and the dynamic redundancy among
these sentences increase the difficulty of this task. We build a dataset for
SPD, dubbed as Persona Match on Persona-Chat (PMPC). Furthermore, we evaluate
several baseline models and propose utterance-to-profile (U2P) matching
networks for this task. The U2P models operate at a fine granularity which
treat both contexts and personas as sets of multiple sequences. Then, each
sequence pair is scored and an interpretable overall score is obtained for a
context-persona pair through aggregation. Evaluation results show that the U2P
models outperform their baseline counterparts significantly.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Modeling, Lexical Translation, Reordering: The Training Process of NMT through the Lens of Classical SMT. (arXiv:2109.01396v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01396">
<div class="article-summary-box-inner">
<span><p>Differently from the traditional statistical MT that decomposes the
translation task into distinct separately learned components, neural machine
translation uses a single neural network to model the entire translation
process. Despite neural machine translation being de-facto standard, it is
still not clear how NMT models acquire different competences over the course of
training, and how this mirrors the different models in traditional SMT. In this
work, we look at the competences related to three core SMT components and find
that during training, NMT first focuses on learning target-side language
modeling, then improves translation quality approaching word-by-word
translation, and finally learns more complicated reordering patterns. We show
that this behavior holds for several models and language pairs. Additionally,
we explain how such an understanding of the training process can be useful in
practice and, as an example, show how it can be used to improve vanilla
non-autoregressive neural machine translation by guiding teacher model
selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Exploratory Study on Utilising the Web of Linked Data for Product Data Mining. (arXiv:2109.01411v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01411">
<div class="article-summary-box-inner">
<span><p>The Linked Open Data practice has led to a significant growth of structured
data on the Web in the last decade. Such structured data describe real-world
entities in a machine-readable way, and have created an unprecedented
opportunity for research in the field of Natural Language Processing. However,
there is a lack of studies on how such data can be used, for what kind of
tasks, and to what extent they can be useful for these tasks. This work focuses
on the e-commerce domain to explore methods of utilising such structured data
to create language resources that may be used for product classification and
linking. We process billions of structured data points in the form of RDF
n-quads, to create multi-million words of product-related corpora that are
later used in three different ways for creating of language resources: training
word embedding models, continued pre-training of BERT-like language models, and
training Machine Translation models that are used as a proxy to generate
product-related keywords. Our evaluation on an extensive set of benchmarks
shows word embeddings to be the most reliable and consistent method to improve
the accuracy on both tasks (with up to 6.9 percentage points in macro-average
F1 on some datasets). The other two methods however, are not as useful. Our
analysis shows that this could be due to a number of reasons, including the
biased domain representation in the structured data and lack of vocabulary
coverage. We share our datasets and discuss how our lessons learned could be
taken forward to inform future research in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LG4AV: Combining Language Models and Graph Neural Networks for Author Verification. (arXiv:2109.01479v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01479">
<div class="article-summary-box-inner">
<span><p>The automatic verification of document authorships is important in various
settings. Researchers are for example judged and compared by the amount and
impact of their publications and public figures are confronted by their posts
on social media platforms. Therefore, it is important that authorship
information in frequently used web services and platforms is correct. The
question whether a given document is written by a given author is commonly
referred to as authorship verification (AV). While AV is a widely investigated
problem in general, only few works consider settings where the documents are
short and written in a rather uniform style. This makes most approaches
unpractical for online databases and knowledge graphs in the scholarly domain.
Here, authorships of scientific publications have to be verified, often with
just abstracts and titles available. To this point, we present our novel
approach LG4AV which combines language models and graph neural networks for
authorship verification. By directly feeding the available texts in a
pre-trained transformer architecture, our model does not need any hand-crafted
stylometric features that are not meaningful in scenarios where the writing
style is, at least to some extent, standardized. By the incorporation of a
graph neural network structure, our model can benefit from relations between
authors that are meaningful with respect to the verification process. For
example, scientific authors are more likely to write about topics that are
addressed by their co-authors and twitter users tend to post about the same
subjects as people they follow. We experimentally evaluate our model and study
to which extent the inclusion of co-authorships enhances verification decisions
in bibliometric environments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Representation Learning for Exemplar-Guided Paraphrase Generation. (arXiv:2109.01484v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01484">
<div class="article-summary-box-inner">
<span><p>Exemplar-Guided Paraphrase Generation (EGPG) aims to generate a target
sentence which conforms to the style of the given exemplar while encapsulating
the content information of the source sentence. In this paper, we propose a new
method with the goal of learning a better representation of the style andthe
content. This method is mainly motivated by the recent success of contrastive
learning which has demonstrated its power in unsupervised feature extraction
tasks. The idea is to design two contrastive losses with respect to the content
and the style by considering two problem characteristics during training. One
characteristic is that the target sentence shares the same content with the
source sentence, and the second characteristic is that the target sentence
shares the same style with the exemplar. These two contrastive losses are
incorporated into the general encoder-decoder paradigm. Experiments on two
datasets, namely QQP-Pos and ParaNMT, demonstrate the effectiveness of our
proposed constrastive losses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical Data-to-Text Generation via Fine-Tuning Transformers. (arXiv:2109.01518v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01518">
<div class="article-summary-box-inner">
<span><p>Data-to-text (D2T) generation in the biomedical domain is a promising - yet
mostly unexplored - field of research. Here, we apply neural models for D2T
generation to a real-world dataset consisting of package leaflets of European
medicines. We show that fine-tuned transformers are able to generate realistic,
multisentence text from data in the biomedical domain, yet have important
limitations. We also release a new dataset (BioLeaflets) for benchmarking D2T
generation models in the biomedical domain.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Longitudinal Multi-modal Dataset for Dementia Monitoring and Diagnosis. (arXiv:2109.01537v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01537">
<div class="article-summary-box-inner">
<span><p>Dementia is a family of neurogenerative conditions affecting memory and
cognition in an increasing number of individuals in our globally aging
population. Automated analysis of language, speech and paralinguistic
indicators have been gaining popularity as potential indicators of cognitive
decline. Here we propose a novel longitudinal multi-modal dataset collected
from people with mild dementia and age matched controls over a period of
several months in a natural setting. The multi-modal data consists of spoken
conversations, a subset of which are transcribed, as well as typed and written
thoughts and associated extra-linguistic information such as pen strokes and
keystrokes. We describe the dataset in detail and proceed to focus on a task
using the speech modality. The latter involves distinguishing controls from
people with dementia by exploiting the longitudinal nature of the data. Our
experiments showed significant differences in how the speech varied from
session to session in the control and dementia groups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Neural Models for Natural Language Processing in the Face of Distributional Shift. (arXiv:2109.01558v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01558">
<div class="article-summary-box-inner">
<span><p>The dominating NLP paradigm of training a strong neural predictor to perform
one task on a specific dataset has led to state-of-the-art performance in a
variety of applications (eg. sentiment classification, span-prediction based
question answering or machine translation). However, it builds upon the
assumption that the data distribution is stationary, ie. that the data is
sampled from a fixed distribution both at training and test time. This way of
training is inconsistent with how we as humans are able to learn from and
operate within a constantly changing stream of information. Moreover, it is
ill-adapted to real-world use cases where the data distribution is expected to
shift over the course of a model's lifetime.
</p>
<p>The first goal of this thesis is to characterize the different forms this
shift can take in the context of natural language processing, and propose
benchmarks and evaluation metrics to measure its effect on current deep
learning architectures. We then proceed to take steps to mitigate the effect of
distributional shift on NLP models. To this end, we develop methods based on
parametric reformulations of the distributionally robust optimization
framework. Empirically, we demonstrate that these approaches yield more robust
models as demonstrated on a selection of realistic problems. In the third and
final part of this thesis, we explore ways of efficiently adapting existing
models to new domains or tasks. Our contribution to this topic takes
inspiration from information geometry to derive a new gradient update rule
which alleviate catastrophic forgetting issues during adaptation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextualized Embeddings based Convolutional Neural Networks for Duplicate Question Identification. (arXiv:2109.01560v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01560">
<div class="article-summary-box-inner">
<span><p>Question Paraphrase Identification (QPI) is a critical task for large-scale
Question-Answering forums. The purpose of QPI is to determine whether a given
pair of questions are semantically identical or not. Previous approaches for
this task have yielded promising results, but have often relied on complex
recurrence mechanisms that are expensive and time-consuming in nature. In this
paper, we propose a novel architecture combining a Bidirectional Transformer
Encoder with Convolutional Neural Networks for the QPI task. We produce the
predictions from the proposed architecture using two different inference
setups: Siamese and Matched Aggregation. Experimental results demonstrate that
our model achieves state-of-the-art performance on the Quora Question Pairs
dataset. We empirically prove that the addition of convolution layers to the
model architecture improves the results in both inference setups. We also
investigate the impact of partial and complete fine-tuning and analyze the
trade-off between computational power and accuracy in the process. Based on the
obtained results, we conclude that the Matched-Aggregation setup consistently
outperforms the Siamese setup. Our work provides insights into what
architecture combinations and setups are likely to produce better results for
the QPI task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Multiple Noisy Augmented Data Sets for Better Cross-Lingual Spoken Language Understanding. (arXiv:2109.01583v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01583">
<div class="article-summary-box-inner">
<span><p>Lack of training data presents a grand challenge to scaling out spoken
language understanding (SLU) to low-resource languages. Although various data
augmentation approaches have been proposed to synthesize training data in
low-resource target languages, the augmented data sets are often noisy, and
thus impede the performance of SLU models. In this paper we focus on mitigating
noise in augmented data. We develop a denoising training approach. Multiple
models are trained with data produced by various augmented methods. Those
models provide supervision signals to each other. The experimental results show
that our method outperforms the existing state of the art by 3.05 and 4.24
percentage points on two benchmark datasets, respectively. The code will be
made open sourced on github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Training with Dense Retrieval for Document Retrieval. (arXiv:2109.01628v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01628">
<div class="article-summary-box-inner">
<span><p>Dense retrieval has shown great success in passage ranking in English.
However, its effectiveness in document retrieval for non-English languages
remains unexplored due to the limitation in training resources. In this work,
we explore different transfer techniques for document ranking from English
annotations to multiple non-English languages. Our experiments on the test
collections in six languages (Chinese, Arabic, French, Hindi, Bengali, Spanish)
from diverse language families reveal that zero-shot model-based transfer using
mBERT improves the search quality in non-English mono-lingual retrieval. Also,
we find that weakly-supervised target language transfer yields competitive
performances against the generation-based target language transfer that
requires external translators and query generators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empirical Study of Named Entity Recognition Performance Using Distribution-aware Word Embedding. (arXiv:2109.01636v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01636">
<div class="article-summary-box-inner">
<span><p>With the fast development of Deep Learning techniques, Named Entity
Recognition (NER) is becoming more and more important in the information
extraction task. The greatest difficulty that the NER task faces is to keep the
detectability even when types of NE and documents are unfamiliar. Realizing
that the specificity information may contain potential meanings of a word and
generate semantic-related features for word embedding, we develop a
distribution-aware word embedding and implement three different methods to make
use of the distribution information in a NER framework. And the result shows
that the performance of NER will be improved if the word specificity is
incorporated into existing NER methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuned Language Models Are Zero-Shot Learners. (arXiv:2109.01652v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01652">
<div class="article-summary-box-inner">
<span><p>This paper explores a simple method for improving the zero-shot learning
abilities of language models. We show that instruction tuning -- finetuning
language models on a collection of tasks described via instructions --
substantially boosts zero-shot performance on unseen tasks.
</p>
<p>We take a 137B parameter pretrained language model and instruction-tune it on
over 60 NLP tasks verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task
types. FLAN substantially improves the performance of its unmodified
counterpart and surpasses zero-shot 175B GPT-3 on 19 of 25 tasks that we
evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,
BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number
of tasks and model scale are key components to the success of instruction
tuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge. (arXiv:2109.01653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.01653">
<div class="article-summary-box-inner">
<span><p>Most benchmark datasets targeting commonsense reasoning focus on everyday
scenarios: physical knowledge like knowing that you could fill a cup under a
waterfall [Talmor et al., 2019], social knowledge like bumping into someone is
awkward [Sap et al., 2019], and other generic situations. However, there is a
rich space of commonsense inferences anchored to knowledge about specific
entities: for example, deciding the truthfulness of a claim "Harry Potter can
teach classes on how to fly on a broomstick." Can models learn to combine
entity knowledge with commonsense reasoning in this fashion? We introduce
CREAK, a testbed for commonsense reasoning about entity knowledge, bridging
fact-checking about entities (Harry Potter is a wizard and is skilled at riding
a broomstick) with commonsense inferences (if you're good at a skill you can
teach others how to do it). Our dataset consists of 13k human-authored English
claims about entities that are either true or false, in addition to a small
contrast set. Crowdworkers can easily come up with these statements and human
performance on the dataset is high (high 90s); we argue that models should be
able to blend entity knowledge and commonsense reasoning to do well here. In
our experiments, we focus on the closed-book setting and observe that a
baseline model finetuned on existing fact verification benchmark struggles on
CREAK. Training a model on CREAK improves accuracy by a substantial margin, but
still falls short of human performance. Our benchmark provides a unique probe
into natural language understanding models, testing both its ability to
retrieve facts (e.g., who teaches at the University of Chicago?) and unstated
commonsense knowledge (e.g., butlers do not yell at guests).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposure Bias versus Self-Recovery: Are Distortions Really Incremental for Autoregressive Text Generation?. (arXiv:1905.10617v10 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.10617">
<div class="article-summary-box-inner">
<span><p>Exposure bias has been regarded as a central problem for auto-regressive
language models (LM). It claims that teacher forcing would cause the test-time
generation to be incrementally distorted due to the training-generation
discrepancy. Although a lot of algorithms have been proposed to avoid teacher
forcing and therefore alleviate exposure bias, there is little work showing how
serious the exposure bias problem actually is. In this work, we focus on the
task of open-ended language generation, propose metrics to quantify the impact
of exposure bias in the aspects of quality, diversity, and consistency. Our key
intuition is that if we feed ground-truth data prefixes (instead of prefixes
generated by the model itself) into the model and ask it to continue the
generation, the performance should become much better because the
training-generation discrepancy in the prefix is removed. Both automatic and
human evaluations are conducted in our experiments. On the contrary to the
popular belief in exposure bias, we find that the the distortion induced by the
prefix discrepancy is limited, and does not seem to be incremental during the
generation. Moreover, our analysis reveals an interesting self-recovery ability
of the LM, which we hypothesize to be countering the harmful effects from
exposure bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Survey on Publicly Available Sinhala Natural Language Processing Tools and Research. (arXiv:1906.02358v10 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1906.02358">
<div class="article-summary-box-inner">
<span><p>Sinhala is the native language of the Sinhalese people who make up the
largest ethnic group of Sri Lanka. The language belongs to the globe-spanning
language tree, Indo-European. However, due to poverty in both linguistic and
economic capital, Sinhala, in the perspective of Natural Language Processing
tools and research, remains a resource-poor language which has neither the
economic drive its cousin English has nor the sheer push of the law of numbers
a language such as Chinese has. A number of research groups from Sri Lanka have
noticed this dearth and the resultant dire need for proper tools and research
for Sinhala natural language processing. However, due to various reasons, these
attempts seem to lack coordination and awareness of each other. The objective
of this paper is to fill that gap of a comprehensive literature survey of the
publicly available Sinhala natural language tools and research so that the
researchers working in this field can better utilize contributions of their
peers. As such, we shall be uploading this paper to arXiv and perpetually
update it periodically to reflect the advances made in the field.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adjusting for Confounders with Text: Challenges and an Empirical Evaluation Framework for Causal Inference. (arXiv:2009.09961v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09961">
<div class="article-summary-box-inner">
<span><p>Leveraging text, such as social media posts, for causal inferences requires
the use of NLP models to 'learn' and adjust for confounders, which could
otherwise impart bias. However, evaluating such models is challenging, as
ground truth is almost never available. We demonstrate the need for empirical
evaluation frameworks for causal inference in natural language by showing that
existing, commonly used models regularly disagree with one another on real
world tasks. We contribute the first such framework, generalizing several
challenges across these real world tasks. Using this framework, we evaluate a
large set of commonly used causal inference models based on propensity scores
and identify their strengths and weaknesses to inform future improvements. We
make all tasks, data, and models public to inform applications and encourage
additional research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CR-Walker: Tree-Structured Graph Reasoning and Dialog Acts for Conversational Recommendation. (arXiv:2010.10333v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.10333">
<div class="article-summary-box-inner">
<span><p>Growing interests have been attracted in Conversational Recommender Systems
(CRS), which explore user preference through conversational interactions in
order to make appropriate recommendation. However, there is still a lack of
ability in existing CRS to (1) traverse multiple reasoning paths over
background knowledge to introduce relevant items and attributes, and (2)
arrange selected entities appropriately under current system intents to control
response generation. To address these issues, we propose CR-Walker in this
paper, a model that performs tree-structured reasoning on a knowledge graph,
and generates informative dialog acts to guide language generation. The unique
scheme of tree-structured reasoning views the traversed entity at each hop as
part of dialog acts to facilitate language generation, which links how entities
are selected and expressed. Automatic and human evaluations show that CR-Walker
can arrive at more accurate recommendation, and generate more informative and
engaging responses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Where Are You? Localization from Embodied Dialog. (arXiv:2011.08277v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2011.08277">
<div class="article-summary-box-inner">
<span><p>We present Where Are You? (WAY), a dataset of ~6k dialogs in which two humans
-- an Observer and a Locator -- complete a cooperative localization task. The
Observer is spawned at random in a 3D environment and can navigate from
first-person views while answering questions from the Locator. The Locator must
localize the Observer in a detailed top-down map by asking questions and giving
instructions. Based on this dataset, we define three challenging tasks:
Localization from Embodied Dialog or LED (localizing the Observer from dialog
history), Embodied Visual Dialog (modeling the Observer), and Cooperative
Localization (modeling both agents). In this paper, we focus on the LED task --
providing a strong baseline model with detailed ablations characterizing both
dataset biases and the importance of various modeling choices. Our best model
achieves 32.7% success at identifying the Observer's location within 3m in
unseen buildings, vs. 70.4% for human Locators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CascadeBERT: Accelerating Inference of Pre-trained Language Models via Calibrated Complete Models Cascade. (arXiv:2012.14682v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14682">
<div class="article-summary-box-inner">
<span><p>Dynamic early exiting aims to accelerate the inference of pre-trained
language models (PLMs) by emitting predictions in internal layers without
passing through the entire model. In this paper, we empirically analyze the
working mechanism of dynamic early exiting and find that it faces a performance
bottleneck under high speed-up ratios. On one hand, the PLMs' representations
in shallow layers lack high-level semantic information and thus are not
sufficient for accurate predictions. On the other hand, the exiting decisions
made by internal classifiers are unreliable, leading to wrongly emitted early
predictions. We instead propose a new framework for accelerating the inference
of PLMs, CascadeBERT, which dynamically selects proper-sized and complete
models in a cascading manner, providing comprehensive representations for
predictions. We further devise a difficulty-aware objective, encouraging the
model to output the class probability that reflects the real difficulty of each
instance for a more reliable cascading mechanism. Experimental results show
that CascadeBERT can achieve an overall 15\% improvement under 4$\times$
speed-up compared with existing dynamic early exiting methods on six
classification tasks, yielding more calibrated and accurate predictions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Which Linguist Invented the Lightbulb? Presupposition Verification for Question-Answering. (arXiv:2101.00391v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00391">
<div class="article-summary-box-inner">
<span><p>Many Question-Answering (QA) datasets contain unanswerable questions, but
their treatment in QA systems remains primitive. Our analysis of the Natural
Questions (Kwiatkowski et al. 2019) dataset reveals that a substantial portion
of unanswerable questions ($\sim$21%) can be explained based on the presence of
unverifiable presuppositions. We discuss the shortcomings of current models in
handling such questions, and describe how an improved system could handle them.
Through a user preference study, we demonstrate that the oracle behavior of our
proposed system that provides responses based on presupposition failure is
preferred over the oracle behavior of existing QA systems. Then we discuss how
our proposed system could be implemented, presenting a novel framework that
breaks down the problem into three steps: presupposition generation,
presupposition verification and explanation generation. We report our progress
in tackling each subproblem, and present a preliminary approach to integrating
these steps into an existing QA system. We find that adding presuppositions and
their verifiability to an existing model yields modest gains in downstream
performance and unanswerability detection. The biggest bottleneck is the
verification component, which needs to be substantially improved for the
integrated system to approach ideal behavior -- even transfer from the best
entailment models currently falls short.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CDLM: Cross-Document Language Modeling. (arXiv:2101.00406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00406">
<div class="article-summary-box-inner">
<span><p>We introduce a new pretraining approach geared for multi-document language
modeling, incorporating two key ideas into the masked language modeling
self-supervised objective. First, instead of considering documents in
isolation, we pretrain over sets of multiple related documents, encouraging the
model to learn cross-document relationships. Second, we improve over recent
long-range transformers by introducing dynamic global attention that has access
to the entire input to predict masked tokens. We release CDLM (Cross-Document
Language Model), a new general language model for multi-document setting that
can be easily applied to downstream tasks. Our extensive analysis shows that
both ideas are essential for the success of CDLM, and work in synergy to set
new state-of-the-art results for several multi-text tasks. Code and models are
available at https://github.com/aviclu/CDLM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04670">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (LMs) such as GPT-3 have acquired a
surprising ability to perform zero-shot learning. For example, to classify
sentiment without any training examples, we can "prompt" the LM with the review
and the label description "Does the user like this movie?", and ask whether the
next word is "yes" or "no". However, the next word prediction training
objective is still misaligned with the target zero-shot learning objective. To
address this weakness, we propose meta-tuning, which directly optimizes the
zero-shot learning objective by fine-tuning pre-trained language models on a
collection of datasets. We focus on classification tasks, and construct the
meta-dataset by aggregating 43 existing datasets and annotating 441 label
descriptions in a question-answering (QA) format. When evaluated on unseen
tasks, meta-tuned models outperform a same-sized QA model and the previous SOTA
zero-shot learning system based on natural language inference. Additionally,
increasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,
and we forecast that even larger models would perform better. Therefore,
measuring zero-shot learning performance on language models out-of-the-box
might underestimate their true potential, and community-wide efforts on
aggregating datasets and unifying their formats can help build models that
answer prompts better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07650">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning has achieved promising results on some few-shot
classification tasks. The core idea of prompt-tuning is to insert text pieces,
i.e., templates, into the input and transform a classification task into a
masked language modeling problem. However, as for relation extraction,
determining the appropriate prompt template requires domain expertise. Single
label word handcrafted or auto-searched is cumbersome and time-consuming to
verify their effectiveness in non-few-shot scenarios. Further, there exist
abundant semantic knowledge among the entities and relation labels which cannot
be ignored. To this end, we focus on incorporating knowledge into prompt-tuning
for relation extraction and propose a knowledge-aware prompt-tuning with
synergistic optimization (KnowPrompt) approach. Specifically, we inject entity
and relation knowledge into prompt construction with learnable virtual template
words and answer words and jointly optimize their representation with knowledge
constraints. Extensive experimental results on five datasets with standard and
low-resource settings demonstrate the effectiveness of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sublanguage: A Serious Issue Affects Pretrained Models in Legal Domain. (arXiv:2104.07782v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07782">
<div class="article-summary-box-inner">
<span><p>Legal English is a sublanguage that is important for everyone but not for
everyone to understand. Pretrained models have become best practices among
current deep learning approaches for different problems. It would be a waste or
even a danger if these models were applied in practice without knowledge of the
sublanguage of the law. In this paper, we raise the issue and propose a trivial
solution by introducing BERTLaw a legal sublanguage pretrained model. The
paper's experiments demonstrate the superior effectiveness of the method
compared to the baseline pretrained model
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KI-BERT: Infusing Knowledge Context for Better Language and Domain Understanding. (arXiv:2104.08145v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08145">
<div class="article-summary-box-inner">
<span><p>Contextualized entity representations learned by state-of-the-art
transformer-based language models (TLMs) like BERT, GPT, T5, etc., leverage the
attention mechanism to learn the data context from training data corpus.
However, these models do not use the knowledge context. Knowledge context can
be understood as semantics about entities and their relationship with
neighboring entities in knowledge graphs. We propose a novel and effective
technique to infuse knowledge context from multiple knowledge graphs for
conceptual and ambiguous entities into TLMs during fine-tuning. It projects
knowledge graph embeddings in the homogeneous vector-space, introduces new
token-types for entities, aligns entity position ids, and a selective attention
mechanism. We take BERT as a baseline model and implement the
"Knowledge-Infused BERT" by infusing knowledge context from ConceptNet and
WordNet, which significantly outperforms BERT and other recent knowledge-aware
BERT variants like ERNIE, SenseBERT, and BERT_CS over eight different subtasks
of GLUE benchmark. The KI-BERT-base model even significantly outperforms
BERT-large for domain-specific tasks like SciTail and academic subsets of QQP,
QNLI, and MNLI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Extract, Denoise and Enforce: Evaluating and Improving Concept Preservation for Text-to-Text Generation. (arXiv:2104.08724v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08724">
<div class="article-summary-box-inner">
<span><p>Prior studies on text-to-text generation typically assume that the model
could figure out what to attend to in the input and what to include in the
output via seq2seq learning, with only the parallel training data and no
additional guidance. However, it remains unclear whether current models can
preserve important concepts in the source input, as seq2seq learning does not
have explicit focus on the concepts and commonly used evaluation metrics also
treat concepts equally important as other tokens. In this paper, we present a
systematic analysis that studies whether current seq2seq models, especially
pre-trained language models, are good enough for preserving important input
concepts and to what extent explicitly guiding generation with the concepts as
lexical constraints is beneficial. We answer the above questions by conducting
extensive analytical experiments on four representative text-to-text generation
tasks. Based on the observations, we then propose a simple yet effective
framework to automatically extract, denoise, and enforce important input
concepts as lexical constraints. This new method performs comparably or better
than its unconstrained counterpart on automatic metrics, demonstrates higher
coverage for concept preservation, and receives better ratings in the human
evaluation. Our code is available at https://github.com/morningmoni/EDE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Verdi: Quality Estimation and Error Detection for Bilingual Corpora. (arXiv:2105.14878v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.14878">
<div class="article-summary-box-inner">
<span><p>Translation Quality Estimation is critical to reducing post-editing efforts
in machine translation and to cross-lingual corpus cleaning. As a research
problem, quality estimation (QE) aims to directly estimate the quality of
translation in a given pair of source and target sentences, and highlight the
words that need corrections, without referencing to golden translations. In
this paper, we propose Verdi, a novel framework for word-level and
sentence-level post-editing effort estimation for bilingual corpora. Verdi
adopts two word predictors to enable diverse features to be extracted from a
pair of sentences for subsequent quality estimation, including a
transformer-based neural machine translation (NMT) model and a pre-trained
cross-lingual language model (XLM). We exploit the symmetric nature of
bilingual corpora and apply model-level dual learning in the NMT predictor,
which handles a primal task and a dual task simultaneously with weight sharing,
leading to stronger context prediction ability than single-direction NMT
models. By taking advantage of the dual learning scheme, we further design a
novel feature to directly encode the translated target information without
relying on the source context. Extensive experiments conducted on WMT20 QE
tasks demonstrate that our method beats the winner of the competition and
outperforms other baseline methods by a great margin. We further use the
sentence-level scores provided by Verdi to clean a parallel corpus and observe
benefits on both model performance and training efficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Coreference Resolution with Harmonized Annotations. (arXiv:2107.12088v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12088">
<div class="article-summary-box-inner">
<span><p>In this paper, we present coreference resolution experiments with a newly
created multilingual corpus CorefUD. We focus on the following languages:
Czech, Russian, Polish, German, Spanish, and Catalan. In addition to
monolingual experiments, we combine the training data in multilingual
experiments and train two joined models -- for Slavic languages and for all the
languages together. We rely on an end-to-end deep learning model that we
slightly adapted for the CorefUD corpus. Our results show that we can profit
from harmonized annotations, and using joined models helps significantly for
the languages with smaller training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Dataset for Answering Time-Sensitive Questions. (arXiv:2108.06314v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.06314">
<div class="article-summary-box-inner">
<span><p>Time is an important dimension in our physical world. Lots of facts can
evolve with respect to time. For example, the U.S. President might change every
four years. Therefore, it is important to consider the time dimension and
empower the existing QA models to reason over time. However, the existing QA
datasets contain rather few time-sensitive questions, hence not suitable for
diagnosing or benchmarking the model's temporal reasoning capability. In order
to promote research in this direction, we propose to construct a time-sensitive
QA dataset. The dataset is constructed by 1) mining time-evolving facts from
WikiData and align them to their corresponding Wikipedia page, 2) employing
crowd workers to verify and calibrate these noisy facts, 3) generating
question-answer pairs based on the annotated time-sensitive facts. Our dataset
poses challenges in the aspect of both temporal understanding and temporal
reasoning. We evaluate different SoTA long-document QA systems like BigBird and
FiD on our dataset. The best-performing model FiD can only achieve 46\%
accuracy, still far behind the human performance of 87\%. We demonstrate that
these models are still lacking the ability to perform consistent temporal
reasoning. Therefore, we believe that our dataset could serve as a benchmark to
develop NLP models more sensitive to temporal shift. The dataset and code are
released in~\url{https://github.com/wenhuchen/Time-Sensitive-QA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Affective Decoding for Empathetic Response Generation. (arXiv:2108.08102v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08102">
<div class="article-summary-box-inner">
<span><p>Understanding speaker's feelings and producing appropriate responses with
emotion connection is a key communicative skill for empathetic dialogue
systems. In this paper, we propose a simple technique called Affective Decoding
for empathetic response generation. Our method can effectively incorporate
emotion signals during each decoding step, and can additionally be augmented
with an auxiliary dual emotion encoder, which learns separate embeddings for
the speaker and listener given the emotion base of the dialogue. Extensive
empirical studies show that our models are perceived to be more empathetic by
human evaluations, in comparison to several strong mainstream methods for
empathetic responding.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features is dependent
upon each other. Experiment results on five public datasets show that our model
performs significantly better than previous approaches. In addition, contrary
to what previous work claims, our auxiliary experiments suggest that relation
prediction is contributory to named entity prediction in a non-negligible way.
The source code can be found at https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications since the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current algorithms can tackle such
problem reliably in a realistic scenario where zero OOD training data is
available. In this study, we propose ProtoInfoMax, a new architecture that
extends Prototypical Networks to simultaneously process In-Domain (ID) and OOD
sentences via Mutual Information Maximization (InfoMax) objective. Experimental
results show that our proposed method can substantially improve performance up
to 20% for OOD detection in low resource settings of text classification. We
also show that ProtoInfoMax is less prone to typical over-confidence Error of
Neural Networks, leading to more reliable ID and OOD prediction outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NEREL: A Russian Dataset with Nested Named Entities, Relations and Events. (arXiv:2108.13112v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13112">
<div class="article-summary-box-inner">
<span><p>In this paper, we present NEREL, a Russian dataset for named entity
recognition and relation extraction. NEREL is significantly larger than
existing Russian datasets: to date it contains 56K annotated named entities and
39K annotated relations. Its important difference from previous datasets is
annotation of nested named entities, as well as relations within nested
entities and at the discourse level. NEREL can facilitate development of novel
models that can extract relations between nested named entities, as well as
relations on both sentence and document levels. NEREL also contains the
annotation of events involving named entities and their roles in the events.
The NEREL collection is available via https://github.com/nerel-ds/NEREL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00627">
<div class="article-summary-box-inner">
<span><p>Contextual knowledge is important for real-world automatic speech recognition
(ASR) applications. In this paper, a novel tree-constrained pointer generator
(TCPGen) component is proposed that incorporates such knowledge as a list of
biasing words into both attention-based encoder-decoder and transducer
end-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing
words into an efficient prefix tree to serve as its symbolic input and creates
a neural shortcut between the tree and the final ASR output distribution to
facilitate recognising biasing words during decoding. Systems were trained and
evaluated on the Librispeech corpus where biasing words were extracted at the
scales of an utterance, a chapter, or a book to simulate different application
scenarios. Experimental results showed that TCPGen consistently improved word
error rates (WERs) compared to the baselines, and in particular, achieved
significant WER reductions on the biasing words. TCPGen is highly efficient: it
can handle 5,000 biasing words and distractors and only add a small overhead to
memory use and computation cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language Model Pre-Training. (arXiv:2109.00993v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00993">
<div class="article-summary-box-inner">
<span><p>Large Transformer-based language models such as BERT have led to broad
performance improvements on many NLP tasks. Domain-specific variants of these
models have demonstrated excellent performance on a variety of specialised
tasks. In legal NLP, BERT-based models have led to new state-of-the-art results
on multiple tasks. The exploration of these models has demonstrated the
importance of capturing the specificity of the legal language and its
vocabulary. However, such approaches suffer from high computational costs,
leading to a higher ecological impact and lower accessibility. Our findings,
focusing on English language legal text, show that lightweight LSTM-based
Language Models are able to capture enough information from a small legal text
pretraining corpus and achieve excellent performance on short legal text
classification tasks. This is achieved with a significantly reduced
computational overhead compared to BERT-based models. However, our method also
shows degraded performance on a more complex task, multi-label classification
of longer documents, highlighting the limitations of this lightweight approach.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-06 01:56:40.213777516 UTC">2021-09-06 01:56:40 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>