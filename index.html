<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-15T04:21:13.586220901Z">09-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-06 Why Rust for offensive security</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7750136c-b31b-4e85-ab94-ae1459b4150e">
<div class="article-summary-box-inner">
<span><h2>Why Rust for offensive security</h2>
<blockquote>
<p>想象一下：你的坦克都是纸板做的。然后你的飞机也都是用纸做的，你的海军也全都是纸船，那也太惨了吧？</p>
<p>虽然很荒唐，但是这就是现在的黑客技术的状态。</p>
<p>Imagine: all the tanks of your army are made of cardboard. Now imagine that not only your tanks but also all your airforce is composed of paper planes and your navy of paper vessels. It would be a pretty bad situation, don’t you think?</p>
<p>While it sounds absurd, this is the sad state of hacking today.</p>
</blockquote>
<h3>TL;DR</h3>
<p>文章指出，过去的编程语言（c, Java， python）等都只能局限在一个领域应用，然而现在我们等来了 Rust 救场——不再有奇怪的包管理器、二级制打包工具或者脆弱的网络代码，这些方面的可靠性一旦被黑客们意识到，就可能带来安全攻防的变革。</p>
<p>为了安利可靠的 Rust，作者还写了本书 <a href="https://academy.kerkour.com/black-hat-rust?coupon=BLOG" rel="noopener noreferrer">Black Hat Rust</a>, 来总结自己通过 Rust 在黑客技术中的实践，以其让读者少踩坑，更好地理解 Rust 的可靠。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-13 Rust 在 linux 内核中的最新进展</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=1c26513e-c4c2-4d52-becf-8c39379474e9">
<div class="article-summary-box-inner">
<span><h1>Rust 在 linux 内核中的最新进展</h1>
<p>虽然Rust编程语言在内核中使用的支持还没有登陆到本周末结束的 <code>Linux 5.15</code> 合并窗口，但这项工作仍在进行中。本周，关于Rust在Linux内核中的使用的最新进展被分享了出来。</p>
<p>作为Rust for Linux项目的主要开发人员之一，Miguel Ojeda在本周的Linaro Connect虚拟会议上介绍了该项目，他目前正在为谷歌的合同工作。</p>
<p>对周五的演讲感兴趣的人可以查看下面的 Presentation。</p>
<p><a href="https://bigthinkbuzz.com/the-latest-progress-on-rust-for-the-linux-kernel/" rel="noopener noreferrer">原文链接</a></p>
<p><a href="https://static.linaro.org/connect/lvc21f/presentations/LVC21F-317.pdf" rel="noopener noreferrer">Presentation地址</a></p>
<h1>Matchbox: Rust wasm 中的 p2p 网络解决方案</h1>
<p>Matchbox 的诞生是因为作者在<code>rust</code> 中制作了一款多人网页游戏，遇到了以下问题:</p>
<p>如何使用不可靠的、无序的 p2p connection 连接 N 个web浏览器?</p>
<p><a href="https://johanhelsing.studio/posts/introducing-matchbox" rel="noopener noreferrer">原文链接</a></p>
<h1>Learn Wgpu 更新了</h1>
<p><code>wgrpu</code> 是 <code>WebGPU API spec</code> 的 Rust 实现, 目前这个教程已经更新到了 0.10 版本, 有大量的原理和代码示例讲解.</p>
<p><a href="https://sotrh.github.io/learn-wgpu/beginner/tutorial2-surface/" rel="noopener noreferrer">原文链接</a></p>
<h1>Sycamore: v0.6.0 版本发布了</h1>
<p>Sycamore是一个用 Rust 和 WebAssembly 构建同构web应用程序的库. 目前发布了 0.6.0 版本了.</p>
<ul>
<li>静态生成</li>
<li>服务端渲染</li>
<li>重验证</li>
<li>增量构建</li>
<li>开放构建矩阵</li>
<li>CLI利用，让您轻松和自信地构建应用程序</li>
<li>充分利用 Fluent 开箱即用的 i18n 支持</li>
</ul>
<p><a href="https://sycamore-rs.netlify.app/news/announcing-v0.6.0" rel="noopener noreferrer">原文链接</a></p>
<p>--</p>
<p>From 日报小组 BobQin，FBI小白</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【成都】招聘Rust开发工程师</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ed86a028-a5b8-41e6-9849-e43a55ff7faf">
<div class="article-summary-box-inner">
<span><h2>Rust开发工程师招聘</h2>
<h3>岗位职责：</h3>
<ul>
<li>1、负责电商产品后端功能接口的开发；</li>
<li>2、负责电商产品业务功能开发、迭代和维护，对业务数据进行处理和分析；</li>
<li>3、配合前端开发完成功能的前后台功能联调；</li>
<li>4、配合完成产品测试，BUG修改。</li>
</ul>
<h3>任职要求：</h3>
<ul>
<li>1、后端开发语言基础扎实，有电商产品后端开发经验；</li>
<li>2、熟练使用使用Mysql关系型数据库；</li>
<li>3、至少了解并使用过RocketMQ、RabbitMQ、Kafka中的一种；</li>
<li>4、有Rust语言的基础，或者愿意转Rust开发；</li>
<li>5、三年以上的互联网开发工作经验；</li>
<li>6、熟习微服务或ServicesMesh架构者优先；</li>
</ul>
<p>工作地点四川成都环球时代中心
有意者请发邮件至：shaipe@sina.com 或直接添加微信号：shaipe</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">read_dir 返回的 io::Result<DirEntry> 会在什么情况下返回 Error 呢？</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c1f3f464-9146-4b43-8467-7eacc8f53bf8">
<div class="article-summary-box-inner">
<span><p><code>read_dir</code> 迭代的时候给到的是一个 <code>io::Result&lt;DirEntry&gt;</code>，文档里面只是简单说了 <strong>New errors may be encountered after an iterator is initially constructed.</strong></p>
<p>但是具体这个 New errors 到底是什么？</p>
<p>我试过了在迭代的时候对文件夹或者里面的文件作删除、重命名、改变权限，都没有返回 error；
（测试平台包括 Mac 和 Linux，没有 windows 暂时没测）。</p>
<p><img src="https://i.loli.net/2021/09/13/AbE9KdTL1sSxWJg.png" alt="screenshot-20210913-174925.png"></p>
<pre><code>/// Iterator over the entries in a directory.
///
/// This iterator is returned from the [`read_dir`] function of this module and
/// will yield instances of [`io::Result`]`&lt;`[`DirEntry`]`&gt;`. Through a [`DirEntry`]
/// information like the entry's path and possibly other metadata can be
/// learned.
///
/// The order in which this iterator returns entries is platform and filesystem
/// dependent.
///
/// # Errors
///
/// This [`io::Result`] will be an [`Err`] if there's some sort of intermittent
/// IO error during iteration.
#[stable(feature = "rust1", since = "1.0.0")]
#[derive(Debug)]
pub struct ReadDir(fs_imp::ReadDir);
</code></pre>
<p>This [<code>io::Result</code>] will be an [<code>Err</code>] if there's some sort of intermittent IO error during iteration.</p>
<p>看起来一定要是比较罕见的 IO 错误？</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">关于我前些天，在 GitHub 上 Rust 的 repo 的那些事</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=111d596f-f53f-44dc-9a59-69ccf2ff7563">
<div class="article-summary-box-inner">
<span><p>上周我在 GitHub 上整理的 <a href="https://github.com/0voice/Understanding_in_Rust" rel="noopener noreferrer">《 Rust 工程师枕边资料》</a> ,涉及了侵权行为。在这里向大家赔礼道歉。并且在第一时间，处理了相关内容。
我在整理的之前的初衷只是单纯为了给大家提供更好、更多、更全、更专业地的 Rust 学习资料。并没有丝毫的商业化手段。
我收集的内容全部来源于互联网，由于我的疏忽没有注明文章出处链接，确实是不应该的。</p>
<p>再一次，给作品的作者道歉。</p>
<p>我将在以后 repo 里将不会出现类似的错误事件，同时也希望广大开发者们监督。如果有任何问题，可以邮箱至：wchao_isvip@163.com ，我会在第一时间处理的。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">filecoin项目RUST大牛招聘</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=3a173b77-c75e-4cbb-a8c9-8c0496b17619">
<div class="article-summary-box-inner">
<span><p>该岗位薪资面谈
岗位职责：
1.参与区块链项目开发，以C++/rust为主；
2.理解业务逻辑与对后端服务的需求，能够分析需求并产生合理技术方案；
3.负责平台对外接口，相关数据服务的设计与实现；
4.根据技术需求部署Filecoin环境，编写脚本，对环境进行测试部署；
5.参与公司项目专利的编写；
岗位要求：
1.大专以上学历，计算机或者相关专业，精通rust语言；
2.至少熟悉两种其他开发语言，如C++、go、Python等；
3.2年以上后端开发工作经验，做过区块链项目开发经验的可优先考虑；
4.熟悉Ethereum、EOS、Bitcoin、Filecoin中至少两个项目的基本原理和设计；
5.熟悉区块链项目中常见的共识机制、加密算法、P2P网络等；
6.思路清晰，具备良好的沟通能力、团队合作意识，能抗压，能主动承担，乐于分享。</p>
<p>详情可联系yhcaozyyz@qq.com or 18109055866</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【招聘 杭州，上海】Rust开发工程师（30K-50K）</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=4f8da484-d0fc-4f56-88ca-c19e7ea32b5a">
<div class="article-summary-box-inner">
<span><p>【岗位职责】</p>
<ol>
<li>负责分布式计算及存储系统的高可扩展后端系统，服务和API；</li>
<li>设计高性能、高可靠性的服务，建立快速、稳定、安全的后端代码&nbsp;；</li>
<li>为其他开发人员提供指导，参与算法设计和实现。</li>
<li>负责设计和优化协议、弱网通信、存储、网络并发、并行计算、加密以及安全等；</li>
<li>保证工程质量和开发效率。</li>
<li>设计和维护性能测试用例；</li>
</ol>
<p>【岗位要求】</p>
<ol>
<li>计算机或者相关专业本科以上学历，两年以上相关工作经验</li>
<li>技术扎实，熟悉Rust语言编程</li>
<li>理解ownership, trait, async等语言机制。</li>
<li>熟练使用tokio。熟练使用rust常用库</li>
<li>有丰富的多线程应用和平台构建经验，可熟练构建稳定、高效率和安全的代码&nbsp;；</li>
<li>有强烈的上进心和求知欲，善于学习和运用新知识，善于沟通和逻辑表达，有强烈的团队意识和执行力。</li>
<li>熟悉Linux下多线程/多进程编程模型，进程间通讯，消息事件通知，同步/异步。</li>
<li>熟悉Linux下内存管理机制，低延迟、高并发无锁化编程。</li>
</ol>
<p>【特别备注】</p>
<ol>
<li>了解安全加密相关算法者优先&nbsp;；</li>
<li>有丰富的c++、python编程经验者优先</li>
<li>参与大型系统的开发，并成功部署、广泛应用者优先；</li>
<li>熟悉大数据、机器学习框架，如:spark，flink, tensorflow者优先。</li>
</ol>
<p>【工作地点】
base1: 杭州市西湖区中电万谷园区
base2: 上海市浦东新区前滩东方广场一期
杭州上海均有岗位。</p>
<p>联系方式：朝歌13732914991（微信同号） 邮箱：zhaoge@fudata.cn</p>
<p>【公司介绍】
上海富数科技有限公司 简称“富数科技”，是国内领先的金融AI和安全计算技术领跑者，核心团队来自CapitalOne，Alibaba和IBM，公司自2016年成立以来受国内顶级风投青睐，已完成C轮融资。富数科技坚持以“以数据安全驱动人工智能”，依托于安全计算和机器学习AI技术，助力金融和各行业机构组织提高智能风控、营销和运营的效率，实现数据合规安全地融合计算和价值流通。</p>
<p>富数科技是中国通信标准化协会会员、工信部信通院大数据安全及流通标准组成员、安全多方计算标准参与方，为行业规范标准制定贡献创新技术成果。富数科技结合最新密码学和区块链技术研发创新，其安全计算和联邦学习开创性地采用“松弛迭代法”，在智能合约、ML算法优化、代码编译和计算硬件芯片融合方面改善性能，在同等条件下实现了收敛速度的大幅提升，精度和准确度损失低于1%，速度较行业水平提高了3倍。</p>
<p>富数科技致力于驱动安全可信的人工智能科技与各行业场景的深度融合赋能，在兼顾隐私保护下发挥大数据的商业价值。富数科技自2017年投入数据安全计算领域研发创新，拥有多项专利发明和软著，并与国内外金融机构和科研机构（上海交大等）联合研发和推动工程化商业化落地。富数科技安全计算解决方案已经落地在智能风控、智能营销、监管和科研统计分析、异业或同业数据安全融合计算等场景，目前已在银行、持牌消金、政务、医疗、运营商等领域积累上百案例，在安全的机器学习领域具有突出的领先优势。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust 日报】2021-09-12 Rust 的 Logging 推荐</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=91b91a60-cbe9-4f8a-a5ec-8825da34457b">
<div class="article-summary-box-inner">
<span><h3>Rust 的 Logging 推荐</h3>
<p>内容整理自 Reddit 的讨论：<a href="https://www.reddit.com/r/rust/comments/pmdh6a/what_is_the_current_recommendation_for_logging_in/" rel="noopener noreferrer">What is the current recommendation for logging in Rust? : rust</a>。</p>
<p>问题简述：除了标准的 <code>log</code>，还有不少选择：<code>env_logger</code>，<code>tracing</code>，<code>slog</code>，<code>simplelog</code> 等等，最佳实践是什么？</p>
<p>来自 <a href="https://www.reddit.com/user/Koxiaet/" rel="noopener noreferrer">Koxiaet</a> 的答复：通常有两类与日志相关的 crate：日志接口和日志消费者。接口提供了想要记录某些东西时调用的函数，消费者处理将结构化日志数据格式化到某个地方（stderr 或文件）。两个主要的接口是 <code>log</code> 和 <code>tracing</code>，后者功能更强大因为它支持结构化日志记录，但前者更普遍。还有另一个结构化日志接口 slog，比 <code>tracing</code> 更古老但用的较少。每个日志接口都有自己生态系统，可以根据自己的需要选择。如果在写一个库，<code>log</code> 是个不错的选择，因为所有的日志记录接口都与它兼容。但如果你确实需要结构化日志记录，则可以改用 <code>tracing</code>，这取决于你的需求，比如你是需要写到文件还是只是终端。</p>
<p>其他网友的推荐：</p>
<ul>
<li>File Logging：<a href="https://github.com/emabee/flexi_logger" rel="noopener noreferrer">emabee/flexi_logger: A flexible logger for rust programs that can write to stderr or to log files</a>。（来自 cfsamson）</li>
<li><code>tracing</code> 的接口：<a href="https://docs.rs/tracing-log/0.1.2/tracing_log/" rel="noopener noreferrer">tracing_log - Rust</a>，有多个同时操作交错日志消息时特别方便，可以按某些属性对它们进行分组并单独查看它们。（来自 class_two_perversion）</li>
<li><a href="https://github.com/estk/log4rs" rel="noopener noreferrer">estk/log4rs: A highly configurable logging framework for Rust</a>，log4rs 是一个高度可配置的日志框架，以 Java 的 Logback 和 log4j 库为模型。通过 Yaml 配置，到 sdout 和文件，带有文件大小限制选项，还可以配置不同级别的日志。（来自 tms102）</li>
<li><a href="https://crates.io/crates/tracing-appender" rel="noopener noreferrer">tracing-appender - crates.io: Rust Package Registry</a>，推荐者所知道的唯一线程外日志记录解决方案，不仅适用于异步应用程序。（来自 Pand9）</li>
<li><a href="https://github.com/daboross/fern" rel="noopener noreferrer">daboross/fern: Simple, efficient logging for Rust</a>，像 Python 的 <code>logging</code> 和 JS 的 <code>Winston</code>。（来自 RapBeautician）</li>
</ul>
<h3>Rust 全栈</h3>
<p>本文是一篇博客翻译，来自：<a href="https://www.justinm.one/blog/2021/09/11/fullstackrust/" rel="noopener noreferrer">Full Stack Rust - Blog</a>。</p>
<p>一年前，我的首选语言如下：</p>
<ul>
<li>Python 用于高级代码快速原型设计，或用于需要第三方功能的代码</li>
<li>C/C++ 用于长期的 low-level 项目</li>
</ul>
<p>当时只听过 Rust 并简单使用过，我的经验来自用 Rust 写了一个处理大文件（&gt;4GB）的事务并从中挖掘一些统计信息的小工具。我用了一个库将文件映射到内存，缤瑞按照顺序对其进行分析。有一些很酷的概念，比如编译器静态地强制内存映射在它被取消映射后无法访问——如果你不小心，C++ 中可能就会发生这种错误。</p>
<p>不过当时并没有真正吸引我，因为那只是一个小新奇。当我向 <a href="https://github.com/DrChat/pdblister" rel="noopener noreferrer">pdblister</a> 添加新功能以并行获取数千个 PDB 文件时诀窍来了。由于 GIL，在 CPython 中几乎不可能，而在 C/C++ 中做到不面临并行错误是极其困难的。然而 Rust 让这变得容易。我添加了 tokio 驱动的异步，使用 <code>tokio::spawn</code> 生成新任务来下载 PDB，并修复了编译器报的错误，它可以正常工作了。Rust 编译器输出一个二进制文件，它可以在任何地方运行，没有运行时依赖。</p>
<p><strong>取代 Python</strong></p>
<p>这是第一点，Rust 是 Python 作为中长期工具语言的绝佳替代品。Python 的好处是庞大的库和生态系统，通过 pip 可以直接拿到，想要快速制作与 API 交互的原型，可以使用 <code>requests</code>，只要 <code>import requests</code> 就可以使用了。Rust 的 <code>reqwest </code> 也是如此，只要输入 <code>cargo add reqwest</code> 就可以在代码中使用它。</p>
<p>然而当进入更长期的生命周期时，Python 就显示出劣势，<code>requests</code> 是程序的依赖，用户需要后去后才能使用。此外，由于弱类型和错误处理能力（与 Rust 比），Python 变得更加劣势。这一点上，我可以使用 Rust 比使用 Python 更快地编写原型工具，并且我可以自信地知道我的工具比等效的 Python 更易于维护且寿命更长。但是，对于短期工具，Python 可能仍然更好，因为它不需要启动项目即可在 VSCode 中获得智能感知支持。 Rust 的 cargo-script 接近将 Rust 推入脚本语言的领域，但不幸的是，我还没有在 VSCode 中找到与之集成的插件。</p>
<p><strong>取代 C</strong></p>
<p>Rust 也是 C 的直接替代品，它在各方面都更好，并且可以与遗留 C 代码原生互操作以进行增量替换。Rust 最大的改进是生态系统：如上所述，利用 Rust 生态中已有的库是很容易的。如果你从未使用过 C，那很幸运，实际上 C 中使用高级功能的最佳方法是自己写。</p>
<p>C 生态系统是支离破碎的，而且很脆弱。ABI 或构建系统没有一致的标准：</p>
<ul>
<li>由于缺乏 ABI 一致性，你不能跨平台或操作系统使用相同的二进制文件。 所以你必须从源代码构建。</li>
<li>由于缺乏一致的构建系统，你不能简单地和应用程序一起构建 C 库，必须修补或重写要使其与你的库兼容的库的构建系统。</li>
<li>C 库很少跨平台兼容，因为它们缺乏可以依赖的共享抽象。</li>
</ul>
<p>然后还有 Rust 最特色的安全改进——我就不展开了。但根据我的经验 - 安全性在很大程度上是一种工具，可以让第三方库开发人员更容易强迫我正确使用他们的库，这是 C 库不能做的事情。</p>
<p><strong>全栈 Rust</strong></p>
<p>总而言之，在过去的一年中，我一直在堆栈的所有部分使用 Rust，而我之前使用过其他语言。我已经使用 Rust 来实现引导加载程序：<a href="https://github.com/xenia-project/xell-rs" rel="noopener noreferrer">xenia-project/xell-rs: Xell Bootloader, rewritten in Rust because ¯_(ツ)_/¯，</a>我已经使用它通过 <a href="https://github.com/DrChat/pdblister" rel="noopener noreferrer">pdblister</a> 和 <a href="https://github.com/panamax-rs/panamax" rel="noopener noreferrer">panamax</a> 中的高级 HTTP/HTTPS 和其他技术来镜像文件。我利用并贡献了优秀的 <a href="https://github.com/DrChat/gdbstub" rel="noopener noreferrer">gdbstub</a> 库，用于控制由自定义 VMM 运行的 VM。这些项目都是在堆栈的不同级别完成的，而 Rust 非常适合所有级别。 我已经开始在我的个人项目中专门使用 Rust，并在适合的时候推动它在我的工作中使用。</p>
<h3>tagged_cell：快速、可初始化和线程安全的静态变量</h3>
<p>通过 <code>TaggedCell</code> 和 <code>Tag</code> 类型实现，为了安全操作，<code>TaggedCell</code> 的每个实例都必须是唯一的。然后必须通过 <code>TaggedCell::init ()</code> 初始化 <code>TaggedCell</code>，它使用用户提供的函数或闭包初始化底层数据，然后返回一个特殊的零大小的 <code>Init&lt;Tag&gt;</code> 用于访问 Cell 的数据。为了确保每个单元格使用唯一的标签类型，<code>tagged_cell!</code> 提供宏。该宏根据变量的名称创建一个新的标记类型，并将其应用到声明中。</p>
<pre><code>use tagged_cell::tagged_cell;
tagged_cell!{
   static BAR: TaggedCell&lt;Vec&lt;usize&gt;, _&gt; = TaggedCell::new();
}

let tag = BAR.init(|| vec![0, 10, 20]);
let vec = BAR.get(tag);

assert_eq!(vec[2], 20);
</code></pre>
<p>为了允许跨线程使用，只有第一次调用 <code>TaggedCell::init</code> 才会初始化 Cell 的数据。所有未来的 <code>TaggedCell::init</code> 调用都将返回一个新标签。未确定哪个线程将初始化 Cell 的数据。</p>
<pre><code>use std::thread;
use tagged_cell::tagged_cell;

tagged_cell!{
    static TABLE: TaggedCell&lt;Vec&lt;usize&gt;, _&gt; = TaggedCell::new();
}

thread::spawn(move || {
    let tag = TABLE.init(|| vec![0, 10, 20]);
    let table = TABLE.get(tag);
    assert_eq!(table[2], 20);
});

thread::spawn(move || {
    let tag = TABLE.init(|| vec![0, 10, 20]);
    let table = TABLE.get(tag);
    assert_eq!(table[1], 10);
});
</code></pre>
<p>GitHub：<a href="https://github.com/Dasch0/tagged_cell" rel="noopener noreferrer">Dasch0/tagged_cell: Fast, initializable, and thread safe static variables</a></p>
<h3>ukanren-rs：µKanren 的 Rust 实现</h3>
<p>µKanren 是一种轻量级关系编程语言</p>
<ul>
<li>原始的 Schema 实现在这里：<a href="https://github.com/jasonhemann/microKanren" rel="noopener noreferrer">jasonhemann/microKanren: The implementation of microKanren, a featherweight relational programming language</a></li>
<li>相关参考：<a href="http://minikanren.org/" rel="noopener noreferrer">miniKanren.org</a></li>
</ul>
<pre><code>use ukanren::*;

fn appendo(first: Value, second: Value, out: Value) -&gt; BoxedGoal&lt;impl Iterator&lt;Item = State&gt;&gt; {
    eq(&amp;first, &amp;())
        .and(eq(&amp;second, &amp;out))
        .or(fresh(move |a: Value, d: Value, res: Value| {
            eq(&amp;(a.clone(), d.clone()), &amp;first)
                .and(eq(&amp;(a.clone(), res.clone()), &amp;out))
                .and(appendo(d.clone(), second.clone(), res))
        }))
        .boxed()
}

let goal = fresh(|x, y| appendo(x, y, [1, 2, 3, 4, 5].to_value()));
assert_eq!(
    goal.run(2).collect::&lt;Vec&lt;_&gt;&gt;(),
    vec![
        state![(), [1, 2, 3, 4, 5]],
        state![[1], [2, 3, 4, 5]],
        state![[1, 2], [3, 4, 5]],
        state![[1, 2, 3], [4, 5]],
        state![[1, 2, 3, 4], [5]],
        state![[1, 2, 3, 4, 5], ()],
    ],
);
</code></pre>
<p>GitHub：<a href="https://github.com/ekzhang/ukanren-rs" rel="noopener noreferrer">ekzhang/ukanren-rs: Rust implementation of µKanren, a featherweight relational programming language.</a></p>
<h3>rust-counter-strings：快速定位字符串位置</h3>
<p>字符串中的每个星号都出现在由紧接前面的数字指定的位置。因此，29 后面的星号是该字符串中的第 29 个字符。可以在任何地方砍掉字符串的末尾，并且确切地知道它在哪里被剪掉了。比如不用数就知道字符串 <code>2*4*6*8*11*14*17*2</code> 正好有 18 个字符。当处理 50 万个字符时会比较省事。</p>
<pre><code>$ ./rust-counter-strings 50
# 2*4*6*8*11*14*17*20*23*26*29*32*35*38*41*44*47*50*
</code></pre>
<p>这就是个小工具，代码也只有几十行。</p>
<p>GitHub：<a href="https://github.com/thomaschaplin/rust-counter-strings" rel="noopener noreferrer">thomaschaplin/rust-counter-strings: 🧵 Generate self-describing strings of a given length to help aid software testing</a></p>
<hr>
<p>From 日报小组 长琴</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc 论坛：支持 rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust 语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-11 Tabled 发布v0.3, bma-benchmark, ferros, Veloren发布v0.11</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=db077b1a-5af6-4fb2-b065-ff8be974fd62">
<div class="article-summary-box-inner">
<span><h3>Tabled 发布v0.3</h3>
<p>Tabled 是一个易于使用的库，用于美化 Rust 结构和枚举的输出。</p>
<p>Github<a href="https://github.com/zhiburt/tabled" rel="noopener noreferrer">链接</a>，https://github.com/zhiburt/tabled</p>
<h3>bma-benchmark 一个友好的基准测试工具</h3>
<p>使用 <code>bma_benchmark</code></p>
<pre><code>#[macro_use]
extern crate bma_benchmark;

use std::sync::Mutex;

let n = 100_000_000;
let mutex = Mutex::new(0);
benchmark_start!();
for _ in 0..n {
    let _a = mutex.lock().unwrap();
}
benchmark_print!(n);
</code></pre>
<p>使用宏 <code>benchmark!</code></p>
<pre><code>#[macro_use]
extern crate bma_benchmark;

use std::sync::Mutex;

let mutex = Mutex::new(0);
benchmark!(100_000_000, {
    let _a = mutex.lock().unwrap();
    });
</code></pre>
<p><img src="https://raw.githubusercontent.com/alttch/bma-benchmark/main/simple.png" alt="结果"></p>
<p>Crate <a href="https://crates.io/crates/bma-benchmark" rel="noopener noreferrer">链接</a>，https://crates.io/crates/bma-benchmark</p>
<h3>ferros</h3>
<p>seL4 是一个用于构建操作系统和嵌入式程序的工具包，这个开源项目是使 Rust 中的 seL4 编程变得更好。</p>
<p>以下代码演练假定使用示例 sel4_start 库执行 selfe，并介绍了 ferros 的某些方面。</p>
<pre><code>use selfe_sys;
use ferros::alloc::{self, micro_alloc, smart_alloc};
use ferros::userland::{root_cnode, BootInfo};

// The raw boot info is provided by the sel4_start library
let raw_boot_info: &amp;'static selfe_sys::seL4_BootInfo = unsafe { &amp;*sel4_start::BOOTINFO };


// Utility for finding and claiming `Untyped` instances supplied by the boot info.
let mut allocator = micro_alloc::Allocator::bootstrap(&amp;raw_boot_info)?;
let initial_untyped = allocator
    .get_untyped::&lt;U20&gt;() // The size of the Untyped instance, as bits
    .expect("Couldn't find an untyped instance of the desired size");

// Create the top-level CNode wrapper with type-level-tracked remaining slot capacity
let (root_cnode, local_slots) = root_cnode(&amp;raw_boot_info);

// Once we have an initial Untyped instance, memory distribution from it
// can be tracked with compile-time checks. The smart_alloc macro synthesizes
// the allocation code, and the capacity bounds are statically verified by
// the type checker. The effect is that you can write 'slots' in the macro body 
// anywhere you need some slots, and you'll get the right number allocated
// with type inference. A reference to 'ut' does the same for untyped memory. 
smart_alloc!(|slots from local_slots, ut from uts| {

    // Create a page table seL4 kernel object and return a capability pointer to it.
    // Here we use a variable binding type annotation and Rust's type system can figure out
    // if it can allocate a large enough Untyped instance and enough cnode slots
    // to represent this particular kernel object.
    let example_page_table: LocalCap&lt;UnmappedPageTable&gt; = retype(ut, slots)?;

    // Create a resource-tracking wrapper around the raw boot info to assist in
    // virtual memory related operations.
    let boot_info  = BootInfo::wrap(raw_boot_info, ut, slots);
    let (root_page_table, boot_info) = boot_info.map_page_table(root_page_table)?;
});

</code></pre>
<p>Github<a href="https://github.com/auxoncorp/ferros" rel="noopener noreferrer">链接</a>，https://github.com/auxoncorp/ferros</p>
<h3>Veloren发布v0.11</h3>
<p>今天，Veloren 发布了 0.11。 这个版本已经制作了 3 个月，其一大重点是让世界各地的战斗更具活力。这是以新的地点系统的形式出现，以及 NPC 和生物如何与世界互动。</p>
<p>要了解还有哪些新功能！请继续阅读 V0.11 变更日志<a href="https://veloren.net/release-0-11/" rel="noopener noreferrer">链接</a>，https://veloren.net/release-0-11/</p>
<hr>
<p>From 日报小组 <a href="https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929" rel="noopener noreferrer">洋芋</a></p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">cargo fix 如何自动修复warning？</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=0d747849-2064-4ba2-a725-2fe64f6ab556">
<div class="article-summary-box-inner">
<span><p>代码中有些外面copy过来的enum，导致很多的“should have an upper camel case name”warning。就是编码风格的问题，可是<code>cargo fix</code>没有办法按照rust给的建议自动帮我改掉。。。。</p>
<p>有办法吗？</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b">
<div class="article-summary-box-inner">
<span><p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>
<ol>
<li>认识面向基础架构语言</li>
<li>理解 Rust 所有权</li>
<li>通过实战理解 Rust 宏</li>
<li>通过 Datafuse 理解全链路跟踪</li>
<li>Rust 异步编程入门 Future Part 1</li>
<li>Rust 异步编程入门 Future Part 2</li>
</ol>
<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。
本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>
<hr>
<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>
<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>
<p>分享讲师： 董泽润</p>
<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现
讲师介绍：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<hr>
<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>
<p>分享时间： 周日晚上2021-09-12 20:00-21:00</p>
<p>分享讲师： 苏林</p>
<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>
<ol>
<li>利用 Tokio 实现一个 Mini Http server</li>
<li>基于 Http server提供内容动态的 API 网关</li>
<li>利用 Redis 实现对 API 网关加速</li>
<li>学习 Rust RPC 调用，实现微服务调用</li>
</ol>
<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<p>Rust 异步编程入门 Future Part 1 | Vol. 5
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<p>Rust 异步编程入门 Future Part 2 | Vol. 6
https://www.bilibili.com/video/bv1oy4y1G7jC</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust 学习随笔</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8">
<div class="article-summary-box-inner">
<span><h1>切换镜像源</h1>
<p>crm =&gt; https://github.com/wtklbm/crm</p>
<p>常用命令就是 <code>crm best</code></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">pretree 补全文档发布了,再次谢谢大神的指点终于入门了。</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827">
<div class="article-summary-box-inner">
<span><h1>Pretree</h1>
<p>pretree is a package for storing and querying routing rules with prefix tree .</p>
<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>
<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>
<p>Inspired by <a href="https://github.com/obity/pretree" rel="noopener noreferrer">obity/pretree</a> (golang)</p>
<h1>Doc</h1>
<p>See this document at <a href="https://docs.rs/pretree" rel="noopener noreferrer">API documentation</a></p>
<h1>Install</h1>
<p>Add the following line to your Cargo.toml file:</p>
<pre><code>pretree = "1.0.0"
</code></pre>
<h1>Example</h1>
<pre><code>use pretree::Pretree;
let mut p = Pretree::new();
p.store("GET","account/{id}/info/:name");
p.store("GET","account/:id/login");
p.store("GET","account/{id}");
p.store("GET","bacteria/count_number_by_month");
let (ok,rule,vars) = p.query("GET","account/929239");
println!("ok:{} rule:{} vars:{:#?}",ok,rule,vars);

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>
<p><strong>课程时间:</strong> 2021年9月5日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识, Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>
<h3>课程大纲</h3>
<p>1、回顾 Rust 异步编程模型.</p>
<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>
<p>3、Tokio 介绍.</p>
<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>
<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/
Rust 异步编程入门 Future Part 1 回放地址：
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课：《 Rust 异步编程入门 Future 》|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>
<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是 Rust 异步编程的核心基础。</p>
<h3>课程大纲</h3>
<p>1、为什么需要异步.</p>
<p>2、理解异步编程模型.</p>
<p>3、Future 编程模型讲解.</p>
<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>
<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>异步引擎 C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D 游戏引擎</h3>
<ul>
<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>
<li><strong>延迟着色</strong></li>
<li><strong>内置保存/加载</strong></li>
<li><strong>独立场景编辑器</strong></li>
<li><strong>高级物理模型</strong></li>
<li><strong>分层模型资源</strong></li>
<li><strong>几何实例化</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>
<p><strong>课程时间：</strong> 2021年8月22日 20:30-21:30</p>
<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>
<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>
<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>
<h3>课程大纲</h3>
<ol>
<li>
<p>什么是分布式追踪系统OpenTracing及应用场景</p>
</li>
<li>
<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>
</li>
<li>
<p>为什么需要tokio-rs/tracing库</p>
</li>
<li>
<p>演示Datafuse项目中tokio-rs/tracing的使用</p>
</li>
</ol>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">论坛github账户无法登录解决笔记</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>有反映这两天github账户无法登录了。</p>
<p>报这个错：</p>
<pre><code>get github user info err
</code></pre>
<p>查了几个地方：</p>
<ol>
<li>代码是否运行正常：Ok</li>
<li>https代理是否正常：Ok</li>
<li>检查了github返回日志，发现是：</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>进入这个地址一看：<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>
<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>
<p>特此记录。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 的 Future 与 Javascript 的 Promise 功能对照参考</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>
<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>
<blockquote>
<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* 什么都不做 */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被 （1）跨线程传递 （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：《通过实战理解 Rust 宏》| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>
<p><strong>课程时间：</strong> 2021年8月15日 20:30-21:30</p>
<p><strong>课程介绍：</strong></p>
<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>这就是通过宏实现配置的统一行为，代码参考：
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>
<h3>课程大纲</h3>
<ul>
<li>什么是 Rust 宏</li>
<li>什么是宏运行原理</li>
<li>如何创建 Rust 宏过程</li>
<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>
</ul>
<p><strong>讲师介绍</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-15T01:30:00Z">09-15</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">The Emergence of the Shape Bias Results from Communicative Efficiency. (arXiv:2109.06232v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06232">
<div class="article-summary-box-inner">
<span><p>By the age of two, children tend to assume that new word categories are based
on objects' shape, rather than their color or texture; this assumption is
called the shape bias. They are thought to learn this bias by observing that
their caregiver's language is biased towards shape based categories. This
presents a chicken and egg problem: if the shape bias must be present in the
language in order for children to learn it, how did it arise in language in the
first place? In this paper, we propose that communicative efficiency explains
both how the shape bias emerged and why it persists across generations. We
model this process with neural emergent language agents that learn to
communicate about raw pixelated images. First, we show that the shape bias
emerges as a result of efficient communication strategies employed by agents.
Second, we show that pressure brought on by communicative need is also
necessary for it to persist across generations; simply having a shape bias in
an agent's input language is insufficient. These results suggest that, over and
above the operation of other learning strategies, the shape bias in human
learners may emerge and be sustained by communicative pressures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language Models via Knowledge Distillation. (arXiv:2109.06243v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06243">
<div class="article-summary-box-inner">
<span><p>The development of over-parameterized pre-trained language models has made a
significant contribution toward the success of natural language processing.
While over-parameterization of these models is the key to their generalization
power, it makes them unsuitable for deployment on low-capacity devices. We push
the limits of state-of-the-art Transformer-based pre-trained language model
compression using Kronecker decomposition. We use this decomposition for
compression of the embedding layer, all linear mappings in the multi-head
attention, and the feed-forward network modules in the Transformer layer. We
perform intermediate-layer knowledge distillation using the uncompressed model
as the teacher to improve the performance of the compressed model. We present
our KroneckerBERT, a compressed version of the BERT_BASE model obtained using
this framework. We evaluate the performance of KroneckerBERT on well-known NLP
benchmarks and show that for a high compression factor of 19 (5% of the size of
the BERT_BASE model), our KroneckerBERT outperforms state-of-the-art
compression methods on the GLUE. Our experiments indicate that the proposed
model has promising out-of-distribution robustness and is superior to the
state-of-the-art compression methods on SQuAD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation. (arXiv:2109.06253v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06253">
<div class="article-summary-box-inner">
<span><p>Neural Machine Translation (NMT) is known to suffer from a beam-search
problem: after a certain point, increasing beam size causes an overall drop in
translation quality. This effect is especially pronounced for long sentences.
While much work was done analyzing this phenomenon, primarily for
autoregressive NMT models, there is still no consensus on its underlying cause.
In this work, we analyze errors that cause major quality degradation with large
beams in NMT and Automatic Speech Recognition (ASR). We show that a factor that
strongly contributes to the quality degradation with large beams is
\textit{dataset length-bias} - \textit{NMT datasets are strongly biased towards
short sentences}. To mitigate this issue, we propose a new data augmentation
technique -- \textit{Multi-Sentence Resampling (MSR)}. This technique extends
the training examples by concatenating several sentences from the original
dataset to make a long training example. We demonstrate that MSR significantly
reduces degradation with growing beam size and improves final translation
quality on the IWSTL$15$ En-Vi, IWSTL$17$ En-Fr, and WMT$14$ En-De datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Multiway Multilingual NMT in the Turkic Languages. (arXiv:2109.06262v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06262">
<div class="article-summary-box-inner">
<span><p>Despite the increasing number of large and comprehensive machine translation
(MT) systems, evaluation of these methods in various languages has been
restrained by the lack of high-quality parallel corpora as well as engagement
with the people that speak these languages. In this study, we present an
evaluation of state-of-the-art approaches to training and evaluating MT systems
in 22 languages from the Turkic language family, most of which being extremely
under-explored. First, we adopt the TIL Corpus with a few key improvements to
the training and the evaluation sets. Then, we train 26 bilingual baselines as
well as a multi-way neural MT (MNMT) model using the corpus and perform an
extensive analysis using automatic metrics as well as human evaluations. We
find that the MNMT model outperforms almost all bilingual baselines in the
out-of-domain test sets and finetuning the model on a downstream task of a
single pair also results in a huge performance boost in both low- and
high-resource scenarios. Our attentive analysis of evaluation criteria for MT
models in Turkic languages also points to the necessity for further research in
this direction. We release the corpus splits, test sets as well as models to
the public.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Post-OCR Document Correction with large Ensembles of Character Sequence Models. (arXiv:2109.06264v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06264">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a novel method based on character
sequence-to-sequence models to correct documents already processed with Optical
Character Recognition (OCR) systems. The main contribution of this paper is a
set of strategies to accurately process strings much longer than the ones used
to train the sequence model while being sample- and resource-efficient,
supported by thorough experimentation. The strategy with the best performance
involves splitting the input document in character n-grams and combining their
individual corrections into the final output using a voting scheme that is
equivalent to an ensemble of a large number of sequence models. We further
investigate how to weigh the contributions from each one of the members of this
ensemble. We test our method on nine languages of the ICDAR 2019 competition on
post-OCR text correction and achieve a new state-of-the-art performance in five
of them. Our code for post-OCR correction is shared at
https://github.com/jarobyte91/post_ocr_correction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">STraTA: Self-Training with Task Augmentation for Better Few-shot Learning. (arXiv:2109.06270v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06270">
<div class="article-summary-box-inner">
<span><p>Despite their recent successes in tackling many NLP tasks, large-scale
pre-trained language models do not perform as well in few-shot settings where
only a handful of training examples are available. To address this shortcoming,
we propose STraTA, which stands for Self-Training with Task Augmentation, an
approach that builds on two key ideas for effective leverage of unlabeled data.
First, STraTA uses task augmentation, a novel technique that synthesizes a
large amount of data for auxiliary-task fine-tuning from target-task unlabeled
texts. Second, STraTA performs self-training by further fine-tuning the strong
base model created by task augmentation on a broad distribution of
pseudo-labeled data. Our experiments demonstrate that STraTA can substantially
improve sample efficiency across 12 few-shot benchmarks. Remarkably, on the
SST-2 sentiment dataset, STraTA, with only 8 training examples per class,
achieves comparable results to standard fine-tuning with 67K training examples.
Our analyses reveal that task augmentation and self-training are both
complementary and independently effective.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks. (arXiv:2109.06275v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06275">
<div class="article-summary-box-inner">
<span><p>An ideal integration of autonomous agents in a human world implies that they
are able to collaborate on human terms. In particular, theory of mind plays an
important role in maintaining common ground during human collaboration and
communication. To enable theory of mind modeling in situated interactions, we
introduce a fine-grained dataset of collaborative tasks performed by pairs of
human subjects in the 3D virtual blocks world of Minecraft. It provides
information that captures partners' beliefs of the world and of each other as
an interaction unfolds, bringing abundant opportunities to study human
collaborative behaviors in situated language communication. As a first step
towards our goal of developing embodied AI agents able to infer belief states
of collaborative partners in situ, we build and present results on
computational models for several theory of mind tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Algorithms for Multiparallel Word Alignment. (arXiv:2109.06283v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06283">
<div class="article-summary-box-inner">
<span><p>With the advent of end-to-end deep learning approaches in machine
translation, interest in word alignments initially decreased; however, they
have again become a focus of research more recently. Alignments are useful for
typological research, transferring formatting like markup to translated texts,
and can be used in the decoding of machine translation systems. At the same
time, massively multilingual processing is becoming an important NLP scenario,
and pretrained language and machine translation models that are truly
multilingual are proposed. However, most alignment algorithms rely on bitexts
only and do not leverage the fact that many parallel corpora are multiparallel.
In this work, we exploit the multiparallelity of corpora by representing an
initial set of bilingual alignments as a graph and then predicting additional
edges in the graph. We present two graph algorithms for edge prediction: one
inspired by recommender systems and one based on network link prediction. Our
experimental results show absolute improvements in $F_1$ of up to 28% over the
baseline bilingual word aligner in different datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration. (arXiv:2109.06304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06304">
<div class="article-summary-box-inner">
<span><p>Phrase representations derived from BERT often do not exhibit complex phrasal
compositionality, as the model relies instead on lexical similarity to
determine semantic relatedness. In this paper, we propose a contrastive
fine-tuning objective that enables BERT to produce more powerful phrase
embeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal
paraphrases, which is automatically generated using a paraphrase generation
model, as well as a large-scale dataset of phrases in context mined from the
Books3 corpus. Phrase-BERT outperforms baselines across a variety of
phrase-level similarity tasks, while also demonstrating increased lexical
diversity between nearest neighbors in the vector space. Finally, as a case
study, we show that Phrase-BERT embeddings can be easily integrated with a
simple autoencoder to build a phrase-based neural topic model that interprets
topics as mixtures of words and phrases by performing a nearest neighbor search
in the embedding space. Crowdsourced evaluations demonstrate that this
phrase-based topic model produces more coherent and meaningful topics than
baseline word and phrase-level topic models, further validating the utility of
Phrase-BERT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Catastrophic Forgetting in Scheduled Sampling with Elastic Weight Consolidation in Neural Machine Translation. (arXiv:2109.06308v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06308">
<div class="article-summary-box-inner">
<span><p>Despite strong performance in many sequence-to-sequence tasks, autoregressive
models trained with maximum likelihood estimation suffer from exposure bias,
i.e. a discrepancy between the ground-truth prefixes used during training and
the model-generated prefixes used at inference time. Scheduled sampling is a
simple and often empirically successful approach which addresses this issue by
incorporating model-generated prefixes into the training process. However, it
has been argued that it is an inconsistent training objective leading to models
ignoring the prefixes altogether. In this paper, we conduct systematic
experiments and find that it ameliorates exposure bias by increasing model
reliance on the input sequence. We also observe that as a side-effect, it
worsens performance when the model-generated prefix is correct, a form of
catastrophic forgetting. We propose using Elastic Weight Consolidation as
trade-off between mitigating exposure bias and retaining output quality.
Experiments on two IWSLT'14 translation tasks demonstrate that our approach
alleviates catastrophic forgetting and significantly improves BLEU compared to
standard scheduled sampling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Constraints and Descriptive Segmentation for Subevent Detection. (arXiv:2109.06316v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06316">
<div class="article-summary-box-inner">
<span><p>Event mentions in text correspond to real-world events of varying degrees of
granularity. The task of subevent detection aims to resolve this granularity
issue, recognizing the membership of multi-granular events in event complexes.
Since knowing the span of descriptive contexts of event complexes helps infer
the membership of events, we propose the task of event-based text segmentation
(EventSeg) as an auxiliary task to improve the learning for subevent detection.
To bridge the two tasks together, we propose an approach to learning and
enforcing constraints that capture dependencies between subevent detection and
EventSeg prediction, as well as guiding the model to make globally consistent
inference. Specifically, we adopt Rectifier Networks for constraint learning
and then convert the learned constraints to a regularization term in the loss
function of the neural model. Experimental results show that the proposed
method outperforms baseline methods by 2.3% and 2.5% on benchmark datasets for
subevent detection, HiEve and IC, respectively, while achieving a decent
performance on EventSeg prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space. (arXiv:2109.06324v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06324">
<div class="article-summary-box-inner">
<span><p>In cross-lingual language models, representations for many different
languages live in the same space. Here, we investigate the linguistic and
non-linguistic factors affecting sentence-level alignment in cross-lingual
pretrained language models for 101 languages and 5,050 language pairs. Using
BERT-based LaBSE and BiLSTM-based LASER as our models, and the Bible as our
corpus, we compute a task-based measure of cross-lingual alignment in the form
of bitext retrieval performance, as well as four intrinsic measures of vector
space alignment and isomorphism. We then examine a range of linguistic,
quasi-linguistic, and training-related features as potential predictors of
these alignment metrics. The results of our analyses show that word order
agreement and agreement in morphological complexity are two of the strongest
linguistic predictors of cross-linguality. We also note in-family training data
as a stronger predictor than language-specific training data across the board.
We verify some of our linguistic findings by looking at the effect of
morphological segmentation on English-Inuktitut alignment, in addition to
examining the effect of word order agreement on isomorphism for 66 zero-shot
language pairs from a different corpus. We make the data and code for our
experiments publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Transferability of BERT Models on Uralic Languages. (arXiv:2109.06327v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06327">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models such as BERT have outperformed previous
models on a large number of English benchmarks, but their evaluation is often
limited to English or a small number of well-resourced languages. In this work,
we evaluate monolingual, multilingual, and randomly initialized language models
from the BERT family on a variety of Uralic languages including Estonian,
Finnish, Hungarian, Erzya, Moksha, Karelian, Livvi, Komi Permyak, Komi Zyrian,
Northern S\'ami, and Skolt S\'ami. When monolingual models are available
(currently only et, fi, hu), these perform better on their native language, but
in general they transfer worse than multilingual models or models of
genetically unrelated languages that share the same character set. Remarkably,
straightforward transfer of high-resource models, even without special efforts
toward hyperparameter optimization, yields what appear to be state of the art
POS and NER tools for the minority Uralic languages where there is sufficient
data for finetuning.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Old BERT, New Tricks: Artificial Language Learning for Pre-Trained Language Models. (arXiv:2109.06333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06333">
<div class="article-summary-box-inner">
<span><p>We extend the artificial language learning experimental paradigm from
psycholinguistics and apply it to pre-trained language models -- specifically,
BERT (Devlin et al., 2019). We treat the model as a subject in an artificial
language learning experimental setting: in order to learn the relation between
two linguistic properties A and B, we introduce a set of new, non-existent,
linguistic items, give the model information about their variation along
property A, then measure to what extent the model learns property B for these
items as a result of training. We show this method at work for degree modifiers
(expressions like "slightly", "very", "rather", "extremely") and test the
hypothesis that the degree expressed by modifiers (low, medium or high degree)
is related to their sensitivity to sentence polarity (whether they show
preference for affirmative or negative sentences or neither). Our experimental
results are compatible with existing linguistic observations that relate degree
semantics to polarity-sensitivity, including the main one: low degree semantics
leads to positive polarity sensitivity (that is, to preference towards
affirmative contexts). The method can be used in linguistics to elaborate on
hypotheses and interpret experimental results, as well as for more insightful
evaluation of linguistic representations in language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning. (arXiv:2109.06349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06349">
<div class="article-summary-box-inner">
<span><p>In this work, we focus on a more challenging few-shot intent detection
scenario where many intents are fine-grained and semantically similar. We
present a simple yet effective few-shot intent detection schema via contrastive
pre-training and fine-tuning. Specifically, we first conduct self-supervised
contrastive pre-training on collected intent datasets, which implicitly learns
to discriminate semantically similar utterances without using any labels. We
then perform few-shot intent detection together with supervised contrastive
learning, which explicitly pulls utterances from the same intent closer and
pushes utterances across different intents farther. Experimental results show
that our proposed method achieves state-of-the-art performance on three
challenging intent detection datasets under 5-shot and 10-shot settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty-Aware Machine Translation Evaluation. (arXiv:2109.06352v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06352">
<div class="article-summary-box-inner">
<span><p>Several neural-based metrics have been recently proposed to evaluate machine
translation quality. However, all of them resort to point estimates, which
provide limited information at segment level. This is made worse as they are
trained on noisy, biased and scarce human judgements, often resulting in
unreliable quality predictions. In this paper, we introduce uncertainty-aware
MT evaluation and analyze the trustworthiness of the predicted quality. We
combine the COMET framework with two uncertainty estimation methods, Monte
Carlo dropout and deep ensembles, to obtain quality scores along with
confidence intervals. We compare the performance of our uncertainty-aware MT
evaluation methods across multiple language pairs from the QT21 dataset and the
WMT20 metrics task, augmented with MQM annotations. We experiment with varying
numbers of references and further discuss the usefulness of uncertainty-aware
quality estimation (without references) to flag possibly critical translation
mistakes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hunspell for Sorani Kurdish Spell Checking and Morphological Analysis. (arXiv:2109.06374v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06374">
<div class="article-summary-box-inner">
<span><p>Spell checking and morphological analysis are two fundamental tasks in text
and natural language processing and are addressed in the early stages of the
development of language technology. Despite the previous efforts, there is no
progress in open-source to create such tools for Sorani Kurdish, also known as
Central Kurdish, as a less-resourced language. In this paper, we present our
efforts in annotating a lexicon with morphosyntactic tags and also, extracting
morphological rules of Sorani Kurdish to build a morphological analyzer, a
stemmer and a spell-checking system using Hunspell. This implementation can be
used for further developments in the field by researchers and also, be
integrated into text editors under a publicly available license.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation. (arXiv:2109.06379v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06379">
<div class="article-summary-box-inner">
<span><p>Natural language generation (NLG) spans a broad range of tasks, each of which
serves for specific objectives and desires different properties of generated
text. The complexity makes automatic evaluation of NLG particularly
challenging. Previous work has typically focused on a single task and developed
individual evaluation metrics based on specific intuitions. In this paper, we
propose a unifying perspective based on the nature of information change in NLG
tasks, including compression (e.g., summarization), transduction (e.g., text
rewriting), and creation (e.g., dialog). Information alignment between input,
context, and output text plays a common central role in characterizing the
generation. With automatic alignment prediction models, we develop a family of
interpretable metrics that are suitable for evaluating key aspects of different
NLG tasks, often without need of gold reference data. Experiments show the
uniformly designed metrics achieve stronger or comparable correlations with
human judgement compared to state-of-the-art metrics in each of diverse tasks,
including text summarization, style transfer, and knowledge-grounded dialog.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rationales for Sequential Predictions. (arXiv:2109.06387v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06387">
<div class="article-summary-box-inner">
<span><p>Sequence models are a critical component of modern NLP systems, but their
predictions are difficult to explain. We consider model explanations though
rationales, subsets of context that can explain individual model predictions.
We find sequential rationales by solving a combinatorial optimization: the best
rationale is the smallest subset of input tokens that would predict the same
output as the full sequence. Enumerating all subsets is intractable, so we
propose an efficient greedy algorithm to approximate this objective. The
algorithm, which is called greedy rationalization, applies to any model. For
this approach to be effective, the model should form compatible conditional
distributions when making predictions on incomplete subsets of the context.
This condition can be enforced with a short fine-tuning step. We study greedy
rationalization on language modeling and machine translation. Compared to
existing baselines, greedy rationalization is best at optimizing the
combinatorial objective and provides the most faithful rationales. On a new
dataset of annotated sequential rationales, greedy rationales are most similar
to human rationales.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos. (arXiv:2109.06398v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06398">
<div class="article-summary-box-inner">
<span><p>We address the problem of temporal sentence localization in videos (TSLV).
Traditional methods follow a top-down framework which localizes the target
segment with pre-defined segment proposals. Although they have achieved decent
performance, the proposals are handcrafted and redundant. Recently, bottom-up
framework attracts increasing attention due to its superior efficiency. It
directly predicts the probabilities for each frame as a boundary. However, the
performance of bottom-up model is inferior to the top-down counterpart as it
fails to exploit the segment-level interaction. In this paper, we propose an
Adaptive Proposal Generation Network (APGN) to maintain the segment-level
interaction while speeding up the efficiency. Specifically, we first perform a
foreground-background classification upon the video and regress on the
foreground frames to adaptively generate proposals. In this way, the
handcrafted proposal design is discarded and the redundant proposals are
decreased. Then, a proposal consolidation module is further developed to
enhance the semantic of the generated proposals. Finally, we locate the target
moments with these generated proposals following the top-down framework.
Extensive experiments on three challenging benchmarks show that our proposed
APGN significantly outperforms previous state-of-the-art methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding. (arXiv:2109.06400v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06400">
<div class="article-summary-box-inner">
<span><p>A key solution to temporal sentence grounding (TSG) exists in how to learn
effective alignment between vision and language features extracted from an
untrimmed video and a sentence description. Existing methods mainly leverage
vanilla soft attention to perform the alignment in a single-step process.
However, such single-step attention is insufficient in practice, since
complicated relations between inter- and intra-modality are usually obtained
through multi-step reasoning. In this paper, we propose an Iterative Alignment
Network (IA-Net) for TSG task, which iteratively interacts inter- and
intra-modal features within multiple steps for more accurate grounding.
Specifically, during the iterative reasoning process, we pad multi-modal
features with learnable parameters to alleviate the nowhere-to-attend problem
of non-matched frame-word pairs, and enhance the basic co-attention mechanism
in a parallel manner. To further calibrate the misaligned attention caused by
each reasoning step, we also devise a calibration module following each
attention module to refine the alignment knowledge. With such iterative
alignment scheme, our IA-Net can robustly capture the fine-grained relations
between vision and language domains step-by-step for progressively reasoning
the temporal boundaries. Extensive experiments conducted on three challenging
benchmarks demonstrate that our proposed model performs better than the
state-of-the-arts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Personality and Online Social Engagement: An Investigation of MBTI Users on Twitter. (arXiv:2109.06402v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06402">
<div class="article-summary-box-inner">
<span><p>Text-based personality prediction by computational models is an emerging
field with the potential to significantly improve on key weaknesses of
survey-based personality assessment. We investigate 3848 profiles from Twitter
with self-labeled Myers-Briggs personality traits (MBTI) - a framework closely
related to the Five Factor Model of personality - to better understand how
text-based digital traces from social engagement online can be used to predict
user personality traits. We leverage BERT, a state-of-the-art NLP architecture
based on deep learning, to analyze various sources of text that hold most
predictive power for our task. We find that biographies, statuses, and liked
tweets contain significant predictive power for all dimensions of the MBTI
system. We discuss our findings and their implications for the validity of the
MBTI and the lexical hypothesis, a foundational theory underlying the Five
Factor Model that links language use and behavior. Our results hold optimistic
implications for personality psychologists, computational linguists, and other
social scientists aiming to predict personality from observational text data
and explore the links between language and core behavioral traits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction. (arXiv:2109.06415v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06415">
<div class="article-summary-box-inner">
<span><p>Low-resource Relation Extraction (LRE) aims to extract relation facts from
limited labeled corpora when human annotation is scarce. Existing works either
utilize self-training scheme to generate pseudo labels that will cause the
gradual drift problem, or leverage meta-learning scheme which does not solicit
feedback explicitly. To alleviate selection bias due to the lack of feedback
loops in existing LRE learning paradigms, we developed a Gradient Imitation
Reinforcement Learning method to encourage pseudo label data to imitate the
gradient descent direction on labeled data and bootstrap its optimization
capability through trial and error. We also propose a framework called GradLRE,
which handles two major scenarios in low-resource relation extraction. Besides
the scenario where unlabeled data is sufficient, GradLRE handles the situation
where no unlabeled data is available, by exploiting a contextualized
augmentation method to generate data. Experimental results on two public
datasets demonstrate the effectiveness of GradLRE on low resource relation
extraction when comparing with baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-document Event Identity via Dense Annotation. (arXiv:2109.06417v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06417">
<div class="article-summary-box-inner">
<span><p>In this paper, we study the identity of textual events from different
documents. While the complex nature of event identity is previously studied
(Hovy et al., 2013), the case of events across documents is unclear. Prior work
on cross-document event coreference has two main drawbacks. First, they
restrict the annotations to a limited set of event types. Second, they
insufficiently tackle the concept of event identity. Such annotation setup
reduces the pool of event mentions and prevents one from considering the
possibility of quasi-identity relations. We propose a dense annotation approach
for cross-document event coreference, comprising a rich source of event
mentions and a dense annotation effort between related document pairs. To this
end, we design a new annotation workflow with careful quality control and an
easy-to-use annotation interface. In addition to the links, we further collect
overlapping event contexts, including time, location, and participants, to shed
some light on the relation between identity decisions and context. We present
an open-access dataset for cross-document event coreference, CDEC-WN, collected
from English Wikinews and open-source our annotation toolkit to encourage
further research on cross-document tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Commonsense-Focused Dialogues for Response Generation: An Empirical Study. (arXiv:2109.06427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06427">
<div class="article-summary-box-inner">
<span><p>Smooth and effective communication requires the ability to perform latent or
explicit commonsense inference. Prior commonsense reasoning benchmarks (such as
SocialIQA and CommonsenseQA) mainly focus on the discriminative task of
choosing the right answer from a set of candidates, and do not involve
interactive language generation as in dialogue. Moreover, existing dialogue
datasets do not explicitly focus on exhibiting commonsense as a facet. In this
paper, we present an empirical study of commonsense in dialogue response
generation. We first auto-extract commonsensical dialogues from existing
dialogue datasets by leveraging ConceptNet, a commonsense knowledge graph.
Furthermore, building on social contexts/situations in SocialIQA, we collect a
new dialogue dataset with 25K dialogues aimed at exhibiting social commonsense
in an interactive setting. We evaluate response generation models trained using
these datasets and find that models trained on both extracted and our collected
data produce responses that consistently exhibit more commonsense than
baselines. Finally we propose an approach for automatic evaluation of
commonsense that relies on features derived from ConceptNet and pre-trained
language and dialog models, and show reasonable correlation with human
evaluation of responses' commonsense quality. We are releasing a subset of our
collected data, Commonsense-Dialogues, containing about 11K dialogs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">YES SIR!Optimizing Semantic Space of Negatives with Self-Involvement Ranker. (arXiv:2109.06436v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06436">
<div class="article-summary-box-inner">
<span><p>Pre-trained model such as BERT has been proved to be an effective tool for
dealing with Information Retrieval (IR) problems. Due to its inspiring
performance, it has been widely used to tackle with real-world IR problems such
as document ranking. Recently, researchers have found that selecting "hard"
rather than "random" negative samples would be beneficial for fine-tuning
pre-trained models on ranking tasks. However, it remains elusive how to
leverage hard negative samples in a principled way. To address the
aforementioned issues, we propose a fine-tuning strategy for document ranking,
namely Self-Involvement Ranker (SIR), to dynamically select hard negative
samples to construct high-quality semantic space for training a high-quality
ranking model. Specifically, SIR consists of sequential compressors implemented
with pre-trained models. Front compressor selects hard negative samples for
rear compressor. Moreover, SIR leverages supervisory signal to adaptively
adjust semantic space of negative samples. Finally, supervisory signal in rear
compressor is computed based on condition probability and thus can control
sample dynamic and further enhance the model performance. SIR is a lightweight
and general framework for pre-trained models, which simplifies the ranking
process in industry practice. We test our proposed solution on MS MARCO with
document ranking setting, and the results show that SIR can significantly
improve the ranking performance of various pre-trained models. Moreover, our
method became the new SOTA model anonymously on MS MARCO Document ranking
leaderboard in May 2021.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncovering Implicit Gender Bias in Narratives through Commonsense Inference. (arXiv:2109.06437v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06437">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models learn socially harmful biases from their training
corpora, and may repeat these biases when used for generation. We study gender
biases associated with the protagonist in model-generated stories. Such biases
may be expressed either explicitly ("women can't park") or implicitly (e.g. an
unsolicited male character guides her into a parking space). We focus on
implicit biases, and use a commonsense reasoning engine to uncover them.
Specifically, we infer and analyze the protagonist's motivations, attributes,
mental states, and implications on others. Our findings regarding implicit
biases are in line with prior work that studied explicit biases, for example
showing that female characters' portrayal is centered around appearance, while
male figures' focus on intellect.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding. (arXiv:2109.06466v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06466">
<div class="article-summary-box-inner">
<span><p>Task-adaptive pre-training (TAPT) and Self-training (ST) have emerged as the
major semi-supervised approaches to improve natural language understanding
(NLU) tasks with massive amount of unlabeled data. However, it's unclear
whether they learn similar representations or they can be effectively combined.
In this paper, we show that TAPT and ST can be complementary with simple TFS
protocol by following TAPT -&gt; Finetuning -&gt; Self-training (TFS) process.
Experimental results show that TFS protocol can effectively utilize unlabeled
data to achieve strong combined gains consistently across six datasets covering
sentiment classification, paraphrase identification, natural language
inference, named entity recognition and dialogue slot classification. We
investigate various semi-supervised settings and consistently show that gains
from TAPT and ST can be strongly additive by following TFS procedure. We hope
that TFS could serve as an important semi-supervised baseline for future NLP
studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Untrustworthy Samples: Data Filtering for Open-domain Dialogues with Bayesian Optimization. (arXiv:2109.06471v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06471">
<div class="article-summary-box-inner">
<span><p>Being able to reply with a related, fluent, and informative response is an
indispensable requirement for building high-quality conversational agents. In
order to generate better responses, some approaches have been proposed, such as
feeding extra information by collecting large-scale datasets with human
annotations, designing neural conversational models (NCMs) with complex
architecture and loss functions, or filtering out untrustworthy samples based
on a dialogue attribute, e.g., Relatedness or Genericness. In this paper, we
follow the third research branch and present a data filtering method for
open-domain dialogues, which identifies untrustworthy samples from training
data with a quality measure that linearly combines seven dialogue attributes.
The attribute weights are obtained via Bayesian Optimization (BayesOpt) that
aims to optimize an objective function for dialogue generation iteratively on
the validation set. Then we score training samples with the quality measure,
sort them in descending order, and filter out those at the bottom. Furthermore,
to accelerate the "filter-train-evaluate" iterations involved in BayesOpt on
large-scale datasets, we propose a training framework that integrates maximum
likelihood estimation (MLE) and negative training method (NEG). The training
method updates parameters of a trained NCMs on two small sets with newly
maintained and removed samples, respectively. Specifically, MLE is applied to
maximize the log-likelihood of newly maintained samples, while NEG is used to
minimize the log-likelihood of newly removed ones. Experimental results on two
datasets show that our method can effectively identify untrustworthy samples,
and NCMs trained on the filtered datasets achieve better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Logic-level Evidence Retrieval and Graph-based Verification Network for Table-based Fact Verification. (arXiv:2109.06480v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06480">
<div class="article-summary-box-inner">
<span><p>Table-based fact verification task aims to verify whether the given statement
is supported by the given semi-structured table. Symbolic reasoning with
logical operations plays a crucial role in this task. Existing methods leverage
programs that contain rich logical information to enhance the verification
process. However, due to the lack of fully supervised signals in the program
generation process, spurious programs can be derived and employed, which leads
to the inability of the model to catch helpful logical operations. To address
the aforementioned problems, in this work, we formulate the table-based fact
verification task as an evidence retrieval and reasoning framework, proposing
the Logic-level Evidence Retrieval and Graph-based Verification network
(LERGV). Specifically, we first retrieve logic-level program-like evidence from
the given table and statement as supplementary evidence for the table. After
that, we construct a logic-level graph to capture the logical relations between
entities and functions in the retrieved evidence, and design a graph-based
verification network to perform logic-level graph-based reasoning based on the
constructed graph to classify the final entailment relation. Experimental
results on the large-scale benchmark TABFACT show the effectiveness of the
proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate. (arXiv:2109.06481v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06481">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive neural machine translation (NART) models suffer from the
multi-modality problem which causes translation inconsistency such as token
repetition. Most recent approaches have attempted to solve this problem by
implicitly modeling dependencies between outputs. In this paper, we introduce
AligNART, which leverages full alignment information to explicitly reduce the
modality of the target distribution. AligNART divides the machine translation
task into $(i)$ alignment estimation and $(ii)$ translation with aligned
decoder inputs, guiding the decoder to focus on simplified one-to-one
translation. To alleviate the alignment estimation problem, we further propose
a novel alignment decomposition method. Our experiments show that AligNART
outperforms previous non-iterative NART models that focus on explicit modality
reduction on WMT14 En$\leftrightarrow$De and WMT16 Ro$\rightarrow$En.
Furthermore, AligNART achieves BLEU scores comparable to those of the
state-of-the-art connectionist temporal classification based models on WMT14
En$\leftrightarrow$De. We also observe that AligNART effectively addresses the
token repetition problem even without sequence-level knowledge distillation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilevel profiling of situation and dialogue-based deep networks for movie genre classification using movie trailers. (arXiv:2109.06488v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06488">
<div class="article-summary-box-inner">
<span><p>Automated movie genre classification has emerged as an active and essential
area of research and exploration. Short duration movie trailers provide useful
insights about the movie as video content consists of the cognitive and the
affective level features. Previous approaches were focused upon either
cognitive or affective content analysis. In this paper, we propose a novel
multi-modality: situation, dialogue, and metadata-based movie genre
classification framework that takes both cognition and affect-based features
into consideration. A pre-features fusion-based framework that takes into
account: situation-based features from a regular snapshot of a trailer that
includes nouns and verbs providing the useful affect-based mapping with the
corresponding genres, dialogue (speech) based feature from audio, metadata
which together provides the relevant information for cognitive and affect based
video analysis. We also develop the English movie trailer dataset (EMTD), which
contains 2000 Hollywood movie trailers belonging to five popular genres:
Action, Romance, Comedy, Horror, and Science Fiction, and perform
cross-validation on the standard LMTD-9 dataset for validating the proposed
framework. The results demonstrate that the proposed methodology for movie
genre classification has performed excellently as depicted by the F1 scores,
precision, recall, and area under the precision-recall curves.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">conSultantBERT: Fine-tuned Siamese Sentence-BERT for Matching Jobs and Job Seekers. (arXiv:2109.06501v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06501">
<div class="article-summary-box-inner">
<span><p>In this paper we focus on constructing useful embeddings of textual
information in vacancies and resumes, which we aim to incorporate as features
into job to job seeker matching models alongside other features. We explain our
task where noisy data from parsed resumes, heterogeneous nature of the
different sources of data, and crosslinguality and multilinguality present
domain-specific challenges.
</p>
<p>We address these challenges by fine-tuning a Siamese Sentence-BERT (SBERT)
model, which we call conSultantBERT, using a large-scale, real-world, and high
quality dataset of over 270,000 resume-vacancy pairs labeled by our staffing
consultants. We show how our fine-tuned model significantly outperforms
unsupervised and supervised baselines that rely on TF-IDF-weighted feature
vectors and BERT embeddings. In addition, we find our model successfully
matches cross-lingual and multilingual textual content.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tribrid: Stance Classification with Neural Inconsistency Detection. (arXiv:2109.06508v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06508">
<div class="article-summary-box-inner">
<span><p>We study the problem of performing automatic stance classification on social
media with neural architectures such as BERT. Although these architectures
deliver impressive results, their level is not yet comparable to the one of
humans and they might produce errors that have a significant impact on the
downstream task (e.g., fact-checking). To improve the performance, we present a
new neural architecture where the input also includes automatically generated
negated perspectives over a given claim. The model is jointly learned to make
simultaneously multiple predictions, which can be used either to improve the
classification of the original perspective or to filter out doubtful
predictions. In the first case, we propose a weakly supervised method for
combining the predictions into a final one. In the second case, we show that
using the confidence scores to remove doubtful predictions allows our method to
achieve human-like performance over the retained information, which is still a
sizable part of the original input.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation. (arXiv:2109.06513v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06513">
<div class="article-summary-box-inner">
<span><p>Dialog grounding enables conversational models to make full use of external
information to establish multiple desired qualities, such as knowledgeable,
engaging and empathetic. However, naturally grounded dialog corpora are usually
not directly available, which puts forward requirements for the few-shot
learning ability of conversational models. Motivated by recent advances in
pre-trained language models and prompt-based learning, in this paper we explore
prompt-based few-shot learning for grounded dialog generation (GDG). We first
formulate the prompt construction for GDG tasks, based on which we then conduct
comprehensive empirical analysis on two common types of prompting methods:
template-based prompting and soft-prompting. We demonstrate the potential of
prompt-based methods in few-shot learning for GDG and provide directions of
improvement for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Netmarble AI Center's WMT21 Automatic Post-Editing Shared Task Submission. (arXiv:2109.06515v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06515">
<div class="article-summary-box-inner">
<span><p>This paper describes Netmarble's submission to WMT21 Automatic Post-Editing
(APE) Shared Task for the English-German language pair. First, we propose a
Curriculum Training Strategy in training stages. Facebook Fair's WMT19 news
translation model was chosen to engage the large and powerful pre-trained
neural networks. Then, we post-train the translation model with different
levels of data at each training stages. As the training stages go on, we make
the system learn to solve multiple tasks by adding extra information at
different training stages gradually. We also show a way to utilize the
additional data in large volume for APE tasks. For further improvement, we
apply Multi-Task Learning Strategy with the Dynamic Weight Average during the
fine-tuning stage. To fine-tune the APE corpus with limited data, we add some
related subtasks to learn a unified representation. Finally, for better
performance, we leverage external translations as augmented machine translation
(MT) during the post-training and fine-tuning. As experimental results show,
our APE system significantly improves the translations of provided MT results
by -2.848 and +3.74 on the development dataset in terms of TER and BLEU,
respectively. It also demonstrates its effectiveness on the test dataset with
higher quality than the development dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Sampling of Dependency Structures. (arXiv:2109.06521v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06521">
<div class="article-summary-box-inner">
<span><p>Probabilistic distributions over spanning trees in directed graphs are a
fundamental model of dependency structure in natural language processing,
syntactic dependency trees. In NLP, dependency trees often have an additional
root constraint: only one edge may emanate from the root. However, no sampling
algorithm has been presented in the literature to account for this additional
constraint. In this paper, we adapt two spanning tree sampling algorithms to
faithfully sample dependency trees from a graph subject to the root constraint.
Wilson (1996)'s sampling algorithm has a running time of $\mathcal{O}(H)$ where
$H$ is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm
has a running time of $\mathcal{O}(N^3)$, which is often greater than the mean
hitting time of a directed graph. Additionally, we build upon Colbourn's
algorithm and present a novel extension that can sample $K$ trees without
replacement in $\mathcal{O}(K N^3 + K^2 N)$ time. To the best of our knowledge,
no algorithm has been given for sampling spanning trees without replacement
from a directed graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks. (arXiv:2109.06524v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06524">
<div class="article-summary-box-inner">
<span><p>Loading models pre-trained on the large-scale corpus in the general domain
and fine-tuning them on specific downstream tasks is gradually becoming a
paradigm in Natural Language Processing. Previous investigations prove that
introducing a further pre-training phase between pre-training and fine-tuning
phases to adapt the model on the domain-specific unlabeled data can bring
positive effects. However, most of these further pre-training works just keep
running the conventional pre-training task, e.g., masked language model, which
can be regarded as the domain adaptation to bridge the data distribution gap.
After observing diverse downstream tasks, we suggest that different tasks may
also need a further pre-training phase with appropriate training tasks to
bridge the task formulation gap. To investigate this, we carry out a study for
improving multiple task-oriented dialogue downstream tasks through designing
various tasks at the further pre-training phase. The experiment shows that
different downstream tasks prefer different further pre-training tasks, which
have intrinsic correlation and most further pre-training tasks significantly
improve certain target tasks rather than all. Our investigation indicates that
it is of great importance and effectiveness to design appropriate further
pre-training tasks modeling specific information that benefit downstream tasks.
Besides, we present multiple constructive empirical conclusions for enhancing
task-oriented dialogues.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Bill Similarity with Annotated and Augmented Corpora of Bills. (arXiv:2109.06527v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06527">
<div class="article-summary-box-inner">
<span><p>Bill writing is a critical element of representative democracy. However, it
is often overlooked that most legislative bills are derived, or even directly
copied, from other bills. Despite the significance of bill-to-bill linkages for
understanding the legislative process, existing approaches fail to address
semantic similarities across bills, let alone reordering or paraphrasing which
are prevalent in legal document writing. In this paper, we overcome these
limitations by proposing a 5-class classification task that closely reflects
the nature of the bill generation process. In doing so, we construct a
human-labeled dataset of 4,721 bill-to-bill relationships at the
subsection-level and release this annotated dataset to the research community.
To augment the dataset, we generate synthetic data with varying degrees of
similarity, mimicking the complex bill writing process. We use BERT variants
and apply multi-stage training, sequentially fine-tuning our models with
synthetic and human-labeled datasets. We find that the predictive performance
significantly improves when training with both human-labeled and synthetic
data. Finally, we apply our trained model to infer section- and bill-level
similarities. Our analysis shows that the proposed methodology successfully
captures the similarities across legal documents at various levels of
aggregation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder. (arXiv:2109.06536v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06536">
<div class="article-summary-box-inner">
<span><p>Recent work has proposed several efficient approaches for generating
gradient-based adversarial perturbations on embeddings and proved that the
model's performance and robustness can be improved when they are trained with
these contaminated embeddings. While they paid little attention to how to help
the model to learn these adversarial samples more efficiently. In this work, we
focus on enhancing the model's ability to defend gradient-based adversarial
attack during the model's training process and propose two novel adversarial
training approaches: (1) CARL narrows the original sample and its adversarial
sample in the representation space while enlarging their distance from
different labeled samples. (2) RAR forces the model to reconstruct the original
sample from its adversarial representation. Experiments show that the proposed
two approaches outperform strong baselines on various text classification
datasets. Analysis experiments find that when using our approaches, the
semantic representation of the input sentence won't be significantly affected
by adversarial perturbations, and the model's performance drops less under
adversarial attack. That is to say, our approaches can effectively improve the
robustness of the model. Besides, RAR can also be used to generate text-form
adversarial samples.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Challenging Instances are Worth Learning: Generating Valuable Negative Samples for Response Selection Training. (arXiv:2109.06538v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06538">
<div class="article-summary-box-inner">
<span><p>Retrieval-based chatbot selects the appropriate response from candidates
according to the context, which heavily depends on a response selection module.
A response selection module is generally a scoring model to evaluate candidates
and is usually trained on the annotated positive response and sampled negative
responses. Sampling negative responses lead to two risks: a). The sampled
negative instances, especially that from random sampling methods, are mostly
irrelevant to the dialogue context and too easy to be fitted at the training
stage while causing a weak model in the real scenario. b). The so-called
negative instances may be positive, which is known as the fake negative
problem. To address the above issue, we employ pre-trained language models,
such as the DialoGPT to construct more challenging negative instances to
enhance the model robustness. Specifically, we provide garbled context to the
pre-trained model to generate responses and filter the fake negative ones. In
this way, our negative instances are fluent, context-related, and more
challenging for the model to learn, while can not be positive. Extensive
experiments show that our method brings significant and stable improvements on
the dialogue response selection capacity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Talking Space: inference from spatial linguistic meanings. (arXiv:2109.06554v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06554">
<div class="article-summary-box-inner">
<span><p>This paper concerns the intersection of natural language and the physical
space around us in which we live, that we observe and/or imagine things within.
Many important features of language have spatial connotations, for example,
many prepositions (like in, next to, after, on, etc.) are fundamentally
spatial. Space is also a key factor of the meanings of many
words/phrases/sentences/text, and space is a, if not the key, context for
referencing (e.g. pointing) and embodiment.
</p>
<p>We propose a mechanism for how space and linguistic structure can be made to
interact in a matching compositional fashion. Examples include Cartesian space,
subway stations, chesspieces on a chess-board, and Penrose's staircase. The
starting point for our construction is the DisCoCat model of compositional
natural language meaning, which we relax to accommodate physical space. We
address the issue of having multiple agents/objects in a space, including the
case that each agent has different capabilities with respect to that space,
e.g., the specific moves each chesspiece can make, or the different velocities
one may be able to reach.
</p>
<p>Once our model is in place, we show how inferences drawing from the structure
of physical space can be made. We also how how linguistic model of space can
interact with other such models related to our senses and/or embodiment, such
as the conceptual spaces of colour, taste and smell, resulting in a rich
compositional model of meaning that is close to human experience and embodiment
in the world.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP. (arXiv:2109.06598v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06598">
<div class="article-summary-box-inner">
<span><p>A key part of the NLP ethics movement is responsible use of data, but exactly
what that means or how it can be best achieved remain unclear. This position
paper discusses the core legal and ethical principles for collection and
sharing of textual data, and the tensions between them. We propose a potential
checklist for responsible data (re-)use that could both standardise the peer
review of conference submissions, as well as enable a more in-depth view of
published research across the community. Our proposal aims to contribute to the
development of a consistent standard for data (re-)use, embraced across NLP
conferences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation. (arXiv:2109.06604v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06604">
<div class="article-summary-box-inner">
<span><p>Recently, $k$NN-MT has shown the promising capability of directly
incorporating the pre-trained neural machine translation (NMT) model with
domain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve
domain adaptation without retraining. Despite being conceptually attractive, it
heavily relies on high-quality in-domain parallel corpora, limiting its
capability on unsupervised domain adaptation, where in-domain parallel corpora
are scarce or nonexistent. In this paper, we propose a novel framework that
directly uses in-domain monolingual sentences in the target language to
construct an effective datastore for $k$-nearest-neighbor retrieval. To this
end, we first introduce an autoencoder task based on the target language, and
then insert lightweight adapters into the original NMT model to map the
token-level representation of this task to the ideal representation of
translation task. Experiments on multi-domain datasets demonstrate that our
proposed approach significantly improves the translation accuracy with
target-side monolingual data, while achieving comparable performance with
back-translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model. (arXiv:2109.06605v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06605">
<div class="article-summary-box-inner">
<span><p>Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a
language model on domain-specific text, improves the modelling of text for
downstream tasks within the domain. Numerous real-world applications are based
on domain-specific text, e.g. working with financial or biomedical documents,
and these applications often need to support multiple languages. However,
large-scale domain-specific multilingual pretraining data for such scenarios
can be difficult to obtain, due to regulations, legislation, or simply a lack
of language- and domain-specific text. One solution is to train a single
multilingual model, taking advantage of the data available in as many languages
as possible. In this work, we explore the benefits of domain adaptive
pretraining with a focus on adapting to multiple languages within a specific
domain. We propose different techniques to compose pretraining corpora that
enable a language model to both become domain-specific and multilingual.
Evaluation on nine domain-specific datasets-for biomedical named entity
recognition and financial sentence classification-covering seven different
languages show that a single multilingual domain-specific model can outperform
the general multilingual model, and performs close to its monolingual
counterpart. This finding holds across two different pretraining methods,
adapter-based pretraining and full model pretraining.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Font Reconstruction with Dual Latent Manifolds. (arXiv:2109.06627v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06627">
<div class="article-summary-box-inner">
<span><p>We propose a deep generative model that performs typography analysis and font
reconstruction by learning disentangled manifolds of both font style and
character shape. Our approach enables us to massively scale up the number of
character types we can effectively model compared to previous methods.
Specifically, we infer separate latent variables representing character and
font via a pair of inference networks which take as input sets of glyphs that
either all share a character type, or belong to the same font. This design
allows our model to generalize to characters that were not observed during
training time, an important task in light of the relative sparsity of most
fonts. We also put forward a new loss, adapted from prior work that measures
likelihood using an adaptive distribution in a projected space, resulting in
more natural images without requiring a discriminator. We evaluate on the task
of font reconstruction over various datasets representing character types of
many languages, and compare favorably to modern style transfer systems
according to both automatic and manually-evaluated metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An MRC Framework for Semantic Role Labeling. (arXiv:2109.06660v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06660">
<div class="article-summary-box-inner">
<span><p>Semantic Role Labeling (SRL) aims at recognizing the predicate-argument
structure of a sentence and can be decomposed into two subtasks: predicate
disambiguation and argument labeling. Prior work deals with these two tasks
independently, which ignores the semantic connection between the two tasks. In
this paper, we propose to use the machine reading comprehension (MRC) framework
to bridge this gap. We formalize predicate disambiguation as multiple-choice
machine reading comprehension, where the descriptions of candidate senses of a
given predicate are used as options to select the correct sense. The chosen
predicate sense is then used to determine the semantic roles for that
predicate, and these semantic roles are used to construct the query for another
MRC model for argument labeling. In this way, we are able to leverage both the
predicate semantics and the semantic role semantics for argument labeling. We
also propose to select a subset of all the possible semantic roles for
computational efficiency. Experiments show that the proposed framework achieves
state-of-the-art results on both span and dependency benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expert Knowledge-Guided Length-Variant Hierarchical Label Generation for Proposal Classification. (arXiv:2109.06661v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06661">
<div class="article-summary-box-inner">
<span><p>To advance the development of science and technology, research proposals are
submitted to open-court competitive programs developed by government agencies
(e.g., NSF). Proposal classification is one of the most important tasks to
achieve effective and fair review assignments. Proposal classification aims to
classify a proposal into a length-variant sequence of labels. In this paper, we
formulate the proposal classification problem into a hierarchical multi-label
classification task. Although there are certain prior studies, proposal
classification exhibit unique features: 1) the classification result of a
proposal is in a hierarchical discipline structure with different levels of
granularity; 2) proposals contain multiple types of documents; 3) domain
experts can empirically provide partial labels that can be leveraged to improve
task performances. In this paper, we focus on developing a new deep proposal
classification framework to jointly model the three features. In particular, to
sequentially generate labels, we leverage previously-generated labels to
predict the label of next level; to integrate partial labels from experts, we
use the embedding of these empirical partial labels to initialize the state of
neural networks. Our model can automatically identify the best length of label
sequence to stop next label prediction. Finally, we present extensive results
to demonstrate that our method can jointly model partial labels, textual
information, and semantic dependencies in label sequences, and, thus, achieve
advanced performances.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Inference for Multilingual Neural Machine Translation. (arXiv:2109.06679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06679">
<div class="article-summary-box-inner">
<span><p>Multilingual NMT has become an attractive solution for MT deployment in
production. But to match bilingual quality, it comes at the cost of larger and
slower models. In this work, we consider several ways to make multilingual NMT
faster at inference without degrading its quality. We experiment with several
"light decoder" architectures in two 20-language multi-parallel settings:
small-scale on TED Talks and large-scale on ParaCrawl. Our experiments
demonstrate that combining a shallow decoder with vocabulary filtering leads to
more than twice faster inference with no loss in translation quality. We
validate our findings with BLEU and chrF (on 380 language pairs), robustness
evaluation and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-autoregressive Transformer with Unified Bidirectional Decoder for Automatic Speech Recognition. (arXiv:2109.06684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06684">
<div class="article-summary-box-inner">
<span><p>Non-autoregressive (NAR) transformer models have been studied intensively in
automatic speech recognition (ASR), and a substantial part of NAR transformer
models is to use the casual mask to limit token dependencies. However, the
casual mask is designed for the left-to-right decoding process of the
non-parallel autoregressive (AR) transformer, which is inappropriate for the
parallel NAR transformer since it ignores the right-to-left contexts. Some
models are proposed to utilize right-to-left contexts with an extra decoder,
but these methods increase the model complexity. To tackle the above problems,
we propose a new non-autoregressive transformer with a unified bidirectional
decoder (NAT-UBD), which can simultaneously utilize left-to-right and
right-to-left contexts. However, direct use of bidirectional contexts will
cause information leakage, which means the decoder output can be affected by
the character information from the input of the same position. To avoid
information leakage, we propose a novel attention mask and modify vanilla
queries, keys, and values matrices for NAT-UBD. Experimental results verify
that NAT-UBD can achieve character error rates (CERs) of 5.0%/5.5% on the
Aishell1 dev/test sets, outperforming all previous NAR transformer models.
Moreover, NAT-UBD can run 49.8x faster than the AR transformer baseline when
decoding in a single step.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A system for information extraction from scientific texts in Russian. (arXiv:2109.06703v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06703">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a system for information extraction from scientific
texts in the Russian language. The system performs several tasks in an
end-to-end manner: term recognition, extraction of relations between terms, and
term linking with entities from the knowledge base. These tasks are extremely
important for information retrieval, recommendation systems, and
classification. The advantage of the implemented methods is that the system
does not require a large amount of labeled data, which saves time and effort
for data labeling and therefore can be applied in low- and mid-resource
settings. The source code is publicly available and can be used for different
research purposes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KFCNet: Knowledge Filtering and Contrastive Learning Network for Generative Commonsense Reasoning. (arXiv:2109.06704v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06704">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models have led to substantial gains over a broad range
of natural language processing (NLP) tasks, but have been shown to have
limitations for natural language generation tasks with high-quality
requirements on the output, such as commonsense generation and ad keyword
generation. In this work, we present a novel Knowledge Filtering and
Contrastive learning Network (KFCNet) which references external knowledge and
achieves better generation performance. Specifically, we propose a BERT-based
filter model to remove low-quality candidates, and apply contrastive learning
separately to each of the encoder and decoder, within a general
encoder--decoder architecture. The encoder contrastive module helps to capture
global target semantics during encoding, and the decoder contrastive module
enhances the utility of retrieved prototypes while learning general features.
Extensive experiments on the CommonGen benchmark show that our model
outperforms the previous state of the art by a large margin: +6.6 points (42.5
vs. 35.9) for BLEU-4, +3.7 points (33.3 vs. 29.6) for SPICE, and +1.3 points
(18.3 vs. 17.0) for CIDEr. We further verify the effectiveness of the proposed
contrastive module on ad keyword generation, and show that our model has
potential commercial value.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (arXiv:2109.06705v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06705">
<div class="article-summary-box-inner">
<span><p>Table filling based relational triple extraction methods are attracting
growing research interests due to their promising performance and their
abilities on extracting triples from complex sentences. However, this kind of
methods are far from their full potential because most of them only focus on
using local features but ignore the global associations of relations and of
token pairs, which increases the possibility of overlooking some important
information during triple extraction. To overcome this deficiency, we propose a
global feature-oriented triple extraction model that makes full use of the
mentioned two kinds of global associations. Specifically, we first generate a
table feature for each relation. Then two kinds of global associations are
mined from the generated table features. Next, the mined global associations
are integrated into the table feature of each relation. This
"generate-mine-integrate" process is performed multiple times so that the table
feature of each relation is refined step by step. Finally, each relation's
table is filled based on its refined table feature, and all triples linked to
this relation are extracted based on its filled table. We evaluate the proposed
model on three benchmark datasets. Experimental results show our model is
effective and it achieves state-of-the-art results on all of these datasets.
The source code of our work is available at: https://github.com/neukg/GRTE.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Answer Type Prediction using BERT: IAI at the ISWC SMART Task 2020. (arXiv:2109.06714v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06714">
<div class="article-summary-box-inner">
<span><p>This paper summarizes our participation in the SMART Task of the ISWC 2020
Challenge. A particular question we are interested in answering is how well
neural methods, and specifically transformer models, such as BERT, perform on
the answer type prediction task compared to traditional approaches. Our main
finding is that coarse-grained answer types can be identified effectively with
standard text classification methods, with over 95% accuracy, and BERT can
bring only marginal improvements. For fine-grained type detection, on the other
hand, BERT clearly outperforms previous retrieval-based approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controllable Dialogue Generation with Disentangled Multi-grained Style Specification and Attribute Consistency Reward. (arXiv:2109.06717v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06717">
<div class="article-summary-box-inner">
<span><p>Controllable text generation is an appealing but challenging task, which
allows users to specify particular attributes of the generated outputs. In this
paper, we propose a controllable dialogue generation model to steer response
generation under multi-attribute constraints. Specifically, we define and
categorize the commonly used control attributes into global and local ones,
which possess different granularities of effects on response generation. Then,
we significantly extend the conventional seq2seq framework by introducing a
novel two-stage decoder, which first uses a multi-grained style specification
layer to impose the stylistic constraints and determine word-level control
states of responses based on the attributes, and then employs a response
generation layer to generate final responses maintaining both semantic
relevancy to the contexts and fidelity to the attributes. Furthermore, we train
our model with an attribute consistency reward to promote response control with
explicit supervision signals. Extensive experiments and in-depth analyses on
two datasets indicate that our model can significantly outperform competitive
baselines in terms of response quality, content diversity and controllability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparse Fuzzy Attention for Structural Sentiment Analysis. (arXiv:2109.06719v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06719">
<div class="article-summary-box-inner">
<span><p>Attention scorers have achieved success in parsing tasks like semantic and
syntactic dependency parsing. However, in tasks modeled into parsing, like
structural sentiment analysis, "dependency edges" are very sparse which hinders
parser performance. Thus we propose a sparse and fuzzy attention scorer with
pooling layers which improves parser performance and sets the new
state-of-the-art on structural sentiment analysis. We further explore the
parsing modeling on structural sentiment analysis with second-order parsing and
introduce a novel sparse second-order edge building procedure that leads to
significant improvement in parsing performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Modelling with Applications to Music Recommendation, Fact-Checking, and Speed Reading. (arXiv:2109.06736v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06736">
<div class="article-summary-box-inner">
<span><p>Sequential modelling entails making sense of sequential data, which naturally
occurs in a wide array of domains. One example is systems that interact with
users, log user actions and behaviour, and make recommendations of items of
potential interest to users on the basis of their previous interactions. In
such cases, the sequential order of user interactions is often indicative of
what the user is interested in next. Similarly, for systems that automatically
infer the semantics of text, capturing the sequential order of words in a
sentence is essential, as even a slight re-ordering could significantly alter
its original meaning. This thesis makes methodological contributions and new
investigations of sequential modelling for the specific application areas of
systems that recommend music tracks to listeners and systems that process text
semantics in order to automatically fact-check claims, or "speed read" text for
efficient further classification. (Rest of abstract omitted due to arXiv
abstract limit)
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adaptive Information Seeking for Open-Domain Question Answering. (arXiv:2109.06747v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06747">
<div class="article-summary-box-inner">
<span><p>Information seeking is an essential step for open-domain question answering
to efficiently gather evidence from a large corpus. Recently, iterative
approaches have been proven to be effective for complex questions, by
recursively retrieving new evidence at each step. However, almost all existing
iterative approaches use predefined strategies, either applying the same
retrieval function multiple times or fixing the order of different retrieval
functions, which cannot fulfill the diverse requirements of various questions.
In this paper, we propose a novel adaptive information-seeking strategy for
open-domain question answering, namely AISO. Specifically, the whole retrieval
and answer process is modeled as a partially observed Markov decision process,
where three types of retrieval operations (e.g., BM25, DPR, and hyperlink) and
one answer operation are defined as actions. According to the learned policy,
AISO could adaptively select a proper retrieval action to seek the missing
evidence at each step, based on the collected evidence and the reformulated
query, or directly output the answer when the evidence set is sufficient for
the question. Experiments on SQuAD Open and HotpotQA fullwiki, which serve as
single-hop and multi-hop open-domain QA benchmarks, show that AISO outperforms
all baseline methods with predefined strategies in terms of both retrieval and
answer evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Zero-shot Cross-lingual Transfer between Closely Related Languages by injecting Character-level Noise. (arXiv:2109.06772v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06772">
<div class="article-summary-box-inner">
<span><p>Cross-lingual transfer between a high-resource language and its dialects or
closely related language varieties should be facilitated by their similarity,
but current approaches that operate in the embedding space do not take surface
similarity into account. In this work, we present a simple yet effective
strategy to improve cross-lingual transfer between closely related varieties by
augmenting the data of the high-resource parent language with character-level
noise to make the model more robust towards spelling variations. Our strategy
shows consistent improvements over several languages and tasks: Zero-shot
transfer of POS tagging and topic identification between language varieties
from the Germanic, Uralic, and Romance language genera. Our work provides
evidence for the usefulness of simple surface-level noise in improving transfer
between language varieties.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction. (arXiv:2109.06798v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06798">
<div class="article-summary-box-inner">
<span><p>Zero-shot cross-lingual information extraction (IE) describes the
construction of an IE model for some target language, given existing
annotations exclusively in some other language, typically English. While the
advance of pretrained multilingual encoders suggests an easy optimism of "train
on English, run on any language", we find through a thorough exploration and
extension of techniques that a combination of approaches, both new and old,
leads to better performance than any one cross-lingual strategy in particular.
We explore techniques including data projection and self-training, and how
different pretrained encoders impact them. We use English-to-Arabic IE as our
initial example, demonstrating strong performance in this setting for event
extraction, named entity recognition, part-of-speech tagging, and dependency
parsing. We then apply data projection and self-training to three tasks across
eight target languages. Because no single set of techniques performs the best
across all tasks, we encourage practitioners to explore various configurations
of the techniques described in this work when seeking to improve on zero-shot
training.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Temporal Variational Model for Story Generation. (arXiv:2109.06807v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06807">
<div class="article-summary-box-inner">
<span><p>Recent language models can generate interesting and grammatically correct
text in story generation but often lack plot development and long-term
coherence. This paper experiments with a latent vector planning approach based
on a TD-VAE (Temporal Difference Variational Autoencoder), using the model for
conditioning and reranking for text generation. The results demonstrate strong
performance in automatic cloze and swapping evaluations. The human judgments
show stories generated with TD-VAE reranking improve on a GPT-2 medium baseline
and show comparable performance to a hierarchical LSTM reranking model.
Conditioning on the latent vectors proves disappointing and deteriorates
performance in human evaluation because it reduces the diversity of generation,
and the models don't learn to progress the narrative. This highlights an
important difference between technical task performance (e.g. cloze) and
generating interesting stories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What are the attackers doing now? Automating cyber threat intelligence extraction from text on pace with the changing threat landscape: A survey. (arXiv:2109.06808v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06808">
<div class="article-summary-box-inner">
<span><p>Cybersecurity researchers have contributed to the automated extraction of CTI
from textual sources, such as threat reports and online articles, where
cyberattack strategies, procedures, and tools are described. The goal of this
article is to aid cybersecurity researchers understand the current techniques
used for cyberthreat intelligence extraction from text through a survey of
relevant studies in the literature. We systematically collect "CTI extraction
from text"-related studies from the literature and categorize the CTI
extraction purposes. We propose a CTI extraction pipeline abstracted from these
studies. We identify the data sources, techniques, and CTI sharing formats
utilized in the context of the proposed pipeline. Our work finds ten types of
extraction purposes, such as extraction indicators of compromise extraction,
TTPs (tactics, techniques, procedures of attack), and cybersecurity keywords.
We also identify seven types of textual sources for CTI extraction, and textual
data obtained from hacker forums, threat reports, social media posts, and
online news articles have been used by almost 90% of the studies. Natural
language processing along with both supervised and unsupervised machine
learning techniques such as named entity recognition, topic modelling,
dependency parsing, supervised classification, and clustering are used for CTI
extraction. We observe the technical challenges associated with these studies
related to obtaining available clean, labelled data which could assure
replication, validation, and further extension of the studies. As we find the
studies focusing on CTI information extraction from text, we advocate for
building upon the current CTI extraction work to help cybersecurity
practitioners with proactive decision making such as threat prioritization,
automated threat modelling to utilize knowledge from past cybersecurity
incidents.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LM-Critic: Language Models for Unsupervised Grammatical Error Correction. (arXiv:2109.06822v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06822">
<div class="article-summary-box-inner">
<span><p>Training a model for grammatical error correction (GEC) requires a set of
labeled ungrammatical / grammatical sentence pairs, but manually annotating
such pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has
demonstrated strong results on learning to repair a broken program without any
labeled examples, but this relies on a perfect critic (e.g., a compiler) that
returns whether an example is valid or not, which does not exist for the GEC
task. In this work, we show how to leverage a pretrained language model (LM) in
defining an LM-Critic, which judges a sentence to be grammatical if the LM
assigns it a higher probability than its local perturbations. We apply this
LM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap
realistic ungrammatical / grammatical pairs for training a corrector. We
evaluate our approach on GEC datasets across multiple domains (CoNLL-2014,
BEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing
methods in both the unsupervised setting (+7.7 F0.5) and the supervised setting
(+0.5 F0.5).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Types of Out-of-Distribution Texts and How to Detect Them. (arXiv:2109.06827v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06827">
<div class="article-summary-box-inner">
<span><p>Despite agreement on the importance of detecting out-of-distribution (OOD)
examples, there is little consensus on the formal definition of OOD examples
and how to best detect them. We categorize these examples by whether they
exhibit a background shift or a semantic shift, and find that the two major
approaches to OOD detection, model calibration and density estimation (language
modeling for text), have distinct behavior on these types of OOD data. Across
14 pairs of in-distribution and OOD English natural language understanding
datasets, we find that density estimation methods consistently beat calibration
methods in background shift settings, while performing worse in semantic shift
settings. In addition, we find that both methods generally fail to detect
examples from challenge data, highlighting a weak spot for current methods.
Since no single method works well across all settings, our results call for an
explicit definition of OOD examples when evaluating different detection
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation. (arXiv:2109.06835v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06835">
<div class="article-summary-box-inner">
<span><p>Recent text generation research has increasingly focused on open-ended
domains such as story and poetry generation. Because models built for such
tasks are difficult to evaluate automatically, most researchers in the space
justify their modeling choices by collecting crowdsourced human judgments of
text quality (e.g., Likert scores of coherence or grammaticality) from Amazon
Mechanical Turk (AMT). In this paper, we first conduct a survey of 45
open-ended text generation papers and find that the vast majority of them fail
to report crucial details about their AMT tasks, hindering reproducibility. We
then run a series of story evaluation experiments with both AMT workers and
English teachers and discover that even with strict qualification filters, AMT
workers (unlike teachers) fail to distinguish between model-generated text and
human-generated references. We show that AMT worker judgments improve when they
are shown model-generated output alongside human-generated references, which
enables the workers to better calibrate their ratings. Finally, interviews with
the English teachers provide deeper insights into the challenges of the
evaluation process, particularly when rating model-generated text.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding. (arXiv:2109.06838v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06838">
<div class="article-summary-box-inner">
<span><p>While large language models have shown exciting progress on several NLP
benchmarks, evaluating their ability for complex analogical reasoning remains
under-explored. Here, we introduce a high-quality crowdsourced dataset of
narratives for employing proverbs in context as a benchmark for abstract
language understanding. The dataset provides fine-grained annotation of aligned
spans between proverbs and narratives, and contains minimal lexical overlaps
between narratives and proverbs, ensuring that models need to go beyond
surface-level reasoning to succeed. We explore three tasks: (1) proverb
recommendation and alignment prediction, (2) narrative generation for a given
proverb and topic, and (3) identifying narratives with similar motifs. Our
experiments show that neural language models struggle in our tasks compared to
humans, and the tasks pose multiple learning challenges.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BenchIE: Open Information Extraction Evaluation Based on Facts, Not Tokens. (arXiv:2109.06850v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06850">
<div class="article-summary-box-inner">
<span><p>Intrinsic evaluations of OIE systems are carried out either manually -- with
human evaluators judging the correctness of extractions -- or automatically, on
standardized benchmarks. The latter, while much more cost-effective, is less
reliable, primarily because of the incompleteness of the existing OIE
benchmarks: the ground truth extractions do not include all acceptable variants
of the same fact, leading to unreliable assessment of models' performance.
Moreover, the existing OIE benchmarks are available for English only. In this
work, we introduce BenchIE: a benchmark and evaluation framework for
comprehensive evaluation of OIE systems for English, Chinese and German. In
contrast to existing OIE benchmarks, BenchIE takes into account informational
equivalence of extractions: our gold standard consists of fact synsets,
clusters in which we exhaustively list all surface forms of the same fact. We
benchmark several state-of-the-art OIE systems using BenchIE and demonstrate
that these systems are significantly less effective than indicated by existing
OIE benchmarks. We make BenchIE (data and evaluation code) publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension. (arXiv:2109.06853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06853">
<div class="article-summary-box-inner">
<span><p>How can we generate concise explanations for multi-hop Reading Comprehension
(RC)? The current strategies of identifying supporting sentences can be seen as
an extractive question-focused summarization of the input text. However, these
extractive explanations are not necessarily concise i.e. not minimally
sufficient for answering a question. Instead, we advocate for an abstractive
approach, where we propose to generate a question-focused, abstractive summary
of input paragraphs and then feed it to an RC system. Given a limited amount of
human-annotated abstractive explanations, we train the abstractive explainer in
a semi-supervised manner, where we start from the supervised model and then
train it further through trial and error maximizing a conciseness-promoted
reward function. Our experiments demonstrate that the proposed abstractive
explainer can generate more compact explanations than an extractive explainer
with limited supervision (only 2k instances) while maintaining sufficiency.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning. (arXiv:2109.06860v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06860">
<div class="article-summary-box-inner">
<span><p>Commonsense is defined as the knowledge that is shared by everyone. However,
certain types of commonsense knowledge are correlated with culture and
geographic locations and they are only shared locally. For example, the
scenarios of wedding ceremonies vary across regions due to different customs
influenced by historical and religious factors. Such regional characteristics,
however, are generally omitted in prior work. In this paper, we construct a
Geo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test
vision-and-language models' ability to understand cultural and
geo-location-specific commonsense. In particular, we study two state-of-the-art
Vision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard
multimodal commonsense benchmark with images primarily from Western regions. We
then evaluate how well the trained models can generalize to answering the
questions in GD-VCR. We find that the performance of both models for
non-Western regions including East Asia, South Asia, and Africa is
significantly lower than that for Western region. We analyze the reasons behind
the performance disparity and find that the performance gap is larger on QA
pairs that: 1) are concerned with culture-related scenarios, e.g., weddings,
religious activities, and festivals; 2) require high-level geo-diverse
commonsense reasoning rather than low-order perception and recognition. Dataset
and code are released at https://github.com/WadeYin9712/GD-VCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Legal Transformer Models May Not Always Help. (arXiv:2109.06862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06862">
<div class="article-summary-box-inner">
<span><p>Deep learning-based Natural Language Processing methods, especially
transformers, have achieved impressive performance in the last few years.
Applying those state-of-the-art NLP methods to legal activities to automate or
simplify some simple work is of great value. This work investigates the value
of domain adaptive pre-training and language adapters in legal NLP tasks. By
comparing the performance of language models with domain adaptive pre-training
on different tasks and different dataset splits, we show that domain adaptive
pre-training is only helpful with low-resource downstream tasks, thus far from
being a panacea. We also benchmark the performance of adapters in a typical
legal NLP task and show that they can yield similar performance to full model
tuning with much smaller training costs. As an additional result, we release
LegalRoBERTa, a RoBERTa model further pre-trained on legal corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition. (arXiv:2109.06870v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06870">
<div class="article-summary-box-inner">
<span><p>This paper is a study of performance-efficiency trade-offs in pre-trained
models for automatic speech recognition (ASR). We focus on wav2vec 2.0, and
formalize several architecture designs that influence both the model
performance and its efficiency. Putting together all our observations, we
introduce SEW (Squeezed and Efficient Wav2vec), a pre-trained model
architecture with significant improvements along both performance and
efficiency dimensions across a variety of training setups. For example, under
the 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x
inference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in
word error rate. With a similar inference time, SEW reduces word error rate by
25-50% across different model sizes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Hyper-Parameter Optimization for Neural Machine Translation on GPU Architectures. (arXiv:1805.02094v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1805.02094">
<div class="article-summary-box-inner">
<span><p>Neural machine translation (NMT) has been accelerated by deep learning neural
networks over statistical-based approaches, due to the plethora and
programmability of commodity heterogeneous computing architectures such as
FPGAs and GPUs and the massive amount of training corpuses generated from news
outlets, government agencies and social media. Training a learning classifier
for neural networks entails tuning hyper-parameters that would yield the best
performance. Unfortunately, the number of parameters for machine translation
include discrete categories as well as continuous options, which makes for a
combinatorial explosive problem. This research explores optimizing
hyper-parameters when training deep learning neural networks for machine
translation. Specifically, our work investigates training a language model with
Marian NMT. Results compare NMT under various hyper-parameter settings across a
variety of modern GPU architecture generations in single node and multi-node
settings, revealing insights on which hyper-parameters matter most in terms of
performance, such as words processed per second, convergence rates, and
translation accuracy, and provides insights on how to best achieve
high-performing NMT systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.00033">
<div class="article-summary-box-inner">
<span><p>With the emergence of the COVID-19 pandemic, the political and the medical
aspects of disinformation merged as the problem got elevated to a whole new
level to become the first global infodemic. Fighting this infodemic has been
declared one of the most important focus areas of the World Health
Organization, with dangers ranging from promoting fake cures, rumors, and
conspiracy theories to spreading xenophobia and panic. Ad-dressing the issue
requires solving a number of challenging problems such as identifying messages
containing claims, determining their check-worthiness and factuality, and their
potential to do harm as well as the nature of that harm, to mention just a few.
To address this gap, we release a large dataset of 16K manually annotated
tweets for fine-grained disinformation analysis that (i) focuses on COVID-19,
(ii) combines the perspectives and the interests of journalists, fact-checkers,
social media platforms, policy makers, and society, and (iii) covers Arabic,
Bulgarian, Dutch, and English. Finally, we show strong evaluation results using
pretrained Transformers, thus con-firming the practical utility of the dataset
in monolingual vs. multilingual, and single task vs. multitask settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">L2R2: Leveraging Ranking for Abductive Reasoning. (arXiv:2005.11223v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.11223">
<div class="article-summary-box-inner">
<span><p>The abductive natural language inference task ($\alpha$NLI) is proposed to
evaluate the abductive reasoning ability of a learning system. In the
$\alpha$NLI task, two observations are given and the most plausible hypothesis
is asked to pick out from the candidates. Existing methods simply formulate it
as a classification problem, thus a cross-entropy log-loss objective is used
during training. However, discriminating true from false does not measure the
plausibility of a hypothesis, for all the hypotheses have a chance to happen,
only the probabilities are different. To fill this gap, we switch to a ranking
perspective that sorts the hypotheses in order of their plausibilities. With
this new perspective, a novel $L2R^2$ approach is proposed under the
learning-to-rank framework. Firstly, training samples are reorganized into a
ranking form, where two observations and their hypotheses are treated as the
query and a set of candidate documents respectively. Then, an ESIM model or
pre-trained language model, e.g. BERT or RoBERTa, is obtained as the scoring
function. Finally, the loss functions for the ranking task can be either
pair-wise or list-wise for training. The experimental results on the ART
dataset reach the state-of-the-art in the public leaderboard.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Document Graph for Neural Machine Translation. (arXiv:2012.03477v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.03477">
<div class="article-summary-box-inner">
<span><p>Previous works have shown that contextual information can improve the
performance of neural machine translation (NMT). However, most existing
document-level NMT methods only consider a few number of previous sentences.
How to make use of the whole document as global contexts is still a challenge.
To address this issue, we hypothesize that a document can be represented as a
graph that connects relevant contexts regardless of their distances. We employ
several types of relations, including adjacency, syntactic dependency, lexical
consistency, and coreference, to construct the document graph. Then, we
incorporate both source and target graphs into the conventional Transformer
architecture with graph convolutional networks. Experiments on various NMT
benchmarks, including IWSLT English--French, Chinese-English, WMT
English--German and Opensubtitle English--Russian, demonstrate that using
document graphs can significantly improve the translation quality. Extensive
analysis verifies that the document graph is beneficial for capturing discourse
phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Quantum Mathematics in Artificial Intelligence. (arXiv:2101.04255v4 [cs.AI] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.04255">
<div class="article-summary-box-inner">
<span><p>In the decade since 2010, successes in artificial intelligence have been at
the forefront of computer science and technology, and vector space models have
solidified a position at the forefront of artificial intelligence. At the same
time, quantum computers have become much more powerful, and announcements of
major advances are frequently in the news.
</p>
<p>The mathematical techniques underlying both these areas have more in common
than is sometimes realized. Vector spaces took a position at the axiomatic
heart of quantum mechanics in the 1930s, and this adoption was a key motivation
for the derivation of logic and probability from the linear geometry of vector
spaces. Quantum interactions between particles are modelled using the tensor
product, which is also used to express objects and operations in artificial
neural networks.
</p>
<p>This paper describes some of these common mathematical areas, including
examples of how they are used in artificial intelligence (AI), particularly in
automated reasoning and natural language processing (NLP). Techniques discussed
include vector spaces, scalar products, subspaces and implication, orthogonal
projection and negation, dual vectors, density matrices, positive operators,
and tensor products. Application areas include information retrieval,
categorization and implication, modelling word-senses and disambiguation,
inference in knowledge bases, and semantic composition.
</p>
<p>Some of these approaches can potentially be implemented on quantum hardware.
Many of the practical steps in this implementation are in early stages, and
some are already realized. Explaining some of the common mathematical tools can
help researchers in both AI and quantum computing further exploit these
overlaps, recognizing and exploring new directions along the way.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation. (arXiv:2102.05766v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05766">
<div class="article-summary-box-inner">
<span><p>Recently, representation learning for text and speech has successfully
improved many language related tasks. However, all existing methods suffer from
two limitations: (a) they only learn from one input modality, while a unified
representation for both speech and text is needed by tasks such as end-to-end
speech translation, and as a result,(b) they can not exploit various
large-scale text and speech data and their performance is limited by the
scarcity of parallel speech translation data.To address these problems, we
propose a Fused Acoustic and Text Masked Language Model (FAT-MLM) which jointly
learns a unified representation for both acoustic and text input from various
types of corpora including parallel data for speech recognition and machine
translation, and even pure speech and text data. Within this cross-modal
representation learning framework, we further present an end-to-end model for
Fused Acoustic and Text Speech Translation (FAT-ST). Experiments on three
translation directions show that by fine-tuning from FAT-MLM, our proposed
speech translation models substantially improve translation quality by up to
+5.9 BLEU.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.01378">
<div class="article-summary-box-inner">
<span><p>Contrastive explanations clarify why an event occurred in contrast to
another. They are more inherently intuitive to humans to both produce and
comprehend. We propose a methodology to produce contrastive explanations for
classification models by modifying the representation to disregard
non-contrastive information, and modifying model behavior to only be based on
contrastive reasoning. Our method is based on projecting model representation
to a latent space that captures only the features that are useful (to the
model) to differentiate two potential decisions. We demonstrate the value of
contrastive explanations by analyzing two different scenarios, using both
high-level abstract concept attribution and low-level input token/span
attribution, on two widely used text classification tasks. Specifically, we
produce explanations for answering: for which label, and against which
alternative label, is some aspect of the input useful? And which aspects of the
input are useful for and against particular decisions? Overall, our findings
shed light on the ability of label-contrastive explanations to provide a more
accurate and finer-grained interpretability of a model's decision.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving and Simplifying Pattern Exploiting Training. (arXiv:2103.11955v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11955">
<div class="article-summary-box-inner">
<span><p>Recently, pre-trained language models (LMs) have achieved strong performance
when fine-tuned on difficult benchmarks like SuperGLUE. However, performance
can suffer when there are very few labeled examples available for fine-tuning.
Pattern Exploiting Training (PET) is a recent approach that leverages patterns
for few-shot learning. However, PET uses task-specific unlabeled data. In this
paper, we focus on few-shot learning without any unlabeled data and introduce
ADAPET, which modifies PET's objective to provide denser supervision during
fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any
task-specific unlabeled data. Our code can be found at
https://github.com/rrmenon10/ADAPET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages. (arXiv:2103.14583v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.14583">
<div class="article-summary-box-inner">
<span><p>Pre-trained speech representations like wav2vec 2.0 are a powerful tool for
automatic speech recognition (ASR). Yet many endangered languages lack
sufficient data for pre-training such models, or are predominantly oral
vernaculars without a standardised writing system, precluding fine-tuning.
Query-by-example spoken term detection (QbE-STD) offers an alternative for
iteratively indexing untranscribed speech corpora by locating spoken query
terms. Using data from 7 Australian Aboriginal languages and a regional variety
of Dutch, all of which are endangered or vulnerable, we show that QbE-STD can
be improved by leveraging representations developed for ASR (wav2vec 2.0: the
English monolingual model and XLSR53 multilingual model). Surprisingly, the
English model outperformed the multilingual model on 4 Australian language
datasets, raising questions around how to optimally leverage self-supervised
speech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0
representations (either English or XLSR53) offer large improvements (56-86%
relative) over state-of-the-art approaches on our endangered language datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Connecting Attributions and QA Model Behavior on Realistic Counterfactuals. (arXiv:2104.04515v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04515">
<div class="article-summary-box-inner">
<span><p>When a model attribution technique highlights a particular part of the input,
a user might understand this highlight as making a statement about
counterfactuals (Miller, 2019): if that part of the input were to change, the
model's prediction might change as well. This paper investigates how well
different attribution techniques align with this assumption on realistic
counterfactuals in the case of reading comprehension (RC). RC is a particularly
challenging test case, as token-level attributions that have been extensively
studied in other NLP tasks such as sentiment analysis are less suitable to
represent the reasoning that RC models perform. We construct counterfactual
sets for three different RC settings, and through heuristics that can connect
attribution methods' outputs to high-level model behavior, we can evaluate how
useful different attribution methods and even different formats are for
understanding counterfactuals. We find that pairwise attributions are better
suited to RC than token-level attributions across these different RC settings,
with our best performance coming from a modification that we propose to an
existing pairwise attribution method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Zero-Shot Multifaceted Visually Grounded Word Embeddings via Multi-Task Training. (arXiv:2104.07500v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07500">
<div class="article-summary-box-inner">
<span><p>Language grounding aims at linking the symbolic representation of language
(e.g., words) into the rich perceptual knowledge of the outside world. The
general approach is to embed both textual and visual information into a common
space -the grounded space-confined by an explicit relationship between both
modalities. We argue that this approach sacrifices the abstract knowledge
obtained from linguistic co-occurrence statistics in the process of acquiring
perceptual information. The focus of this paper is to solve this issue by
implicitly grounding the word embeddings. Rather than learning two mappings
into a joint space, our approach integrates modalities by determining a
reversible grounded mapping between the textual and the grounded space by means
of multi-task learning. Evaluations on intrinsic and extrinsic tasks show that
our embeddings are highly beneficial for both abstract and concrete words. They
are strongly correlated with human judgments and outperform previous works on a
wide range of benchmarks. Our grounded embeddings are publicly available here.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. (arXiv:2104.08455v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08455">
<div class="article-summary-box-inner">
<span><p>Dialogue systems powered by large pre-trained language models (LM) exhibit an
innate ability to deliver fluent and natural-looking responses. Despite their
impressive generation performance, these models can often generate factually
incorrect statements impeding their widespread adoption. In this paper, we
focus on the task of improving the faithfulness -- and thus reduce
hallucination -- of Neural Dialogue Systems to known facts supplied by a
Knowledge Graph (KG). We propose Neural Path Hunter which follows a
generate-then-refine strategy whereby a generated response is amended using the
k-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level
fact critic to identify plausible sources of hallucination followed by a
refinement stage consisting of a chain of two neural LM's that retrieves
correct entities by crafting a query signal that is propagated over the k-hop
subgraph. Our proposed model can easily be applied to any dialogue generated
responses without retraining the model. We empirically validate our proposed
approach on the OpenDialKG dataset against a suite of metrics and report a
relative improvement of faithfulness over dialogue responses by 20.35% based on
FeQA (Durmus et al., 2020).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation. (arXiv:2104.08771v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08771">
<div class="article-summary-box-inner">
<span><p>We study the power of cross-attention in the Transformer architecture within
the context of transfer learning for machine translation, and extend the
findings of studies into cross-attention when training from scratch. We conduct
a series of experiments through fine-tuning a translation model on data where
either the source or target language has changed. These experiments reveal that
fine-tuning only the cross-attention parameters is nearly as effective as
fine-tuning all parameters (i.e., the entire translation model). We provide
insights into why this is the case and observe that limiting fine-tuning in
this manner yields cross-lingually aligned embeddings. The implications of this
finding for researchers and practitioners include a mitigation of catastrophic
forgetting, the potential for zero-shot translation, and the ability to extend
machine translation models to several new language pairs with reduced parameter
storage overhead.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Low-Dimensional Linear Geometry of Contextualized Word Representations. (arXiv:2105.07109v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.07109">
<div class="article-summary-box-inner">
<span><p>Black-box probing models can reliably extract linguistic features like tense,
number, and syntactic role from pretrained word representations. However, the
manner in which these features are encoded in representations remains poorly
understood. We present a systematic study of the linear geometry of
contextualized word representations in ELMO and BERT. We show that a variety of
linguistic features (including structured dependency relationships) are encoded
in low-dimensional subspaces. We then refine this geometric picture, showing
that there are hierarchical relations between the subspaces encoding general
linguistic categories and more specific ones, and that low-dimensional feature
encodings are distributed rather than aligned to individual neurons. Finally,
we demonstrate that these linear subspaces are causally related to model
behavior, and can be used to perform fine-grained manipulation of BERT's output
distribution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Coreference-Aware Dialogue Summarization. (arXiv:2106.08556v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.08556">
<div class="article-summary-box-inner">
<span><p>Summarizing conversations via neural approaches has been gaining research
traction lately, yet it is still challenging to obtain practical solutions.
Examples of such challenges include unstructured information exchange in
dialogues, informal interactions between speakers, and dynamic role changes of
speakers as the dialogue evolves. Many of such challenges result in complex
coreference links. Therefore, in this work, we investigate different approaches
to explicitly incorporate coreference information in neural abstractive
dialogue summarization models to tackle the aforementioned challenges.
Experimental results show that the proposed approaches achieve state-of-the-art
performance, implying it is useful to utilize coreference information in
dialogue summarization. Evaluation results on factual correctness suggest such
coreference-aware models are better at tracing the information flow among
interlocutors and associating accurate status/actions with the corresponding
interlocutors and person mentions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05541">
<div class="article-summary-box-inner">
<span><p>Chatbots are intelligent software built to be used as a replacement for human
interaction. However, existing studies typically do not provide enough support
for low-resource languages like Bangla. Moreover, due to the increasing
popularity of social media, we can also see the rise of interactions in Bangla
transliteration (mostly in English) among the native Bangla speakers. In this
paper, we propose a novel approach to build a Bangla chatbot aimed to be used
as a business assistant which can communicate in Bangla and Bangla
Transliteration in English with high confidence consistently. Since annotated
data was not available for this purpose, we had to work on the whole machine
learning life cycle (data preparation, machine learning modeling, and model
deployment) using Rasa Open Source Framework, fastText embeddings, Polyglot
embeddings, Flask, and other systems as building blocks. While working with the
skewed annotated dataset, we try out different setups and pipelines to evaluate
which works best and provide possible reasoning behind the observed results.
Finally, we present a pipeline for intent classification and entity extraction
which achieves reasonable performance (accuracy: 83.02%, precision: 80.82%,
recall: 83.02%, F1-score: 80%).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation. (arXiv:2107.10821v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.10821">
<div class="article-summary-box-inner">
<span><p>Automatic metrics are commonly used as the exclusive tool for declaring the
superiority of one machine translation system's quality over another. The
community choice of automatic metric guides research directions and industrial
developments by deciding which models are deemed better. Evaluating metrics
correlations with sets of human judgements has been limited by the size of
these sets. In this paper, we corroborate how reliable metrics are in contrast
to human judgements on -- to the best of our knowledge -- the largest
collection of judgements reported in the literature. Arguably, pairwise
rankings of two systems are the most common evaluation tasks in research or
deployment scenarios. Taking human judgement as a gold standard, we investigate
which metrics have the highest accuracy in predicting translation quality
rankings for such system pairs. Furthermore, we evaluate the performance of
various metrics across different language pairs and domains. Lastly, we show
that the sole use of BLEU impeded the development of improved models leading to
bad deployment decisions. We release the collection of 2.3M sentence-level
human judgements for 4380 systems for further analysis and replication of our
work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08614">
<div class="article-summary-box-inner">
<span><p>Question answering over knowledge graphs and other RDF data has been greatly
advanced, with a number of good systems providing crisp answers for natural
language questions or telegraphic queries. Some of these systems incorporate
textual sources as additional evidence for the answering process, but cannot
compute answers that are present in text alone. Conversely, systems from the IR
and NLP communities have addressed QA over text, but barely utilize semantic
data and knowledge. This paper presents the first QA system that can seamlessly
operate over RDF datasets and text corpora, or both together, in a unified
framework. Our method, called UNIQORN, builds a context graph on the fly, by
retrieving question-relevant triples from the RDF data and/or the text corpus,
where the latter case is handled by automatic information extraction. The
resulting graph is typically rich but highly noisy. UNIQORN copes with this
input by advanced graph algorithms for Group Steiner Trees, that identify the
best answer candidates in the context graph. Experimental results on several
benchmarks of complex questions with multiple entities and relations, show that
UNIQORN, an unsupervised method with only five parameters, produces results
comparable to the state-of-the-art on KGs, text corpora, and heterogeneous
sources. The graph-based methodology provides user-interpretable evidence for
the complete answering process.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DEGREE: A Data-Efficient Generative Event Extraction Model. (arXiv:2108.12724v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12724">
<div class="article-summary-box-inner">
<span><p>Event extraction (EE) aims to identify structured events, including event
triggers and their corresponding arguments, from unstructured text. Most of the
existing works rely on a large number of labeled instances to train models,
while the labeled data could be expensive to be obtained. In this work, we
present a data-efficient event extraction method by formulating event
extraction as a natural language generation problem. The formulation allows us
to inject knowledge of label semantics, event structure, and output
dependencies into the model. Given a passage and an event type, our model
learns to summarize this passage into a templated sentence in a predefined
structure. The template is event-type-specific, manually created, and contains
event trigger and argument information. Lastly, a rule-based algorithm is used
to derive the trigger and argument predictions from the generated sentence. Our
method inherently enjoys the following benefits: (1) The pretraining of the
generative language models help incorporate the semantics of the labels for
generative EE. (2) The autoregressive generation process and our end-to-end
design for extracting triggers and arguments force the model to capture the
dependencies among the output triggers and their arguments. (3) The predefined
templates form concrete yet flexible rules to hint the models about the valid
patterns for each event type, reducing the models' burden to learn structures
from the data. Empirical results show that our model achieves superior
performance over strong baselines on EE tasks in the low data regime and
achieves competitive results to the current state-of-the-art when more data
becomes available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Retrieval Augmented Generation for Zero-shot Slot Filling. (arXiv:2108.13934v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13934">
<div class="article-summary-box-inner">
<span><p>Automatically inducing high quality knowledge graphs from a given collection
of documents still remains a challenging problem in AI. One way to make headway
for this problem is through advancements in a related task known as slot
filling. In this task, given an entity query in form of [Entity, Slot, ?], a
system is asked to fill the slot by generating or extracting the missing value
exploiting evidence extracted from relevant passage(s) in the given document
collection. The recent works in the field try to solve this task in an
end-to-end fashion using retrieval-based language models. In this paper, we
present a novel approach to zero-shot slot filling that extends dense passage
retrieval with hard negatives and robust training procedures for retrieval
augmented generation models. Our model reports large improvements on both T-REx
and zsRE slot filling datasets, improving both passage retrieval and slot value
generation, and ranking at the top-1 position in the KILT leaderboard.
Moreover, we demonstrate the robustness of our system showing its domain
adaptation capability on a new variant of the TACRED dataset for slot filling,
through a combination of zero/few-shot learning. We release the source code and
pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixup Decoding for Diverse Machine Translation. (arXiv:2109.03402v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03402">
<div class="article-summary-box-inner">
<span><p>Diverse machine translation aims at generating various target language
translations for a given source language sentence. Leveraging the linear
relationship in the sentence latent space introduced by the mixup training, we
propose a novel method, MixDiversity, to generate different translations for
the input sentence by linearly interpolating it with different sentence pairs
sampled from the training corpus when decoding. To further improve the
faithfulness and diversity of the translations, we propose two simple but
effective approaches to select diverse sentence pairs in the training corpus
and adjust the interpolation weight for each pair correspondingly. Moreover, by
controlling the interpolation weight, our method can achieve the trade-off
between faithfulness and diversity without any additional training, which is
required in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14
en-de, and WMT'17 zh-en are conducted to show that our method substantially
outperforms all previous diverse machine translation methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories. (arXiv:2109.03754v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03754">
<div class="article-summary-box-inner">
<span><p>Measuring event salience is essential in the understanding of stories. This
paper takes a recent unsupervised method for salience detection derived from
Barthes Cardinal Functions and theories of surprise and applies it to longer
narrative forms. We improve the standard transformer language model by
incorporating an external knowledgebase (derived from Retrieval Augmented
Generation) and adding a memory mechanism to enhance performance on longer
works. We use a novel approach to derive salience annotation using
chapter-aligned summaries from the Shmoop corpus for classic literary works.
Our evaluation against this data demonstrates that our salience detection model
improves performance over and above a non-knowledgebase and memory augmented
language model, both of which are crucial to this improvement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PPT: Pre-trained Prompt Tuning for Few-shot Learning. (arXiv:2109.04332v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04332">
<div class="article-summary-box-inner">
<span><p>Prompts for pre-trained language models (PLMs) have shown remarkable
performance by bridging the gap between pre-training tasks and various
downstream tasks. Among these methods, prompt tuning, which freezes PLMs and
only tunes soft prompts, provides an efficient and effective solution for
adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to
be fully explored. In our pilot experiments, we find that prompt tuning
performs comparably with conventional full-model fine-tuning when downstream
data are sufficient, whereas it performs much worse under few-shot learning
settings, which may hinder the application of prompt tuning in practice. We
attribute this low performance to the manner of initializing soft prompts.
Therefore, in this work, we propose to pre-train prompts by adding soft prompts
into the pre-training stage to obtain a better initialization. We name this
Pre-trained Prompt Tuning framework "PPT". To ensure the generalization of PPT,
we formulate similar classification tasks into a unified task form and
pre-train soft prompts for this unified task. Extensive experiments show that
tuning pre-trained prompts for downstream tasks can reach or even outperform
full-model fine-tuning under both full-data and few-shot settings. Our approach
is effective and efficient for using large-scale PLMs in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks. (arXiv:2109.04604v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04604">
<div class="article-summary-box-inner">
<span><p>The general goal of text simplification (TS) is to reduce text complexity for
human consumption. This paper investigates another potential use of neural TS:
assisting machines performing natural language processing (NLP) tasks. We
evaluate the use of neural TS in two ways: simplifying input texts at
prediction time and augmenting data to provide machines with additional
information during training. We demonstrate that the latter scenario provides
positive effects on machine performance on two separate datasets. In
particular, the latter use of TS improves the performances of LSTM (1.82-1.98%)
and SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,
real-world relation extraction task. Further, the same setting yields
improvements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT
text classifier on MNLI, a practical natural language inference dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems. (arXiv:2109.04645v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04645">
<div class="article-summary-box-inner">
<span><p>As labeling cost for different modules in task-oriented dialog (ToD) systems
is high, a major challenge in practice is to learn different tasks with the
least amount of labeled data. Recently, prompting methods over pre-trained
language models (PLMs) have shown promising results for few-shot learning in
ToD. To better utilize the power of PLMs, this paper proposes Comprehensive
Instruction (CINS) that exploits PLMs with extra task-specific instructions. We
design a schema (definition, constraint, prompt) of instructions and their
customized realizations for three important downstream tasks in ToD, i.e.
intent classification, dialog state tracking, and natural language generation.
A sequence-to-sequence model (T5) is adopted to solve these three tasks in a
unified framework. Extensive experiments are conducted on these ToD tasks in
realistic few-shot learning scenarios with small validation data. Empirical
results demonstrate that the proposed CINS approach consistently improves
techniques that finetune PLMs with raw input or short prompts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RoR: Read-over-Read for Long Document Machine Reading Comprehension. (arXiv:2109.04780v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04780">
<div class="article-summary-box-inner">
<span><p>Transformer-based pre-trained models, such as BERT, have achieved remarkable
results on machine reading comprehension. However, due to the constraint of
encoding length (e.g., 512 WordPiece tokens), a long document is usually split
into multiple chunks that are independently read. It results in the reading
field being limited to individual chunks without information collaboration for
long document machine reading comprehension. To address this problem, we
propose RoR, a read-over-read method, which expands the reading field from
chunk to document. Specifically, RoR includes a chunk reader and a document
reader. The former first predicts a set of regional answers for each chunk,
which are then compacted into a highly-condensed version of the original
document, guaranteeing to be encoded once. The latter further predicts the
global answers from this condensed document. Eventually, a voting strategy is
utilized to aggregate and rerank the regional and global answers for final
prediction. Extensive experiments on two benchmarks QuAC and TriviaQA
demonstrate the effectiveness of RoR for long document reading. Notably, RoR
ranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of
submission (May 17th, 2021).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05238">
<div class="article-summary-box-inner">
<span><p>Simultaneous machine translation (SiMT) generates translation before reading
the entire source sentence and hence it has to trade off between translation
quality and latency. To fulfill the requirements of different translation
quality and latency in practical applications, the previous methods usually
need to train multiple SiMT models for different latency levels, resulting in
large computational costs. In this paper, we propose a universal SiMT model
with Mixture-of-Experts Wait-k Policy to achieve the best translation quality
under arbitrary latency with only one trained model. Specifically, our method
employs multi-head attention to accomplish the mixture of experts where each
head is treated as a wait-k expert with its own waiting words number, and given
a test latency and source inputs, the weights of the experts are accordingly
adjusted to produce the best translation. Experiments on three datasets show
that our method outperforms all the strong baselines under different latency,
including the state-of-the-art adaptive policy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. (arXiv:2109.05244v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05244">
<div class="article-summary-box-inner">
<span><p>Cross-attention is an important component of neural machine translation
(NMT), which is always realized by dot-product attention in previous methods.
However, dot-product attention only considers the pair-wise correlation between
words, resulting in dispersion when dealing with long sentences and neglect of
source neighboring relationships. Inspired by linguistics, the above issues are
caused by ignoring a type of cross-attention, called concentrated attention,
which focuses on several central words and then spreads around them. In this
work, we apply Gaussian Mixture Model (GMM) to model the concentrated attention
in cross-attention. Experiments and analyses we conducted on three datasets
show that the proposed method outperforms the baseline and has significant
improvement on alignment quality, N-gram accuracy, and long sentence
translation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora. (arXiv:2109.05406v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05406">
<div class="article-summary-box-inner">
<span><p>Human conversations consist of reasonable and natural topic flows, which are
observed as the shifts of the mentioned concepts across utterances. Previous
chatbots that incorporate the external commonsense knowledge graph prove that
modeling the concept shifts can effectively alleviate the dull and
uninformative response dilemma. However, there still exists a gap between the
concept relations in the natural conversation and those in the external
commonsense knowledge graph, which is an issue to solve. Specifically, the
concept relations in the external commonsense knowledge graph are not
intuitively built from the conversational scenario but the world knowledge,
which makes them insufficient for the chatbot construction. To bridge the above
gap, we propose the method to supply more concept relations extracted from the
conversational corpora and reconstruct an enhanced concept graph for the
chatbot construction. In addition, we present a novel, powerful, and fast graph
encoding architecture named the Edge-Transformer to replace the traditional GNN
architecture. Experimental results on the Reddit conversation dataset indicate
our proposed method significantly outperforms strong baseline systems and
achieves new SOTA results. Further analysis individually proves the
effectiveness of the enhanced concept graph and the Edge-Transformer
architecture.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating Language-Dependent Ethnic Bias in BERT. (arXiv:2109.05704v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05704">
<div class="article-summary-box-inner">
<span><p>BERT and other large-scale language models (LMs) contain gender and racial
bias. They also exhibit other dimensions of social bias, most of which have not
been studied in depth, and some of which vary depending on the language. In
this paper, we study ethnic bias and how it varies across languages by
analyzing and mitigating ethnic bias in monolingual BERT for English, German,
Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we
develop a novel metric called Categorical Bias score. Then we propose two
methods for mitigation; first using a multilingual model, and second using
contextual word alignment of two monolingual models. We compare our proposed
methods with monolingual BERT and show that these methods effectively alleviate
the ethnic bias. Which of the two methods works better depends on the amount of
NLP resources available for that language. We additionally experiment with
Arabic and Greek to verify that our proposed methods work for a wider variety
of languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation. (arXiv:2109.05729v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05729">
<div class="article-summary-box-inner">
<span><p>In this paper, we take the advantage of previous pre-trained models (PTMs)
and propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different
from previous Chinese PTMs, CPT is designed for both natural language
understanding (NLU) and natural language generation (NLG) tasks. CPT consists
of three parts: a shared encoder, an understanding decoder, and a generation
decoder. Two specific decoders with a shared encoder are pre-trained with
masked language modeling (MLM) and denoising auto-encoding (DAE) tasks,
respectively. With the partially shared architecture and multi-task
pre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks
with two decoders and (2) be fine-tuned flexibly that fully exploits the
potential of the model. Moreover, the unbalanced Transformer saves the
computational and storage cost, which makes CPT competitive and greatly
accelerates the inference of text generation. Experimental results on a wide
range of Chinese NLU and NLG tasks show the effectiveness of CPT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Categorical Semantics of Reversible Pattern-Matching. (arXiv:2109.05837v2 [cs.LO] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05837">
<div class="article-summary-box-inner">
<span><p>This paper is concerned with categorical structures for reversible
computation. In particular, we focus on a typed, functional reversible language
based on Theseus. We discuss how join inverse rig categories do not in general
capture pattern-matching, the core construct Theseus uses to enforce
reversibility. We then derive a categorical structure to add to join inverse
rig categories in order to capture pattern-matching. We show how such a
structure makes an adequate model for reversible pattern-matching.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Question Answering over Electronic Devices: A New Benchmark Dataset and a Multi-Task Learning based QA Framework. (arXiv:2109.05897v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05897">
<div class="article-summary-box-inner">
<span><p>Answering questions asked from instructional corpora such as E-manuals,
recipe books, etc., has been far less studied than open-domain factoid
context-based question answering. This can be primarily attributed to the
absence of standard benchmark datasets. In this paper we meticulously create a
large amount of data connected with E-manuals and develop suitable algorithm to
exploit it. We collect E-Manual Corpus, a huge corpus of 307,957 E-manuals and
pretrain RoBERTa on this large corpus. We create various benchmark QA datasets
which include question answer pairs curated by experts based upon two
E-manuals, real user questions from Community Question Answering Forum
pertaining to E-manuals etc. We introduce EMQAP (E-Manual Question Answering
Pipeline) that answers questions pertaining to electronics devices. Built upon
the pretrained RoBERTa, it harbors a supervised multi-task learning framework
which efficiently performs the dual tasks of identifying the section in the
E-manual where the answer can be found and the exact answer span within that
section. For E-Manual annotated question-answer pairs, we show an improvement
of about 40% in ROUGE-L F1 scores over the most competitive baseline. We
perform a detailed ablation study and establish the versatility of EMQAP across
different circumstances. The code and datasets are shared at
https://github.com/abhi1nandy2/EMNLP-2021-Findings, and the corresponding
project website is https://sites.google.com/view/emanualqa/home.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color. (arXiv:2109.06129v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.06129">
<div class="article-summary-box-inner">
<span><p>Pretrained language models have been shown to encode relational information,
such as the relations between entities or concepts in knowledge-bases --
(Paris, Capital, France). However, simple relations of this type can often be
recovered heuristically and the extent to which models implicitly reflect
topological structure that is grounded in world, such as perceptual structure,
is unknown. To explore this question, we conduct a thorough case study on
color. Namely, we employ a dataset of monolexemic color terms and color chips
represented in CIELAB, a color space with a perceptually meaningful distance
metric.
</p>
<p>Using two methods of evaluating the structural alignment of colors in this
space with text-derived color term representations, we find significant
correspondence. Analyzing the differences in alignment across the color
spectrum, we find that warmer colors are, on average, better aligned to the
perceptual color space than cooler ones, suggesting an intriguing connection to
findings from recent work on efficient communication in color naming. Further
analysis suggests that differences in alignment are, in part, mediated by
collocationality and differences in syntactic usage, posing questions as to the
relationship between color perception and usage and context.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-15 04:21:13.639879607 UTC">2021-09-15 04:21:13 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>