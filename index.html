<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-08T01:51:03.535825020Z">09-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">ğŸ„ğŸ’¨Rustå·¥ç¨‹å¸ˆæ•è¾¹èµ„æ–™</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c1c166d7-83ab-4818-a50e-0d76ef059a67">
<div class="article-summary-box-inner">
<span><p>æˆ‘æ­£åœ¨æ•´ç†æ›´æ–°ä¸­ï¼Œæ¬¢è¿å¤§å®¶å’Œæˆ‘ä¸€èµ·ï¼Œç»´æŠ¤å®ƒã€‚
å†…å®¹åŒ…æ‹¬ï¼š</p>
<p>ğŸ„ğŸ’¨Rustå·¥ç¨‹å¸ˆæ•è¾¹èµ„æ–™</p>
<p><img src="https://cdn.learnku.com/uploads/images/202109/07/86344/SzlA0SUmUD.png!large" alt="img"></p>
<p>é¡¹ç›®åœ°å€ï¼šhttps://github.com/0voice/Understanding_in_Rust</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust è°ƒç”¨cè¿”å›çš„åŠ¨æ€æ•°ç»„</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=a048c6cc-cbdd-4965-80d7-5aa0097678a7">
<div class="article-summary-box-inner">
<span><p>C ä½¿ç”¨mallocç”Ÿæˆä¸€ä¸ªåŠ¨æ€é•¿åº¦çš„æ•°ç»„ï¼Œå…·ä½“ç±»å‹å¯çŸ¥ã€‚Cå¦‚ä½•ä¼ é€’ç»™rust æ‰èƒ½ä½¿rustçŸ¥é“è¯¥æ•°ç»„çš„é•¿åº¦å’Œåœ°å€ï¼Ÿ</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">å¤šåœ°æ‹›è˜ ï½œå®‰å…¨æ€§æé«˜çš„Rustï¼Œæƒ³æ¥å­¦å—ï¼Ÿ</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=a48abd3f-43dc-4456-bade-0f94ccfa72b4">
<div class="article-summary-box-inner">
<span><p>éå‡¸ç§‘æŠ€ï¼ˆhttps://ft.techï¼‰æ­£æˆä¸ºå›½å†…é‡‘èå¸‚åœºæ™ºèƒ½æŠ•èµ„äº¤æ˜“å¹³å°çš„å¼•é¢†è€…ï¼Œåœ¨åŸæœ‰åŸºç¡€ä¸Šå…¨é¢å‡çº§åˆ°äº’è”ç½‘æ–°ä¸€ä»£æŠ€æœ¯æ¶æ„ï¼Œé‡‡ç”¨Rustæ„å»ºæ™ºèƒ½ç®—æ³•äº¤æ˜“å¹³å°ï¼Œé€æ­¥è¿­ä»£ï¼ŒæŒç»­ä¸ºåˆ¸å•†ã€é‡åŒ–ç§å‹Ÿç­‰ä¼—å¤šå¤§å‹é‡‘èæœºæ„æä¾›ä¼˜è´¨çš„ç®—æ³•æœåŠ¡ã€‚</p>
<p>Rustå·²ç»è¿ç»­äº”å¹´è¢«å¼€å‘äººå‘˜è¯„ä¸ºâ€œæœ€ä¸ºå–œçˆ±â€çš„ç¼–ç¨‹è¯­è¨€ï¼Œå› ä¸ºå®ƒå¯ä»¥é¿å…æŸäº›ç±»å‹çš„å†…å­˜å®‰å…¨é”™è¯¯ï¼Œèƒ½ä»æ ¹æœ¬ä¸Šæ”¹å–„è½¯ä»¶æ¼æ´çš„ç°çŠ¶ã€‚</p>
<p>Why Rustï¼Ÿå®‰å…¨ã€å¹¶å‘ã€å®ç”¨ï¼Œæ–°æ—¶ä»£çš„ç³»ç»Ÿçº§ç¼–ç¨‹è¯­è¨€ã€‚</p>
<p>é«˜æ€§èƒ½: Rust é€Ÿåº¦æƒŠäººä¸”å†…å­˜åˆ©ç”¨ç‡æé«˜ã€‚ç”±äºæ²¡æœ‰è¿è¡Œæ—¶å’Œåƒåœ¾å›æ”¶ï¼Œå®ƒèƒ½å¤Ÿèƒœä»»å¯¹æ€§èƒ½è¦æ±‚ç‰¹åˆ«é«˜çš„æœåŠ¡ï¼Œå¯ä»¥åœ¨åµŒå…¥å¼è®¾å¤‡ä¸Šè¿è¡Œï¼Œè¿˜èƒ½è½»æ¾å’Œå…¶ä»–è¯­è¨€é›†æˆã€‚</p>
<p>å¯é æ€§ï¼šRust ä¸°å¯Œçš„ç±»å‹ç³»ç»Ÿå’Œæ‰€æœ‰æƒæ¨¡å‹ä¿è¯äº†å†…å­˜å®‰å…¨å’Œçº¿ç¨‹å®‰å…¨ï¼Œè®©æ‚¨åœ¨ç¼–è¯‘æœŸå°±èƒ½å¤Ÿæ¶ˆé™¤å„ç§å„æ ·çš„é”™è¯¯ã€‚</p>
<p>ç”Ÿäº§åŠ›ï¼šRust æ‹¥æœ‰å‡ºè‰²çš„æ–‡æ¡£ã€å‹å¥½çš„ç¼–è¯‘å™¨å’Œæ¸…æ™°çš„é”™è¯¯æç¤ºä¿¡æ¯ï¼Œ è¿˜é›†æˆäº†ä¸€æµçš„å·¥å…·â€”â€”åŒ…ç®¡ç†å™¨å’Œæ„å»ºå·¥å…·ï¼Œæ™ºèƒ½åœ°è‡ªåŠ¨è¡¥å…¨å’Œç±»å‹æ£€éªŒçš„å¤šç¼–è¾‘å™¨æ”¯æŒï¼Œ ä»¥åŠè‡ªåŠ¨æ ¼å¼åŒ–ä»£ç ç­‰ã€‚</p>
<p>è®©Rustå£°åè¿œæ’­çš„ä¼˜ç‚¹è¿˜åŒ…æ‹¬ï¼šæä¾›Cå’ŒC++çš„é€Ÿåº¦å’Œæ§åˆ¶èƒ½åŠ›ï¼ŒåŒæ—¶è¿˜æä¾›äº†å…¶ä»–è¯­è¨€ï¼ˆå¦‚Goå’ŒPythonï¼‰çš„å®‰å…¨æ€§å’Œå®‰å…¨æ€§ä¿è¯ã€‚MSRCå°†è¿‘70ï¼…çš„æ¼æ´å½’ç±»ä¸ºå†…å­˜å®‰å…¨é—®é¢˜ï¼Œå› æ­¤æ¶ˆé™¤æ­¤ç±»æ¼æ´è‡³å…³é‡è¦ã€‚</p>
<p>ç°é˜¶æ®µï¼Œéå‡¸ç§‘æŠ€æ­£åœ¨å¯»æ‰¾è¡Œä¸šå†…ä¼˜ç§€çš„Rustå¼€å‘å·¥ç¨‹å¸ˆï¼Œè–ªèµ„ç¦åˆ©è¶…çº§ä¼˜åšã€‚å…³é”®æ˜¯å›¢é˜Ÿæœ‰å¾ˆå¥½çš„Rustå¼€å‘æ°›å›´ï¼ŒRustå¤§ç¥æ‰‹æŠŠæ‰‹è¾…å¯¼ï¼ŒåŠ©ä½ ä»Rustæ–°äººä¸æ–­å‡çº§ã€‚æ¬¢è¿åŠ å…¥æˆ‘ä»¬ï¼</p>
<p>å…¬å¸ç¦åˆ©ï¼š
1.æä¾›ç§Ÿæˆ¿è¡¥è´´ï¼›
2.æ—¥å¸¸ä¸é—´æ–­ç½‘çº¢é›¶é£Ÿã€é¥®æ–™ã€èŒ¶æ°´ä¾›ç»™ï¼›
3.ä¸å®šæœŸç»„ç»‡å„éƒ¨é—¨æŠ€æœ¯äº¤æµå­¦ä¹ ç ”è®¨ä¼šï¼Œåˆ†äº«å¿ƒå¾—ï¼Œäº’ç›¸æˆé•¿ï¼›
4.å›¢å»ºæ´»åŠ¨ä¸°å¯Œå¤šå½©ï¼Œæ”¾æ¾å¿ƒæƒ…ç¼“è§£ç–²åŠ³ã€‚</p>
<p>å·¥ä½œåœ°ç‚¹ï¼šåŒ—äº¬ã€ä¸Šæµ·ã€æˆéƒ½ã€æ·±åœ³ã€ç¾å›½åŠ å·</p>
<p>å²—ä½ï¼šRustå¼€å‘å·¥ç¨‹å¸ˆ</p>
<p>å²—ä½èŒè´£ï¼š
1.è®¾è®¡å¹¶å¼€å‘åŸºäºRUSTçš„é«˜æ€§èƒ½ï¼Œä½æ—¶å»¶ç®—æ³•äº¤æ˜“ç³»ç»Ÿï¼›
2.è®¾è®¡å¹¶å¼€å‘æ•°æ®å¤„ç†å¹³å°ï¼Œç›‘æ§è¿ç»´å¹³å°ï¼›
3.è®¾è®¡å¹¶å¼€å‘é¢å‘å®¢æˆ·çš„é«˜å¯ç”¨äº¤æ˜“å·¥å…·ç­‰ï¼›
4.è®¾è®¡å¹¶å¼€å‘ç­–ç•¥ç›¸å…³çš„å›æµ‹å¹³å°ã€‚</p>
<p>å²—ä½è¦æ±‚ï¼š
1.æœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼ˆ985ä¼˜å…ˆï¼‰ã€‚ç¼–ç¨‹åŸºç¡€æ‰å®ï¼Œå…·æœ‰è‰¯å¥½çš„è®¡ç®—æœºç†è®ºåŸºç¡€ï¼›
2.ç†Ÿç»ƒæŒæ¡Linuxæ“ä½œï¼Œæ€§èƒ½åˆ†æï¼Œå…·å¤‡Rust/C++/Java/Goä¸°å¯Œå¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰å¸¸ç”¨çš„è®¾è®¡æ¨¡å¼ï¼Œæœ‰åˆ†å¸ƒå¼ç›¸å…³ç»éªŒåŠ åˆ†ï¼›
3.æœ‰ç ”å‘é«˜æ€§èƒ½ï¼Œä½æ—¶å»¶ç³»ç»Ÿç»éªŒåŠ åˆ†ï¼›
4.å¯¹æŠ€æœ¯å……æ»¡çƒ­æƒ…ï¼Œæ€è€ƒæ·±å…¥ã€‚è‡ªæˆ‘é©±åŠ¨ï¼Œèƒ½å¿«é€Ÿå­¦ä¹ æ–°é²œäº‹ç‰©ã€‚</p>
<p>å²—ä½è–ªé…¬ï¼šBase 30K-60K+ï¼Œæœ‰æœŸæƒæ¿€åŠ±</p>
<p>æŠ•é€’é‚®ç®±ï¼šrecruit@non-convex.com
è”ç³»äººåŠ å¾®ä¿¡ï¼šSweeneyTodd333333</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rustæ—¥æŠ¥ã€‘2021-09-07</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=fd95c63b-08d3-468a-9743-82feb56df3f2">
<div class="article-summary-box-inner">
<span><h2>ã€å¤§å®¶çš„é¡¹ç›®ã€‘ <code>Poem-openapi</code> v0.2.1 å‘å¸ƒ</h2>
<p><a href="https://www.reddit.com/r/rust/comments/pjhoiu/poemopenapi_021_released/" rel="noopener noreferrer">Poem-openapi 0.2.1 released!</a></p>
<p><strong>Poem-openapi</strong>:</p>
<blockquote>
<p><code>Poem-openapi</code> ä½¿ä½ èƒ½å¤Ÿæ–¹ä¾¿å¿«æ·çš„æ„å»ºç¬¦åˆ <code>OpenAPIv3</code>æ ‡å‡†çš„åº”ç”¨ç¨‹åºæ¥å£; é€šè¿‡ä½¿ç”¨è¿‡ç¨‹å®æ¥ç”Ÿæˆå¤§é‡æ ·æ¿ä»£ç ï¼Œä½ å°†æœ‰æ›´å¤šæ—¶é—´å’Œç²¾åŠ›æ¥ä¸“æ³¨äºå®ç°æ›´é‡è¦çš„ä¸šåŠ¡é€»è¾‘ã€‚</p>
</blockquote>
<p><strong>åŠŸèƒ½ç‰¹æ€§</strong>:</p>
<ul>
<li>å®Œå…¨æ”¯æŒ <code>async/await</code>;</li>
<li>ç±»å‹å®‰å…¨;</li>
<li>å¯¹<code>Rustfmt</code>ç”¨æˆ·å‹å¥½ ï¼ˆè¿‡ç¨‹å®ï¼‰;</li>
<li>æœ€å°å¼€é”€;</li>
</ul>
<p><strong>ç¤ºä¾‹</strong>:</p>
<p>ä»£ç ï¼š</p>
<pre><code>use poem_openapi::{payload::PlainText, OpenAPI, API};

struct Api;

#[API]
impl Api {
    #[oai(path = "/", method = "get")]
    async fn index(&amp;self, #[oai(name = "name", in = "query")] name: Option&lt;String&gt;) -&gt; PlainText {
        match name {
            Some(name) =&gt; format!("hello, {}!", name).into(),
            None =&gt; "hello!".into(),
        }
    }
}

#[tokio::main]
async fn main() {
    poem::Server::bind("127.0.0.1:3000")
        .await
        .unwrap()
        .run(OpenAPI::new(Api).title("hello World").ui_path("/ui"))
        .await
        .unwrap();
}
</code></pre>
<p>è¿è¡Œï¼š</p>
<pre><code>[test@localhost poem-openapi]$ cargo run --example hello_world

[test@localhost poem-openapi]$ curl http://localhost:3000
hello!

[test@localhost poem-openapi]$ curl http://localhost:3000\?name\=sunli
hello, sunli!
</code></pre>
<p><strong>æ›´å¤šä¿¡æ¯</strong>ï¼š</p>
<ul>
<li>é¡¹ç›®åœ°å€ï¼š<a href="https://github.com/poem-web/poem-openapi" rel="noopener noreferrer">https://github.com/poem-web/poem-openapi</a></li>
<li>å¼€æºå£°æ˜ï¼š<a href="https://rustcc.cn/article?id=334551fe-a8a9-4561-aa63-11987b10e81f" rel="noopener noreferrer">Poem-openapiå¼€æºäº†!</a></li>
<li>ä½œè€…ä¿¡æ¯ï¼š<a href="https://github.com/sunli829" rel="noopener noreferrer">æ²¹æ¡å“¥ä¸»é¡µ</a></li>
</ul>
<hr>
<h2>Relm4 v0.1 å‘å¸ƒ</h2>
<p><a href="https://aaronerhardt.github.io/blog/posts/announcing_relm4/" rel="noopener noreferrer">Announcing Relm4 v0.1</a></p>
<p>åœ¨ç¬¬ä¸€ä¸ªæµ‹è¯•ç‰ˆå‘å¸ƒå¤§çº¦ä¸€ä¸ªæœˆåï¼Œç»è¿‡æ— æ•°ä¸ªå°æ—¶çš„å·¥ä½œï¼Œä½œè€…é«˜å…´åœ°å®£å¸ƒRelm4çš„ç¬¬ä¸€ä¸ªç¨³å®šç‰ˆæœ¬æ­£å¼å‘å¸ƒï¼</p>
<p><strong>å…³äºRelm4</strong>ï¼š</p>
<p><code>Relm4</code>æ˜¯ä¸€ä¸ªå—<code>Elm</code>å¯å‘å¹¶åŸºäº<code>gtk4-rs</code>çš„æƒ¯ç”¨GUIåº“ã€‚å®ƒæ˜¯ä¸€ä¸ªä»å¤´å¼€å§‹æ„å»ºçš„<code>relm</code>çš„æ–°ç‰ˆæœ¬ï¼Œå¹¶ä¸”å…¼å®¹<code>gtk4</code>å’Œ<code>libadwaita</code>ã€‚<code>Relm4</code>çš„ä¸»è¦ç›®æ ‡æ˜¯ç”Ÿäº§æ•ˆç‡ã€çµæ´»æ€§ã€ç®€å•æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚</p>
<p><strong>åŠŸèƒ½ç‰¹æ€§</strong></p>
<ul>
<li>æ”¯æŒ<code>libadwaita</code>;</li>
<li>é…å¥—ä¹¦ç±<a href="https://aaronerhardt.github.io/relm4-book/book/" rel="noopener noreferrer"><em>GUI development with Relm4</em></a> å·²å®Œç»“;</li>
<li>æ–°å¢æ”¯æŒéé˜»å¡IOçš„æ¶ˆæ¯å¥æŸ„;</li>
<li>æ›´å¤šçš„å¯å¤ç”¨ç»„ä»¶;</li>
<li>è®¸å¤šå…¶ä»–çš„æ”¹è¿›å’Œä¿®å¤;</li>
</ul>
<p>å®Œæ•´çš„ChangeLogå¯ä»¥å‚è§ï¼š
<a href="https://github.com/AaronErhardt/relm4/blob/main/CHANGES.md" rel="noopener noreferrer">https://github.com/AaronErhardt/relm4/blob/main/CHANGES.md</a></p>
<p><strong>æ›´å¤šä¿¡æ¯</strong>ï¼š</p>
<ul>
<li>é¡¹ç›®åœ°å€ï¼š<a href="https://github.com/AaronErhardt/relm4" rel="noopener noreferrer">https://github.com/AaronErhardt/relm4</a></li>
<li>é¡¹ç›®æ–‡æ¡£ï¼š<a href="https://aaronerhardt.github.io/docs/relm4/relm4/" rel="noopener noreferrer">https://aaronerhardt.github.io/docs/relm4/relm4/</a></li>
<li>å‚è€ƒä¹¦ç±ï¼š<a href="https://aaronerhardt.github.io/relm4-book/book/" rel="noopener noreferrer"><em>GUI development with Relm4</em></a></li>
</ul>
<hr>
<h2>Ockamç¤ºä¾‹: æ„å»ºä¸€ä¸ªå¯ä»¥å®‰å…¨è®¿é—®è¿œç¨‹ç§æœ‰ç½‘ç»œçš„é€šé“</h2>
<p><a href="https://github.com/ockam-network/ockam/tree/develop/documentation/use-cases/secure-remote-access-tunnels#readme" rel="noopener noreferrer">Build a secure access tunnel to a service in a remote private network</a></p>
<p>Ockamæ˜¯ä¸€ä¸ªæ”¯æŒç«¯åˆ°ç«¯åŠ å¯†ã€åŒå‘è®¤è¯ã€ç½‘ç»œå®‰å…¨çš„Rustå’ŒElixirè¯­è¨€é€šä¿¡åº“ã€‚</p>
<p>åœ¨æœ¬ç¯‡åšæ–‡ä¸­ï¼Œä½œè€…è¯¦ç»†çš„ä»‹ç»äº†å¦‚ä½•ä½¿ç”¨<code>Ockam</code>åœ¨Rustä¸­é€šè¿‡çº¦<code>20è¡Œ</code>ä»£ç æ¥æ„å»ºä¸€ä¸ªå¯ä»¥å®‰å…¨è®¿é—®è¿œç¨‹ç§æœ‰ç½‘ç»œä¸­è®¾å¤‡çš„é€šé“ã€‚</p>
<hr>
<h2>Skiff: ä¸€é—¨ç”¨Rustç¼–å†™çš„é€æ¸ç±»å‹åŒ–çš„å‡½æ•°å¼ç¼–ç¨‹è¯­è¨€</h2>
<p><a href="https://www.reddit.com/r/ProgrammingLanguages/comments/pjcewi/introducing_skiff_a_gradually_typed_functional/" rel="noopener noreferrer">Introducing Skiff, a gradually typed functional language written in Rust</a></p>
<p>Skiffï¼Œæ˜¯ä¸€é—¨ç”¨Rustç¼–å†™çš„é€æ¸ç±»å‹åŒ–çš„å‡½æ•°å¼ç¼–ç¨‹è¯­è¨€ã€‚æ‰€è°“é€æ¸ç±»å‹åŒ–æ˜¯æŒ‡ä½œè€…è®¡åˆ’ä¸‹ä¸€æ­¥é€šè¿‡æ·»åŠ ç±»å‹åŒ–å…³é”®å­—æ¥åŒºåˆ†å®Œå…¨ç±»å‹å‡½æ•°å’Œéƒ¨åˆ†ç±»å‹å‡½æ•°ã€‚</p>
<p>Skiffå—<code>Elm</code>/<code>Pyret</code>/<code>Python</code>è¯­è¨€å¯å‘ï¼Œå¹¶å—<code>Rust</code>/<code>Javascript</code>/<code>Typescript</code>/<code>Haskell</code>/<code>OCaml</code>/<code>Lua</code>ç­‰è¯­è¨€å½±å“ï¼Œå½“å‰è¯­è¨€åŠŸèƒ½è¿˜åœ¨æŒç»­å®Œå–„ä¸­ï¼Œä½œè€…æä¾›äº†ä¸€ä¸ªç”±<code>wasm!</code>é©±åŠ¨çš„<a href="https://skiff.paulbiberstein.me/" rel="noopener noreferrer">ç½‘é¡µç¼–è¾‘å™¨</a>å¯ä¾›è¯»è€…å­¦ä¹ ä½¿ç”¨ï¼Œæ›´å¤šä¿¡æ¯è¯·è®¿é—®é¡¹ç›®ä¸»é¡µçš„<a href="https://github.com/P-bibs/skiff/" rel="noopener noreferrer">Readme</a>ã€‚</p>
<p><strong>æ›´å¤šä¿¡æ¯</strong>ï¼š</p>
<ul>
<li>é¡¹ç›®åœ°å€ï¼š<a href="https://github.com/P-bibs/skiff/" rel="noopener noreferrer">https://github.com/P-bibs/skiff/</a></li>
<li>ç½‘é¡µç¼–è¾‘å™¨ï¼š<a href="https://skiff.paulbiberstein.me/" rel="noopener noreferrer">https://skiff.paulbiberstein.me/</a></li>
</ul>
<hr>
<p>From æ—¥æŠ¥å°ç»„ odd-cat</p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<p><a href="https://rustcc.cn/" rel="noopener noreferrer">Rust.cc è®ºå›: æ”¯æŒ rss</a></p>
<p><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRust è¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">40k-80kæ‹›è˜å…¨èŒè¿œç¨‹åŒºå—é“¾å¼€å‘å·¥ç¨‹å¸ˆ</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=5c42bdfd-0e97-4fc4-a494-4c9d14a5679f">
<div class="article-summary-box-inner">
<span><p>å…¬å¸ç®€ä»‹
æˆ‘ä»¬æ˜¯ä¸€å®¶æ€»éƒ¨ä½äºç¡…è°·çš„åŒºå—é“¾åˆåˆ›ä¼ä¸š,æˆ‘ä»¬è·Ÿç»å¤§éƒ¨åˆ†åŒºå—é“¾å…¬å¸ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬æœ‰é¡¶çº§çš„äº§å“æ¥æ”¯æ’‘æˆ‘ä»¬çš„ä¸šåŠ¡å’Œå‘å±•è¿œæ™¯ã€‚é‡è¦çš„äº‹æƒ…è¯´ä¸‰éï¼Œæœ‰äº§å“ï¼Œæœ‰äº§å“ï¼Œæœ‰äº§å“ï¼Œè€Œä¸”æ˜¯ä¸šç•Œé¢†å…ˆçš„äº§å“ï¼</p>
<p>ç›®å‰æˆ‘ä»¬åœ¨å…¨çƒ150å¤šä¸ªå›½å®¶æ‹¥æœ‰20,000+çš„ç”¨æˆ·ï¼Œ30,000+èŠ‚ç‚¹ã€‚</p>
<p>ç›®å‰å…¬å¸å·²ç»ç›ˆåˆ©ï¼Œç°é‡‘è´®å¤‡ä¸°åšï¼Œæ­£åœ¨å¯¹æ¥ä¸šç•Œæœ€ç­‰çº§çš„é£é™©æŠ•èµ„æœºæ„ï¼Œå¤„äºèµ·é£çš„å‰å¤•ã€‚</p>
<p>åŸºæœ¬è¦æ±‚
1ã€æ‰å®çš„è®¡ç®—æœºç§‘å­¦åŸºç¡€çŸ¥è¯†ï¼›</p>
<p>2ã€åŠ¨æ‰‹èƒ½åŠ›å¼ºï¼Œæœ‰æ­»ç£•ç²¾ç¥ï¼›</p>
<p>3ã€æœ‰ä¸°å¯Œçš„ Rust å¼€å‘ç»éªŒï¼›</p>
<p>4ã€æ›¾ç»ç‹¬ç«‹å®Œæˆæˆ–è€…ä¸»å¯¼å®Œæˆè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„é¡¹ç›®ï¼›</p>
<p>5ã€å¯¹å·¥ä½œæœ‰å¼ºå¤§çš„è´£ä»»å¿ƒã€‚</p>
<p>åŠ åˆ†é¡¹
1ã€åŒºå—é“¾ç›¸å…³æ•°æ®ç»“æ„ä¸ç®—æ³•ï¼›</p>
<p>2ã€Substrateæˆ–å…¶ä»–åŒºå—é“¾èŠ‚ç‚¹å¼€å‘ç»éªŒï¼›</p>
<p>3ã€è·¨é“¾ã€Layer 2 å¼€å‘ç»éªŒã€‚</p>
<p>è–ªèµ„ç¦åˆ©
1ã€æœˆè–ª40k-80kï¼›</p>
<p>2ã€å…¥èŒæ»¡ä¸€å¹´ï¼Œè¡¨ç°åˆæ ¼è€…å¯ä»¥è·å¾—å…¬å¸çš„è‚¡ç¥¨æˆ–æœŸæƒï¼›</p>
<p>3ã€ä¼˜ç§€è€…æä¾›ç§»æ°‘ç¾å›½ã€åŠ æ‹¿å¤§çš„æœºä¼šã€‚</p>
<p>å·¥ä½œæ–¹å¼
1ã€è¿œç¨‹åŠå…¬</p>
<p>2ã€å·¥ä½œæ—¶é—´ï¼šæ— å›ºå®šæ—¶é—´ï¼Œå·¥ä½œå®Œæˆåè‡ªç”±å®‰æ’</p>
<p>3ã€å¾ˆå°‘æœ‰è·¨æ—¶åŒºçš„ä¼šè®®ï¼Œé™¤ç‰¹æ®Šã€ç´§æ€¥å·¥ä½œä»»åŠ¡å¯¹æ¥å¤–</p>
<p>4ã€å·¥ä½œä¼šè®®ä¸»è¦ä¸ºä¸­æ–‡äº¤æµ</p>
<p>å·¥ä½œè¯­è¨€
1ã€è‹±æ–‡ä¸ºä¸»ï¼Œä¸­æ–‡ä¸ºè¾…</p>
<p>2ã€è‹±æ–‡è¦æ±‚è¯»å†™èƒ½åŠ›åˆæ ¼åŠä¼˜ç§€</p>
<p>é¢è¯•æµç¨‹
1ã€æ”¶åˆ°ç®€å†åï¼Œå°†å®‰æ’1åˆ°3è½®ç”µè¯è¯­éŸ³é¢è¯•ï¼›</p>
<p>2ã€ç”µè¯è¯­éŸ³é¢è¯•ç»“æŸåï¼Œå°†è¿›å…¥è¯•ç”¨æœŸï¼ˆå…¨é¢å·¥èµ„ï¼‰ï¼›</p>
<p>3ã€è¯•ç”¨æœŸç»“æŸåï¼Œæ­£å¼å¼€å§‹å·¥ä½œã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rustæ—¥æŠ¥ã€‘2021-09-06 åŠ å¿« Rust çš„ç¼–è¯‘</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=28141388-ce4a-4421-95b4-e2059e3b2347">
<div class="article-summary-box-inner">
<span><h1>åŠ å¿« Rust çš„ç¼–è¯‘</h1>
<p>ä¼—æ‰€å‘¨çŸ¥ï¼ŒRustä»£ç ç¼–è¯‘èµ·æ¥å¾ˆæ…¢ã€‚ä½†æˆ‘æœ‰ä¸€ç§å¼ºçƒˆçš„ç›´è§‰ï¼Œå¤§å¤šæ•°Rustä»£ç çš„ç¼–è¯‘é€Ÿåº¦æ¯”å®ƒæœ¬å¯ä»¥çš„è¦æ…¢å¾—å¤šã€‚</p>
<p>ä¾‹å¦‚, Rust çš„ <code>rust-analyzer</code> CI åœ¨ GitHub ä¸Šæ“ä½œéœ€è¦8åˆ†é’Ÿã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å½“å¤§å’Œå¤æ‚çš„é¡¹ç›®ï¼Œæœ‰20ä¸‡è¡Œè‡ªå·±çš„ä»£ç å’Œ100ä¸‡è¡Œä¾èµ–ã€‚</p>
<p>è·Ÿéšä½œè€…, è®©æˆ‘ä»¬è¿›ä¸€æ­¥äº†è§£å¦‚ä½•ä½¿ç¼–è¯‘æ—¶é—´ä¿æŒåœ¨åˆç†çš„èŒƒå›´å†…!</p>
<p><a href="https://matklad.github.io/2021/09/04/fast-rust-builds.html" rel="noopener noreferrer">åŸæ–‡é“¾æ¥</a></p>
<h1>async çš„å¦å¤–ä¸€ä¸ªè¯­æ³•æ–¹æ¡ˆ</h1>
<p>ä½œè€…å¯¹äº <code>async</code>çš„è¯­æ³•æ–¹æ¡ˆæå‡ºäº†å®Œæ•´çš„è‡ªå·±çš„è§£å†³æ–¹æ¡ˆ, éå¸¸æœ‰è¶£.</p>
<p><a href="https://ibraheem.ca/writings/an-alternative-async-fn-syntax/" rel="noopener noreferrer">åŸæ–‡é“¾æ¥</a></p>
<h1>Rust æ’ä»¶</h1>
<p>è¿™æ˜¯ä½œè€…çš„ç¬¬äºŒç¯‡å…³äº Rustæ’ä»¶çš„æ–‡ç« !
åœ¨è¿™é‡Œï¼Œä½œè€…å°†å°è¯•ç¼–å†™ä¸€äº› PDK (Plugin Development Kit, æ’ä»¶å¼€å‘å·¥å…·åŒ…) å¯èƒ½æ˜¯ä»€ä¹ˆæ ·å­çš„ç®€å•ä»£ç ï¼Œå¹¶å¯¹åœ¨ç¼–å†™è¿‡ç¨‹ä¸­å‡ºç°çš„é—®é¢˜åšä¸€äº›ç ”ç©¶ã€‚</p>
<p><a href="https://nullderef.com/blog/plugin-start/" rel="noopener noreferrer">åŸæ–‡é“¾æ¥</a></p>
<h1>sentinel-rust: Rust ç‰ˆæœ¬çš„ sentinel</h1>
<p><code>Sentinel</code> æ˜¯ä¸€ä¸ªé¢å‘åˆ†å¸ƒå¼æœåŠ¡æ¶æ„çš„é«˜å¯ç”¨æµé‡æ§åˆ¶ç»„ä»¶. ç°åœ¨ Rust ç‰ˆæœ¬å·²åŠ å…¥</p>
<p><a href="https://github.com/sentinel-group/sentinel-rust" rel="noopener noreferrer">github åœ°å€</a></p>
<h1><code>&lt;&lt;Programming Rust&gt;&gt;</code> ç¬¬äºŒç‰ˆç”µå­ä¹¦å·²ä¸Šæ¶</h1>
<p><code>&lt;&lt;Programming Rust&gt;&gt;</code> ç¬¬äºŒç‰ˆçš„åœ¨çº¿ç”µå­ä¹¦ç°å·²ä¸Šæ¶.</p>
<p><a href="https://www.lunaticai.com/2021/09/programming-rust-2nd-edition-pdf-github.html" rel="noopener noreferrer">åŸæ–‡é“¾æ¥</a></p>
<p>--</p>
<p>From æ—¥æŠ¥å°ç»„ BobQinï¼ŒFBIå°ç™½</p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustccè®ºå›: æ”¯æŒrss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust ffi è°ƒç”¨C++å‡½æ•°,C++å‡½æ•°è¿”å› ä¹‹æŒ‡å‘æ™ºèƒ½å¯¹è±¡çš„æŒ‡é’ˆ</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=6060f204-1000-4bc2-a492-1e452c7ae9ef">
<div class="article-summary-box-inner">
<span><p>\stable-x86_64-pc-windows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib\libcompiler_builtins-0f3806ca1d72c7be.rlib" "kernel32.lib" "ws2_32.lib" "advapi32.lib" "userenv.lib" "kernel32.lib" "msvcrt.lib" "/NXCOMPAT" "/LIBPATH:C:\Users\xiake\.rustup\toolchains\stable-x86_64-pc-windows-msvc\lib\rustlib\x86_64-pc-windows-msvc\lib" "/OUT:D:\other\github\rust_nav\testffi\target\debug\deps\testffi.exe" "/OPT:REF,NOICF" "/DEBUG" "/NATVIS:C:\Users\xiake\.rustup\toolchains\stable-x86_64-pc-windows-msvc\lib\rustlib\etc\intrinsic.natvis" "/NATVIS:C:\Users\xiake\.rustup\toolchains\stable-x86_64-pc-windows-msvc\lib\rustlib\etc\liballoc.natvis" "/NATVIS:C:\Users\xiake\.rustup\toolchains\stable-x86_64-pc-windows-msvc\lib\rustlib\etc\libcore.natvis" "/NATVIS:C:\Users\xiake\.rustup\toolchains\stable-x86_64-pc-windows-msvc\lib\rustlib\etc\libstd.natvis"
= note: navapp.lib(pch.obj) : MSIL .netmodule or module compiled with /GL found; restarting link with /LTCG; add /LTCG to the link command line to improve linker performance
Creating library D:\other\github\rust_nav\testffi\target\debug\deps\testffi.lib and object D:\other\github\rust_nav\testffi\target\debug\deps\testffi.exp
navapp.lib(navapp.obj) : error LNK2001: unresolved external symbol "public: class KBEngine::SmartPointer __cdecl KBEngine::Navigation::findNavigation(class std::basic_string&lt;char,struct std::char_traits,class std::allocator &gt;)" (?findNavigation@Navigation@KBEngine@@QEAA?AV?$SmartPointer@VNavigationHandle@KBEngine@@@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)
navapp.lib(navapp.obj) : error LNK2001: unresolved external symbol "public: class KBEngine::SmartPointer <em><em>cdecl KBEngine::Navigation::loadNavigation(class std::basic_string&lt;char,struct std::char_traits,class std::allocator &gt;,class std::map&lt;int,class std::basic_string&lt;char,struct std::char_traits,class std::allocator &gt;,struct std::less,class std::allocator&lt;struct std::pair&lt;int const ,class std::basic_string&lt;char,struct std::char_traits,class std::allocator &gt; &gt; &gt; &gt; const &amp;)" (?loadNavigation@Navigation@KBEngine@@QEAA?AV?$SmartPointer@VNavigationHandle@KBEngine@@@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$map@HV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$less@H@2@V?$allocator@U?$pair@$$CBHV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@std@@@2@@5@@Z)
navapp.lib(navapp.obj) : error LNK2001: unresolved external symbol "protected: static class KBEngine::Navigation * KBEngine::Singleton::singleton</em>" (?singleton</em>@?$Singleton@VNavigation@KBEngine@@@KBEngine@@1PEAVNavigation@2@EA)
D:\other\github\rust_nav\testffi\target\debug\deps\testffi.exe : fatal error LNK1120: 3 unresolved externals</p>
<p>C++éƒ¨åˆ†</p>
<p>DLLEXPORT KBEngine::NavigationHandlePtr* get_nav_handle(char* resPath_);</p>
<p>typedef SmartPointer NavigationHandlePtr;</p>
<p>template
class SmartPointer : public ConstSmartPointer
{
public:
typedef ConstSmartPointer ConstProxy;</p>
<pre><code>SmartPointer(T* obj = 0, typename ConstProxy::REF_TAG tag = ConstProxy::NEW_REF):
ConstProxy(obj, tag)
{
}

SmartPointer( const SmartPointer&lt;T&gt;&amp; P ) : ConstProxy( P ) { }

template&lt;class DerivedType&gt;
SmartPointer( ConstSmartPointer&lt;DerivedType&gt;&amp; dt ) :
	ConstProxy( dt.get() )
{
}
</code></pre>
<p>ä»£ç å¤ªé•¿çœç•¥</p>
<p>æ±‚å¤§ä½¬ç»™ç‚¹æç¤º å®åœ¨æ‰¾ä¸åˆ°åŸå› </p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">æœ‰æ²¡æœ‰æ–¹æ³•å°†ä¸€ä¸ªusizeçš„å€¼è½¬æˆi32çš„-1</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7f247165-5483-4b7e-b39b-7f2db042e5df">
<div class="article-summary-box-inner">
<span><p>ä»Šå¤©å†™leetcodeçš„æ¯æ—¥ä¸€é¢˜çš„æ—¶å€™çªå‘å¥‡æƒ³</p>
<pre><code>impl Solution {
    pub fn search(nums: Vec&lt;i32&gt;, target: i32) -&gt; i32 {
        nums.binary_search(&amp;target).unwrap_or_default() as i32
    }
}
</code></pre>
<p>åƒè¿™é‡Œèƒ½ä¸èƒ½ç”¨ä¸€ä¸ªunwrap_or(x) ç„¶åè½¬æˆå¤±è´¥è¿”å›-1</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">åˆè§RuståŒºå—é“¾æ‹›è˜ -_-!, but è¿™ä¸ªç‚¹è¿›æ¥ä¸åæ‚”^_^</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=640daecb-eec5-4b6d-87a1-02b640bb0433">
<div class="article-summary-box-inner">
<span><p>å¤§å®¶å¥½ï¼Œæˆ‘ä»¬æ˜¯Deeper Network, å®˜ç½‘æ˜¯deeper.network, ä¸€ä¸ªæ€»éƒ¨ä½äºç¡…è°·çš„åŒºå—é“¾åˆåˆ›ä¼ä¸š,æˆ‘ä»¬è·Ÿç»å¤§éƒ¨åˆ†åŒºå—é“¾å…¬å¸ä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬æœ‰é¡¶çº§çš„äº§å“æ¥æ”¯æ’‘æˆ‘ä»¬çš„ä¸šåŠ¡å’Œå‘å±•æ„¿æ™¯ã€‚é‡è¦çš„äº‹æƒ…è¯´ä¸‰éï¼Œæœ‰äº§å“ï¼Œæœ‰äº§å“ï¼Œæœ‰äº§å“ï¼Œè€Œä¸”æ˜¯ä¸šç•Œé¢†å…ˆçš„äº§å“ï¼</p>
<p>æˆ‘ä»¬çš„æ——èˆ°äº§å“Deeper Connectå·²ç»è¿­ä»£åˆ°æœ€æ–°çš„ç¬¬å››ä»£äº§å“ï¼Œå…¶ä¸­Deeper Connect miniåœ¨Indiegogoå¹³å°é¦–å‘ï¼Œä»…é¢„å”®æˆç»©å°±è¶…è¿‡270ä¸‡ç¾å…ƒï¼Œåœ¨Indiegogoå†å²ä¸Šçš„ä¸€ç™¾å¤šä¸‡ä¸ªé¡¹ç›®ä¸­æ’åå‰10ã€‚(å‰åä¸­çš„ç»å¤§éƒ¨åˆ†é¡¹ç›®éƒ½æ˜¯Sony, åä¸ºè¿™æ ·çº§åˆ«çš„å…¬å¸)</p>
<p>æˆ‘ä»¬å³å°†å‘å”®çš„æœ€æ–°äº§å“picoçš„ä»‹ç»ï¼Œå¯ä»¥åœ¨ä¸‹é¢æ‰¾åˆ°ï¼š</p>
<p>http://dev.deepernetwork.com:8088/down/tmp/pico.png</p>
<p>ä¸–ç•Œä¸Šæœ€å°ï¼Œæœ€è½»ï¼Œæœ€è–„ï¼ŒåŠŸèƒ½æœ€å¼ºå¤§çš„ç½‘ç»œå®‰å…¨+åŒºå—é“¾äº§å“ï¼šDeeper Connect Pico</p>
<p>ç›®å‰æˆ‘ä»¬åœ¨å…¨çƒ150å¤šä¸ªå›½å®¶æ‹¥æœ‰20ï¼Œ000+çš„ç”¨æˆ·ï¼Œ30ï¼Œ000+èŠ‚ç‚¹ã€‚</p>
<p>æˆ‘ä»¬äº2020å¹´å…¥é€‰äº†æ³¢å¡çš„builders programå¹¶ä¸”è·å¾—äº†Web 3.0åŸºé‡‘ä¼šèµåŠ©ï¼Œæ˜¯æ³¢å¡ç”Ÿæ€é‡è¦çš„ä¸€å‘˜ã€‚</p>
<p>2021å¹´ï¼Œæˆ‘ä»¬çš„åŒºå—é“¾ç½‘ç»œDeeper Chainè·å¾—æ³¢å¡ç¬¬ä¸€å±Šé»‘å®¢æ¾æ¯”èµ›çš„ç¤¾åŒºæœ€å—æ¬¢è¿å¥–å’Œå†³èµ›äºšå†›ã€‚</p>
<p>Deeper Connect + Deeper Chainæ˜¯ç›®å‰ä¸–ç•Œä¸Šå”¯ä¸€çš„å…¨æ ˆWEB3.0è§£å†³æ–¹æ¡ˆ,åŒ…æ‹¬ï¼šweb3.0ç½‘å…³ï¼Œå»ä¸­å¿ƒåŒ–å®‰å…¨ç½‘ç»œï¼Œå»ä¸­å¿ƒåŒ–å¹¿å‘Šï¼Œå»ä¸­å¿ƒåŒ–è§†é¢‘ç‚¹æ’­å¹³å°ï¼Œå»ä¸­å¿ƒåŒ–CDNç­‰ç­‰ã€‚</p>
<p>ç›®å‰å…¬å¸å·²ç»ç›ˆåˆ©ï¼Œç°é‡‘è´®å¤‡ä¸°åšï¼Œæ­£åœ¨å¯¹æ¥ä¸šç•Œæœ€é¡¶çº§çš„é£é™©æŠ•èµ„æœºæ„ï¼Œå¤„äºèµ·é£çš„å‰å¤•ã€‚</p>
<p>æˆ‘ä»¬å¸Œæœ›æ‚¨å…·æœ‰ä»¥ä¸‹æŠ€èƒ½ï¼š</p>
<p>åŸºæœ¬è¦æ±‚ï¼š</p>
<p>1ã€æ‰å®çš„è®¡ç®—æœºç§‘å­¦åŸºç¡€çŸ¥è¯†</p>
<p>2ã€åŠ¨æ‰‹èƒ½åŠ›å¼ºï¼Œæœ‰æ­»ç£•ç²¾ç¥</p>
<p>3ã€æœ‰ä¸°å¯Œçš„ Rust å¼€å‘ç»éªŒ</p>
<p>4ã€æ›¾ç»ç‹¬ç«‹å®Œæˆæˆ–è€…ä¸»å¯¼å®Œæˆè¿‡å…·æœ‰æŒ‘æˆ˜æ€§çš„é¡¹ç›®</p>
<p>5ã€å¯¹å·¥ä½œæœ‰æå¼ºçš„è´£ä»»å¿ƒ</p>
<p>åŠ åˆ†é¡¹ï¼š</p>
<p>1ã€åŒºå—é“¾ç›¸å…³æ•°æ®ç»“æ„ä¸ç®—æ³•</p>
<p>2ã€Substrateæˆ–å…¶ä»–åŒºå—é“¾èŠ‚ç‚¹å¼€å‘ç»éªŒ</p>
<p>3ã€è·¨é“¾ã€Layer 2 å¼€å‘ç»éªŒ</p>
<p>å¾…é‡ï¼š</p>
<p>ä¸°åšçš„è–ªèµ„å¾…é‡ï¼š40K~80K/æœˆ</p>
<p>çµæ´»çš„å·¥ä½œæ–¹å¼ï¼šæ‚¨å¯ä»¥åœ¨ä»»ä½•æ—¶é—´ï¼Œä»»ä½•åœ°ç‚¹ï¼Œåªè¦æœ‰ç½‘ç»œå°±è¡Œ</p>
<p>è¡¨ç°åˆæ ¼è€…æä¾›è‚¡ç¥¨/å¸æƒçš„ä¸°åšæ¿€åŠ±</p>
<p>è¡¨ç°ä¼˜å¼‚è€…æä¾›ç¾å›½/åŠ æ‹¿å¤§ç§»æ°‘æœºä¼šï¼ˆç›®å‰é—¹ç˜Ÿç–«ï¼Œè¯´å®è¯ä¹Ÿæ²¡å•¥å¥½ç§»çš„ï¼‰</p>
<p>æœ‰æ„å‘çš„é€‰æ‰‹ï¼Œè¯·å‘ä¸ªäººç®€å†åˆ°:jobs@deeper.network, æˆ‘ä»¬åœ¨è¿™é‡Œç­‰ä½ ï¼</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rustæ—¥æŠ¥ã€‘2021-09-05</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=f480ea38-5b5e-423e-9c78-e78bd1344c6e">
<div class="article-summary-box-inner">
<span><h3>rust-tui-templateï¼šä½¿ç”¨ tui-rs å’Œ crossterm å¼•å¯¼ Rust TUI åº”ç”¨ç¨‹åºçš„æ¨¡æ¿</h3>
<p>é¡¹ç›®ç»“æ„å¦‚ä¸‹ï¼š</p>
<pre><code>src/
â”œâ”€â”€ app.rs     -&gt; holds the states and renders the widgets
â”œâ”€â”€ event.rs   -&gt; handles the terminal events (key press, mouse click, resize, etc.)
â”œâ”€â”€ handler.rs -&gt; handles the key press events and updates the application
â”œâ”€â”€ lib.rs     -&gt; module definitions
â”œâ”€â”€ main.rs    -&gt; entry-point
â””â”€â”€ tui.rs     -&gt; initializes/exits the terminal interface
</code></pre>
<p>æŒ‰ README ä¸‹è½½æ‰§è¡Œåæ•ˆæœå¦‚ä¸‹ï¼š</p>
<p><img src="http://qnimg.lovevivian.cn/rust-daily-20210905-1.jpg" alt></p>
<p>GitHubï¼š<a href="https://github.com/orhun/rust-tui-template" rel="noopener noreferrer">orhun/rust-tui-template: A template for bootstrapping a Rust TUI application with tui-rs &amp; crossterm</a></p>
<h3>perseusï¼šå®Œå…¨æ”¯æŒ SSR å’Œ SSG çš„ Rust é«˜ç«¯å‰ç«¯å¼€å‘æ¡†æ¶</h3>
<p>Perseus æ˜¯ä¸€ä¸ªä½¿ç”¨ Rust æ„å»ºçš„æå¿«çš„å‰ç«¯ Web å¼€å‘æ¡†æ¶ï¼Œå®ƒæ”¯æŒä¸»è¦çš„æ¸²æŸ“ç­–ç•¥ã€åœ¨æ²¡æœ‰è™šæ‹Ÿ DOM çš„æƒ…å†µä¸‹å…·æœ‰ååº”æ€§ï¼Œå¹¶ä¸”å…·æœ‰æé«˜çš„å¯å®šåˆ¶æ€§ã€‚å®ƒå°è£…äº† Sycamore çš„åº•å±‚åŠŸèƒ½ï¼Œæä¾›äº†ä¸€ä¸ªç±»ä¼¼ NextJS çš„ APIï¼</p>
<p>âœ¨ æ”¯æŒé™æ€ç”Ÿæˆï¼ˆåªæä¾›é™æ€èµ„æºï¼‰
âœ¨ æ”¯æŒæœåŠ¡ç«¯æ¸²æŸ“ï¼ˆæœåŠ¡åŠ¨æ€èµ„æºï¼‰
âœ¨ æ”¯æŒä¸€æ®µæ—¶é—´åé‡æ–°éªŒè¯å’Œ / æˆ–ä½¿ç”¨è‡ªå®šä¹‰é€»è¾‘ï¼ˆæ›´æ–°å·²æ¸²æŸ“é¡µé¢ï¼‰
âœ¨ æ”¯æŒå¢é‡é‡å»ºï¼ˆæŒ‰éœ€æ„å»ºï¼‰
âœ¨å¼€æ”¾æ„å»ºçŸ©é˜µï¼ˆä¸»è¦ä½¿ç”¨ä»»ä½•æ¸²æŸ“ç­–ç•¥å’Œå…¶ä»–ä»»ä½•ä¸œè¥¿ï¼‰
âœ¨ CLI å·¥å…·ï¼Œè®©æ‚¨è½»æ¾è‡ªä¿¡åœ°æ„å»ºåº”ç”¨ç¨‹åº</p>
<p>é¡¹ç›®çš„ä¸»è¦ç›®æ ‡æ˜¯ï¼šæ”¯æŒæ¯ä¸€ä¸ªä¸»è¦çš„æ¸²æŸ“ç­–ç•¥ï¼Œå¹¶ä¸ºå¼€å‘äººå‘˜æä¾›ä½¿ç”¨ Rust é«˜æ•ˆåˆ›å»ºè¶…å¿«é€Ÿåº”ç”¨ç¨‹åºçš„èƒ½åŠ›å’Œç‚«é…·çš„çš„å¼€å‘ä½“éªŒï¼</p>
<p>æ–‡æ¡£ï¼š<a href="https://arctic-hen7.github.io/perseus/" rel="noopener noreferrer">Introduction - Perseus Book</a></p>
<p>GitHubï¼š<a href="https://github.com/arctic-hen7/perseus" rel="noopener noreferrer">arctic-hen7/perseus: A high-level frontend development framework for Rust with full support for SSR and SSG.</a></p>
<h3>Rust æ„å»º LC-3 è™šæ‹Ÿæœº</h3>
<p>Little Computer 3ï¼Œæˆ– LC-3ï¼Œæ˜¯ä¸€ç§è®¡ç®—æœºæ•™è‚²ç¼–ç¨‹è¯­è¨€ï¼Œä¸€ç§æ±‡ç¼–è¯­è¨€ã€‚å®ƒå…·æœ‰ç›¸å¯¹ç®€å•çš„æŒ‡ä»¤é›†ï¼Œä½†å¯ç”¨äºç¼–å†™ä¸­ç­‰å¤æ‚çš„æ±‡ç¼–ç¨‹åºï¼Œæ˜¯ C ç¼–è¯‘å™¨çš„å¯è¡Œç›®æ ‡ã€‚ è¯¥è¯­è¨€ä¸å¦‚ x86 æ±‡ç¼–è¯­è¨€å¤æ‚ï¼Œä½†å…·æœ‰è®¸å¤šç±»ä¼¼äºæ›´å¤æ‚è¯­è¨€çš„åŠŸèƒ½ã€‚ è¿™äº›åŠŸèƒ½ä½¿å…¶å¯¹å…¥é—¨æ•™å­¦éå¸¸æœ‰ç”¨ï¼Œå› æ­¤å®ƒæœ€å¸¸ç”¨äºå‘è®¡ç®—æœºç§‘å­¦å’Œè®¡ç®—æœºå·¥ç¨‹ä¸“ä¸šçš„å­¦ç”Ÿæ•™æˆç¼–ç¨‹å’Œè®¡ç®—æœºä½“ç³»ç»“æ„çš„åŸºç¡€çŸ¥è¯†ã€‚</p>
<p>æ•™ç¨‹åœ°å€ï¼š<a href="https://www.rodrigoaraujo.me/posts/lets-build-an-lc-3-virtual-machine/" rel="noopener noreferrer">Let's build an LC-3 Virtual Machine :: Rodrigo Araujo â€” Computer Scientist and Software Engineer</a></p>
<p>å¦å¤–é™„ä¸Š 2 ä¸ªä¹‹å‰çš„ä¸€ä¸ªæ•™ç¨‹ï¼š</p>
<ul>
<li><a href="https://github.com/KuldeepSinh/lc3_vm" rel="noopener noreferrer">KuldeepSinh/lc3_vm: LC-3 (Little Computer 3) VM implemented in Rust</a></li>
<li><a href="https://github.com/justinmeiners/lc3-vm" rel="noopener noreferrer">justinmeiners/lc3-vm: Write your own virtual machine for the LC-3 computer!</a></li>
</ul>
<h3>RustGameJam ä¸­ä½¿ç”¨çš„æ¸¸æˆå¼•æ“åˆ†å¸ƒ</h3>
<p>GameJam æ˜¯ä¸€ä¸ªæ¸¸æˆå¼€å‘è€…çš„ hackathonï¼Œ<a href="https://itch.io/jam/rusty-jam" rel="noopener noreferrer">ç¬¬ä¸€å±Š Rust Game Jam</a> æ˜¯äº2021å¹´8æœˆ22å·åˆ°8æœˆ29å·ä¸¾åŠï¼Œæ¸¸æˆå¼€å‘è€…ä»¬ä½¿ç”¨çš„æ¸¸æˆå¼•æ“æœ€å¤šçš„æ˜¯ Bevyï¼Œå…¶æ¬¡æ˜¯ macroquadï¼Œå½“ç„¶è¿˜æœ‰å…¶ä»–å¼•æ“ï¼Œæ¯”å¦‚ï¼špixelsã€ RG3Dã€minifbã€‚æƒ³çœ‹GameJamçš„æ¸¸æˆä½œå“ï¼Œè¯·ç‚¹å‡»ä¸‹é¢é“¾æ¥ã€‚</p>
<ul>
<li>https://itch.io/jam/rusty-jam</li>
</ul>
<h3>memuse ä¸€ä¸ªåˆ†æåŠ¨æ€å†…å­˜ä½¿ç”¨çš„åº“</h3>
<pre><code>use memuse::DynamicUsage;

assert_eq!(7u64.dynamic_usage(), 0);
assert_eq("I'm simple!".dynamic_usage(), 0);
assert_eq(vec![7u64; 2].dynamic_usage(), 16);

let empty: Vec&lt;u32&gt; = Vec::with_capacity(100);
assert_eq!(empty.len(), 0);
assert_eq!(empty.dynamic_usage, 400);
</code></pre>
<ul>
<li>Repo <a href="https://crates.io/crates/memuse" rel="noopener noreferrer">crates.io/crates/memuse</a></li>
</ul>
<hr>
<p>From æ—¥æŠ¥å°ç»„ å¤ªå­é•¿ç´ï¼Œæå†¬æ°</p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<ul>
<li><a href="https://rust.cc/" rel="noopener noreferrer">Rustcc è®ºå›: æ”¯æŒ rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f620" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust åŸ¹å…»æé«˜è®¡åˆ’ Vol. 7 - 8 | Rust é¡¹ç›®å·¥ç¨‹æ¥äº†</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b">
<div class="article-summary-box-inner">
<span><p>æˆ‘ä»¬çš„ Rust å…¬å¼€è¯¾è¿›è¡Œäº† 6 æœŸäº†ï¼Œå¸¦å¤§å®¶äº†è§£äº† ï¼š</p>
<ol>
<li>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€</li>
<li>ç†è§£ Rust æ‰€æœ‰æƒ</li>
<li>é€šè¿‡å®æˆ˜ç†è§£ Rust å®</li>
<li>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª</li>
<li>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1</li>
<li>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 2</li>
</ol>
<p>ç›®å‰è§†é¢‘å›æ”¾ä¼ åˆ° B ç«™æ”¶è·è®¸å¤šå¥½è¯„ï¼Œèµï¼Œä¹Ÿç»™æˆ‘ä»¬å¾ˆå¤§çš„é¼“åŠ±ã€‚å¸Œæœ›æˆ‘ä»¬çš„ Rust åŸ¹å…»æé«˜è®¡åˆ’ | Datafuse å¯ä»¥å¸®åŠ©æ›´å¤šçš„æœ‹å‹å¿«é€Ÿçš„ä½¿ç”¨ä¸Š Rust ã€‚
æœ¬å‘¨ç»™å¤§å®¶æ’ä¸¤ä¸ªå…¬å¼€è¯¾ï¼šå‘¨å››æ™šä¸Šï¼Œå‘¨æ—¥æ™šä¸Šã€‚æˆ‘ä»¬ Rust åŸ¹å…»æé«˜è®¡åˆ’é‚€è¯·åˆ°ç¬¬äºŒä½åˆ†äº«å˜‰å®¾ è‘£æ³½æ¶¦è€å¸ˆï¼Œ å¦å¤– Rust åŸ¹å…»æé«˜è®¡åˆ’ çš„å†…å®¹ä¸Šä¹Ÿåšäº†ä¸€äº›è°ƒæ•´ã€‚</p>
<hr>
<p>åˆ†äº«ä¸»é¢˜ï¼šã€Šæ·±å…¥äº†è§£rust é—­åŒ…ã€‹ | Vol. 7</p>
<p>åˆ†äº«æ—¶é—´ï¼š å‘¨å››æ™šä¸Š2021-09-09 20:00-21:00</p>
<p>åˆ†äº«è®²å¸ˆï¼š è‘£æ³½æ¶¦</p>
<p>å†…å®¹ä»‹ç»ï¼š æ·±å…¥æµ…å‡ºäº†è§£ rust é—­åŒ…å·¥ä½œåŸç†ï¼Œè®©å¤§å®¶äº†è§£åº•å±‚å®ç°
è®²å¸ˆä»‹ç»ï¼š
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<hr>
<p>åˆ†äº«ä¸»é¢˜ï¼šã€Šåˆ©ç”¨ Tokio å®ç°ä¸€ä¸ªé«˜æ€§èƒ½ Mini Http serverã€‹ | Vol. 8</p>
<p>åˆ†äº«æ—¶é—´ï¼š å‘¨æ—¥æ™šä¸Š2021-09-12 20:00-21:00</p>
<p>åˆ†äº«è®²å¸ˆï¼š è‹æ—</p>
<p>é¦–å…ˆæ„Ÿè°¢è‹æ—è€å¸ˆçš„åšæŒä»˜å‡ºï¼Œ å¸¦æˆ‘ä»¬å­¦ä¹  Rust çš„é‡ç‚¹çŸ¥è¯†ã€‚ ç»è¿‡å’Œè‹ç³è€å¸ˆæ²Ÿé€šï¼Œæˆ‘ä»¬åç»­çš„è¯¾ç¨‹ï¼Œä¼šæ›´åŠ å¾€å®æˆ˜æ–¹å‘è½¬å˜ã€‚æ¥ä¸‹æ˜¯ä¸€ä¸ªç³»åˆ—çš„å†…å®¹ï¼š</p>
<ol>
<li>åˆ©ç”¨ Tokio å®ç°ä¸€ä¸ª Mini Http server</li>
<li>åŸºäº Http serveræä¾›å†…å®¹åŠ¨æ€çš„ API ç½‘å…³</li>
<li>åˆ©ç”¨ Redis å®ç°å¯¹ API ç½‘å…³åŠ é€Ÿ</li>
<li>å­¦ä¹  Rust RPC è°ƒç”¨ï¼Œå®ç°å¾®æœåŠ¡è°ƒç”¨</li>
</ol>
<p>è¿™ä¸ªå†…å®¹å¯èƒ½éœ€è¦4æ¬¡å·¦å³çš„å…¬å¼€è¯¾ï¼Œç›®çš„æ˜¯å¸¦ç€å¤§å®¶åšä¸€äº›å°é¡¹ç›®ï¼Œå¸¦å¤§å®¶ç†Ÿæ‚‰ä¸€ä¸‹ Rust å·¥ç¨‹ï¼Œè®©å¤§å®¶å¯ä»¥å¿«é€ŸæŠŠ Rust ç”¨åˆ°åç«¯å¼€å‘ä¸­ã€‚</p>
<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>
<ol>
<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>
<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>
<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>
<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>
</ol>
<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>
<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<p>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1 | Vol. 5
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<p>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 2 | Vol. 6
https://www.bilibili.com/video/bv1oy4y1G7jC</p>
<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust å¼‚æ­¥ç¼–ç¨‹æ•™æï¼šhttps://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust å­¦ä¹ éšç¬”</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8">
<div class="article-summary-box-inner">
<span><h1>åˆ‡æ¢é•œåƒæº</h1>
<p>crm =&gt; https://github.com/wtklbm/crm</p>
<p>å¸¸ç”¨å‘½ä»¤å°±æ˜¯ <code>crm best</code></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">pretree è¡¥å…¨æ–‡æ¡£å‘å¸ƒäº†,å†æ¬¡è°¢è°¢å¤§ç¥çš„æŒ‡ç‚¹ç»ˆäºå…¥é—¨äº†ã€‚</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827">
<div class="article-summary-box-inner">
<span><h1>Pretree</h1>
<p>pretree is a package for storing and querying routing rules with prefix tree .</p>
<p>pretree æ˜¯ä¸€ä¸ªç”¨äºå­˜å‚¨å’ŒæŸ¥è¯¢è·¯ç”±è§„åˆ™çš„åŒ…ã€‚å®ƒç”¨å‰ç¼€æ ‘å­˜å‚¨è·¯ç”±è§„åˆ™ï¼Œæ”¯æŒåŒ…å«å˜é‡çš„è·¯ç”±ã€‚</p>
<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>
<p>Inspired by <a href="https://github.com/obity/pretree" rel="noopener noreferrer">obity/pretree</a> (golang)</p>
<h1>Doc</h1>
<p>See this document at <a href="https://docs.rs/pretree" rel="noopener noreferrer">API documentation</a></p>
<h1>Install</h1>
<p>Add the following line to your Cargo.toml file:</p>
<pre><code>pretree = "1.0.0"
</code></pre>
<h1>Example</h1>
<pre><code>use pretree::Pretree;
let mut p = Pretree::new();
p.store("GET","account/{id}/info/:name");
p.store("GET","account/:id/login");
p.store("GET","account/{id}");
p.store("GET","bacteria/count_number_by_month");
let (ok,rule,vars) = p.query("GET","account/929239");
println!("ok:{} rule:{} vars:{:#?}",ok,rule,vars);

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust å¼‚æ­¥ç¼–ç¨‹äºŒ: Tokio å…¥é—¨è¿è¡Œæ—¶ä»‹ç» | Rust åŸ¹å…»æé«˜è®¡åˆ’ Vol. 6</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a">
<div class="article-summary-box-inner">
<span><h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹äºŒ: Tokio å…¥é—¨è¿è¡Œæ—¶ä»‹ç»ã€‹|Vol. 6</h3>
<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´9æœˆ5æ—¥ 20:00-21:00</p>
<p><strong>è¯¾ç¨‹ä»‹ç»:</strong> ä¸Šå‘¨å…¬å¼€è¯¾æˆ‘ä»¬è®²è§£äº† Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹ï¼ˆ å±äºä¸€ä¸ªéå¸¸ç»å…¸çš„å†…å®¹ï¼Œå»ºè®®è§‚çœ‹ ï¼‰, å¤§å®¶å¯¹ Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹æœ‰äº†ä¸€ä¸ªåˆæ­¥è®¤è¯†, Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹é‡Œéœ€è¦ Executorã€Reactorã€Future ç­‰, æœ¬å‘¨å…¬å¼€è¯¾å°†ä»¥ Tokio æ¡†æ¶ä¸ºåŸºç¡€, å’Œå¤§å®¶ä¸€èµ·èŠèŠ Tokio é‡Œçš„ Executorã€Reactorã€Future æ˜¯ä»€ä¹ˆ?</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<p>1ã€å›é¡¾ Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>
<p>2ã€è°ˆè°ˆå¯¹ Rust å¼‚æ­¥æ¡†æ¶çš„è®¤è¯† ( futures-rsã€async-stdã€tokio ) .</p>
<p>3ã€Tokio ä»‹ç».</p>
<p>4ã€Tokio é‡Œçš„ Executorã€Reactorã€Future å¦‚ä½•ä½¿ç”¨.</p>
<p>5ã€ä½¿ç”¨ Tokio å®ç°ä¸€ä¸ªç®€å•çš„æœåŠ¡ç«¯ä¸å®¢æˆ·ç«¯ç¨‹åº.</p>
<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>
<ol>
<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>
<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>
<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>
<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>
</ol>
<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>
<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/
Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1 å›æ”¾åœ°å€ï¼š
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust å¼‚æ­¥ç¼–ç¨‹æ•™æï¼šhttps://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</h3>
<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´8æœˆ29æ—¥ 20:00-21:00</p>
<p><strong>è¯¾ç¨‹ä»‹ç»:</strong> è®²åˆ° Rust ä½¿ç”¨ Future å¼‚æ­¥ç¼–ç¨‹ï¼Œå°±ä¸å¾—ä¸è¯´ futures å’Œ tokio è¿™ä¸¤ä¸ª crateï¼Œå…¶å®æ ‡å‡†åº“ä¸­çš„ futureï¼Œä»¥åŠ async/await å°±æ˜¯ä» futures åº“ä¸­æ•´åˆè¿›æ ‡å‡†åº“çš„, Tokio æ‹¥æœ‰æå¿«çš„æ€§èƒ½ï¼Œæ˜¯å¤§éƒ¨åˆ†ç³»ç»Ÿå¼‚æ­¥å¤„ç†çš„é€‰æ‹©ï¼Œå…¶æ„å»ºäº future ä¹‹ä¸Šã€‚Future æ˜¯ Rust å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒåŸºç¡€ã€‚</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<p>1ã€ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥.</p>
<p>2ã€ç†è§£å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>
<p>3ã€Future ç¼–ç¨‹æ¨¡å‹è®²è§£.</p>
<p>4ã€å¸¦é¢†å¤§å®¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ future , å†æ¬¡å¸®å¿™å¤§å®¶ç†è§£</p>
<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>
<ol>
<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>
<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>
<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>
<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>
</ol>
<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>
<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rustæ—¥æŠ¥ã€‘2021-08-19 -- Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</h3>
<p>å·²ç»åœ¨ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªcrate ä¸Šæµ‹è¯•äº†ç‰ˆæœ¬è¿ç§»,å¹¶ä¸”å°†æµ‹è¯•æ‰€æœ‰å…¬å…±çš„ crateã€‚</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>å¼‚æ­¥å¼•æ“ C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D æ¸¸æˆå¼•æ“</h3>
<ul>
<li><strong>PCï¼ˆWindowsã€Linuxã€macOSï¼‰å’Œ Web (WebAssembly)</strong> æ”¯æŒã€‚</li>
<li><strong>å»¶è¿Ÿç€è‰²</strong></li>
<li><strong>å†…ç½®ä¿å­˜/åŠ è½½</strong></li>
<li><strong>ç‹¬ç«‹åœºæ™¯ç¼–è¾‘å™¨</strong></li>
<li><strong>é«˜çº§ç‰©ç†æ¨¡å‹</strong></li>
<li><strong>åˆ†å±‚æ¨¡å‹èµ„æº</strong></li>
<li><strong>å‡ ä½•å®ä¾‹åŒ–</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From æ—¥æŠ¥å°ç»„ å†°å±±ä¸Šçš„ mook &amp;&amp; æŒºè‚¥</p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustccè®ºå›: æ”¯æŒrss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">å…¬å¼€è¯¾: é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Šé€šè¿‡Datafuseç†è§£å…¨é“¾è·¯è·Ÿè¸ªã€‹| Vol. 4</strong></p>
<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong> 2021å¹´8æœˆ22æ—¥ 20:30-21:30</p>
<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong> æ•°æ®åº“ç³»ç»Ÿä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚ï¼Œåºå¤§çš„ç³»ç»Ÿã€‚ç‰¹åˆ«æ˜¯åœ¨è°ƒè¯•å’Œè§‚å¯ŸSQLæ‰§è¡Œï¼Œå¤šçº¿ç¨‹ä»»åŠ¡åˆ‡æ¢ï¼Œå› ä¸ºæ²¡æœ‰å†…å­˜è°ƒç”¨æˆ–å †æ ˆè·Ÿè¸ªï¼Œè¿™ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¿½è¸ªçš„ç”±æ¥ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°å¤šè¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ªä¸ºæè¿°å’Œåˆ†æè·¨è¿›ç¨‹äº‹åŠ¡æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚Google Dapper(Dapper: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿé“¾è·¯è¿½è¸ªåŸºç¡€è®¾æ–½)è®ºæ–‡(å„tracerçš„åŸºç¡€)ä¸­æè¿°äº†åˆ†å¸ƒå¼è¿½è¸ªçš„ä¸€äº›ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­ç¨³æ€é—®é¢˜ã€åˆ†å¸ƒå¼åˆ†æã€èµ„æºå±æ€§å’Œå¾®æœåŠ¡çš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ã€‚</p>
<p>æœ¬æ¬¡å…¬å¼€è¯¾é€š Google çš„ OpenTraceing ä»‹ç»ï¼Œç»“åˆRustçš„ tokio-rs/tracing ä½¿ç”¨ï¼Œæœ€ç»ˆç»“åˆ Datafuse é¡¹ç›®ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹å¤§å‹åº”ç”¨çš„å…¨é“¾è·¯è·Ÿè¸ªåˆ†æè¿‡ç¨‹ã€‚</p>
<p>å…³äºDatafuse : https://github.com/datafuselabs/datafuse</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<ol>
<li>
<p>ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»ŸOpenTracingåŠåº”ç”¨åœºæ™¯</p>
</li>
<li>
<p>ä»‹ç» tokio-rs/tracing åŠåœ¨ç¨‹åºå¼€å‘ä¸­çš„ä½œç”¨</p>
</li>
<li>
<p>ä¸ºä»€ä¹ˆéœ€è¦tokio-rs/tracingåº“</p>
</li>
<li>
<p>æ¼”ç¤ºDatafuseé¡¹ç›®ä¸­tokio-rs/tracingçš„ä½¿ç”¨</p>
</li>
</ol>
<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>
<ol>
<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>
<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>
<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>
<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>
</ol>
<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>
<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">è®ºå›githubè´¦æˆ·æ— æ³•ç™»å½•è§£å†³ç¬”è®°</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>æœ‰åæ˜ è¿™ä¸¤å¤©githubè´¦æˆ·æ— æ³•ç™»å½•äº†ã€‚</p>
<p>æŠ¥è¿™ä¸ªé”™ï¼š</p>
<pre><code>get github user info err
</code></pre>
<p>æŸ¥äº†å‡ ä¸ªåœ°æ–¹ï¼š</p>
<ol>
<li>ä»£ç æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼šOk</li>
<li>httpsä»£ç†æ˜¯å¦æ­£å¸¸ï¼šOk</li>
<li>æ£€æŸ¥äº†githubè¿”å›æ—¥å¿—ï¼Œå‘ç°æ˜¯ï¼š</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>è¿›å…¥è¿™ä¸ªåœ°å€ä¸€çœ‹ï¼š<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>åŸæ¥2020å¹´2æœˆå°±å·²ç»è¯´äº†ï¼Œè¦æ”¹è¦æ”¹ã€‚ä¸è¿‡æˆ‘ç¡®å®æ²¡ç•™æ„åˆ°è¿™ä¸ªä¿¡æ¯ã€‚ï¼šï¼ˆ</p>
<p>æ„æ€å°±æ˜¯è¯´access_tokenä¸è¦æ”¾åœ¨queryå‚æ•°ä¸­ï¼Œè€Œæ˜¯è¦æ”¾åœ¨headeré‡Œé¢ã€‚ç…§å®ƒè¯´çš„ï¼Œæ”¹äº†åå°±å¥½äº†ã€‚</p>
<p>ç‰¹æ­¤è®°å½•ã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust çš„ Future ä¸ Javascript çš„ Promise åŠŸèƒ½å¯¹ç…§å‚è€ƒ</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>çš„<code>Future</code>ä¸<code>Javascript</code>çš„<code>Promise</code>åŠŸèƒ½å¯¹ç…§å‚è€ƒ</h1>
<p>å­¦ä¹ æ–°é²œæŠ€æœ¯æ—¶ï¼Œæˆ‘æ€»æ˜¯ä¼šä¹ æƒ¯æ€§å‘æ›¾ç»ç†Ÿæ‚‰çš„å†…å®¹ä¸Šé ï¼Œç”šè‡³å¥—ç”¨ç°æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¿™æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œå¯¹ç…§<code>Javascript - Promise/A+ API</code>æ¥è®°å¿†ä¸€éƒ¨åˆ†<code>Rust Future</code>å¸¸ç”¨<code>API</code>ã€‚</p>
<blockquote>
<p>æ³¨æ„ï¼šæ‰€æœ‰çš„<code>Rust - Future</code>æ“ä½œéƒ½æ˜¯ä»¥<code>.await</code>ç»“å°¾çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œä¸åŒäº<code>Javascript - Promise/A+</code>ï¼Œ<code>Rust - Future</code>æ˜¯æƒ°æ€§çš„ã€‚åªæœ‰è¢«<code>.await</code>æŒ‡ä»¤æ¿€æ´»åï¼Œåœ¨<code>Rust - Future</code>å†…å°è£…çš„æ“ä½œæ‰ä¼šè¢«çœŸæ­£åœ°æ‰§è¡Œã€‚</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">æè¿°</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* ä»€ä¹ˆéƒ½ä¸åš */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll ä¸èƒ½è¢«ç”¨æ¥æ„é€ åŒ…å«äº†å¼‚æ­¥æ“ä½œçš„ Future å®ä¾‹ï¼Œå› ä¸ºã€å›è°ƒé—­åŒ…ã€‘å†…çš„ã€å¯ä¿®æ”¹å¼•ç”¨ã€‘&amp;mut Context&lt;'_&gt; ä¸èƒ½è¢« ï¼ˆ1ï¼‰è·¨çº¿ç¨‹ä¼ é€’ ï¼ˆ2ï¼‰ä¼ é€’å‡ºé—­åŒ…ä½œç”¨åŸŸ2. task::spawn_blocking() ã€å›è°ƒé—­åŒ…ã€‘è¾“å…¥å‚æ•°å†…çš„ thread::sleep() ä¸æ˜¯é˜»å¡è¿è¡Œ task::spawn_blocking() çš„ä¸»çº¿ç¨‹ï¼Œè€Œæ˜¯é˜»å¡ä»ã€é˜»å¡ä»»åŠ¡çº¿ç¨‹æ± ã€‘ä¸­åˆ†é…æ¥è¿è¡Œé˜»å¡ä»»åŠ¡çš„ã€å·¥ä½œçº¿ç¨‹ã€‘ã€‚</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. æœ‰ä¸€ä¸ª promise/future å¤±è´¥å°±æ•´ä½“æ€§åœ°å¤±è´¥ã€‚2. try_join æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future çš„æˆåŠŸä¸å¤±è´¥ç»“æœéƒ½æ”¶é›†2. è¿”å›ç»“æœï¼š(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. ä»…åªæ”¶é›†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ promise/future2. try_race æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. æ”¶é›†ç¬¬ä¸€ä¸ªç»“æŸçš„ promise/futureï¼Œæ— è®ºå®ƒæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯å¤±è´¥æ”¶åœºã€‚2. è¿”å›ç»“æœï¼šT</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rustå…¬å¼€è¯¾ï¼šã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼š</strong>ã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹</p>
<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong> 2021å¹´8æœˆ15æ—¥ 20:30-21:30</p>
<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong></p>
<p>å¦‚æœæƒ³ç”¨ Rust å¼€å‘å¤§å‹ç›®ï¼Œæˆ–è€…å­¦ä¹ å¤§å‹é¡¹ç›®ä»£ç ï¼Œç‰¹åˆ«æ˜¯æ¡†æ¶çº§åˆ«çš„é¡¹ç›®ï¼Œé‚£ä¹ˆ Rust çš„å®æœºåˆ¶è‚¯å®šæ˜¯ä¸€ä¸ªå¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚ ä¾‹å¦‚ datafuse ä¸­çš„ä¸€äº›é…ç½®ç®¡ç†ï¼š
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>è¿™å°±æ˜¯é€šè¿‡å®å®ç°é…ç½®çš„ç»Ÿä¸€è¡Œä¸ºï¼Œä»£ç å‚è€ƒï¼š
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust è¯­è¨€å¼ºå¤§çš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¯ä»¥åˆ›å»ºå’Œåˆ©ç”¨å®ï¼Œä¸è¿‡åˆ›å»ºå®çœ‹èµ·æ¥æŒºå¤æ‚ï¼Œå¸¸å¸¸ä»¤åˆšæ¥è§¦ Rust çš„å¼€å‘è€…ç”Ÿç•æƒ§ã€‚ åœ¨æœ¬æ¬¡å…¬å¼€è¯¾ä¸­å¸®åŠ©ä½ ç†è§£ Rust Macro çš„åŸºæœ¬åŸç†ï¼Œå­¦ä¹ å¦‚ä½•åˆ›è‡ªå·²çš„ Rust å®ï¼Œä»¥åŠæŸ¥çœ‹æºç å­¦ä¹ å®çš„å®ç°ã€‚</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<ul>
<li>ä»€ä¹ˆæ˜¯ Rust å®</li>
<li>ä»€ä¹ˆæ˜¯å®è¿è¡ŒåŸç†</li>
<li>å¦‚ä½•åˆ›å»º Rust å®è¿‡ç¨‹</li>
<li>é˜…è¯» datafuse é¡¹ç›®æºç ï¼Œ å­¦ä¹ é¡¹ç›®ä¸­å®çš„å®ç°</li>
</ul>
<p><strong>è®²å¸ˆä»‹ç»</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-08T01:30:00Z">09-08</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">SS-BERT: Mitigating Identity Terms Bias in Toxic Comment Classification by Utilising the Notion of "Subjectivity" and "Identity Terms". (arXiv:2109.02691v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02691">
<div class="article-summary-box-inner">
<span><p>Toxic comment classification models are often found biased toward identity
terms which are terms characterizing a specific group of people such as
"Muslim" and "black". Such bias is commonly reflected in false-positive
predictions, i.e. non-toxic comments with identity terms. In this work, we
propose a novel approach to tackle such bias in toxic comment classification,
leveraging the notion of subjectivity level of a comment and the presence of
identity terms. We hypothesize that when a comment is made about a group of
people that is characterized by an identity term, the likelihood of that
comment being toxic is associated with the subjectivity level of the comment,
i.e. the extent to which the comment conveys personal feelings and opinions.
Building upon the BERT model, we propose a new structure that is able to
leverage these features, and thoroughly evaluate our model on 4 datasets of
varying sizes and representing different social media platforms. The results
show that our model can consistently outperform BERT and a SOTA model devised
to address identity term bias in a different way, with a maximum improvement in
F1 of 2.43% and 1.91% respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Text-to-Table: A New Way of Information Extraction. (arXiv:2109.02707v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02707">
<div class="article-summary-box-inner">
<span><p>We study a new problem setting of information extraction (IE), referred to as
text-to-table, which can be viewed as an inverse problem of the well-studied
table-to-text. In text-to-table, given a text, one creates a table or several
tables expressing the main content of the text, while the model is learned from
text-table pair data. The problem setting differs from those of the existing
methods for IE. First, the extraction can be carried out from long texts to
large tables with complex structures. Second, the extraction is entirely
data-driven, and there is no need to explicitly define the schemas. As far as
we know, there has been no previous work that studies the problem. In this
work, we formalize text-to-table as a sequence-to-sequence (seq2seq) problem.
We first employ a seq2seq model fine-tuned from a pre-trained language model to
perform the task. We also develop a new method within the seq2seq approach,
exploiting two additional techniques in table generation: table constraint and
table relation embeddings. We make use of four existing table-to-text datasets
in our experiments on text-to-table. Experimental results show that the vanilla
seq2seq model can outperform the baseline methods of using relation extraction
and named entity extraction. The results also show that our method can further
boost the performances of the vanilla seq2seq model. We further discuss the
main challenges of the proposed task. The code and data will be made publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Inspiring Content on Social Media. (arXiv:2109.02734v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02734">
<div class="article-summary-box-inner">
<span><p>Inspiration moves a person to see new possibilities and transforms the way
they perceive their own potential. Inspiration has received little attention in
psychology, and has not been researched before in the NLP community. To the
best of our knowledge, this work is the first to study inspiration through
machine learning methods. We aim to automatically detect inspiring content from
social media data. To this end, we analyze social media posts to tease out what
makes a post inspiring and what topics are inspiring. We release a dataset of
5,800 inspiring and 5,800 non-inspiring English-language public post unique ids
collected from a dump of Reddit public posts made available by a third party
and use linguistic heuristics to automatically detect which social media
English-language posts are inspiring.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does BERT Learn as Humans Perceive? Understanding Linguistic Styles through Lexica. (arXiv:2109.02738v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02738">
<div class="article-summary-box-inner">
<span><p>People convey their intention and attitude through linguistic styles of the
text that they write. In this study, we investigate lexicon usages across
styles throughout two lenses: human perception and machine word importance,
since words differ in the strength of the stylistic cues that they provide. To
collect labels of human perception, we curate a new dataset, Hummingbird, on
top of benchmarking style datasets. We have crowd workers highlight the
representative words in the text that makes them think the text has the
following styles: politeness, sentiment, offensiveness, and five emotion types.
We then compare these human word labels with word importance derived from a
popular fine-tuned style classifier like BERT. Our results show that the BERT
often finds content words not relevant to the target style as important words
used in style prediction, but humans do not perceive the same way even though
for some styles (e.g., positive sentiment and joy) human- and
machine-identified words share significant overlap for some styles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02747">
<div class="article-summary-box-inner">
<span><p>We aim to automatically identify human action reasons in online videos. We
focus on the widespread genre of lifestyle vlogs, in which people perform
actions while verbally describing them. We introduce and make publicly
available the {\sc WhyAct} dataset, consisting of 1,077 visual actions manually
annotated with their reasons. We describe a multimodal model that leverages
visual and textual information to automatically infer the reasons corresponding
to an action presented in the video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">End-to-end Neural Information Status Classification. (arXiv:2109.02753v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02753">
<div class="article-summary-box-inner">
<span><p>Most previous studies on information status (IS) classification and bridging
anaphora recognition assume that the gold mention or syntactic tree information
is given (Hou et al., 2013; Roesiger et al., 2018; Hou, 2020; Yu and Poesio,
2020). In this paper, we propose an end-to-end neural approach for information
status classification. Our approach consists of a mention extraction component
and an information status assignment component. During the inference time, our
system takes a raw text as the input and generates mentions together with their
information status. On the ISNotes corpus (Markert et al., 2012), we show that
our information status assignment component achieves new state-of-the-art
results on fine-grained IS classification based on gold mentions. Furthermore,
our system performs significantly better than other baselines for both mention
extraction and fine-grained IS classification in the end-to-end setting.
Finally, we apply our system on BASHI (Roesiger, 2018) and SciCorp (Roesiger,
2016) to recognize referential bridging anaphora. We find that our end-to-end
system trained on ISNotes achieves competitive results on bridging anaphora
recognition compared to the previous state-of-the-art system that relies on
syntactic information and is trained on the in-domain datasets (Yu and Poesio,
2020).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mixed Attention Transformer for LeveragingWord-Level Knowledge to Neural Cross-Lingual Information Retrieval. (arXiv:2109.02789v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02789">
<div class="article-summary-box-inner">
<span><p>Pretrained contextualized representations offer great success for many
downstream tasks, including document ranking. The multilingual versions of such
pretrained representations provide a possibility of jointly learning many
languages with the same model. Although it is expected to gain big with such
joint training, in the case of cross lingual information retrieval (CLIR), the
models under a multilingual setting are not achieving the same level of
performance as those under a monolingual setting. We hypothesize that the
performance drop is due to the translation gap between query and documents. In
the monolingual retrieval task, because of the same lexical inputs, it is
easier for model to identify the query terms that occurred in documents.
However, in the multilingual pretrained models that the words in different
languages are projected into the same hyperspace, the model tends to translate
query terms into related terms, i.e., terms that appear in a similar context,
in addition to or sometimes rather than synonyms in the target language. This
property is creating difficulties for the model to connect terms that cooccur
in both query and document. To address this issue, we propose a novel Mixed
Attention Transformer (MAT) that incorporates external word level knowledge,
such as a dictionary or translation table. We design a sandwich like
architecture to embed MAT into the recent transformer based deep neural models.
By encoding the translation knowledge into an attention matrix, the model with
MAT is able to focus on the mutually translated words in the input sequence.
Experimental results demonstrate the effectiveness of the external knowledge
and the significant improvement of MAT embedded neural reranking model on CLIR
task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Puzzle Solving without Search or Human Knowledge: An Unnatural Language Approach. (arXiv:2109.02797v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02797">
<div class="article-summary-box-inner">
<span><p>The application of Generative Pre-trained Transformer (GPT-2) to learn
text-archived game notation provides a model environment for exploring sparse
reward gameplay. The transformer architecture proves amenable to training on
solved text archives describing mazes, Rubik's Cube, and Sudoku solvers. The
method benefits from fine-tuning the transformer architecture to visualize
plausible strategies derived outside any guidance from human heuristics or
domain expertise. The large search space ($&gt;10^{19}$) for the games provides a
puzzle environment in which the solution has few intermediate rewards and a
final move that solves the challenge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Scalable AI Approach for Clinical Trial Cohort Optimization. (arXiv:2109.02808v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02808">
<div class="article-summary-box-inner">
<span><p>FDA has been promoting enrollment practices that could enhance the diversity
of clinical trial populations, through broadening eligibility criteria.
However, how to broaden eligibility remains a significant challenge. We propose
an AI approach to Cohort Optimization (AICO) through transformer-based natural
language processing of the eligibility criteria and evaluation of the criteria
using real-world data. The method can extract common eligibility criteria
variables from a large set of relevant trials and measure the generalizability
of trial designs to real-world patients. It overcomes the scalability limits of
existing manual methods and enables rapid simulation of eligibility criteria
design for a disease of interest. A case study on breast cancer trial design
demonstrates the utility of the method in improving trial generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Strategies for Generalizable Commonsense Reasoning with Pre-trained Models. (arXiv:2109.02837v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02837">
<div class="article-summary-box-inner">
<span><p>Commonsense reasoning benchmarks have been largely solved by fine-tuning
language models. The downside is that fine-tuning may cause models to overfit
to task-specific data and thereby forget their knowledge gained during
pre-training. Recent works only propose lightweight model updates as models may
already possess useful knowledge from past experience, but a challenge remains
in understanding what parts and to what extent models should be refined for a
given task. In this paper, we investigate what models learn from commonsense
reasoning datasets. We measure the impact of three different adaptation methods
on the generalization and accuracy of models. Our experiments with two models
show that fine-tuning performs best, by learning both the content and the
structure of the task, but suffers from overfitting and limited generalization
to novel answers. We observe that alternative adaptation methods like
prefix-tuning have comparable accuracy, but generalize better to unseen answers
and are more robust to adversarial splits.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Datasets: A Community Library for Natural Language Processing. (arXiv:2109.02846v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02846">
<div class="article-summary-box-inner">
<span><p>The scale, variety, and quantity of publicly-available NLP datasets has grown
rapidly as researchers propose new tasks, larger models, and novel benchmarks.
Datasets is a community library for contemporary NLP designed to support this
ecosystem. Datasets aims to standardize end-user interfaces, versioning, and
documentation, while providing a lightweight front-end that behaves similarly
for small datasets as for internet-scale corpora. The design of the library
incorporates a distributed, community-driven approach to adding datasets and
documenting usage. After a year of development, the library now includes more
than 650 unique datasets, has more than 250 contributors, and has helped
support a variety of novel cross-dataset research projects and shared tasks.
The library is available at https://github.com/huggingface/datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Regular Expressions with Neural Networks via DFA. (arXiv:2109.02882v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02882">
<div class="article-summary-box-inner">
<span><p>Human-designed rules are widely used to build industry applications. However,
it is infeasible to maintain thousands of such hand-crafted rules. So it is
very important to integrate the rule knowledge into neural networks to build a
hybrid model that achieves better performance. Specifically, the human-designed
rules are formulated as Regular Expressions (REs), from which the equivalent
Minimal Deterministic Finite Automatons (MDFAs) are constructed. We propose to
use the MDFA as an intermediate model to capture the matched RE patterns as
rule-based features for each input sentence and introduce these additional
features into neural networks. We evaluate the proposed method on the ATIS
intent classification task. The experiment results show that the proposed
method achieves the best performance compared to neural networks and four other
methods that combine REs and neural networks when the training dataset is
relatively small.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">IndicBART: A Pre-trained Model for Natural Language Generation of Indic Languages. (arXiv:2109.02903v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02903">
<div class="article-summary-box-inner">
<span><p>In this paper we present IndicBART, a multilingual, sequence-to-sequence
pre-trained model focusing on 11 Indic languages and English. Different from
existing pre-trained models, IndicBART utilizes the orthographic similarity
between Indic scripts to improve transfer learning between similar Indic
languages. We evaluate IndicBART on two NLG tasks: Neural Machine Translation
(NMT) and extreme summarization. Our experiments on NMT for 12 language pairs
and extreme summarization for 7 languages using multilingual fine-tuning show
that IndicBART is competitive with or better than mBART50 despite containing
significantly fewer parameters. Our analyses focus on identifying the impact of
script unification (to Devanagari), corpora size as well as multilingualism on
the final performance. The IndicBART model is available under the MIT license
at https://indicnlp.ai4bharat.org/indic-bart .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Reasoning Chains for Multi-hop Science Question Answering. (arXiv:2109.02905v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02905">
<div class="article-summary-box-inner">
<span><p>We propose a novel Chain Guided Retriever-reader ({\tt CGR}) framework to
model the reasoning chain for multi-hop Science Question Answering. Our
framework is capable of performing explainable reasoning without the need of
any corpus-specific annotations, such as the ground-truth reasoning chain, or
human-annotated entity mentions. Specifically, we first generate reasoning
chains from a semantic graph constructed by Abstract Meaning Representation of
retrieved evidence facts. A \textit{Chain-aware loss}, concerning both local
and global chain information, is also designed to enable the generated chains
to serve as distant supervision signals for training the retriever, where
reinforcement learning is also adopted to maximize the utility of the reasoning
chains. Our framework allows the retriever to capture step-by-step clues of the
entire reasoning process, which is not only shown to be effective on two
challenging multi-hop Science QA tasks, namely OpenBookQA and ARC-Challenge,
but also favors explainability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Driven Content Creation using Statistical and Natural Language Processing Techniques for Financial Domain. (arXiv:2109.02935v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02935">
<div class="article-summary-box-inner">
<span><p>Over the years customers' expectation of getting information instantaneously
has given rise to the increased usage of channels like virtual assistants.
Typically, customers try to get their questions answered by low-touch channels
like search and virtual assistant first, before getting in touch with a live
chat agent or the phone representative. Higher usage of these low-touch systems
is a win-win for both customers and the organization since it enables
organizations to attain a low cost of service while customers get served
without delay. In this paper, we propose a two-part framework where the first
part describes methods to combine the information from different interaction
channels like call, search, and chat. We do this by summarizing (using a
stacked Bi-LSTM network) the high-touch interaction channel data such as call
and chat into short searchquery like customer intents and then creating an
organically grown intent taxonomy from interaction data (using Hierarchical
Agglomerative Clustering). The second part of the framework focuses on
extracting customer questions by analyzing interaction data sources. It
calculates similarity scores using TF-IDF and BERT(Devlin et al., 2019). It
also maps these identified questions to the output of the first part of the
framework using syntactic and semantic similarity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Naturalness Evaluation of Natural Language Generation in Task-oriented Dialogues using BERT. (arXiv:2109.02938v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02938">
<div class="article-summary-box-inner">
<span><p>This paper presents an automatic method to evaluate the naturalness of
natural language generation in dialogue systems. While this task was previously
rendered through expensive and time-consuming human labor, we present this
novel task of automatic naturalness evaluation of generated language. By
fine-tuning the BERT model, our proposed naturalness evaluation method shows
robust results and outperforms the baselines: support vector machines,
bi-directional LSTMs, and BLEURT. In addition, the training speed and
evaluation performance of naturalness model are improved by transfer learning
from quality and informativeness linguistic knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Countering Online Hate Speech: An NLP Perspective. (arXiv:2109.02941v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02941">
<div class="article-summary-box-inner">
<span><p>Online hate speech has caught everyone's attention from the news related to
the COVID-19 pandemic, US elections, and worldwide protests. Online toxicity -
an umbrella term for online hateful behavior, manifests itself in forms such as
online hate speech. Hate speech is a deliberate attack directed towards an
individual or a group motivated by the targeted entity's identity or opinions.
The rising mass communication through social media further exacerbates the
harmful consequences of online hate speech. While there has been significant
research on hate-speech identification using Natural Language Processing (NLP),
the work on utilizing NLP for prevention and intervention of online hate speech
lacks relatively. This paper presents a holistic conceptual framework on
hate-speech NLP countering methods along with a thorough survey on the current
progress of NLP for countering online hate speech. It classifies the countering
techniques based on their time of action, and identifies potential future
research areas on this topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Paraphrase Generation as Unsupervised Machine Translation. (arXiv:2109.02950v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02950">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose a new paradigm for paraphrase generation by
treating the task as unsupervised machine translation (UMT) based on the
assumption that there must be pairs of sentences expressing the same meaning in
a large-scale unlabeled monolingual corpus. The proposed paradigm first splits
a large unlabeled corpus into multiple clusters, and trains multiple UMT models
using pairs of these clusters. Then based on the paraphrase pairs produced by
these UMT models, a unified surrogate model can be trained to serve as the
final Seq2Seq model to generate paraphrases, which can be directly used for
test in the unsupervised setup, or be finetuned on labeled datasets in the
supervised setup. The proposed method offers merits over
machine-translation-based paraphrase generation methods, as it avoids reliance
on bilingual sentence pairs. It also allows human intervene with the model so
that more diverse paraphrases can be generated using different filtering
criteria. Extensive experiments on existing paraphrase dataset for both the
supervised and unsupervised setups demonstrate the effectiveness the proposed
paradigm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FH-SWF SG at GermEval 2021: Using Transformer-Based Language Models to Identify Toxic, Engaging, & Fact-Claiming Comments. (arXiv:2109.02966v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02966">
<div class="article-summary-box-inner">
<span><p>In this paper we describe the methods we used for our submissions to the
GermEval 2021 shared task on the identification of toxic, engaging, and
fact-claiming comments. For all three subtasks we fine-tuned freely available
transformer-based models from the Huggingface model hub. We evaluated the
performance of various pre-trained models after fine-tuning on 80% of the
training data with different hyperparameters and submitted predictions of the
two best performing resulting models. We found that this approach worked best
for subtask 3, for which we achieved an F1-score of 0.736.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02972">
<div class="article-summary-box-inner">
<span><p>Despite constant improvements in machine translation quality, automatic
poetry translation remains a challenging problem due to the lack of
open-sourced parallel poetic corpora, and to the intrinsic complexities
involved in preserving the semantics, style, and figurative nature of poetry.
We present an empirical investigation for poetry translation along several
dimensions: 1) size and style of training data (poetic vs. non-poetic),
including a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)
language-family-specific models vs. mixed-multilingual models. To accomplish
this, we contribute a parallel dataset of poetry translations for several
language pairs. Our results show that multilingual fine-tuning on poetic text
significantly outperforms multilingual fine-tuning on non-poetic text that is
35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and
human evaluation metrics such as faithfulness (meaning and poetic style).
Moreover, multilingual fine-tuning on poetic data outperforms \emph{bilingual}
fine-tuning on poetic data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Revisiting Context Choices for Context-aware Machine Translation. (arXiv:2109.02995v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02995">
<div class="article-summary-box-inner">
<span><p>One of the most popular methods for context-aware machine translation (MT) is
to use separate encoders for the source sentence and context as multiple
sources for one target sentence. Recent work has cast doubt on whether these
models actually learn useful signals from the context or are improvements in
automatic evaluation metrics just a side-effect. We show that multi-source
transformer models improve MT over standard transformer-base models even with
empty lines provided as context, but the translation quality improves
significantly (1.51 - 2.65 BLEU) when a sufficient amount of correct context is
provided. We also show that even though randomly shuffling in-domain context
can also improve over baselines, the correct context further improves
translation quality and random out-of-domain context further degrades it.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Empathetic Dialogue Generation with Pre-trained RoBERTa-GPT2 and External Knowledge. (arXiv:2109.03004v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03004">
<div class="article-summary-box-inner">
<span><p>One challenge for dialogue agents is to recognize feelings of the
conversation partner and respond accordingly. In this work, RoBERTa-GPT2 is
proposed for empathetic dialogue generation, where the pre-trained
auto-encoding RoBERTa is utilised as encoder and the pre-trained
auto-regressive GPT-2 as decoder. With the combination of the pre-trained
RoBERTa and GPT-2, our model realizes a new state-of-the-art emotion accuracy.
To enable the empathetic ability of RoBERTa-GPT2 model, we propose a
commonsense knowledge and emotional concepts extractor, in which the
commonsensible and emotional concepts of dialogue context are extracted for the
GPT-2 decoder. The experiment results demonstrate that the empathetic dialogue
generation benefits from both pre-trained encoder-decoder architecture and
external knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sequential Attention Module for Natural Language Processing. (arXiv:2109.03009v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03009">
<div class="article-summary-box-inner">
<span><p>Recently, large pre-trained neural language models have attained remarkable
performance on many downstream natural language processing (NLP) applications
via fine-tuning. In this paper, we target at how to further improve the token
representations on the language models. We, therefore, propose a simple yet
effective plug-and-play module, Sequential Attention Module (SAM), on the token
embeddings learned from a pre-trained language model. Our proposed SAM consists
of two main attention modules deployed sequentially: Feature-wise Attention
Module (FAM) and Token-wise Attention Module (TAM). More specifically, FAM can
effectively identify the importance of features at each dimension and promote
the effect via dot-product on the original token embeddings for downstream NLP
applications. Meanwhile, TAM can further re-weight the features at the
token-wise level. Moreover, we propose an adaptive filter on FAM to prevent
noise impact and increase information absorption. Finally, we conduct extensive
experiments to demonstrate the advantages and properties of our proposed SAM.
We first show how SAM plays a primary role in the champion solution of two
subtasks of SemEval'21 Task 7. After that, we apply SAM on sentiment analysis
and three popular NLP tasks and demonstrate that SAM consistently outperforms
the state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generate & Rank: A Multi-task Framework for Math Word Problems. (arXiv:2109.03034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03034">
<div class="article-summary-box-inner">
<span><p>Math word problem (MWP) is a challenging and critical task in natural
language processing. Many recent studies formalize MWP as a generation task and
have adopted sequence-to-sequence models to transform problem descriptions to
mathematical expressions. However, mathematical expressions are prone to minor
mistakes while the generation objective does not explicitly handle such
mistakes. To address this limitation, we devise a new ranking task for MWP and
propose Generate &amp; Rank, a multi-task framework based on a generative
pre-trained language model. By joint training with generation and ranking, the
model learns from its own mistakes and is able to distinguish between correct
and incorrect expressions. Meanwhile, we perform tree-based disturbance
specially designed for MWP and an online update to boost the ranker. We
demonstrate the effectiveness of our proposed method on the benchmark and the
results show that our method consistently outperforms baselines in all
datasets. Particularly, in the classical Math23k, our method is 7% (78.4%
$\rightarrow$ 85.4%) higher than the state-of-the-art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">POSSCORE: A Simple Yet Effective Evaluation of Conversational Search with Part of Speech Labelling. (arXiv:2109.03039v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03039">
<div class="article-summary-box-inner">
<span><p>Conversational search systems, such as Google Assistant and Microsoft
Cortana, provide a new search paradigm where users are allowed, via natural
language dialogues, to communicate with search systems. Evaluating such systems
is very challenging since search results are presented in the format of natural
language sentences. Given the unlimited number of possible responses,
collecting relevance assessments for all the possible responses is infeasible.
In this paper, we propose POSSCORE, a simple yet effective automatic evaluation
method for conversational search. The proposed embedding-based metric takes the
influence of part of speech (POS) of the terms in the response into account. To
the best knowledge, our work is the first to systematically demonstrate the
importance of incorporating syntactic information, such as POS labels, for
conversational search evaluation. Experimental results demonstrate that our
metrics can correlate with human preference, achieving significant improvements
over state-of-the-art baseline metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Patient Outcome and Zero-shot Diagnosis Prediction with Hypernetwork-guided Multitask Learning. (arXiv:2109.03062v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03062">
<div class="article-summary-box-inner">
<span><p>Multitask deep learning has been applied to patient outcome prediction from
text, taking clinical notes as input and training deep neural networks with a
joint loss function of multiple tasks. However, the joint training scheme of
multitask learning suffers from inter-task interference, and diagnosis
prediction among the multiple tasks has the generalizability issue due to rare
diseases or unseen diagnoses. To solve these challenges, we propose a
hypernetwork-based approach that generates task-conditioned parameters and
coefficients of multitask prediction heads to learn task-specific prediction
and balance the multitask learning. We also incorporate semantic task
information to improves the generalizability of our task-conditioned multitask
model. Experiments on early and discharge notes extracted from the real-world
MIMIC database show our method can achieve better performance on multitask
patient outcome prediction than strong baselines in most cases. Besides, our
method can effectively handle the scenario with limited information and improve
zero-shot prediction on unseen diagnosis categories.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GOLD: Improving Out-of-Scope Detection in Dialogues using Data Augmentation. (arXiv:2109.03079v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03079">
<div class="article-summary-box-inner">
<span><p>Practical dialogue systems require robust methods of detecting out-of-scope
(OOS) utterances to avoid conversational breakdowns and related failure modes.
Directly training a model with labeled OOS examples yields reasonable
performance, but obtaining such data is a resource-intensive process. To tackle
this limited-data problem, previous methods focus on better modeling the
distribution of in-scope (INS) examples. We introduce GOLD as an orthogonal
technique that augments existing data to train better OOS detectors operating
in low-data regimes. GOLD generates pseudo-labeled candidates using samples
from an auxiliary dataset and keeps only the most beneficial candidates for
training through a novel filtering mechanism. In experiments across three
target benchmarks, the top GOLD model outperforms all existing methods on all
key metrics, achieving relative gains of 52.4%, 48.9% and 50.3% against median
baseline performance. We also analyze the unique properties of OOS data to
identify key factors for optimally applying our proposed method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning grounded word meaning representations on similarity graphs. (arXiv:2109.03084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03084">
<div class="article-summary-box-inner">
<span><p>This paper introduces a novel approach to learn visually grounded meaning
representations of words as low-dimensional node embeddings on an underlying
graph hierarchy. The lower level of the hierarchy models modality-specific word
representations through dedicated but communicating graphs, while the higher
level puts these representations together on a single graph to learn a
representation jointly from both modalities. The topology of each graph models
similarity relations among words, and is estimated jointly with the graph
embedding. The assumption underlying this model is that words sharing similar
meaning correspond to communities in an underlying similarity graph in a
low-dimensional space. We named this model Hierarchical Multi-Modal Similarity
Graph Embedding (HM-SGE). Experimental results validate the ability of HM-SGE
to simulate human similarity judgements and concept categorization,
outperforming the state of the art.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FHAC at GermEval 2021: Identifying German toxic, engaging, and fact-claiming comments with ensemble learning. (arXiv:2109.03094v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03094">
<div class="article-summary-box-inner">
<span><p>The availability of language representations learned by large pretrained
neural network models (such as BERT and ELECTRA) has led to improvements in
many downstream Natural Language Processing tasks in recent years. Pretrained
models usually differ in pretraining objectives, architectures, and datasets
they are trained on which can affect downstream performance. In this
contribution, we fine-tuned German BERT and German ELECTRA models to identify
toxic (subtask 1), engaging (subtask 2), and fact-claiming comments (subtask 3)
in Facebook data provided by the GermEval 2021 competition. We created
ensembles of these models and investigated whether and how classification
performance depends on the number of ensemble members and their composition. On
out-of-sample data, our best ensemble achieved a macro-F1 score of 0.73 (for
all subtasks), and F1 scores of 0.72, 0.70, and 0.76 for subtasks 1, 2, and 3,
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Infusing Future Information into Monotonic Attention Through Language Models. (arXiv:2109.03121v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03121">
<div class="article-summary-box-inner">
<span><p>Simultaneous neural machine translation(SNMT) models start emitting the
target sequence before they have processed the source sequence. The recent
adaptive policies for SNMT use monotonic attention to perform read/write
decisions based on the partial source and target sequences. The lack of
sufficient information might cause the monotonic attention to take poor
read/write decisions, which in turn negatively affects the performance of the
SNMT model. On the other hand, human translators make better read/write
decisions since they can anticipate the immediate future words using linguistic
information and domain knowledge.Motivated by human translators, in this work,
we propose a framework to aid monotonic attention with an external language
model to improve its decisions.We conduct experiments on the MuST-C
English-German and English-French speech-to-text translation tasks to show the
effectiveness of the proposed framework.The proposed SNMT method improves the
quality-latency trade-off over the state-of-the-art monotonic multihead
attention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rare Words Degenerate All Words. (arXiv:2109.03127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03127">
<div class="article-summary-box-inner">
<span><p>Despite advances in neural network language model, the representation
degeneration problem of embeddings is still challenging. Recent studies have
found that the learned output embeddings are degenerated into a narrow-cone
distribution which makes the similarity between each embeddings positive. They
analyzed the cause of the degeneration problem has been demonstrated as common
to most embeddings. However, we found that the degeneration problem is
especially originated from the training of embeddings of rare words. In this
study, we analyze the intrinsic mechanism of the degeneration of rare word
embeddings with respect of their gradient about the negative log-likelihood
loss function. Furthermore, we theoretically and empirically demonstrate that
the degeneration of rare word embeddings causes the degeneration of non-rare
word embeddings, and that the overall degeneration problem can be alleviated by
preventing the degeneration of rare word embeddings. Based on our analyses, we
propose a novel method, Adaptive Gradient Partial Scaling(AGPS), to address the
degeneration problem. Experimental results demonstrate the effectiveness of the
proposed method qualitatively and quantitatively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NumGPT: Improving Numeracy Ability of Generative Pre-trained Models. (arXiv:2109.03137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03137">
<div class="article-summary-box-inner">
<span><p>Existing generative pre-trained language models (e.g., GPT) focus on modeling
the language structure and semantics of general texts. However, those models do
not consider the numerical properties of numbers and cannot perform robustly on
numerical reasoning tasks (e.g., math word problems and measurement
estimation). In this paper, we propose NumGPT, a generative pre-trained model
that explicitly models the numerical properties of numbers in texts.
Specifically, it leverages a prototype-based numeral embedding to encode the
mantissa of the number and an individual embedding to encode the exponent of
the number. A numeral-aware loss function is designed to integrate numerals
into the pre-training objective of NumGPT. We conduct extensive experiments on
four different datasets to evaluate the numeracy ability of NumGPT. The
experiment results show that NumGPT outperforms baseline models (e.g., GPT and
GPT with DICE) on a range of numerical reasoning tasks such as measurement
estimation, number comparison, math word problems, and magnitude
classification. Ablation studies are also conducted to evaluate the impact of
pre-training and model hyperparameters on the performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PAUSE: Positive and Annealed Unlabeled Sentence Embedding. (arXiv:2109.03155v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03155">
<div class="article-summary-box-inner">
<span><p>Sentence embedding refers to a set of effective and versatile techniques for
converting raw text into numerical vector representations that can be used in a
wide range of natural language processing (NLP) applications. The majority of
these techniques are either supervised or unsupervised. Compared to the
unsupervised methods, the supervised ones make less assumptions about
optimization objectives and usually achieve better results. However, the
training requires a large amount of labeled sentence pairs, which is not
available in many industrial scenarios. To that end, we propose a generic and
end-to-end approach -- PAUSE (Positive and Annealed Unlabeled Sentence
Embedding), capable of learning high-quality sentence embeddings from a
partially labeled dataset. We experimentally show that PAUSE achieves, and
sometimes surpasses, state-of-the-art results using only a small fraction of
labeled sentence pairs on various benchmark tasks. When applied to a real
industrial use case where labeled samples are scarce, PAUSE encourages us to
extend our dataset without the liability of extensive manual annotation work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03158">
<div class="article-summary-box-inner">
<span><p>An individual's variation in writing style is often a function of both social
and personal attributes. While structured social variation has been extensively
studied, e.g., gender based variation, far less is known about how to
characterize individual styles due to their idiosyncratic nature. We introduce
a new approach to studying idiolects through a massive cross-author comparison
to identify and encode stylistic features. The neural model achieves strong
performance at authorship identification on short texts and through an
analogy-based probing task, showing that the learned representations exhibit
surprising regularities that encode qualitative and quantitative shifts of
idiolectal styles. Through text perturbation, we quantify the relative
contributions of different linguistic elements to idiolectal variation.
Furthermore, we provide a description of idiolects through measuring inter- and
intra-author variation, showing that variation in idiolects is often
distinctive yet consistent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How much pretraining data do language models need to learn syntax?. (arXiv:2109.03160v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03160">
<div class="article-summary-box-inner">
<span><p>Transformers-based pretrained language models achieve outstanding results in
many well-known NLU benchmarks. However, while pretraining methods are very
convenient, they are expensive in terms of time and resources. This calls for a
study of the impact of pretraining data size on the knowledge of the models. We
explore this impact on the syntactic capabilities of RoBERTa, using models
trained on incremental sizes of raw text data. First, we use syntactic
structural probes to determine whether models pretrained on more data encode a
higher amount of syntactic information. Second, we perform a targeted syntactic
evaluation to analyze the impact of pretraining data size on the syntactic
generalization performance of the models. Third, we compare the performance of
the different models on three downstream applications: part-of-speech tagging,
dependency parsing and paraphrase identification. We complement our study with
an analysis of the cost-benefit trade-off of training such models. Our
experiments show that while models pretrained on more data encode more
syntactic knowledge and perform better on downstream applications, they do not
always offer a better performance across the different syntactic phenomena and
come at a higher financial and environmental cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Aspect-Controllable Opinion Summarization. (arXiv:2109.03171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03171">
<div class="article-summary-box-inner">
<span><p>Recent work on opinion summarization produces general summaries based on a
set of input reviews and the popularity of opinions expressed in them. In this
paper, we propose an approach that allows the generation of customized
summaries based on aspect queries (e.g., describing the location and room of a
hotel). Using a review corpus, we create a synthetic training dataset of
(review, summary) pairs enriched with aspect controllers which are induced by a
multi-instance learning model that predicts the aspects of a document at
different levels of granularity. We fine-tune a pretrained model using our
synthetic dataset and generate aspect-specific summaries by modifying the
aspect controllers. Experiments on two benchmarks show that our model
outperforms the previous state of the art and generates personalized summaries
by controlling the number of aspects discussed in them.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When differential privacy meets NLP: The devil is in the detail. (arXiv:2109.03175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03175">
<div class="article-summary-box-inner">
<span><p>Differential privacy provides a formal approach to privacy of individuals.
Applications of differential privacy in various scenarios, such as protecting
users' original utterances, must satisfy certain mathematical properties. Our
contribution is a formal analysis of ADePT, a differentially private
auto-encoder for text rewriting (Krishna et al, 2021). ADePT achieves promising
results on downstream tasks while providing tight privacy guarantees. Our proof
reveals that ADePT is not differentially private, thus rendering the
experimental results unsubstantiated. We also quantify the impact of the error
in its private mechanism, showing that the true sensitivity is higher by at
least factor 6 in an optimistic case of a very small encoder's dimension and
that the amount of utterances that are not privatized could easily reach 100%
of the entire dataset. Our intention is neither to criticize the authors, nor
the peer-reviewing process, but rather point out that if differential privacy
applications in NLP rely on formal guarantees, these should be outlined in full
and put under detailed scrutiny.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Conversation Disentanglement through Co-Training. (arXiv:2109.03199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03199">
<div class="article-summary-box-inner">
<span><p>Conversation disentanglement aims to separate intermingled messages into
detached sessions, which is a fundamental task in understanding multi-party
conversations. Existing work on conversation disentanglement relies heavily
upon human-annotated datasets, which are expensive to obtain in practice. In
this work, we explore to train a conversation disentanglement model without
referencing any human annotations. Our method is built upon a deep co-training
algorithm, which consists of two neural networks: a message-pair classifier and
a session classifier. The former is responsible for retrieving local relations
between two messages while the latter categorizes a message to a session by
capturing context-aware information. Both networks are initialized respectively
with pseudo data built from an unannotated corpus. During the deep co-training
process, we use the session classifier as a reinforcement learning component to
learn a session assigning policy by maximizing the local rewards given by the
message-pair classifier. For the message-pair classifier, we enrich its
training data by retrieving message pairs with high confidence from the
disentangled sessions predicted by the session classifier. Experimental results
on the large Movie Dialogue Dataset demonstrate that our proposed approach
achieves competitive performance compared to the previous supervised methods.
Further experiments show that the predicted disentangled conversations can
promote the performance on the downstream task of multi-party response
selection.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ExCode-Mixed: Explainable Approaches towards Sentiment Analysis on Code-Mixed Data using BERT models. (arXiv:2109.03200v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03200">
<div class="article-summary-box-inner">
<span><p>The increasing use of social media sites in countries like India has given
rise to large volumes of code-mixed data. Sentiment analysis of this data can
provide integral insights into people's perspectives and opinions. Developing
robust explainability techniques which explain why models make their
predictions becomes essential. In this paper, we propose an adequate
methodology to integrate explainable approaches into code-mixed sentiment
analysis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint model for intent and entity recognition. (arXiv:2109.03221v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03221">
<div class="article-summary-box-inner">
<span><p>The semantic understanding of natural dialogues composes of several parts.
Some of them, like intent classification and entity detection, have a crucial
role in deciding the next steps in handling user input. Handling each task as
an individual problem can be wasting of training resources, and also each
problem can benefit from each other. This paper tackles these problems as one.
Our new model, which combine intent and entity recognition into one system, is
achieving better metrics in both tasks with lower training requirements than
solving each task separately. We also optimize the model based on the inputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Preserved Accuracy: Evaluating Loyalty and Robustness of BERT Compression. (arXiv:2109.03228v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03228">
<div class="article-summary-box-inner">
<span><p>Recent studies on compression of pretrained language models (e.g., BERT)
usually use preserved accuracy as the metric for evaluation. In this paper, we
propose two new metrics, label loyalty and probability loyalty that measure how
closely a compressed model (i.e., student) mimics the original model (i.e.,
teacher). We also explore the effect of compression with regard to robustness
under adversarial attacks. We benchmark quantization, pruning, knowledge
distillation and progressive module replacing with loyalty and robustness. By
combining multiple compression techniques, we provide a practical strategy to
achieve better accuracy, loyalty and robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06438">
<div class="article-summary-box-inner">
<span><p>Advertising is an important revenue source for many companies. However, it is
expensive to manually create advertisements that meet the needs of various
queries for massive items. In this paper, we propose the query-variant
advertisement text generation task that aims to generate candidate
advertisements for different queries with various needs given the item
keywords. In this task, for many different queries there is only one general
purposed advertisement with no predefined query-advertisement pair, which would
discourage traditional End-to-End models from generating query-variant
advertisements for different queries with different needs. To deal with the
problem, we propose a query-variant advertisement text generation model that
takes keywords and associated external knowledge as input during training and
adds different queries during inference. Adding external knowledge helps the
model adapted to the information besides the item keywords during training,
which makes the transition between training and inference more smoothing when
the query is added during inference. Both automatic and human evaluation show
that our model can generate more attractive and query-focused advertisements
than the strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intuitive Contrasting Map for Antonym Embeddings. (arXiv:2004.12835v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.12835">
<div class="article-summary-box-inner">
<span><p>This paper shows that, modern word embeddings contain information that
distinguishes synonyms and antonyms despite small cosine similarities between
corresponding vectors. This information is encoded in the geometry of the
embeddings and could be extracted with a straight-forward and intuitive
manifold learning procedure or a contrasting map. Such a map is trained on a
small labeled subset of the data and can produce new embeddings that explicitly
highlight specific semantic attributes of the word. The new embeddings produced
by the map are shown to improve the performance on downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Entity Types into Entity-Guided Text Generation. (arXiv:2009.13401v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13401">
<div class="article-summary-box-inner">
<span><p>Recent successes in deep generative modeling have led to significant advances
in natural language generation (NLG). Incorporating entities into neural
generation models has demonstrated great improvements by assisting to infer the
summary topic and to generate coherent content. To enhance the role of entity
in NLG, in this paper, we aim to model the entity type in the decoding phase to
generate contextual words accurately. We develop a novel NLG model to produce a
target sequence based on a given list of entities. Our model has a multi-step
decoder that injects the entity types into the process of entity mention
generation. Experiments on two public news datasets demonstrate type injection
performs better than existing type embedding concatenation baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fully Synthetic Data Improves Neural Machine Translation withKnowledge Distillation. (arXiv:2012.15455v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.15455">
<div class="article-summary-box-inner">
<span><p>This paper explores augmenting monolingual data for knowledge distillation in
neural machine translation. Source language monolingual text can be
incorporated as a forward translation. Interestingly, we find the best way to
incorporate target language monolingual text is to translate it to the source
language and round-trip translate it back to the target language, resulting in
a fully synthetic corpus. We find that combining monolingual data from both
source and target languages yields better performance than a corpus twice as
large only in one language. Moreover, experiments reveal that the improvement
depends upon the provenance of the test set. If the test set was originally in
the source language (with the target side written by translators), then forward
translating source monolingual data matters. If the test set was originally in
the target language (with the source written by translators), then
incorporating target monolingual data matters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00234">
<div class="article-summary-box-inner">
<span><p>Transformers have shown improved performance when compared to previous
architectures for sequence processing such as RNNs. Despite their sizeable
performance gains, as recently suggested, the model is computationally
expensive to train and with a high parameter budget. In light of this, we
explore parameter-sharing methods in Transformers with a specific focus on
generative models. We perform an analysis of different parameter
sharing/reduction methods and develop the Subformer. Our model combines
sandwich-style parameter sharing, which overcomes naive cross-layer parameter
sharing in generative models, and self-attentive embedding factorization
(SAFE). Experiments on machine translation, abstractive summarization and
language modeling show that the Subformer can outperform the Transformer even
when using significantly fewer parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Toward Improving Coherence and Diversity of Slogan Generation. (arXiv:2102.05924v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05924">
<div class="article-summary-box-inner">
<span><p>Previous work in slogan generation focused on utilising slogan skeletons
mined from existing slogans. While some generated slogans can be catchy, they
are often not coherent with the company's focus or style across their marketing
communications because the skeletons are mined from other companies' slogans.
We propose a sequence-to-sequence (seq2seq) transformer model to generate
slogans from a brief company description. A naive seq2seq model fine-tuned for
slogan generation is prone to introducing false information. We use company
name delexicalisation and entity masking to alleviate this problem and improve
the generated slogans' quality and truthfulness. Furthermore, we apply
conditional training based on the first words' POS tag to generate
syntactically diverse slogans. Our best model achieved a ROUGE-1/-2/-L F1 score
of 35.58/18.47/33.32. Besides, automatic and human evaluations indicate that
our method generates significantly more factual, diverse and catchy slogans
than strong LSTM and transformer seq2seq baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WordBias: An Interactive Visual Tool for Discovering Intersectional Biases Encoded in Word Embeddings. (arXiv:2103.03598v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.03598">
<div class="article-summary-box-inner">
<span><p>Intersectional bias is a bias caused by an overlap of multiple social factors
like gender, sexuality, race, disability, religion, etc. A recent study has
shown that word embedding models can be laden with biases against
intersectional groups like African American females, etc. The first step
towards tackling such intersectional biases is to identify them. However,
discovering biases against different intersectional groups remains a
challenging task. In this work, we present WordBias, an interactive visual tool
designed to explore biases against intersectional groups encoded in static word
embeddings. Given a pretrained static word embedding, WordBias computes the
association of each word along different groups based on race, age, etc. and
then visualizes them using a novel interactive interface. Using a case study,
we demonstrate how WordBias can help uncover biases against intersectional
groups like Black Muslim Males, Poor Females, etc. encoded in word embedding.
In addition, we also evaluate our tool using qualitative feedback from expert
interviews. The source code for this tool can be publicly accessed for
reproducibility at github.com/bhavyaghai/WordBias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentence-Permuted Paragraph Generation. (arXiv:2104.07228v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07228">
<div class="article-summary-box-inner">
<span><p>Generating paragraphs of diverse contents is important in many applications.
Existing generation models produce similar contents from homogenized contexts
due to the fixed left-to-right sentence order. Our idea is permuting the
sentence orders to improve the content diversity of multi-sentence paragraph.
We propose a novel framework PermGen whose objective is to maximize the
expected log-likelihood of output paragraph distributions with respect to all
possible sentence orders. PermGen uses hierarchical positional embedding and
designs new procedures for training, decoding, and candidate ranking in the
sentence-permuted generation. Experiments on three paragraph generation
benchmarks demonstrate PermGen generates more diverse outputs with a higher
quality than existing models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation. (arXiv:2104.07555v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07555">
<div class="article-summary-box-inner">
<span><p>QuestEval is a reference-less metric used in text-to-text tasks, that
compares the generated summaries directly to the source text, by automatically
asking and answering questions. Its adaptation to Data-to-Text tasks is not
straightforward, as it requires multimodal Question Generation and Answering
systems on the considered tasks, which are seldom available. To this purpose,
we propose a method to build synthetic multimodal corpora enabling to train
multimodal components for a data-QuestEval metric. The resulting metric is
reference-less and multimodal; it obtains state-of-the-art correlations with
human judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's
code and models available for reproducibility purpose, as part of the QuestEval
project.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Graph Augmented Political Perspective Detection in News Media. (arXiv:2108.03861v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03861">
<div class="article-summary-box-inner">
<span><p>Identifying political perspective in news media has become an important task
due to the rapid growth of political commentary and the increasingly polarized
ideologies. Previous approaches only focus on leveraging the semantic
information and leaves out the rich social and political context that helps
individuals understand political stances. In this paper, we propose a
perspective detection method that incorporates external knowledge of real-world
politics. Specifically, we construct a contemporary political knowledge graph
with 1,071 entities and 10,703 triples. We then build a heterogeneous
information network for each news document that jointly models article
semantics and external knowledge in knowledge graphs. Finally, we apply gated
relational graph convolutional networks and conduct political perspective
detection as graph-level classification. Extensive experiments show that our
method achieves the best performance and outperforms state-of-the-art methods
by 5.49%. Numerous ablation studies further bear out the necessity of external
knowledge and the effectiveness of our graph-based approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Encoding Heterogeneous Social and Political Context for Entity Stance Prediction. (arXiv:2108.03881v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.03881">
<div class="article-summary-box-inner">
<span><p>Political stance detection has become an important task due to the
increasingly polarized political ideologies. Most existing works focus on
identifying perspectives in news articles or social media posts, while social
entities, such as individuals and organizations, produce these texts and
actually take stances. In this paper, we propose the novel task of entity
stance prediction, which aims to predict entities' stances given their social
and political context. Specifically, we retrieve facts from Wikipedia about
social entities regarding contemporary U.S. politics. We then annotate social
entities' stances towards political ideologies with the help of domain experts.
After defining the task of entity stance prediction, we propose a graph-based
solution, which constructs a heterogeneous information network from collected
facts and adopts gated relational graph convolutional networks for
representation learning. Our model is then trained with a combination of
supervised, self-supervised and unsupervised loss functions, which are
motivated by multiple social and political phenomenons. We conduct extensive
experiments to compare our method with existing text and graph analysis
baselines. Our model achieves highest stance detection accuracy and yields
inspiring insights regarding social entity stances. We further conduct ablation
study and parameter analysis to study the mechanism and effectiveness of our
proposed approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation. (arXiv:2108.13140v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13140">
<div class="article-summary-box-inner">
<span><p>While deep learning models have greatly improved the performance of most
artificial intelligence tasks, they are often criticized to be untrustworthy
due to the black-box problem. Consequently, many works have been proposed to
study the trustworthiness of deep learning. However, as most open datasets are
designed for evaluating the accuracy of model outputs, there is still a lack of
appropriate datasets for evaluating the inner workings of neural networks. The
lack of datasets obviously hinders the development of trustworthiness research.
Therefore, in order to systematically evaluate the factors for building
trustworthy systems, we propose a novel and well-annotated sentiment analysis
dataset to evaluate robustness and interpretability. To evaluate these factors,
our dataset contains diverse annotations about the challenging distribution of
instances, manual adversarial instances and sentiment explanations. Several
evaluation metrics are further proposed for interpretability and robustness.
Based on the dataset and metrics, we conduct comprehensive comparisons for the
trustworthiness of three typical models, and also study the relations between
accuracy, robustness and interpretability. We release this trustworthiness
evaluation dataset at \url{https://github/xyz} and hope our work can facilitate
the progress on building more trustworthy systems for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FinQA: A Dataset of Numerical Reasoning over Financial Data. (arXiv:2109.00122v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00122">
<div class="article-summary-box-inner">
<span><p>The sheer volume of financial statements makes it difficult for humans to
access and analyze a business's financials. Robust numerical reasoning likewise
faces unique challenges in this domain. In this work, we focus on answering
deep questions over financial data, aiming to automate the analysis of a large
corpus of financial documents. In contrast to existing tasks on general domain,
the finance domain includes complex numerical reasoning and understanding of
heterogeneous representations. To facilitate analytical progress, we propose a
new large-scale dataset, FinQA, with Question-Answering pairs over Financial
reports, written by financial experts. We also annotate the gold reasoning
programs to ensure full explainability. We further introduce baselines and
conduct comprehensive experiments in our dataset. The results demonstrate that
popular, large, pre-trained models fall far short of expert humans in acquiring
finance knowledge and in complex multi-step numerical reasoning on that
knowledge. Our dataset -- the first of its kind -- should therefore enable
significant, new community research into complex application domains. The
dataset and code are publicly available\url{https://github.com/czyssrs/FinQA}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. (arXiv:2109.00904v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00904">
<div class="article-summary-box-inner">
<span><p>We introduce MULTI-EURLEX, a new multilingual dataset for topic
classification of legal documents. The dataset comprises 65k European Union
(EU) laws, officially translated in 23 languages, annotated with multiple
labels from the EUROVOC taxonomy. We highlight the effect of temporal concept
drift and the importance of chronological, instead of random splits. We use the
dataset as a testbed for zero-shot cross-lingual transfer, where we exploit
annotated training documents in one language (source) to classify documents in
another language (target). We find that fine-tuning a multilingually pretrained
model (XLM-ROBERTA, MT5) in a single source language leads to catastrophic
forgetting of multilingual knowledge and, consequently, poor zero-shot transfer
to other languages. Adaptation strategies, namely partial fine-tuning,
adapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new
end-tasks, help retain multilingual knowledge from pretraining, substantially
improving zero-shot cross-lingual transfer, but their impact also depends on
the pretrained model used and the size of the label set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Eliminating Sentiment Bias for Aspect-Level Sentiment Classification with Unsupervised Opinion Extraction. (arXiv:2109.02403v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02403">
<div class="article-summary-box-inner">
<span><p>Aspect-level sentiment classification (ALSC) aims at identifying the
sentiment polarity of a specified aspect in a sentence. ALSC is a practical
setting in aspect-based sentiment analysis due to no opinion term labeling
needed, but it fails to interpret why a sentiment polarity is derived for the
aspect. To address this problem, recent works fine-tune pre-trained Transformer
encoders for ALSC to extract an aspect-centric dependency tree that can locate
the opinion words. However, the induced opinion words only provide an intuitive
cue far below human-level interpretability. Besides, the pre-trained encoder
tends to internalize an aspect's intrinsic sentiment, causing sentiment bias
and thus affecting model performance. In this paper, we propose a span-based
anti-bias aspect representation learning framework. It first eliminates the
sentiment bias in the aspect embedding by adversarial learning against aspects'
prior sentiment. Then, it aligns the distilled opinion candidates with the
aspect by span-based dependency modeling to highlight the interpretable opinion
terms. Our method achieves new state-of-the-art performance on five benchmarks,
with the capability of unsupervised opinion extraction.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-08 01:51:03.577242521 UTC">2021-09-08 01:51:03 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>