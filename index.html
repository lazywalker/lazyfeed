<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-29T04:27:39.172019049Z">09-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">【求助】Rust 如何在一个 crate 里划分模块，让 no_std 和 std 共存？</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=e8a4a5cc-3316-4d63-a122-ee4a4027e2f6">
<div class="article-summary-box-inner">
<span><p>我希望写一个库，同时支持 <code>no_std</code> 和 <code>std</code>。</p>
<p>参考别的库试着写了一下，但似乎写得不对。</p>
<p><code>lib.rs</code>:</p>
<pre><code>#![cfg_attr(not(feature = "std"), no_std)]
pub mod no_std;

#[cfg(feature = "std")]
pub mod std;
</code></pre>
<p>我想让 mod std 支持 std，而 mod no_std 仅允许使用 core，应如何正确设置呢？</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">#![windows_subsystem = "windows"] 的副作用</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2e80c186-a06a-40cf-ae32-26a96388fb8e">
<div class="article-summary-box-inner">
<span><p>我发现一个奇怪的问题。
禁用控制台，会在启动时导致鼠标出现一秒的漏斗状态。
而如果显示控制台，启动过程非常丝滑，完全感受不到任何异状。</p>
<p>二者在CPU占用方面并没有显示差异。</p>
<p>请问是什么原因引发了这个问题？</p>
<p>我没有在内部使用任何 println!。</p>
<p>====</p>
<p>这也会显示在加载外部进程时。
如果禁用控制台，运行一个外部进程，通常鼠标会出现漏斗。
显示控制台，则不会出现漏斗。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust 日报】2021-09-28 RustConf 2021 还是精选</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=42fcae33-c4cd-4b8d-8932-0ee84749cdd7">
<div class="article-summary-box-inner">
<span><h2>语言团队 leader Niko Matsakis 的演讲</h2>
<p>此视频是 Niko 关于语言团队的一些思考：</p>
<ul>
<li>Rust 进入 Linux 内核就已经“完成使命”了嘛？</li>
<li>Rust 的现状
<ul>
<li>兼具 c/c++ 的性能与 JavaScript/Ruby/Python 的易用性(然而实际上易用性还是差了一点）
<ul>
<li>可以在一个两周 sprint 将 JavaScript 编写的日志服务降低 75% 使用与 95%内存使用</li>
<li>而且不需要太多的额外工程成本——只要 Rust 程序通过编译了，它通常就能工作（If it compiles, it works!)</li>
</ul>
</li>
<li>但其实还有不少人认为 “rust 不能提高生产力”（据2019 survey）</li>
<li>2015年证明了 Rust 能用，2021年我们还证明了 Rust 还很持久</li>
</ul>
</li>
<li>现在我们可能需要花六个月时间让使用 Rust 变成有生产力的行为，但我们的目标是六周就能感到有生产力</li>
<li>并且我们希望Rust 是“可靠、高性能、并为使用者赋能”，为此，我们（语言团队）需要专注、富有创造力以及拓宽思考广度</li>
<li>介绍了一些 2021 版本的特性</li>
<li>关于 async Rust -- MVP版本后我们应该做什么？ 可以参考一下 async vision doc</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">我在看structopt源码时，发现它源码里并没有`#[proc_macro_attribute]`，那它使用属性宏不应该报错嘛？</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=f59dd1fd-394d-44f6-aac3-cfd1f2ac8cf5">
<div class="article-summary-box-inner">
<span><pre><code>#[derive(StructOpt, Debug)]
#[structopt(name = "basic")] // 这里不是属性宏么
struct Opt {
</code></pre>
<p>难道有其他的定义属性宏的方式？</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">[成都/杭州/远程] 长桥 Rust 跨平台基础架构师</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=200431f7-9b4a-4198-bd75-679781fcc244">
<div class="article-summary-box-inner">
<span><h2>公司介绍</h2>
<p><a href="https://longbridgeapp.com" rel="noopener noreferrer">长桥</a> 成立于 2019 年 2 月，是一家致力于提供全球股票经纪的互联网券商，核心成员均来自于阿里巴巴，蚂蚁金服，华为等知名公司。</p>
<p>我司技术文化开放，实际上目前大部分服务端是采用 Go 语言开发，我们通过一些初步尝试与验证，发现了 Rust 对于跨平台开发有很不错的前景，准备持续投入资源开拓、探索 Rust 生态对于我们公司技术的各种可能性。</p>
<blockquote>
<p>我司除此岗位之外，也长期开放很多其他岗位如：Go、Ruby、前端，有兴趣可以邮件联系我。</p>
</blockquote>
<h2>岗位职责</h2>
<p>参与公司跨平台的基础架构设计与开发工作，采用 Rust + FFI / WebAssembly，用来解决多端基础库的支持。并基于这个可能性在基础架构内实现更多有益与多端支持的功能，比如：API 封装、复杂行情资产计算、数据离线/持久化/预加载、网络协议改进（HTTP/3）、数据传输格式改进（JSON to Protobuf 或其他）、传输加密等等。</p>
<p>同时这个岗位也将会持续探索 Rust 在跨平台方向可行性。</p>
<p>全职岗位，你可以选择成都、杭州、新加坡、香港、北京等多个办公地入职，或也可以选择远程。</p>
<h2>岗位要求</h2>
<p>这个岗位是要招来做跨平台基础架构的，得有一定的设计能力和技术功底。</p>
<p>满足以下任意一项：</p>
<ul>
<li>有 Rust 项目经验</li>
<li>GitHub Profile 过硬，是个开源社区参与者</li>
<li>独立 App 开发经历，有较好的作品</li>
<li>有 Rust FFI / WebAssembly 的相关开发经验</li>
<li>擅长多种语言，爱折腾</li>
<li>对 Rust 有兴趣，有一定其他语言（Go、Ruby、Node.js），此项不支持远程</li>
</ul>
<h2>待遇</h2>
<ul>
<li>入职配 MacBook Pro (Apple M1) + LG 4K 27 寸显示器；</li>
<li>薪资范围开放，面议</li>
<li>完整社保、公积金、以及带薪年假；</li>
</ul>
<p>简历投递：huacnlee@gmail.com</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-27 20W+行 Rust 构建的游戏居然这么好玩！</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=373312af-8ae5-4f78-af87-37171af41f4e">
<div class="article-summary-box-inner">
<span><h1>20W+行 Rust 构建的游戏居然这么好玩！</h1>
<p>真的是一款很有趣的游戏。
视频中还未展示很多游戏内容，欢迎大家来游玩。
有能力的玩家欢迎提供建模模型，修改源码提供MOD。</p>
<p><a href="https://www.bilibili.com/video/BV1mL411x7rF?p=1&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=1b28a632-f60f-4ee5-9d71-4bcb96a744b1&amp;share_source=WEIXIN&amp;share_tag=s_i&amp;timestamp=1632706296&amp;unique_k=tsDj2D" rel="noopener noreferrer">B站视频</a></p>
<h1>rustls 0.20 发布了</h1>
<p>russtls 是一个 Rust 编写的现代的 TLS库。它使用ring进行加密，使用libwebpki进行证书验证。</p>
<p>目前已发布 0.20 版本.</p>
<p><a href="https://github.com/rustls/rustls" rel="noopener noreferrer">github地址</a></p>
<h1>dune: 一个 Rust 写的 shell</h1>
<p>如下图所示:</p>
<p><img src="https://github.com/adam-mcdaniel/dune/raw/main/assets/welcome2.png" alt="img"></p>
<p><a href="https://github.com/adam-mcdaniel/dune" rel="noopener noreferrer">github 地址</a></p>
<h1>SeaORM: 异步动态的 ORM</h1>
<p>SeaORM 是一个关系型 ORM,可以帮助你构建轻量的高并发的 web 服务.</p>
<p><a href="https://www.sea-ql.org/SeaORM/blog/2021-09-20-introducing-sea-orm/" rel="noopener noreferrer">原文链接</a></p>
<h1>emoji_pix: 一个可以将图片转换为像素风的工具</h1>
<p>如下图效果:</p>
<p><img src="https://github.com/multimeric/emoji_pix/raw/master/discord_example.png" alt="img"></p>
<p><a href="https://github.com/multimeric/emoji_pix" rel="noopener noreferrer">github地址</a></p>
<p>--</p>
<p>From 日报小组 BobQin，FBI小白</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust开发postgres扩展</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ffa8347d-c09c-4636-9346-f3d861cd9dda">
<div class="article-summary-box-inner">
<span><p>文章中介绍了如何基于 cargo-pgx 开发postgres相关扩展，如何开发自定义聚集函数等</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">有没有兴趣来讨论一下NFT？？</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=1682a83c-4e59-4b36-992d-93dbddaf8cce">
<div class="article-summary-box-inner">
<span><blockquote>
<p>本文不包含区块链专有名词，不包含 cx 内容，可放心食用。</p>
</blockquote>
<p>最近区块链世界里面发生了几件大事，一是 Elon Musk 宣布15亿美元购入 Bitcoin 作为公司储备资产；二是迈阿密市长提议使用比特币作为该市的税收和公务员工资支付手段；三是 Visa 开始接受 Bitcoin 在其支付网络上进行结算。这几件事情说明区块链叙事开始加速走向主流世界，也说明是时候认真考虑这个行业的职业机会了。毕竟美国的科技和政治精英并不是傻子，他们押注这个行业说明它“有点东西”。</p>
<h2>1. 价值观</h2>
<p>不得不承认，自中本聪在2009年初通过比特币创世块写下反传统金融体系的宣言开始，指引区块链前进的根本动力就一直是其价值观。如果你也认可以下的价值观，那说明你适合在这个行业生存。</p>
<p><strong>Can't be evil.</strong></p>
<p>谷歌的“Don't be evil”不作恶原则是远远不够的，因为是否作恶的决定权仍然在中心化权威手里。不作恶成为是一种施舍，而不是一种约束。而区块链世界则强调开源、自由 fork、去中心。代码一旦完成即自主运行。即使是中本聪也无法私自增发比特币，因为它从协议层就限制了作恶的可能性。</p>
<p><strong>Permissionless.</strong></p>
<p>无需许可是区块链世界的最高原则，任何人不需要第三方的授权就可以操作区块链。它保障了任何人都有相同的机会使用区块链上的信息、金融等服务，同时为不同服务之间的相互组合提供了可能性。对比来说，微信、支付宝互相封锁，Tw、Fb等社交平台随意禁言。他们不但违背了互联网的初衷，也扼杀了思想交流和技术创新。</p>
<p><strong>Don't trust, verify.</strong></p>
<p>你不应该相信任何人声明的事情、提供的服务、做出的承诺，你需要自己确认。正如这篇文章的所有观点你都不应该选择直接相信，你应该自己去考证。</p>
<h2>2. 对古典互联网的革命性优势</h2>
<p>互联网世界发展到现在遇到了哪些无法解决的问题？第一，大公司的数据霸权。南山必胜客刚刚打赢了“好友关系不属于个人隐私”的官司，用户在大公司面前形同裸奔。同时用户没有平台迁移的权利，我不爽，但是我还离不开，因为我的社交关系和历史都沉淀在这个平台上。第二，算法霸权。大公司利用算法优势对用户进行差别对待，精准杀熟。第三，平台割裂。平台越大，平台间的鸿沟越大。不同平台间的协作难度也越大。同一个小程序要在微信、支付宝各做一份，同一个用户要在不同的应用中分别注册。</p>
<p>区块链世界采用了完全不同的用户-服务范式。所有的服务都是以用户为中心进行整合，这个范式也被称为“用户端集成”。这个范式一举打破了传统互联网产品的壁垒，数据、资产、关系都掌握在用户手中，服务商围绕着用户提供服务。想象一个这样的互联网世界：你的好友关系在各个软件平台中共享，你的数据由自己掌握，即使服务商跑路了，你的资产和数据也不会丢失。这样的一个新的互联网世界对古典互联网世界显然具有维度上的优势。</p>
<h2>3. 那么问题是什么</h2>
<p>区块链这么厉害，为什么到现在还只有炒币这一个场景呢。个人认为区块链和主流应用结合主要面临两方面的障碍。</p>
<p>第一是用户门槛高、体验差。创建一个账户需要理解私钥、地址、交易、手续费等概念，创建过程涉及助记词、长密码、助记词校验等流程，对普通人绝对是劝退模式。做过互联网产品的都知道，一个不合理的按钮位置都会降低用户留存，而区块链的用户体验是灾难级别的。做个上链业务居然要平均半分钟才有成功失败的反馈。</p>
<p>第二是收费模型极不友好。用户早已习惯互联网产品免费使用、付费升级或者广告收费的方式。而区块链应用则不然，做任何业务都必须用户付费，而且<strong>必须购买区块链代币才能付费</strong>。有这个障碍存在，大众用户绝对进不来，所以这两年区块链世界内卷越来越严重。</p>
<h2>4. 怎么解决</h2>
<p>终于可以介绍一下我们是谁，以及在做什么了。我们是原 <a href="https://www.nervos.org" rel="noopener noreferrer">Nervos</a> 应用开发工程师团队独立成立的区块链应用公司 <strong>Nervina Labs</strong>，Nervina 是个合成词，Nervos + China。我们的使命是在中国市场进行商业合作、技术服务和产品开发工作。关于 Nervos 公链多说无益，大家秉持区块链 Don't trust, verify 的精神可以自己翻一翻 <a href="https://github.com/nervosnetwork/" rel="noopener noreferrer">GitHub</a> 和媒体报道。这里重点说说我们的产品和业务内容。我们认为 Nervos 可以很好地解决前面提出的那两个问题，从而让区块链出圈，进入主流互联网世界。</p>
<p>首先，比特币、以太坊等传统的区块链平台在账户层只支持硬编码的特定密码学算法，用户必须创建并自行管理账户的公私钥对，因此认知和使用门槛非常高。而 Nervos 采用了原生的账户抽象方案，支持任意的密码学算法，包括在互联网世界已经应用很久的 RSA、P256、和 SM2 等算法。大家熟悉的 https 协议、email 协议、iOS/Android 的生物识别模块、甚至护照身份证等都支持相应的密码学算法。所以我们可以把它们拿来在 Nervos 上创建和管理账户，这样用户操作区块链的体验和操作互联网应用基本没有区别。</p>
<p>其次，收费模型方面尽管 Nervos 默认也是需要原生代币作为手续费，但 Nervos 允许服务商代付，并且在链上凭证转移的时候可以把小额手续费一起转移。从用户角度上看，大家完全不需要理解什么叫手续费，甚至不需要知道业务发生在区块链上。更大的好处是用户不需要购买区块链代币，规避了合规风险。</p>
<p>我们的今年主推的产品是为互联网企业准备的“+区块链”方案，帮助对区块链有一定了解的头部互联网公司实现部分业务向区块链做迁移。我们将帮助互联网公司改造现有的会员卡、论坛勋章、电子票务、商品预售、粉丝经济等等生态，用区块链为更多的大众用户提供价值。</p>
<h2>5. 收入</h2>
<p>不谈收入只谈理想就是耍流氓。好在区块链是最不缺钱的行业，我们会提供和一线大厂相一致的薪资水平，包括五险一金等各方面国家规定的福利待遇一个不少。但对于很多人来说，单纯的线性工资是远远不够在一线城市安身立命的。大厂的股票期权，创业公司的干股才是大家上升到富裕阶层的砝码。在我厂工作可以有两个获得非线性收入的机会，我们分别谈谈。</p>
<p>首先是期权。我厂会在内部推动项目单独融资进而独立发展，项目参与者会获得相应的期权。在区块链世界中，优秀项目从创立到股份/期权获得实际价值的周期要远远短于传统企业上市的周期。拼夕夕光速发展也走了3年才上市，在区块链世界中几乎只要开始对外运营其期权、股份就立刻有了二级市场，可以完成价值兑现。从项目融资到“上市”中间基本上就是一个开发周期。</p>
<p>其次是行业信息优势。“康波周期理论”告诉我们人生一般也就一到两次大的机遇，抓住的话就可以完成阶层跨越。上一波机遇大家都知道是买房，而这一波应该就是区块链了。尽管从圈外人看比特币上涨到6万美金一个已经和普通人无缘了，但只有你站在这个圈子里面你才会对整个浪潮获得全面的感知，从而得出自己理性的结论。所谓“链圈一日、人间一年”，区块链世界进化速度极快，你待在这个圈子里面会发现无数的机会，比特币只是其中名声最大的一个而已。</p>
<h2>6. 职业发展</h2>
<p>我们一直戏谑 Nervos 为链圈的黄埔军校，因为确实为圈子输送了很多技术和市场人才。时不时会遇到一个圈内的 panel 四个嘉宾三个是我们的老员工。究其原因，Nervos 生态会更关注诸如“区块链究竟会带来什么价值”、“区块链的长期发展必须解决什么核心问题”等这种原则性问题而不是短平快地追逐最新热点。所以在我厂绝对不用担心只能当工具人而学不到真东西。在 Nervos 生态下大家接触的一定是业界最前沿的技术和产品。</p>
<p>Nervina Labs 对待人才管理有三个关键词：开放、自驱和涌现。开放指的是所有产品所有部门之间绝大多数信息都是互相开放的，并且几乎所有的代码库都是开源的，员工可以使用私人的 github 账号贡献代码。自驱是我们把每个员工当做对自己负责任的成年人看待，相信自己可以管理好自己的时间，不需要用严格的打卡、kpi 等制度进行管理。涌现则是指除了自上而下的任务以外，我们提倡员工自己为生态添砖加瓦。你认为整个区块链生态缺什么就可以提出来方案、预算、招聘需求，我们内部讨论通过后就给你资源让你去实现。这也是人才晋升的重要通道。你甚至可以提出融资需求，拉团队出去创业，我们提供必要的启动资金。这种模式只有在区块链行业才有可能，因为从公链整体的角度看，所有的生态企业都会带来价值，最优的模式就是由社区自发维护整个生态的发展。</p>
<h2>7. 工作环境与强度</h2>
<p>我们是<strong>100% 远程工作</strong>。沟通工具主要是 G Suite、GitHub、Notion 和 Telegram。既然是远程，也就没什么打卡、加班，全凭自觉。我们强调交付，产品和技术充分沟通后大家约定交付时间，按照交付时间和质量评估绩效。因此自驱型的人最适应我厂的工作。</p>
<p>作为团队的传统，我们每年计划有两次封闭开发的“团建”活动。一般会选在像青岛、杭州等风景饮食俱佳的城市，包下一个民宿或者别墅，用一周左右时间大家聚在一起冲刺开发产品。大家远程久了通过一两次这种活动加深了解促进协作效率，事实证明很有效果。</p>
<h2>8. 投简历 or 交朋友</h2>
<p>区块链是我们这代人肉眼可见的一次绝佳的机遇，不论你是否考虑我厂，我都建议你认真关注一下这个行业。我是qiao，很乐意交朋友，愿意回答你关于区块链的任何问题。或者你恰巧对我们感兴趣，对以下的职位感兴趣，你可以直接发邮件给我你的简历，我们聊聊看。</p>
<h2>9.招聘岗位</h2>
<p>平面设计
岗位职责：
1、负责线上线下活动海报、宣传物料等宣传品的设计；
2、辅助新媒体进行企业外宣的输出；
3、品牌VI设计、制作及其它图文处理；
4、协助其他部门人员对设计及美学方面的工作顺利完成；
职位要求：
1、大专及以上学历，美术、设计类相关专业；
2、2年以上平面设计工作经验；
3、具有良好的美术基础，精通Photoshop、Illustrator、AE等设计软件，热衷于设计工作；会C4D加分，具备较好的手绘能力优先；
4、强烈的自我驱动力，进取心。</p>
<h3>附加信息：</h3>
<ul>
<li>工作时间：周末双休</li>
<li>上下班时间：10:00-19:00</li>
</ul>
<h3>面试信息：</h3>
<ul>
<li>面试方式： 视频面试</li>
<li>面试轮数： 1-2轮</li>
<li>时间安排： 分多次完成</li>
<li>补充标签： 可周末面试 | 可下班后面试</li>
</ul>
<p>前端工程师</p>
<h2>岗位职责</h2>
<ul>
<li>根据产品需求高质量完成 web 应用以及 electron 应用;</li>
<li>对具体产品进行性能优化;</li>
<li>维护团队的工具链;</li>
<li>对可复用组件进行抽象并独立维护.</li>
</ul>
<h2>职位要求</h2>
<ul>
<li>具备良好的前端开发技能, 熟悉 HTML, CSS 和 TypeScript, 了解 Web 标准化(可访问性, 安全性);</li>
<li>在泛前端范围有开发经验, 比如 Node 应用, Electron 应用;</li>
<li>熟练使用前端的各种工具, 比如各类脚手架, CSS 处理器, 模板引擎;</li>
<li>Web 技术栈偏向 React 及 TypeScript.</li>
</ul>
<h2>加分项</h2>
<ul>
<li>具有开源项目经验;</li>
<li>提供 GitHub 或技术博客;</li>
<li>有区块链产品开发经验；
-本岗位可接受应届生，欢迎投递</li>
</ul>
<p>市场运营总监</p>
<h3>职位描述：</h3>
<p>1、全面负责平台运营工作，根据公司的OKR，制定运营策略和平台发展规划；
2、搭建运营推广渠道，进行品牌传播；
3、建立业务增长模型、用户增长模型，驱动用户增长的持续性和规模化；
4、负责平台用户（B端/C端）的活动策划，活动流程和规则设计，协同技术团队、其他运营推广团队做好活动组织安排，活动效果评估等相关工作；
5、参与公司战略及经营目标讨论及制定，向公司高层提供运营及发展建议；
任职要求：
1、本科及以上学历；3-5年相关岗位从业经验；
2、区块链相关背景、熟悉NFT等优先考虑；
3、本岗位为远程办公模式，每年两次集中封闭，每期5-10天。</p>
<h3>附加信息：</h3>
<ul>
<li>工作时间：周末双休</li>
<li>上下班时间：10:00-19:00</li>
</ul>
<p>rust工程师</p>
<h3>职位描述：</h3>
<p>岗位职责：1、负责智能合约的开发及设计；2、负责区块链业务系统分析与设计工作；3、负责智能合约代码测试、运行和维护。任职要求：1、计算机相关专业本科及以上学历，3年以上工作经验；2、熟练掌握 C/C++、Rust 等系统开发语言至少一种，至少有过两年相关开发经验；3、对数据结构和算法，对密码学，安全协议和加密算法有研究者优先；4、优秀的英语文档撰写与阅读能力者优先；5、了解区块链，有合约开发经验更佳。</p>
<h3>附加信息：</h3>
<ul>
<li>工作时间：周末双休</li>
<li>上下班时间：10:00-19:00</li>
</ul>
<h3>面试信息：</h3>
<ul>
<li>面试方式： 视频面试</li>
<li>面试轮数： 1-2轮</li>
<li>时间安排： 分多次完成</li>
<li>补充标签： 可周末面试 | 可下班后面试</li>
</ul>
<p>联系方式：15005209448（微信同） wangmeng@nervina.io</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust 日报】2021-09-26 RustConf 2021 项目精选</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=218ae5ee-1938-4ab4-a893-18e80cca61f6">
<div class="article-summary-box-inner">
<span><h3>RustConf 2021 项目精选</h3>
<p>以下项目来自 RustConf 2021。</p>
<p><strong>移动构造函数：有可能吗？</strong></p>
<p>“自引用” 类型是一种对自身引用的类型；异步 Features 是当今 Rust 中最常见的自引用类型。但是，它们不能在不使引用无效的情况下移动，因此常被固定在堆或栈上。你不能返回它们或将之放入 Collection（当然了不用 Boxing）。C++ 通过移动构造函数大量使用可安全移动的自引用类型，将移动构造函数桥接到 Rust 是 C++ FFI 未解决的重大问题之一。使用对 Pin P 保证的新颖解释，我们将所有 C++ 构造函数（而不仅仅是移动构造函数）移植到 Rust，而不影响 Rust 的「使用后移动」保护（C++ 所缺乏的）。今天，稳定的 Rust 完全支持按移动返回和集合，除了零成本的 C++ FFI，Rust 的 “构造函数” 还可用于在纯 Rust 中表达新颖的数据结构。</p>
<p>视频：<a href="https://www.youtube.com/watch?v=UrDhMWISR3w" rel="noopener noreferrer">(4) RustConf 2021 - Move Constructors: Is it Possible? by Miguel Young de la Sota - YouTube</a></p>
<p><strong>在 Rust 中不要过度优化的重要性</strong></p>
<p>Rust 编程语言具有许多高级功能，可以实现一些出色的优化。对于新的 Rust 开发人员来说，这些既令人兴奋又令人沮丧。因为 Rust 允许我们编写经验丰富的 Rust 开发人员可以理解的高度优化的代码，所以很容易一直这样做。本演讲将展示这些优化通常对于获得优于 Python 等高度动态语言的性能来说是不必要的。对于新的 Rust 开发人员来说，打破过度优化的诱惑可以提高 Rust 的生产力和满意度。</p>
<p>视频：<a href="https://www.youtube.com/watch?v=CV5CjUlcqsw" rel="noopener noreferrer">(4) RustConf 2021 - The Importance of Not Over-Optimizing in Rust by Lily Mara - YouTube</a></p>
<p><strong>模糊驱动开发</strong></p>
<p>有时候你可以想出一种简单的方法验证正确性，但很难找到单元测试的实际示例。你知道「对于所有 x 都成立」但是无法为 x 想出好的可能性。这就是依靠模糊测试可以通过提供一些代码尚未涵盖的真实示例来快速推动开发的地方。本演讲我们将一起完成使用 cargo fuzz 的过程，以构建一个可以压缩 JSON Patch 操作的快速程序，涉及 Rust 社区的一些实用程序/库（模糊测试工具，serde_json 等），以及学习一些关于在开始模糊测试时你可能会有什么样的误解。</p>
<p>视频：<a href="https://www.youtube.com/watch?v=qUu1vJNg8yo" rel="noopener noreferrer">(4) RustConf 2021 - Fuzz Driven Development by Midas Lambrichts - YouTube</a></p>
<p><strong>使用 Rust 写最快的 GBDT 库</strong></p>
<p>本演讲将分享作者优化梯度提升决策树机器学习算法的 Rust 实现的经验。 通过代码片段、堆栈跟踪和基准测试，探索如何使用 rayon、perf、cargo-asm、编译器内在函数和 unsafe rust 来编写一个 GBDT 库，该库的训练速度比用 C/C++ 编写的类似库更快。PS：作者是个美女。</p>
<p>视频：<a href="https://www.youtube.com/watch?v=D1NAREuicNs" rel="noopener noreferrer">(4) RustConf 2021 - Writing the Fastest GBDT Library in Rust by Isabella Tromba - YouTube</a></p>
<p><strong>Twitter 工程师使用 Rust 重构的故事</strong></p>
<p>三名工程师，在不同的方面，各自采用自己的方法将 Rust 添加到 C 代码库中，每个人都越来越雄心勃勃。最初只是想用同样快速的 Rust 实现替换服务器的网络和事件循环。 我们会重用 C 中的许多核心组件，然后从 Rust 中调用它们。肯定不会有那么多代码...... Pelikan 是 Twitter 用于内存缓存的开源和模块化框架，允许我们用单个代码库替换 Memcached 和 Redis 分支并获得更好的性能。在 Twitter，我们运行数百个缓存集群，在内存中存储数百 TB 的小对象。内存缓存至关重要，需要性能、可靠性和效率，本演讲将分享在 Pelikan 工作的冒险经历以及如何用 Rust 重写它。</p>
<p>视频：<a href="https://www.youtube.com/watch?v=m-Qg3OoPIdc" rel="noopener noreferrer">(4) RustConf 2021 - Whoops! I Rewrote It in Rust by Brian Martin - YouTube</a></p>
<p><strong>五个鲜为人知的属性增强你的代码</strong></p>
<p>属性是 Rust 语言中最有用和最方便的特性之一，它使程序员能够自动推导 Trait，在几分钟内设置测试套件，并有条件地为不同平台编译代码。但是在标准库的内部和外部，还有许多其他有用的属性，它们常常被忽视。</p>
<p>视频：<a href="https://www.youtube.com/watch?v=8d7DqeYXq7A" rel="noopener noreferrer">(4) RustConf 2021 - Supercharging Your Code With Five Little-Known Attributes by Jackson Lewis - YouTube</a></p>
<p><strong>编译时协调</strong></p>
<p>你可以写出好的代码，我可以，但是我们可以一起编写正确的代码吗？今天普通开发者面临的最困难的问题不是算法或框架。错误通常在代码间发现。项目包含必须在任何地方遵守但未在任何地方指定的规则。它们是惯例、部落知识和最佳实践。让我们了解 Rust 如何让编写在文件、crates 和人员之间代码的一致变得更容易。这是关于如何停止踩到每个人的脚趾并学会爱上 borrow checker 的故事。</p>
<p>视频：<a href="https://www.youtube.com/watch?v=4_Jg-rLDy-Y" rel="noopener noreferrer">(4) RustConf 2021 - Compile-Time Social Coordination by Zac Burns - YouTube</a></p>
<p><strong>Hacking Rustc</strong></p>
<p>编译器通常被视为一个令人生畏的 “黑匣子”，只有少数人才能理解。实际上，编译器 “只是” 另一种类型的程序。 只要你知道编译器是用什么语言编写的，编译器编译的语言是什么，并且有时间，你也可以处理它们。如果你曾经对修改编译器感兴趣，或者遇到了一个真心希望尽快修复的错误，那么这是卷起袖子自己动手的机会，因为我们将介绍 rustc 上的 hacking 基础知识。</p>
<p>视频：<a href="https://www.youtube.com/watch?v=9H9SO2u6Q20" rel="noopener noreferrer">(4) RustConf 2021 - Hacking rustc: Contributing to the Compiler by Esteban Kuber - YouTube</a></p>
<p>完整内容可以查看：<a href="https://this-week-in-rust.org/blog/2021/09/22/this-week-in-rust-409/" rel="noopener noreferrer">This Week in Rust 409 · This Week in Rust</a></p>
<h3>全栈 Rust：包含示例的 Tutorial</h3>
<p>本 Tutorial 是一个简单的全栈 Web 应用，包含具有数据库支持的 REST 后端和基于 Wasm 的单页应用程序前端。具体是构建一个简单的宠物主人应用程序，使用户能够添加主人和他们的宠物。程序需要将所有者和他们的宠物列表提供详细视图，能够根据需要删除和添加宠物。</p>
<p>教程涵盖以下内容：</p>
<ul>
<li>创建一个全栈的 Rust APP</li>
<li>常用功能</li>
<li>构建 REST 后端</li>
<li>前端实现</li>
<li>测试</li>
</ul>
<p>教程地址：<a href="https://blog.logrocket.com/full-stack-rust-a-complete-tutorial-with-examples/" rel="noopener noreferrer">Full-stack Rust: A complete tutorial with examples - LogRocket Blog</a></p>
<h3>Rust For Javaer</h3>
<p>写给 Java 程序员的 Rust 介绍 Tutorial。包括以下内容：</p>
<ul>
<li>简介</li>
<li>Rust 构建和运行</li>
<li>Rust 变量</li>
<li>Rust 默认的不可变性</li>
<li>Rust 函数</li>
<li>Rust if/else 和表达式</li>
<li>Rust 字符串</li>
<li>Rust 结构体（Java 类）</li>
<li>Rust 结构体函数（Java 静态函数）</li>
<li>Rust 结构体方法（Java 方法）</li>
<li>Rust Trait（Java 接口）</li>
<li>Rust 数组（Java 数组）</li>
<li>Rust 元组</li>
<li>Rust Vec</li>
<li>Rust 枚举（Java 枚举）</li>
<li>Rust Match</li>
<li>Java 17 Switch 表达式（Rust Match）</li>
<li>Rest Generics</li>
<li>Rust Option（vs Null/Optional）</li>
<li>Rust Result（Java Exception）</li>
</ul>
<p>非常不错的介绍，值得一看看。</p>
<p>视频地址：<a href="https://www.youtube.com/playlist?list=PL7r-PXl6ZPcD63DS2djSiz4SlXkaTfobc" rel="noopener noreferrer">(4) Rust for Java Developers - YouTube</a></p>
<h3>eztd：让 Rust 更易学习</h3>
<p>项目致力于『可学习性和可控制』，目标包括：</p>
<ul>
<li>低语法噪声</li>
<li>对 Python 开发者熟悉</li>
<li>允许优化内循环</li>
<li>与 Rust 生态互操作</li>
</ul>
<p>设计指南：<a href="https://github.com/epage/eztd/blob/main/CONTRIBUTING.md#design-guidelines" rel="noopener noreferrer">eztd/CONTRIBUTING.md at main · epage/eztd</a></p>
<p>GitHub：<a href="https://github.com/epage/eztd#about" rel="noopener noreferrer">epage/eztd: Source code spell checker</a></p>
<p>网址：<a href="https://epage.github.io/blog/2021/09/learning-rust/" rel="noopener noreferrer">Learnability of Rust</a></p>
<hr>
<p>From 日报小组 长琴</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc 论坛：支持 rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust 语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-25 GitHub Advisory Database 现已支持 Rust</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=43ce24e2-fba4-494e-841b-a36f7445958c">
<div class="article-summary-box-inner">
<span><h3>GitHub Advisory Database 现已支持 Rust</h3>
<p>GitHub Advisory Database（安全咨询数据库）现在已经支持 Rust 了。</p>
<p>下一步将支持 dependabot ， dependabot是 GitHub 推出的一个提醒依赖更新机器人，当你项目的依赖有更新的时候就会自动推送一个 Pull requests。</p>
<p>GitHub Advisory Database 官方写道：</p>
<p>这一覆盖范围确保了Rust社区的任何成员都可以在他们的代码所在的同一个地方检查安全问题：GitHub上。这仅仅是第一步! 请查看我们的公共路线图，我们正在努力实现Rust对依赖关系图和Dependabot警报的支持。</p>
<p>谢谢你，RustSec和Rust社区!
在我们努力将Rust生态系统加入咨询数据库的过程中，我们得到了RustSec和Rust社区的大量支持。</p>
<p>我们非常感谢RustSec，这是一个独立的组织，负责收集、规范和发布与Rust库相关的安全建议。它的免费公共数据库是我们自己的Rust漏洞数据集的起点。</p>
<p>我们计划继续与RustSec和更广泛的Rust社区合作，使我们自己的GitHub安全咨询数据可用并易于使用，以进一步补充他们的数据。通过合作，我们可以为减少漏洞的可见性问题做更多的工作，而不是单独行动。</p>
<p>原文<a href="https://github.blog/2021-09-23-github-advisory-database-now-supports-rust/" rel="noopener noreferrer">链接</a>，https://github.blog/2021-09-23-github-advisory-database-now-supports-rust/</p>
<h3>Klask</h3>
<p>Klask，从 clap v3 apps 自动创建 GUI 应用，将 egui 用于图形。</p>
<p>Github <a href="https://github.com/MichalGniadek/klask" rel="noopener noreferrer">链接</a>，https://github.com/MichalGniadek/klask</p>
<h3>MiniJinja</h3>
<p>MiniJinja 是一个强大但最小依赖的 Rust 模板引擎，基于 Python 的 <a href="https://jinja.palletsprojects.com/en/3.0.x/" rel="noopener noreferrer">Jinja2</a> 模板引擎的语法和行为。</p>
<pre><code>use minijinja::Environment;
use serde::Serialize;

#[derive(Serialize)]
pub struct Context {
    name: String,
}

fn main() {
    let mut env = Environment::new();
    env.add_template("hello.txt", "Hello {{ name }}!").unwrap();
    let template = env.get_template("hello.txt").unwrap();
    println!("{}", template.render(&amp;Context {
        name: "World".into()
    }).unwrap());
}
</code></pre>
<p>Github <a href="https://github.com/mitsuhiko/minijinja" rel="noopener noreferrer">链接</a>，https://github.com/mitsuhiko/minijinja</p>
<hr>
<p>From 日报小组 <a href="https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929" rel="noopener noreferrer">洋芋</a></p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b">
<div class="article-summary-box-inner">
<span><p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>
<ol>
<li>认识面向基础架构语言</li>
<li>理解 Rust 所有权</li>
<li>通过实战理解 Rust 宏</li>
<li>通过 Datafuse 理解全链路跟踪</li>
<li>Rust 异步编程入门 Future Part 1</li>
<li>Rust 异步编程入门 Future Part 2</li>
</ol>
<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。
本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>
<hr>
<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>
<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>
<p>分享讲师： 董泽润</p>
<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现
讲师介绍：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<hr>
<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>
<p>分享时间： 周日晚上2021-09-12 20:00-21:00</p>
<p>分享讲师： 苏林</p>
<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>
<ol>
<li>利用 Tokio 实现一个 Mini Http server</li>
<li>基于 Http server提供内容动态的 API 网关</li>
<li>利用 Redis 实现对 API 网关加速</li>
<li>学习 Rust RPC 调用，实现微服务调用</li>
</ol>
<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<p>Rust 异步编程入门 Future Part 1 | Vol. 5
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<p>Rust 异步编程入门 Future Part 2 | Vol. 6
https://www.bilibili.com/video/bv1oy4y1G7jC</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust 学习随笔</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8">
<div class="article-summary-box-inner">
<span><h1>切换镜像源</h1>
<p>crm =&gt; https://github.com/wtklbm/crm</p>
<p>常用命令就是 <code>crm best</code></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">pretree 补全文档发布了,再次谢谢大神的指点终于入门了。</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827">
<div class="article-summary-box-inner">
<span><h1>Pretree</h1>
<p>pretree is a package for storing and querying routing rules with prefix tree .</p>
<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>
<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>
<p>Inspired by <a href="https://github.com/obity/pretree" rel="noopener noreferrer">obity/pretree</a> (golang)</p>
<h1>Doc</h1>
<p>See this document at <a href="https://docs.rs/pretree" rel="noopener noreferrer">API documentation</a></p>
<h1>Install</h1>
<p>Add the following line to your Cargo.toml file:</p>
<pre><code>pretree = "1.0.0"
</code></pre>
<h1>Example</h1>
<pre><code>use pretree::Pretree;
let mut p = Pretree::new();
p.store("GET","account/{id}/info/:name");
p.store("GET","account/:id/login");
p.store("GET","account/{id}");
p.store("GET","bacteria/count_number_by_month");
let (ok,rule,vars) = p.query("GET","account/929239");
println!("ok:{} rule:{} vars:{:#?}",ok,rule,vars);

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>
<p><strong>课程时间:</strong> 2021年9月5日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识, Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>
<h3>课程大纲</h3>
<p>1、回顾 Rust 异步编程模型.</p>
<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>
<p>3、Tokio 介绍.</p>
<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>
<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/
Rust 异步编程入门 Future Part 1 回放地址：
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课：《 Rust 异步编程入门 Future 》|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>
<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是 Rust 异步编程的核心基础。</p>
<h3>课程大纲</h3>
<p>1、为什么需要异步.</p>
<p>2、理解异步编程模型.</p>
<p>3、Future 编程模型讲解.</p>
<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>
<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>异步引擎 C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D 游戏引擎</h3>
<ul>
<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>
<li><strong>延迟着色</strong></li>
<li><strong>内置保存/加载</strong></li>
<li><strong>独立场景编辑器</strong></li>
<li><strong>高级物理模型</strong></li>
<li><strong>分层模型资源</strong></li>
<li><strong>几何实例化</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>
<p><strong>课程时间：</strong> 2021年8月22日 20:30-21:30</p>
<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>
<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>
<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>
<h3>课程大纲</h3>
<ol>
<li>
<p>什么是分布式追踪系统OpenTracing及应用场景</p>
</li>
<li>
<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>
</li>
<li>
<p>为什么需要tokio-rs/tracing库</p>
</li>
<li>
<p>演示Datafuse项目中tokio-rs/tracing的使用</p>
</li>
</ol>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">论坛github账户无法登录解决笔记</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>有反映这两天github账户无法登录了。</p>
<p>报这个错：</p>
<pre><code>get github user info err
</code></pre>
<p>查了几个地方：</p>
<ol>
<li>代码是否运行正常：Ok</li>
<li>https代理是否正常：Ok</li>
<li>检查了github返回日志，发现是：</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>进入这个地址一看：<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>
<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>
<p>特此记录。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 的 Future 与 Javascript 的 Promise 功能对照参考</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>
<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>
<blockquote>
<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* 什么都不做 */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被 （1）跨线程传递 （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：《通过实战理解 Rust 宏》| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>
<p><strong>课程时间：</strong> 2021年8月15日 20:30-21:30</p>
<p><strong>课程介绍：</strong></p>
<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>这就是通过宏实现配置的统一行为，代码参考：
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>
<h3>课程大纲</h3>
<ul>
<li>什么是 Rust 宏</li>
<li>什么是宏运行原理</li>
<li>如何创建 Rust 宏过程</li>
<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>
</ul>
<p><strong>讲师介绍</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-29T01:30:00Z">09-29</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13238">
<div class="article-summary-box-inner">
<span><p>The design of widespread vision-and-language datasets and pre-trained
encoders directly adopts, or draws inspiration from, the concepts and images of
ImageNet. While one can hardly overestimate how much this benchmark contributed
to progress in computer vision, it is mostly derived from lexical databases and
image queries in English, resulting in source material with a North American or
Western European bias. Therefore, we devise a new protocol to construct an
ImageNet-style hierarchy representative of more languages and cultures. In
particular, we let the selection of both concepts and images be entirely driven
by native speakers, rather than scraping them automatically. Specifically, we
focus on a typologically diverse set of languages, namely, Indonesian, Mandarin
Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images
obtained through this new protocol, we create a multilingual dataset for
{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting
statements from native speaker annotators about pairs of images. The task
consists of discriminating whether each grounded statement is true or false. We
establish a series of baselines using state-of-the-art models and find that
their cross-lingual transfer performance lags dramatically behind supervised
performance in English. These results invite us to reassess the robustness and
accuracy of current state-of-the-art models beyond a narrow domain, but also
open up new exciting challenges for the development of truly multilingual and
multicultural systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation. (arXiv:2109.13296v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13296">
<div class="article-summary-box-inner">
<span><p>Recent progress in generative language models has enabled machines to
generate astonishingly realistic texts. While there are many legitimate
applications of such models, there is also a rising need to distinguish
machine-generated texts from human-written ones (e.g., fake news detection).
However, to our best knowledge, there is currently no benchmark environment
with datasets and tasks to systematically study the so-called "Turing Test"
problem for neural text generation methods. In this work, we present the
TuringBench benchmark environment, which is comprised of (1) a dataset with
200K human- or machine-generated samples across 20 labels {Human, GPT-1,
GPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3,
GROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large,
FAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two
benchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and
(3) a website with leaderboards. Our preliminary experimental results using
TuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all
language models tested, in generating the most human-like indistinguishable
texts with the lowest F1 score by five state-of-the-art TT detection models.
The TuringBench is available at: https://turingbench.ist.psu.edu/
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Isotropy Calibration of Transformers. (arXiv:2109.13304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13304">
<div class="article-summary-box-inner">
<span><p>Different studies of the embedding space of transformer models suggest that
the distribution of contextual representations is highly anisotropic - the
embeddings are distributed in a narrow cone. Meanwhile, static word
representations (e.g., Word2Vec or GloVe) have been shown to benefit from
isotropic spaces. Therefore, previous work has developed methods to calibrate
the embedding space of transformers in order to ensure isotropy. However, a
recent study (Cai et al. 2021) shows that the embedding space of transformers
is locally isotropic, which suggests that these models are already capable of
exploiting the expressive capacity of their embedding space. In this work, we
conduct an empirical evaluation of state-of-the-art methods for isotropy
calibration on transformers and find that they do not provide consistent
improvements across models and tasks. These results support the thesis that,
given the local isotropy, transformers do not benefit from additional isotropy
calibration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation. (arXiv:2109.13318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13318">
<div class="article-summary-box-inner">
<span><p>Automating sign language translation (SLT) is a challenging real world
application. Despite its societal importance, though, research progress in the
field remains rather poor. Crucially, existing methods that yield viable
performance necessitate the availability of laborious to obtain gloss sequence
groundtruth. In this paper, we attenuate this need, by introducing an
end-to-end SLT model that does not entail explicit use of glosses; the model
only needs text groundtruth. This is in stark contrast to existing end-to-end
models that use gloss sequence groundtruth, either in the form of a modality
that is recognized at an intermediate model stage, or in the form of a parallel
output process, jointly trained with the SLT model. Our approach constitutes a
Transformer network with a novel type of layers that combines: (i) local
winner-takes-all (LWTA) layers with stochastic winner sampling, instead of
conventional ReLU layers, (ii) stochastic weights with posterior distributions
estimated via variational inference, and (iii) a weight compression technique
at inference time that exploits estimated posterior variance to perform
massive, almost lossless compression. We demonstrate that our approach can
reach the currently best reported BLEU-4 score on the PHOENIX 2014T benchmark,
but without making use of glosses for model training, and with a memory
footprint reduced by more than 70%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in the UMLS Metathesaurus. (arXiv:2109.13348v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13348">
<div class="article-summary-box-inner">
<span><p>The current UMLS (Unified Medical Language System) Metathesaurus construction
process for integrating over 200 biomedical source vocabularies is expensive
and error-prone as it relies on the lexical algorithms and human editors for
deciding if the two biomedical terms are synonymous. Recent advances in Natural
Language Processing such as Transformer models like BERT and its biomedical
variants with contextualized word embeddings have achieved state-of-the-art
(SOTA) performance on downstream tasks. We aim to validate if these approaches
using the BERT models can actually outperform the existing approaches for
predicting synonymy in the UMLS Metathesaurus. In the existing Siamese Networks
with LSTM and BioWordVec embeddings, we replace the BioWordVec embeddings with
the biomedical BERT embeddings extracted from each BERT model using different
ways of extraction. In the Transformer architecture, we evaluate the use of the
different biomedical BERT models that have been pre-trained using different
datasets and tasks. Given the SOTA performance of these BERT models for other
downstream tasks, our experiments yield surprisingly interesting results: (1)
in both model architectures, the approaches employing these biomedical
BERT-based models do not outperform the existing approaches using Siamese
Network with BioWordVec embeddings for the UMLS synonymy prediction task, (2)
the original BioBERT large model that has not been pre-trained with the UMLS
outperforms the SapBERT models that have been pre-trained with the UMLS, and
(3) using the Siamese Networks yields better performance for synonymy
prediction when compared to using the biomedical BERT models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SYGMA: System for Generalizable Modular Question Answering OverKnowledge Bases. (arXiv:2109.13430v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13430">
<div class="article-summary-box-inner">
<span><p>Knowledge Base Question Answering (KBQA) tasks that in-volve complex
reasoning are emerging as an important re-search direction. However, most KBQA
systems struggle withgeneralizability, particularly on two dimensions: (a)
acrossmultiple reasoning types where both datasets and systems haveprimarily
focused on multi-hop reasoning, and (b) across mul-tiple knowledge bases, where
KBQA approaches are specif-ically tuned to a single knowledge base. In this
paper, wepresent SYGMA, a modular approach facilitating general-izability
across multiple knowledge bases and multiple rea-soning types. Specifically,
SYGMA contains three high levelmodules: 1) KB-agnostic question understanding
module thatis common across KBs 2) Rules to support additional reason-ing types
and 3) KB-specific question mapping and answeringmodule to address the
KB-specific aspects of the answer ex-traction. We demonstrate effectiveness of
our system by evalu-ating on datasets belonging to two distinct knowledge
bases,DBpedia and Wikidata. In addition, to demonstrate extensi-bility to
additional reasoning types we evaluate on multi-hopreasoning datasets and a new
Temporal KBQA benchmarkdataset on Wikidata, namedTempQA-WD1, introduced in
thispaper. We show that our generalizable approach has bettercompetetive
performance on multiple datasets on DBpediaand Wikidata that requires both
multi-hop and temporal rea-soning
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When in Doubt: Improving Classification Performance with Alternating Normalization. (arXiv:2109.13449v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13449">
<div class="article-summary-box-inner">
<span><p>We introduce Classification with Alternating Normalization (CAN), a
non-parametric post-processing step for classification. CAN improves
classification accuracy for challenging examples by re-adjusting their
predicted class probability distribution using the predicted class
distributions of high-confidence validation examples. CAN is easily applicable
to any probabilistic classifier, with minimal computation overhead. We analyze
the properties of CAN using simulated experiments, and empirically demonstrate
its effectiveness across a diverse set of classification tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Teacher-Student Learning Approach for Multi-lingual Speech-to-Intent Classification. (arXiv:2109.13486v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13486">
<div class="article-summary-box-inner">
<span><p>End-to-end speech-to-intent classification has shown its advantage in
harvesting information from both text and speech. In this paper, we study a
technique to develop such an end-to-end system that supports multiple
languages. To overcome the scarcity of multi-lingual speech corpus, we exploit
knowledge from a pre-trained multi-lingual natural language processing model.
Multi-lingual bidirectional encoder representations from transformers (mBERT)
models are trained on multiple languages and hence expected to perform well in
the multi-lingual scenario. In this work, we employ a teacher-student learning
approach to sufficiently extract information from an mBERT model to train a
multi-lingual speech model. In particular, we use synthesized speech generated
from an English-Mandarin text corpus for analysis and training of a
multi-lingual intent classification model. We also demonstrate that the
teacher-student learning approach obtains an improved performance (91.02%) over
the traditional end-to-end (89.40%) intent classification approach in a
practical multi-lingual scenario.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">"How Robust r u?": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations. (arXiv:2109.13489v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13489">
<div class="article-summary-box-inner">
<span><p>Most prior work in dialogue modeling has been on written conversations mostly
because of existing data sets. However, written dialogues are not sufficient to
fully capture the nature of spoken conversations as well as the potential
speech recognition errors in practical spoken dialogue systems. This work
presents a new benchmark on spoken task-oriented conversations, which is
intended to study multi-domain dialogue state tracking and knowledge-grounded
dialogue modeling. We report that the existing state-of-the-art models trained
on written conversations are not performing well on our spoken data, as
expected. Furthermore, we observe improvements in task performances when
leveraging n-best speech recognition hypotheses such as by combining
predictions based on individual hypotheses. Our data set enables speech-based
benchmarking of task-oriented dialogue systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Instance-Based Neural Dependency Parsing. (arXiv:2109.13497v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13497">
<div class="article-summary-box-inner">
<span><p>Interpretable rationales for model predictions are crucial in practical
applications. We develop neural models that possess an interpretable inference
process for dependency parsing. Our models adopt instance-based inference,
where dependency edges are extracted and labeled by comparing them to edges in
a training set. The training edges are explicitly used for the predictions;
thus, it is easy to grasp the contribution of each edge to the predictions. Our
experiments show that our instance-based models achieve competitive accuracy
with standard neural models and have the reasonable plausibility of
instance-based explanations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">VoxCeleb Enrichment for Age and Gender Recognition. (arXiv:2109.13510v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13510">
<div class="article-summary-box-inner">
<span><p>VoxCeleb datasets are widely used in speaker recognition studies. Our work
serves two purposes. First, we provide speaker age labels and (an alternative)
annotation of speaker gender. Second, we demonstrate the use of this metadata
by constructing age and gender recognition models with different features and
classifiers. We query different celebrity databases and apply consensus rules
to derive age and gender labels. We also compare the original VoxCeleb gender
labels with our labels to identify records that might be mislabeled in the
original VoxCeleb data. On modeling side, we design a comprehensive study of
multiple features and models for recognizing gender and age. Our best system,
using i-vector features, achieved an F1-score of 0.9829 for gender recognition
task using logistic regression, and the lowest mean absolute error (MAE) in age
regression, 9.443 years, is obtained with ridge regression. This indicates
challenge in age estimation from in-the-wild style speech data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Template-free Prompt Tuning for Few-shot NER. (arXiv:2109.13532v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13532">
<div class="article-summary-box-inner">
<span><p>Prompt-based methods have been successfully applied in sentence-level
few-shot learning tasks, mostly owing to the sophisticated design of templates
and label words. However, when applied to token-level labeling tasks such as
NER, it would be time-consuming to enumerate the template queries over all
potential entity spans. In this work, we propose a more elegant method to
reformulate NER tasks as LM problems without any templates. Specifically, we
discard the template construction process while maintaining the word prediction
paradigm of pre-training models to predict a class-related pivot word (or label
word) at the entity position. Meanwhile, we also explore principled ways to
automatically search for appropriate label words that the pre-trained models
can easily adapt to. While avoiding complicated template-based process, the
proposed LM objective also reduces the gap between different objectives used in
pre-training and fine-tuning, thus it can better benefit the few-shot
performance. Experimental results demonstrate the effectiveness of the proposed
method over bert-tagger and template-based method under few-shot setting.
Moreover, the decoding speed of the proposed method is up to 1930.12 times
faster than the template-based method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement. (arXiv:2109.13563v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13563">
<div class="article-summary-box-inner">
<span><p>Since state-of-the-art approaches to offensive language detection rely on
supervised learning, it is crucial to quickly adapt them to the continuously
evolving scenario of social media. While several approaches have been proposed
to tackle the problem from an algorithmic perspective, so to reduce the need
for annotated data, less attention has been paid to the quality of these data.
Following a trend that has emerged recently, we focus on the level of agreement
among annotators while selecting data to create offensive language datasets, a
task involving a high level of subjectivity. Our study comprises the creation
of three novel datasets of English tweets covering different topics and having
five crowd-sourced judgments each. We also present an extensive set of
experiments showing that selecting training and test data according to
different levels of annotators' agreement has a strong effect on classifiers
performance and robustness. Our findings are further validated in cross-domain
experiments and studied using a popular benchmark dataset. We show that such
hard cases, where low agreement is present, are not necessarily due to
poor-quality annotation and we advocate for a higher presence of ambiguous
cases in future datasets, particularly in test sets, to better account for the
different points of view expressed online.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generating texts under constraint through discriminator-guided MCTS. (arXiv:2109.13582v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13582">
<div class="article-summary-box-inner">
<span><p>Large pre-trained language models (LM) based on Transformers allow to
generate very plausible long texts. In this paper, we explore how this
generation can be further controlled to satisfy certain constraints (eg. being
non-toxic, positive or negative, convey certain emotions, etc.) without
fine-tuning the LM. Precisely, we formalize constrained generation as a tree
exploration process guided by a discriminator according to how well the
associated sequence respects the constraint. Using a discriminator to guide
this generation, rather than fine-tuning the LM, in addition to be easier and
cheaper to train, allows to apply the constraint more finely and dynamically.
We propose several original methods to search this generation tree, notably the
Monte Carlo Tree Search (MCTS) which provides theoretical guarantees on the
search efficiency, but also simpler methods based on re-ranking a pool of
diverse sequences using the discriminator scores. We evaluate these methods on
two types of constraints and languages: review polarity and emotion control in
French and English. We show that MCTS achieves state-of-the-art results in
constrained generation, without having to tune the language model, in both
tasks and languages. We also demonstrate that our other proposed methods based
on re-ranking can be really effective when diversity among the generated
propositions is encouraged.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Argument Mining: A Practical Approach. (arXiv:2109.13611v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13611">
<div class="article-summary-box-inner">
<span><p>Despite considerable recent progress, the creation of well-balanced and
diverse resources remains a time-consuming and costly challenge in Argument
Mining. Active Learning reduces the amount of data necessary for the training
of machine learning models by querying the most informative samples for
annotation and therefore is a promising method for resource creation. In a
large scale comparison of several Active Learning methods, we show that Active
Learning considerably decreases the effort necessary to get good deep learning
performance on the task of Argument Unit Recognition and Classification (AURC).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking. (arXiv:2109.13620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13620">
<div class="article-summary-box-inner">
<span><p>Recent progress in task-oriented neural dialogue systems is largely focused
on a handful of languages, as annotation of training data is tedious and
expensive. Machine translation has been used to make systems multilingual, but
this can introduce a pipeline of errors. Another promising solution is using
cross-lingual transfer learning through pretrained multilingual models.
Existing methods train multilingual models with additional code-mixed task data
or refine the cross-lingual representations through parallel ontologies. In
this work, we enhance the transfer learning process by intermediate fine-tuning
of pretrained multilingual models, where the multilingual models are fine-tuned
with different but related data and/or tasks. Specifically, we use parallel and
conversational movie subtitles datasets to design cross-lingual intermediate
tasks suitable for downstream dialogue tasks. We use only 200K lines of
parallel data for intermediate fine-tuning which is already available for 1782
language pairs. We test our approach on the cross-lingual dialogue state
tracking task for the parallel MultiWoZ (English -&gt; Chinese, Chinese -&gt;
English) and Multilingual WoZ (English -&gt; German, English -&gt; Italian) datasets.
We achieve impressive improvements (&gt; 20% on joint goal accuracy) on the
parallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla
baseline with only 10% of the target language task data and zero-shot setup
respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v1 [eess.SY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13662">
<div class="article-summary-box-inner">
<span><p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce
an end-to-end trainable system that integrates reasoning and perception. PSL
represents first-order logic in terms of a convex graphical model -- Hinge Loss
Markov random fields (HL-MRFs). PSL stands out among probabilistic logic
frameworks due to its tractability having been applied to systems of more than
1 billion ground rules. The key to our approach is to represent predicates in
first-order logic using deep neural networks and then to approximately
back-propagate through the HL-MRF and thus train every aspect of the
first-order system being represented. We believe that this approach represents
an interesting direction for the integration of deep learning and reasoning
techniques with applications to knowledge base learning, multi-task learning,
and explainability. We evaluate DeepPSL on a zero shot learning problem in
image classification. State of the art results demonstrate the utility and
flexibility of our approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Counter Narrative Type Classification. (arXiv:2109.13664v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13664">
<div class="article-summary-box-inner">
<span><p>The growing interest in employing counter narratives for hatred intervention
brings with it a focus on dataset creation and automation strategies. In this
scenario, learning to recognize counter narrative types from natural text is
expected to be useful for applications such as hate speech countering, where
operators from non-governmental organizations are supposed to answer to hate
with several and diverse arguments that can be mined from online sources. This
paper presents the first multilingual work on counter narrative type
classification, evaluating SoTA pre-trained language models in monolingual,
multilingual and cross-lingual settings. When considering a fine-grained
annotation of counter narrative classes, we report strong baseline
classification results for the majority of the counter narrative types,
especially if we translate every language to English before cross-lingual
prediction. This suggests that knowledge about counter narratives can be
successfully transferred across languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Nana-HDR: A Non-attentive Non-autoregressive Hybrid Model for TTS. (arXiv:2109.13673v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13673">
<div class="article-summary-box-inner">
<span><p>This paper presents Nana-HDR, a new non-attentive non-autoregressive model
with hybrid Transformer-based Dense-fuse encoder and RNN-based decoder for TTS.
It mainly consists of three parts: Firstly, a novel Dense-fuse encoder with
dense connections between basic Transformer blocks for coarse feature fusion
and a multi-head attention layer for fine feature fusion. Secondly, a
single-layer non-autoregressive RNN-based decoder. Thirdly, a duration
predictor instead of an attention model that connects the above hybrid encoder
and decoder. Experiments indicate that Nana-HDR gives full play to the
advantages of each component, such as strong text encoding ability of
Transformer-based encoder, stateful decoding without being bothered by exposure
bias and local information preference, and stable alignment provided by
duration predictor. Due to these advantages, Nana-HDR achieves competitive
performance in naturalness and robustness on two Mandarin corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CIDEr-R: Robust Consensus-based Image Description Evaluation. (arXiv:2109.13701v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13701">
<div class="article-summary-box-inner">
<span><p>This paper shows that CIDEr-D, a traditional evaluation metric for image
description, does not work properly on datasets where the number of words in
the sentence is significantly greater than those in the MS COCO Captions
dataset. We also show that CIDEr-D has performance hampered by the lack of
multiple reference sentences and high variance of sentence length. To bypass
this problem, we introduce CIDEr-R, which improves CIDEr-D, making it more
flexible in dealing with datasets with high sentence length variance. We
demonstrate that CIDEr-R is more accurate and closer to human judgment than
CIDEr-D; CIDEr-R is more robust regarding the number of available references.
Our results reveal that using Self-Critical Sequence Training to optimize
CIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized,
the generated captions' length tends to be similar to the reference length.
However, the models also repeat several times the same word to increase the
sentence length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One to rule them all: Towards Joint Indic Language Hate Speech Detection. (arXiv:2109.13711v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13711">
<div class="article-summary-box-inner">
<span><p>This paper is a contribution to the Hate Speech and Offensive Content
Identification in Indo-European Languages (HASOC) 2021 shared task. Social
media today is a hotbed of toxic and hateful conversations, in various
languages. Recent news reports have shown that current models struggle to
automatically identify hate posted in minority languages. Therefore,
efficiently curbing hate speech is a critical challenge and problem of
interest. We present a multilingual architecture using state-of-the-art
transformer language models to jointly learn hate and offensive speech
detection across three languages namely, English, Hindi, and Marathi. On the
provided testing corpora, we achieve Macro F1 scores of 0.7996, 0.7748, 0.8651
for sub-task 1A and 0.6268, 0.5603 during the fine-grained classification of
sub-task 1B. These results show the efficacy of exploiting a multilingual
training scheme.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analyzing the Use of Character-Level Translation with Sparse and Noisy Datasets. (arXiv:2109.13723v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13723">
<div class="article-summary-box-inner">
<span><p>This paper provides an analysis of character-level machine translation models
used in pivot-based translation when applied to sparse and noisy datasets, such
as crowdsourced movie subtitles. In our experiments, we find that such
character-level models cut the number of untranslated words by over 40% and are
especially competitive (improvements of 2-3 BLEU points) in the case of limited
training data. We explore the impact of character alignment, phrase table
filtering, bitext size and the choice of pivot language on translation quality.
We further compare cascaded translation models to the use of synthetic training
data via multiple pivots, and we find that the latter works significantly
better. Finally, we demonstrate that neither word-nor character-BLEU correlate
perfectly with human judgments, due to BLEU's sensitivity to length.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translating from Morphologically Complex Languages: A Paraphrase-Based Approach. (arXiv:2109.13724v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13724">
<div class="article-summary-box-inner">
<span><p>We propose a novel approach to translating from a morphologically complex
language. Unlike previous research, which has targeted word inflections and
concatenations, we focus on the pairwise relationship between morphologically
related words, which we treat as potential paraphrases and handle using
paraphrasing techniques at the word, phrase, and sentence level. An important
advantage of this framework is that it can cope with derivational morphology,
which has so far remained largely beyond the capabilities of statistical
machine translation systems. Our experiments translating from Malay, whose
morphology is mostly derivational, into English show significant improvements
over rivaling approaches based on five automatic evaluation measures (for
320,000 sentence pairs; 9.5 million English word tokens).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sentiment Analysis in Twitter for Macedonian. (arXiv:2109.13725v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13725">
<div class="article-summary-box-inner">
<span><p>We present work on sentiment analysis in Twitter for Macedonian. As this is
pioneering work for this combination of language and genre, we created suitable
resources for training and evaluating a system for sentiment analysis of
Macedonian tweets. In particular, we developed a corpus of tweets annotated
with tweet-level sentiment polarity (positive, negative, and neutral), as well
as with phrase-level sentiment, which we made freely available for research
purposes. We further bootstrapped several large-scale sentiment lexicons for
Macedonian, motivated by previous work for English. The impact of several
different pre-processing steps as well as of various features is shown in
experiments that represent the first attempt to build a system for sentiment
analysis in Twitter for the morphologically rich Macedonian language. Overall,
our experimental results show an F1-score of 92.16, which is very strong and is
on par with the best results for English, which were achieved in recent SemEval
competitions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exposing Paid Opinion Manipulation Trolls. (arXiv:2109.13726v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13726">
<div class="article-summary-box-inner">
<span><p>Recently, Web forums have been invaded by opinion manipulation trolls. Some
trolls try to influence the other users driven by their own convictions, while
in other cases they can be organized and paid, e.g., by a political party or a
PR agency that gives them specific instructions what to write. Finding paid
trolls automatically using machine learning is a hard task, as there is no
enough training data to train a classifier; yet some test data is possible to
obtain, as these trolls are sometimes caught and widely exposed. In this paper,
we solve the training data problem by assuming that a user who is called a
troll by several different people is likely to be such, and one who has never
been called a troll is unlikely to be such. We compare the profiles of (i) paid
trolls vs. (ii)"mentioned" trolls vs. (iii) non-trolls, and we further show
that a classifier trained to distinguish (ii) from (iii) does quite well also
at telling apart (i) from (iii).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Triplet Loss for Named Entity Recognition using Supplementary Text. (arXiv:2109.13736v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13736">
<div class="article-summary-box-inner">
<span><p>Retail item data contains many different forms of text like the title of an
item, the description of an item, item name and reviews. It is of interest to
identify the item name in the other forms of text using a named entity tagger.
However, the title of an item and its description are syntactically different
(but semantically similar) in that the title is not necessarily a well formed
sentence while the description is made up of well formed sentences. In this
work, we use a triplet loss to contrast the embeddings of the item title with
the description to establish a proof of concept. We find that using the triplet
loss in a multi-task NER algorithm improves both the precision and recall by a
small percentage. While the improvement is small, we think it is a step in the
right direction of using various forms of text in a multi-task algorithm. In
addition to precision and recall, the multi task triplet loss method is also
found to significantly improve the exact match accuracy i.e. the accuracy of
tagging the entire set of tokens in the text with correct tags.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On Homophony and R\'enyi Entropy. (arXiv:2109.13766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13766">
<div class="article-summary-box-inner">
<span><p>Homophony's widespread presence in natural languages is a controversial
topic. Recent theories of language optimality have tried to justify its
prevalence, despite its negative effects on cognitive processing time; e.g.,
Piantadosi et al. (2012) argued homophony enables the reuse of efficient
wordforms and is thus beneficial for languages. This hypothesis has recently
been challenged by Trott and Bergen (2020), who posit that good wordforms are
more often homophonous simply because they are more phonotactically probable.
In this paper, we join in on the debate. We first propose a new
information-theoretic quantification of a language's homophony: the sample
R\'enyi entropy. Then, we use this quantification to revisit Trott and Bergen's
claims. While their point is theoretically sound, a specific methodological
issue in their experiments raises doubts about their results. After addressing
this issue, we find no clear pressure either towards or against homophony -- a
much more nuanced result than either Piantadosi et al.'s or Trott and Bergen's
findings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings. (arXiv:2109.13767v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13767">
<div class="article-summary-box-inner">
<span><p>Euclidean word embedding models such as GloVe and Word2Vec have been shown to
reflect human-like gender biases. In this paper, we extend the study of gender
bias to the recently popularized hyperbolic word embeddings. We propose
gyrocosine bias, a novel measure for quantifying gender bias in hyperbolic word
representations and observe a significant presence of gender bias. To address
this problem, we propose Poincar\'e Gender Debias (PGD), a novel debiasing
procedure for hyperbolic word representations. Experiments on a suit of
evaluation tests show that PGD effectively reduces bias while adding a minimal
semantic offset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health. (arXiv:2109.13770v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13770">
<div class="article-summary-box-inner">
<span><p>Many statistical models have high accuracy on test benchmarks, but are not
explainable, struggle in low-resource scenarios, cannot be reused for multiple
tasks, and cannot easily integrate domain expertise. These factors limit their
use, particularly in settings such as mental health, where it is difficult to
annotate datasets and model outputs have significant impact. We introduce a
micromodel architecture to address these challenges. Our approach allows
researchers to build interpretable representations that embed domain knowledge
and provide explanations throughout the model's decision process. We
demonstrate the idea on multiple mental health tasks: depression
classification, PTSD classification, and suicidal risk assessment. Our systems
consistently produce strong results, even in low-resource scenarios, and are
more interpretable than alternative methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Chekhov's Gun Recognition. (arXiv:2109.13855v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13855">
<div class="article-summary-box-inner">
<span><p>Chekhov's gun is a dramatic principle stating that every element in a story
must be necessary, and irrelevant elements should be removed. This paper
presents a new natural language processing task - Chekhov's gun recognition or
(CGR) - recognition of entities that are pivotal for the development of the
plot. Though similar to classical Named Entity Recognition (NER) it has
profound differences and is crucial for the tasks of narrative processing,
since Chekhov's guns have a profound impact on the causal relationship in a
story. The paper presents a new benchmark dataset for the CGR task that
includes 5550 descriptions with one or more Chekhov's Gun in each and validates
the task on two more datasets available in the natural language processing
(NLP) literature.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Expectation-based Minimalist Grammars. (arXiv:2109.13871v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13871">
<div class="article-summary-box-inner">
<span><p>Expectation-based Minimalist Grammars (e-MGs) are simplified versions of the
(Conflated) Minimalist Grammars, (C)MGs, formalized by Stabler (Stabler, 2011,
2013, 1997) and Phase-based Minimalist Grammars, PMGs (Chesi, 2005, 2007;
Stabler, 2011). The crucial simplification consists of driving structure
building only by relying on lexically encoded categorial top-down expectations.
The commitment on a top-down derivation (as in e-MGs and PMGs, as opposed to
(C)MGs, Chomsky, 1995; Stabler, 2011) allows us to define a core derivation
that should be the same in both parsing and generation (Momma &amp; Phillips,
2018).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Single-dataset Experts for Multi-dataset Question Answering. (arXiv:2109.13880v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13880">
<div class="article-summary-box-inner">
<span><p>Many datasets have been created for training reading comprehension models,
and a natural question is whether we can combine them to build models that (1)
perform better on all of the training datasets and (2) generalize and transfer
better to new datasets. Prior work has addressed this goal by training one
network simultaneously on multiple datasets, which works well on average but is
prone to over- or under-fitting different sub-distributions and might transfer
worse compared to source models with more overlap with the target dataset. Our
approach is to model multi-dataset question answering with a collection of
single-dataset experts, by training a collection of lightweight,
dataset-specific adapter modules (Houlsby et al., 2019) that share an
underlying Transformer model. We find that these Multi-Adapter Dataset Experts
(MADE) outperform all our baselines in terms of in-distribution accuracy, and
simple methods based on parameter-averaging lead to better zero-shot
generalization and few-shot transfer performance, offering a strong and
versatile starting point for building new reading comprehension systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Different Text-preprocessing Techniques Using The BERT Model Affect The Gender Profiling of Authors. (arXiv:2109.13890v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13890">
<div class="article-summary-box-inner">
<span><p>Forensic author profiling plays an important role in indicating possible
profiles for suspects. Among the many automated solutions recently proposed for
author profiling, transfer learning outperforms many other state-of-the-art
techniques in natural language processing. Nevertheless, the sophisticated
technique has yet to be fully exploited for author profiling. At the same time,
whereas current methods of author profiling, all largely based on features
engineering, have spawned significant variation in each model used, transfer
learning usually requires a preprocessed text to be fed into the model. We
reviewed multiple references in the literature and determined the most common
preprocessing techniques associated with authors' genders profiling.
Considering the variations in potential preprocessing techniques, we conducted
an experimental study that involved applying five such techniques to measure
each technique's effect while using the BERT model, chosen for being one of the
most-used stock pretrained models. We used the Hugging face transformer library
to implement the code for each preprocessing case. In our five experiments, we
found that BERT achieves the best accuracy in predicting the gender of the
author when no preprocessing technique is applied. Our best case achieved
86.67% accuracy in predicting the gender of authors.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Temporal Information and Event Markup Language: TIE-ML Markup Process and Schema Version 1.0. (arXiv:2109.13892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13892">
<div class="article-summary-box-inner">
<span><p>Temporal Information and Event Markup Language (TIE-ML) is a markup strategy
and annotation schema to improve the productivity and accuracy of temporal and
event related annotation of corpora to facilitate machine learning based model
training. For the annotation of events, temporal sequencing, and durations, it
is significantly simpler by providing an extremely reduced tag set for just
temporal relations and event enumeration. In comparison to other standards, as
for example the Time Markup Language (TimeML), it is much easier to use by
dropping sophisticated formalisms, theoretical concepts, and annotation
approaches. Annotations of corpora using TimeML can be mapped to TIE-ML with a
loss, and TIE-ML annotations can be fully mapped to TimeML with certain
under-specification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsolved Problems in ML Safety. (arXiv:2109.13916v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13916">
<div class="article-summary-box-inner">
<span><p>Machine learning (ML) systems are rapidly increasing in size, are acquiring
new capabilities, and are increasingly deployed in high-stakes settings. As
with other powerful technologies, safety for ML should be a leading research
priority. In response to emerging safety challenges in ML, such as those
introduced by recent large-scale models, we provide a new roadmap for ML Safety
and refine the technical problems that the field needs to address. We present
four problems ready for research, namely withstanding hazards ("Robustness"),
identifying hazards ("Monitoring"), steering ML systems ("Alignment"), and
reducing risks to how ML systems are handled ("External Safety"). Throughout,
we clarify each problem's motivation and provide concrete research directions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.13847">
<div class="article-summary-box-inner">
<span><p>Word embeddings are a powerful natural language processing technique, but
they are extremely difficult to interpret. To enable interpretable NLP models,
we create vectors where each dimension is inherently interpretable. By
inherently interpretable, we mean a system where each dimension is associated
with some human understandable hint that can describe the meaning of that
dimension. In order to create more interpretable word embeddings, we transform
pretrained dense word embeddings into sparse embeddings. These new embeddings
are inherently interpretable: each of their dimensions is created from and
represents a natural language word or specific grammatical concept. We
construct these embeddings through sparse coding, where each vector in the
basis set is itself a word embedding. Therefore, each dimension of our sparse
vectors corresponds to a natural language word. We also show that models
trained using these sparse embeddings can achieve good performance and are more
interpretable in practice, including through human evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open Knowledge Graphs Canonicalization using Variational Autoencoders. (arXiv:2012.04780v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.04780">
<div class="article-summary-box-inner">
<span><p>Noun phrases and Relation phrases in open knowledge graphs are not
canonicalized, leading to an explosion of redundant and ambiguous
subject-relation-object triples. Existing approaches to solve this problem take
a two-step approach. First, they generate embedding representations for both
noun and relation phrases, then a clustering algorithm is used to group them
using the embeddings as features. In this work, we propose Canonicalizing Using
Variational Autoencoders (CUVA), a joint model to learn both embeddings and
cluster assignments in an end-to-end approach, which leads to a better vector
representation for the noun and relation phrases. Our evaluation over multiple
benchmarks shows that CUVA outperforms the existing state-of-the-art
approaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate
entity canonicalization systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models. (arXiv:2102.07988v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.07988">
<div class="article-summary-box-inner">
<span><p>Model parallelism has become a necessity for training modern large-scale deep
language models. In this work, we identify a new and orthogonal dimension from
existing model parallel approaches: it is possible to perform pipeline
parallelism within a single training sequence for Transformer-based language
models thanks to its autoregressive property. This enables a more fine-grained
pipeline compared with previous work. With this key idea, we design TeraPipe, a
high-performance token-level pipeline parallel algorithm for synchronous
model-parallel training of Transformer-based language models. We develop a
novel dynamic programming-based algorithm to calculate the optimal pipelining
execution scheme given a specific model and cluster configuration. We show that
TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175
billion parameters on an AWS cluster with 48 p3.16xlarge instances compared
with state-of-the-art model-parallel methods. The code for reproduction can be
found at https://github.com/zhuohan123/terapipe
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving and Simplifying Pattern Exploiting Training. (arXiv:2103.11955v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.11955">
<div class="article-summary-box-inner">
<span><p>Recently, pre-trained language models (LMs) have achieved strong performance
when fine-tuned on difficult benchmarks like SuperGLUE. However, performance
can suffer when there are very few labeled examples available for fine-tuning.
Pattern Exploiting Training (PET) is a recent approach that leverages patterns
for few-shot learning. However, PET uses task-specific unlabeled data. In this
paper, we focus on few-shot learning without any unlabeled data and introduce
ADAPET, which modifies PET's objective to provide denser supervision during
fine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any
task-specific unlabeled data. Our code can be found at
https://github.com/rrmenon10/ADAPET.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intent Recognition and Unsupervised Slot Identification for Low Resourced Spoken Dialog Systems. (arXiv:2104.01287v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.01287">
<div class="article-summary-box-inner">
<span><p>Intent Recognition and Slot Identification are crucial components in spoken
language understanding (SLU) systems. In this paper, we present a novel
approach towards both these tasks in the context of low resourced and unwritten
languages. We present an acoustic based SLU system that converts speech to its
phonetic transcription using a universal phone recognition system. We build a
word-free natural language understanding module that does intent recognition
and slot identification from these phonetic transcription. Our proposed SLU
system performs competitively for resource rich scenarios and significantly
outperforms existing approaches as the amount of available data reduces. We
observe more than 10% improvement for intent classification in Tamil and more
than 5% improvement for intent classification in Sinhala. We also present a
novel approach towards unsupervised slot identification using normalized
attention scores. This approach can be used for unsupervised slot labelling,
data augmentation and to generate data for a new slot in a one-shot way with
only one speech recording
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hidden Backdoors in Human-Centric Language Models. (arXiv:2105.00164v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00164">
<div class="article-summary-box-inner">
<span><p>Natural language processing (NLP) systems have been proven to be vulnerable
to backdoor attacks, whereby hidden features (backdoors) are trained into a
language model and may only be activated by specific inputs (called triggers),
to trick the model into producing unexpected behaviors. In this paper, we
create covert and natural triggers for textual backdoor attacks, \textit{hidden
backdoors}, where triggers can fool both modern language models and human
inspection. We deploy our hidden backdoors through two state-of-the-art trigger
embedding methods. The first approach via homograph replacement, embeds the
trigger into deep neural networks through the visual spoofing of lookalike
character replacement. The second approach uses subtle differences between text
generated by language models and real natural text to produce trigger sentences
with correct grammar and high fluency. We demonstrate that the proposed hidden
backdoors can be effective across three downstream security-critical NLP tasks,
representative of modern human-centric NLP systems, including toxic comment
detection, neural machine translation (NMT), and question answering (QA). Our
two hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at
least $97\%$ with an injection rate of only $3\%$ in toxic comment detection,
$95.1\%$ ASR in NMT with less than $0.5\%$ injected data, and finally $91.12\%$
ASR against QA updated with only 27 poisoning data samples on a model
previously trained with 92,024 samples (0.029\%). We are able to demonstrate
the adversary's high success rate of attacks, while maintaining functionality
for regular users, with triggers inconspicuous by the human administrators.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking. (arXiv:2107.05002v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.05002">
<div class="article-summary-box-inner">
<span><p>Extractive Reading Comprehension (ERC) has made tremendous advances enabled
by the availability of large-scale high-quality ERC training data. Despite of
such rapid progress and widespread application, the datasets in languages other
than high-resource languages such as English remain scarce. To address this
issue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by
modelling existing high-quality extractive reading comprehension datasets in a
multilingual environment. To be specific, we present multilingual adaptive
attention (MAA) to combine intra-attention and inter-attention to learn more
general generalizable semantic and lexical knowledge from each pair of language
families. Furthermore, to make full use of existing datasets, we adopt a new
training framework to train our model by calculating task-level similarities
between each existing dataset and target dataset. The experimental results show
that our XLTT model surpasses six baselines on two multilingual ERC benchmarks,
especially more effective for low-resource languages with 3.9 and 4.1 average
improvement in F1 and EM, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Arabic aspect based sentiment analysis using BERT. (arXiv:2107.13290v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13290">
<div class="article-summary-box-inner">
<span><p>Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that
defines the polarity of opinions on certain aspects related to specific
targets. The majority of research on ABSA is in English, with a small amount of
work available in Arabic. Most previous Arabic research has relied on deep
learning models that depend primarily on context-independent word embeddings
(e.g.word2vec), where each word has a fixed representation independent of its
context. This article explores the modeling capabilities of contextual
embeddings from pre-trained language models, such as BERT, and making use of
sentence pair input on Arabic ABSA tasks. In particular, we are building a
simple but effective BERT-based neural baseline to handle this task. Our BERT
architecture with a simple linear classification layer surpassed the
state-of-the-art works, according to the experimental results on the
benchmarked Arabic hotel reviews dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SeqScore: Addressing Barriers to Reproducible Named Entity Recognition Evaluation. (arXiv:2107.14154v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.14154">
<div class="article-summary-box-inner">
<span><p>To address a looming crisis of unreproducible evaluation for named entity
recognition, we propose guidelines and introduce SeqScore, a software package
to improve reproducibility. The guidelines we propose are extremely simple and
center around transparency regarding how chunks are encoded and scored. We
demonstrate that despite the apparent simplicity of NER evaluation, unreported
differences in the scoring procedure can result in changes to scores that are
both of noticeable magnitude and statistically significant. We describe
SeqScore, which addresses many of the issues that cause replication failures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HeadlineCause: A Dataset of News Headlines for Detecting Causalities. (arXiv:2108.12626v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12626">
<div class="article-summary-box-inner">
<span><p>Detecting implicit causal relations in texts is a task that requires both
common sense and world knowledge. Existing datasets are focused either on
commonsense causal reasoning or explicit causal relations. In this work, we
present HeadlineCause, a dataset for detecting implicit causal relations
between pairs of news headlines. The dataset includes over 5000 headline pairs
from English news and over 9000 headline pairs from Russian news labeled
through crowdsourcing. The pairs vary from totally unrelated or belonging to
the same general topic to the ones including causation and refutation
relations. We also present a set of models and experiments that demonstrates
the dataset validity, including a multilingual XLM-RoBERTa based model for
causality detection and a GPT-2 based model for possible effects prediction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.05473">
<div class="article-summary-box-inner">
<span><p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by
learning with merely a handful of annotated instances. Meta-learning has been
widely adopted for such a task, which trains on randomly generated few-shot
tasks to learn generic data representations. Despite impressive results
achieved, existing models still perform suboptimally when handling hard FSRE
tasks, where the relations are fine-grained and similar to each other. We argue
this is largely because existing models do not distinguish hard tasks from easy
ones in the learning process. In this paper, we introduce a novel approach
based on contrastive learning that learns better representations by exploiting
relation label information. We further design a method that allows the model to
adaptively learn how to focus on hard tasks. Experiments on two standard
datasets demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Something Old, Something New: Grammar-based CCG Parsing with Transformer Models. (arXiv:2109.10044v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10044">
<div class="article-summary-box-inner">
<span><p>This report describes the parsing problem for Combinatory Categorial Grammar
(CCG), showing how a combination of Transformer-based neural models and a
symbolic CCG grammar can lead to substantial gains over existing approaches.
The report also documents a 20-year research program, showing how NLP methods
have evolved over this time. The staggering accuracy improvements provided by
neural models for CCG parsing can be seen as a reflection of the improvements
seen in NLP more generally. The report provides a minimal introduction to CCG
and CCG parsing, with many pointers to the relevant literature. It then
describes the CCG supertagging problem, and some recent work from Tian et al.
(2020) which applies Transformer-based models to supertagging with great
effect. I use this existing model to develop a CCG multitagger, which can serve
as a front-end to an existing CCG parser. Simply using this new multitagger
provides substantial gains in parsing accuracy. I then show how a
Transformer-based model from the parsing literature can be combined with the
grammar-based CCG parser, setting a new state-of-the-art for the CCGbank
parsing task of almost 93% F-score for labelled dependencies, with complete
sentence accuracies of over 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Recursively Summarizing Books with Human Feedback. (arXiv:2109.10862v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10862">
<div class="article-summary-box-inner">
<span><p>A major challenge for scaling machine learning is training models to perform
tasks that are very difficult or time-consuming for humans to evaluate. We
present progress on this problem on the task of abstractive summarization of
entire fiction novels. Our method combines learning from human feedback with
recursive task decomposition: we use models trained on smaller parts of the
task to assist humans in giving feedback on the broader task. We collect a
large volume of demonstrations and comparisons from human labelers, and
fine-tune GPT-3 using behavioral cloning and reward modeling to do
summarization recursively. At inference time, the model first summarizes small
sections of the book and then recursively summarizes these summaries to produce
a summary of the entire book. Our human labelers are able to supervise and
evaluate the models quickly, despite not having read the entire books
themselves. Our resulting model generates sensible summaries of entire books,
even matching the quality of human-written summaries in a few cases ($\sim5\%$
of books). We achieve state-of-the-art results on the recent BookSum dataset
for book-length summarization. A zero-shot question-answering model using these
summaries achieves state-of-the-art results on the challenging NarrativeQA
benchmark for answering questions about books and movie scripts. We release
datasets of samples from our model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AES Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11728">
<div class="article-summary-box-inner">
<span><p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively
used by states and language testing agencies alike to evaluate millions of
candidates for life-changing decisions ranging from college applications to
visa approvals. However, little research has been put to understand and
interpret the black-box nature of deep-learning based scoring algorithms.
Previous studies indicate that scoring models can be easily fooled. In this
paper, we explore the reason behind their surprising adversarial brittleness.
We utilize recent advances in interpretability to find the extent to which
features such as coherence, content, vocabulary, and relevance are important
for automated scoring mechanisms. We use this to investigate the
oversensitivity i.e., large change in output score with a little change in
input essay content) and overstability i.e., little change in output scores
with large changes in input essay content) of AES. Our results indicate that
autoscoring models, despite getting trained as "end-to-end" models with rich
contextual embeddings such as BERT, behave like bag-of-words models. A few
words determine the essay score without the requirement of any context making
the model largely overstable. This is in stark contrast to recent probing
studies on pre-trained representation learning models, which show that rich
linguistic features such as parts-of-speech and morphology are encoded by them.
Further, we also find that the models have learnt dataset biases, making them
oversensitive. To deal with these issues, we propose detection-based protection
models that can detect oversensitivity and overstability causing samples with
high accuracies. We find that our proposed models are able to detect unusual
attribution patterns and flag adversarial samples successfully.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts. (arXiv:2109.12761v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.12761">
<div class="article-summary-box-inner">
<span><p>In order to better simulate the real human conversation process, models need
to generate dialogue utterances based on not only preceding textual contexts
but also visual contexts. However, with the development of multi-modal dialogue
learning, the dataset scale gradually becomes a bottleneck. In this report, we
release OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset
compared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a
total number of 5.6 million dialogue turns extracted from either movies or TV
series from different resources, and each dialogue turn is paired with its
corresponding visual context. We hope this large-scale dataset can help
facilitate future researches on open-domain multi-modal dialog generation,
e.g., multi-modal pretraining for dialogue generation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations. (arXiv:2109.13059v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13059">
<div class="article-summary-box-inner">
<span><p>In NLP, a large volume of tasks involve pairwise comparison between two
sequences (e.g. sentence similarity and paraphrase identification).
Predominantly, two formulations are used for sentence-pair tasks: bi-encoders
and cross-encoders. Bi-encoders produce fixed-dimensional sentence
representations and are computationally efficient, however, they usually
underperform cross-encoders. Cross-encoders can leverage their attention heads
to exploit inter-sentence interactions for better performance but they require
task fine-tuning and are computationally more expensive. In this paper, we
present a completely unsupervised sentence representation model termed as
Trans-Encoder that combines the two learning paradigms into an iterative joint
framework to simultaneously learn enhanced bi- and cross-encoders.
Specifically, on top of a pre-trained Language Model (PLM), we start with
converting it to an unsupervised bi-encoder, and then alternate between the bi-
and cross-encoder task formulations. In each alternation, one task formulation
will produce pseudo-labels which are used as learning signals for the other
task formulation. We then propose an extension to conduct such
self-distillation approach on multiple PLMs in parallel and use the average of
their pseudo-labels for mutual-distillation. Trans-Encoder creates, to the best
of our knowledge, the first completely unsupervised cross-encoder and also a
state-of-the-art unsupervised bi-encoder for sentence similarity. Both the
bi-encoder and cross-encoder formulations of Trans-Encoder outperform recently
proposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT
and SimCSE by up to 5% on the sentence similarity benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions. (arXiv:2109.13066v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13066">
<div class="article-summary-box-inner">
<span><p>Existing text-to-SQL research only considers complete questions as the input,
but lay-users might strive to formulate a complete question. To build a smarter
natural language interface to database systems (NLIDB) that also processes
incomplete questions, we propose a new task, prefix-to-SQL which takes question
prefix from users as the input and predicts the intended SQL. We construct a
new benchmark called PAGSAS that contains 124K user question prefixes and the
intended SQL for 5 sub-tasks Advising, GeoQuery, Scholar, ATIS, and Spider.
Additionally, we propose a new metric SAVE to measure how much effort can be
saved by users. Experimental results show that PAGSAS is challenging even for
strong baseline models such as T5. As we observe the difficulty of
prefix-to-SQL is related to the number of omitted tokens, we incorporate
curriculum learning of feeding examples with an increasing number of omitted
tokens. This improves scores on various sub-tasks by as much as 9% recall
scores on sub-task GeoQuery in PAGSAS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP). (arXiv:2109.13123v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.13123">
<div class="article-summary-box-inner">
<span><p>Digital learning platforms enable students to learn on a flexible and
individual schedule as well as providing instant feedback mechanisms. The field
of STEM education requires students to solve numerous training exercises to
grasp underlying concepts. It is apparent that there are restrictions in
current online education in terms of exercise diversity and individuality. Many
exercises show little variance in structure and content, hindering the adoption
of abstraction capabilities by students. This thesis proposes an approach to
generate diverse, context rich word problems. In addition to requiring the
generated language to be grammatically correct, the nature of word problems
implies additional constraints on the validity of contents. The proposed
approach is proven to be effective in generating valid word problems for
mathematical statistics. The experimental results present a tradeoff between
generation time and exercise validity. The system can easily be parametrized to
handle this tradeoff according to the requirements of specific use cases.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-29 04:27:39.212558660 UTC">2021-09-29 04:27:39 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>