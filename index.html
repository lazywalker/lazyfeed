<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-30T01:49:53.272297370Z">08-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">构建安全易用的链表</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=273831e7-932d-476f-9d31-323151afb123">
<div class="article-summary-box-inner">
<span><p>写了一个链表的Crate，愿景是构建安全且易用的链表。</p>
<p>欢迎大家来找茬（Bug）或提需求 :)</p>
<p>Crate IO链接：<a href="https://crates.io/crates/cyclic_list" rel="noopener noreferrer">https://crates.io/crates/cyclic_list</a>;</p>
<p>GitHub链接：<a href="https://github.com/whjpji/cyclic_list" rel="noopener noreferrer">https://github.com/whjpji/cyclic_list</a></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust 日报】2021-08-29 Tangram：训练、部署和监控机器学习模型</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=4a218f6c-3c77-4aa0-84d6-90ac2bf1fc7c">
<div class="article-summary-box-inner">
<span><h3>Embedded Rust 第一步：选择一块板子</h3>
<p>内容整理自 <a href="https://github.com/robyoung" rel="noopener noreferrer">robyoung (Rob Young)</a> 的文章：First steps with Embedded Rust: Selecting a board</p>
<p>有这么多令人眼花缭乱的微控制器和项目，对于嵌入式经验很少的人来说应该从哪里开始？</p>
<p><strong>我们在开发板中想要什么？</strong></p>
<ul>
<li>良好的架构支持</li>
<li>良好的芯片支持</li>
<li>活跃的社区</li>
<li>内置调试器</li>
</ul>
<p><strong>我们需要什么架构？</strong></p>
<p>拥有最完整库、最详尽指南和最大社区的架构是 ARM Cortex-M。 ARM Cortex-M 是面向微控制器应用的低功耗、低成本处理器。 查看 crates.io 上的下载量虽说不是一个完美的指标，但可以让我们了解规模上的差异。在过去的 90 天内，cortex-m 的下载量超过 250k。 RISC-V、AVR 或 Xtensa 最多有 3k 次下载，cortex-a 有大约 18k 次下载。ARM Cortex-M 独树一帜。</p>
<ul>
<li>AVR：AVR 是用于嵌入式系统的 8 位微控制器系列。在 Rust 生态系统中，它们并没有得到很好的支持。直到最近，还需要使用 rustc 的一个分支来构建 AVR。 现在有几个不同的选择，awesome-avr-rust 是一个很好的起点。</li>
<li>ARM Cortex-A：更强大的多核 ARM 处理器，专为运行更大的东西而设计。 通常会在它们上运行完整的操作系统。 例如这是大多数智能手机和掌上游戏机中使用的架构。查看 <a href="https://crates.io/crates/cortex-a" rel="noopener noreferrer">cortex-a - crates.io: Rust Package Registry</a> 了解更多。</li>
<li>RISC-V：似乎是机器架构的新热点，它是一种免费且开放的指令集架构 (ISA)。 它也从一开始就被设计成模块化的，这意味着芯片设计人员可以创建各种各样的专用芯片，虽然目前开发板的范围很小。有一个活跃的 Rust RISC-V 社区，SiFive 或 www.riscv.org 都是不错的起点，Rust 方面，可以查看 riscv crate。</li>
<li>Xtensa：最受欢迎的主板组是来自 Espressif 的 ESP32 系列芯片。它们是小型、廉价、支持 WiFi 的电路板。 需要注意的是，并非所有 ESP32 开发板都使用 Xtensa 芯片，新的 ESP32-C3 是基于 RISC-V 的。在 Xtensa 芯片上使用 Rust 的最大障碍可能是 llvm 不支持它，因此需要构建 Rust 的 fork：<a href="https://github.com/esp-rs/rust" rel="noopener noreferrer">esp-rs/rust</a>。</li>
</ul>
<p><strong>我们需要什么芯片？</strong></p>
<p>因此，我们将使用 ARM Cortex-M。 这缩小了搜索范围，但仍有很多选择。如果我们查看 cortex-m <a href="https://crates.io/crates/cortex-m/reverse_dependencies" rel="noopener noreferrer">crate</a> 的依赖项，我们会看到有两组芯片比其他任何一组都使用得更多； <a href="https://www.st.com/content/st_com/en/products/microcontrollers-microprocessors/stm32-32-bit-arm-cortex-mcus.html" rel="noopener noreferrer">STM32</a> 系列芯片和 <a href="https://www.nordicsemi.com/Products/Bluetooth-Low-Energy" rel="noopener noreferrer">nRF5</a> 系列，这是我们要重点搜索的地方。</p>
<ul>
<li>STM32：STM32 系列芯片可能是应用最广泛的嵌入式 Rust ARM Cortex-M 芯片。两种最受欢迎的 STM32 板是 Blue Pill 和 Black Pill。主要的缺点是没有板载调试器。如果想要带有调试器的基于 STM32 的电路板，那么获得 STMicroelectronics <a href="https://www.st.com/en/evaluation-tools/stm32-discovery-kits.html#overview" rel="noopener noreferrer">官方套件</a>是一个不错的选择（STM32F3 或 STM32F4 是不错的选择）。Rust Embedded Discovery 书的原始版本是针对 STM32F3 板编写的，因此有非常高质量的初学者文档，可以从那里开始。</li>
<li>nRF5：用于嵌入式 Rust 的第二个最广泛使用的 ARM Cortex-M 芯片系列是 Nordic Semiconductor 的 <a href="https://www.nordicsemi.com/Products/Bluetooth-Low-Energy" rel="noopener noreferrer">nRF5 系列</a>。官方开发<a href="https://www.nordicsemi.com/Products/Bluetooth-Low-Energy/Development-hardware" rel="noopener noreferrer">套件</a> (DK) 是很棒的入门板。 Ferrous Systems 的 Knurling-rs 会议使用 nRF52840 <a href="https://www.nordicsemi.com/Products/Development-hardware/nRF52840-DK" rel="noopener noreferrer">开发套件</a>。Knurling 课程质量非常高，手把手指导，通过有趣好玩的项目教授嵌入 Rust，是使用 Rust 进行嵌入式开发的最佳切入点。另一个很棒的基于 nRF 的开发板是 <a href="https://www.microbit.org/" rel="noopener noreferrer">BBC micro:bit</a>。它配备了板载调试器和一系列有趣的板载外围设备，如板上的 LED 显示屏、按钮和传感器。BBC micro:bit 被设计为一个教育平台，因此硬件在他们的<a href="https://tech.microbit.org/" rel="noopener noreferrer">开发者社区</a>中以非常适合初学者的方式进行记录，并且互联网上有大量项目创意。</li>
<li>RP2040：<a href="https://www.raspberrypi.org/documentation/rp2040/getting-started/" rel="noopener noreferrer">RP2040</a> 于 2020 年底发布，是 Raspberry Pi 基金会首次尝试设计自己的芯片。由于如此新，Rust 对它的支持仍在开发中。与 BBC micro:bit 一样，RP2040 旨在成为一个教育平台，因此硬件文档是一流的，并且有大量初学者友好的代码示例和其他编程语言的库（没有多少适合初学者的嵌入式 Rust 文档）。这是一个非常令人兴奋的平台，并且在 Embedded Rust 社区中围绕它进行了大量活动，所以一定要密切关注，但它可能不适合作为入门第一块板。</li>
</ul>
<p><strong>板载调试器？</strong></p>
<p>在主机上运行程序时，可以在 shell 中运行它并查看打印输出。这在嵌入式目标上更加困难，调试器填补了这一空白。除了允许单步调试、断点调试外，它还允许将程序加载到设备上并轻松查看输出。不过有一个问题，它通常是连接到主机然后连接到目标设备的单独设备。第一次开始时，这是一笔不可忽视的费用，也是必须正确设置的另一件事。幸运的是，有些设备带有内置调试器，将它们直接插入主机并在瞬间探测运行的代码（通常需要在主机上进行一些设置才能使调试器正常工作，ferrous 有一个很好的设置<a href="https://session20q4.ferrous-systems.com/sessions/installation.html" rel="noopener noreferrer">指南</a>）。</p>
<p><strong>结论</strong></p>
<p>以下这些板都有很棒的 HAL 和 BSP crate、活跃友好的社区和板载调试器。</p>
<ul>
<li><a href="https://www.microbit.org/" rel="noopener noreferrer">BBC micro:bit</a>（约 13 英镑）：它是新版 Rust Embedded Discovery 书中使用的板。</li>
<li><a href="https://www.nordicsemi.com/Products/Development-hardware/nRF52840-DK" rel="noopener noreferrer">nRF52840 开发套件</a>（约 35 英镑）； 它是 Ferrous Systems 在 Kunrling 会议和培训中使用的板。</li>
<li><a href="https://www.st.com/en/evaluation-tools/stm32f3discovery.html" rel="noopener noreferrer">STM32F3 探索套件</a>（约 14 英镑）； 它是 Rust Embedded Discovery 书的第一版中使用的板。</li>
</ul>
<p>密切关注：</p>
<ul>
<li><a href="https://www.raspberrypi.org/products/raspberry-pi-pico/" rel="noopener noreferrer">Raspberry Pi Pico</a>（约 6 英镑，带预焊引脚）； ARM Cortex-M 但没有内置调试器，HAL 仍在开发中。不过目前有很多活动，进展很快。</li>
<li><a href="https://www.sifive.com/boards/hifive1-rev-b" rel="noopener noreferrer">HiFive1 Rev B</a>（约 50 英镑）； RISC-V 是新的热点。 Rust 中似乎有很多围绕它的活动，但它目前还没有 ARM Cortex-M 的支持。 其他需要关注的开发板是 <a href="https://longan.sipeed.com/en/" rel="noopener noreferrer">Logan Nano</a> 和 <a href="https://hackaday.com/2021/02/08/hands-on-the-risc-v-esp32-c3-will-be-your-new-esp8266/" rel="noopener noreferrer">ESP32-C3</a>。</li>
</ul>
<p>部分内容略有轻微调整，更多可阅读原文：<a href="https://robyoung.digital/blog/embedded-rust-selecting-a-board/" rel="noopener noreferrer">Rob Young | digital</a></p>
<h3>Tangram：训练、部署和监控机器学习模型</h3>
<p>一个机器学习套件，使用方法如下：</p>
<pre><code># 训练
$ tangram train --file heart_disease.csv --target diagnosis --output heart_disease.tangram
</code></pre>
<p>推理支持多种语言：<a href="https://hex.pm/packages/tangram" rel="noopener noreferrer">Elixir</a>, <a href="https://pkg.go.dev/github.com/tangramdotdev/tangram-go" rel="noopener noreferrer">Go</a>, <a href="https://www.npmjs.com/package/@tangramdotdev/tangram" rel="noopener noreferrer">JavaScript</a>, <a href="https://pypi.org/project/tangram" rel="noopener noreferrer">Python</a>, <a href="https://rubygems.org/gems/tangram" rel="noopener noreferrer">Ruby</a> 和 <a href="https://lib.rs/tangram" rel="noopener noreferrer">Rust</a>，以 Rust 为例：</p>
<pre><code>let model: tangram::Model = tangram::Model::from_path("heart_disease.tangram", None).unwrap();

let input = tangram::predict_input! {
  "age": 63.0,
  "gender": "male",
  // ...
};

let output = model.predict_one(input, None);
# { className: 'Negative', probability: 0.9381780624389648 }
</code></pre>
<p>很好奇训练的时候居然没有要指定模型，发现其将模型共分为三类：回归、二分类和多分类，训练时会根据数据自动选择合适（使用评估方法）的模型，每种模型又有两种不同的训练方法：线性方法和树方法。</p>
<p>自带的监控功能看起来还不错，比如下面这张可以展示特征对输出的贡献：</p>
<p><img src="https://github.com/tangramdotdev/tangram/raw/main/readme/predictions.png" alt></p>
<p>项目理论上可以用在简单机器学习场景下，尤其是那些还没有支持机器学习的语言，不过推理并没有 Benchmark，生产中使用需要做好性能测试。</p>
<p>GitHub：<a href="https://github.com/tangramdotdev/tangram" rel="noopener noreferrer">tangramdotdev/tangram: Tangram makes it easy for programmers to train, deploy, and monitor machine learning models.</a></p>
<p>文档：<a href="https://www.tangram.dev/docs/" rel="noopener noreferrer">Tangram</a></p>
<h3>lateral：一个在 x86_64 上启动的模块化内核</h3>
<p>在本地执行：</p>
<pre><code>$ make run-release ARCH=x86_64
</code></pre>
<p>可以根据自己的情况调整 Makefile 第一行 Bash 的配置。执行后如果有安装 QEMU 的话会自动加载：</p>
<p><img src="http://qnimg.lovevivian.cn/tmp-os-1.jpg" alt></p>
<p>每个组件都建立在窗口管理器之上，而不是像大多数操作系统那样建立在终端之上。</p>
<p>GitHub：<a href="https://github.com/carterisonline/lateral" rel="noopener noreferrer">carterisonline/lateral: A clean, custom-built modular kernel ready to boot on x86_64.</a></p>
<h3>tv：显示表格的 cli 工具</h3>
<p>就是把 json 或 csv 显示成表格，看起来很不错：</p>
<pre><code>$ cat test.json
[
  {
    "name": "test",
    "age": 10,
    "lang": "ja"
  },
  {
    "name": "uzimaru",
    "age": 23,
    "lang": "ja"
  },
  {
    "name": "hogehoge",
    "age": 21,
    "lang": "en"
  },
  {
    "name": "hugehuge",
    "age": 32,
    "lang": "en"
  }
]

$ tv test.json
|age|lang|    name|
|---|----|--------|
| 10|  ja|    test|
| 23|  ja| uzimaru|
| 21|  en|hogehoge|
| 32|  en|hugehuge|

$ cat test.csv
name,age,lang
test,10,ja
uzimaru,23,ja
hogehoge,21,en
hugehuge,32,en

$ tv test.csv
|age|lang|    name|
|---|----|--------|
| 10|  ja|    test|
| 23|  ja| uzimaru|
| 21|  en|hogehoge|
| 32|  en|hugehuge|
</code></pre>
<p>Mac 用户 brew 安装：</p>
<pre><code>$ brew install uzimaru0000/tap/tv
</code></pre>
<p>GitHub：<a href="https://github.com/uzimaru0000/tv" rel="noopener noreferrer">uzimaru0000/tv: CLI tool for displaying table</a></p>
<h3>minesweeper：使用 Rust，WebAssembly 和 Canvas 的扫雷游戏</h3>
<p>界面长这样：</p>
<p><img src="https://github.com/KarthikNedunchezhiyan/minesweeper/raw/main/www/assets/stage_bomb_triggered.png" alt></p>
<p>是很好的学习资料。在这里玩儿：<a href="https://karthiknedunchezhiyan.me/minesweeper/" rel="noopener noreferrer">Minesweeper</a></p>
<p>GitHub：<a href="https://github.com/karthikNedunchezhiyan/minesweeper" rel="noopener noreferrer">KarthikNedunchezhiyan/minesweeper: Minesweeper game developed with Rust, WebAssembly (Wasm), and Canvas</a></p>
<h3>copy-translator：划词翻译</h3>
<p>复制后翻译，使用 DeepL 的 API，不过目前只有 Local 版本好用：</p>
<p><img src="http://qnimg.lovevivian.cn/tmp-rust-1.jpg" alt></p>
<p>当然，也可以使用 Eudic（欧路词典）。</p>
<p>GitHub：<a href="https://github.com/zu1k/copy-translator" rel="noopener noreferrer">zu1k/copy-translator: Copy Translator, using DeepL api</a></p>
<h3>veccentric：小巧的 2-D 向量 Library</h3>
<p>项目受 <a href="https://p5js.org/reference/#/p5.Vector" rel="noopener noreferrer">p5.Vector</a> 启发，使用方法如下：</p>
<pre><code>use veccentric::Vecc;

let a = Vecc::new(3_i32, 4);
let b = a * 5;
let c = Vecc::new(-10, -8);
let d = b - c;
let e = -d;
</code></pre>
<p>GitHub：<a href="https://github.com/micouy/veccentric" rel="noopener noreferrer">micouy/veccentric: Tiny 2D vector library. Inspired by p5.js's p5.Vector.</a></p>
<hr>
<p>From 日报小组 长琴</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc 论坛：支持 rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust 语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">axum如何使用静态文件目录</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=f3fa9c8e-004b-4d95-8d5f-bdf6609c2e8e">
<div class="article-summary-box-inner">
<span><p>再重构一个简单的单页面小程序的时候打算用<code>axum</code>代替<code>warp</code>的时候遇到了个问题。</p>
<pre><code>    let post = warp::post()
        .and(warp::body::bytes())
        .map(move |content: Bytes| {
            Response::builder().body(server::handle_post_request(content))
        });

    let routers = warp::get().and(warp::fs::dir("./wwwroot")).or(post);

    warp::serve(routers).run(([127, 0, 0, 1], 3030)).await;
</code></pre>
<p>有如上的简单代码，使用<code>wwwroot</code>文件夹目录来生成页面，文件夹里包含有<code>index.html</code>,JS和CSS文件，怎么使用<code>axum</code>改写呢？看了下doc，只看到</p>
<pre><code>let app = Router::new()
    // this route cannot fail
    .route("/foo", get(|| async {}))
    // this route can fail with io::Error
    .route(
        "/",
        service::get(service_fn(|_req: Request&lt;Body&gt;| async {
            let contents = tokio::fs::read_to_string("some_file").await?;
            Ok::&lt;_, io::Error&gt;(Response::new(Body::from(contents)))
        }))
        .handle_error(handle_io_error),
    );

fn handle_io_error(error: io::Error) -&gt; Result&lt;impl IntoResponse, Infallible&gt; {
    // ...
}
</code></pre>
<p>这种写法。看着头大不说，那个<code>some_file</code>只是简单读取文件，完成不了我的要求。</p>
<p>有大神说说axum完成了这个部分了吗？这框架的代码看着感觉有点过于复杂了。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">自己管理内存的测试方法</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d4e8f317-f43f-44ea-a041-f39dd3ce1578">
<div class="article-summary-box-inner">
<span><p>很漂亮的一段case，来自std</p>
<p>library/alloc/tests/linked_list.rs</p>
<pre><code>#[test]
fn test_drop() {
    static mut DROPS: i32 = 0;
    struct Elem;
    impl Drop for Elem {
        fn drop(&amp;mut self) {
            unsafe {
                DROPS += 1;
            }
        }
    }

    let mut ring = LinkedList::new();
    ring.push_back(Elem);
    ring.push_front(Elem);
    ring.push_back(Elem);
    ring.push_front(Elem);
    drop(ring);

    assert_eq!(unsafe { DROPS }, 4);
}

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">问一个Display trait的问题</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=6b59dfb1-87d8-4b79-8820-e9d5397f178a">
<div class="article-summary-box-inner">
<span><p>请问&amp;str, &amp;&amp;str, &amp;&amp;&amp;str 并没有实现Display 的trait, 为什么这个函数调用没问题?</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-28 开源操作系统夏令营最终报告会安排</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ef3dd4e8-a8e8-4fec-bc7e-75703e1117ff">
<div class="article-summary-box-inner">
<span><h3>开源操作系统夏令营最终报告会安排</h3>
<p>会议主题：开源操作系统夏令营最终报告会
会议时间：2021/08/29 09:00-11:30 (GMT+08:00) 中国标准时间 - 北京
点击链接入会，或添加至会议列表： https://meeting.tencent.com/dm/Mp7T1h5zeQOk?rs=25
会议 ID：635 194 989</p>
<p>下面是9位全程参与夏令营活动同学的报告顺序。每人报告时间最长15分钟。</p>
<ol>
<li>杨云枫 王涛 Rustsbi的哪吒开发版移植</li>
<li>兰陈昕 zCore图形支持</li>
<li>都秉甲 容器技术学习</li>
<li>薛潇巍 RVM 的 RISC-V 支持</li>
<li>陈乐 共享调度器</li>
<li>吴非凡 基于用户态中断的异步系统调用设计与实现</li>
<li>彭淳毅 陈志扬 基于rCore-Tutorial的性能分析软件实现</li>
</ol>
<h3>crates.live：可视化 Rust crates 依赖项</h3>
<p>crates.live 是来自 crates.io 的 Rust crates 的依赖可视化工具。 它显示了 Rust crates（包）的依赖树。功能包括：</p>
<ul>
<li>依赖解析， crates.live 引擎通过匹配依赖版本来完成完整的依赖解析。</li>
<li>交互式图表，带有标记的板条箱的可缩放交互式图表。</li>
<li>图像导出， 将图形导出为 PNG。</li>
<li>开放 API：（即将推出）GraphQL API。</li>
</ul>
<p>crates.live 使用了一堆技术框架，技术栈包括：</p>
<ul>
<li>Rust， crates.live 后端和爬虫是用 Rust 和开源 Rust 库开发的。</li>
<li>GraphQl， WASM 驱动的 GraphQL 服务器。</li>
<li>React/Bulma， 前端库。</li>
<li>Terraform， 帮助启动和维护我们的基础设施。</li>
<li>Cloudflare， Cloudflare 工作人员运行 WASM 后端。</li>
</ul>
<p>如果在使用此应用程序时有任何疑问、建议或问题； 可以通过 contact@crates.live 联系。 crates.live 由 Abid Omar 开发，可通过 contact@omarabid.com 联系。</p>
<p><a href="https://crates.live/" rel="noopener noreferrer">链接</a>：https://crates.live/</p>
<h3>Obake，版本化数据结构</h3>
<p>Obake 是一个用于声明和维护版本化数据结构的过程宏。 “obake”这个名字取自日语“お化け（おばけ）”，这是日本民间传说中一类会变形的超自然生物。</p>
<p>在开发应用程序时，配置格式和内部数据结构通常会在版本之间演变。 然而，保持这些版本之间的向后兼容性需要声明和维护遗留格式的数据结构和用于在它们之间迁移的代码。 Obake 的目标是让这个过程变得轻松。</p>
<pre><code>#[obake::versioned]                 // create a versioned data-structure
#[obake(version("0.1.0"))]          // declare some versions
#[obake(version("0.2.0"))]
#[derive(PartialEq, Eq, Hash)]      // additional attributes are applied to all versions
struct Foo {
    #[obake(cfg("0.1.0"))]          // enable fields for specific versions with
    foo: String,                    // semantic version constraints
   
    #[obake(cfg("&gt;=0.2, &lt;=0.3.0"))] // any semantic version constraint can appear in
    bar: u32,                       // a `cfg` attribute 
   
    #[obake(cfg("0.1.0"))]          // multiple `cfg` attributes are treated as a
    #[obake(cfg("&gt;=0.3"))]          // disjunction over version constraints
    baz: char,
}

// describe migrations between versions using the `From` trait
// and an automatically generated type-level macro for referring to
// specific versions of `Foo`
impl From&lt;Foo!["0.1.0"]&gt; for Foo!["0.2.0"] {
    fn from(foo: Foo!["0.1.0"]) -&gt; Self {
        Self { bar: 0 }
    }
}

// an enumeration of all versions of `Foo` is accessed using the
// `obake::Versioned` trait:
let versioned_example: &lt;Foo as obake::Versioned&gt;::Versioned = unimplemented!();

// this enumeration implements `Into&lt;Foo&gt;`, where `Foo` is the latest declared
// version of `Foo` (in this case, `Foo!["0.2.0"]`)
let example: Foo = versioned_example.into();
</code></pre>
<p>Github<a href="https://github.com/doctorn/obake" rel="noopener noreferrer">链接</a>：https://github.com/doctorn/obake</p>
<h3>iced，跨平台 GUI 库</h3>
<p>iced，Rust 的跨平台 GUI 库，专注于简单性和类型安全。 灵感来自<a href="https://elm-lang.org/" rel="noopener noreferrer">Elm</a>。</p>
<p><img src="https://raw.githubusercontent.com/hecrj/iced/master/docs/graphs/ecosystem.png" alt="eco"></p>
<p>Github<a href="https://github.com/hecrj/iced/" rel="noopener noreferrer">链接</a>：https://github.com/hecrj/iced/</p>
<p>示例：https://github.com/hecrj/iced/tree/master/examples</p>
<hr>
<p>From 日报小组 <a href="https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929" rel="noopener noreferrer">洋芋</a></p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust 日报】2021-8-27 Rudra Rust 的内存安全和未定义行为检测工具</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ce7eb559-fdda-45d7-a53e-293af787a813">
<div class="article-summary-box-inner">
<span><h4>Rudra Rust 的内存安全和未定义行为检测工具</h4>
<p>Rudra 是一个静态分析器，用于检测 Rust 程序中常见的未定义行为。它能够分析单个 Rust 包以及 crates.io 上的所有包。Rudra 及其相关论文将在 Proceedings of the 28th ACM Symposium on Operating Systems Principles 2021 (SOSP '21) 上发表。</p>
<ul>
<li>https://github.com/sslab-gatech/Rudra#readme</li>
</ul>
<h4>nom 7.0 版本发布</h4>
<p>nom 是一个用 Rust 编写的解析器组合库。它的目标是提供工具来构建安全的解析器，而不会影响速度或内存消耗。为此，它广泛使用 Rust 的强类型和内存安全来生成快速且正确的解析器，并提供函数、宏和特征来抽象大部分容易出错的管道。目前7.0已经发布</p>
<ul>
<li>https://crates.io/crates/nom</li>
</ul>
<h4>egui 0.14 版本发布</h4>
<p>egui 是一个易于使用的纯 Rust 图形用户界面。egui 可以在 Web 上、本机上以及您最喜欢的游戏引擎中运行。egui 旨在成为最容易使用的 Rust GUI 库，以及在 Rust 中制作 Web 应用程序的最简单方法，它可以在任何可以绘制纹理三角形的地方使用，这意味着您可以轻松地将其集成到您选择的游戏引擎中。</p>
<ul>
<li>演示文档：https://emilk.github.io/egui/</li>
<li>https://github.com/emilk/egui</li>
</ul>
<hr>
<p>From 日报小组 北纬27度，侯盛鑫</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">开源项目xiu登上了GitHub rust trending榜</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=86c83d9a-8370-42cf-8993-ef15af6932c4">
<div class="article-summary-box-inner">
<span><p><a href="https://github.com/harlanc/xiu" rel="noopener noreferrer">https://github.com/harlanc/xiu</a></p>
<p><a href="https://github.com/trending/rust?since=daily" rel="noopener noreferrer">https://github.com/trending/rust?since=daily</a></p>
<p>感谢大家的支持！！</p>
<p>PS：</p>
<p>前三名有两个都在论坛里发过，这个论坛有点狠，哈哈</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salvo - 一个简单的 Web 后端框架</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=e5dc5be9-b1ab-488f-8944-dd7cd97b0128">
<div class="article-summary-box-inner">
<span><h2>为什么要写这个框架</h2>
<p>因为我笨，无法学会使用 actix-web 等现存的框架。当我想把以前的 go 的 web 服务使用 rust 实现时，一眼看去，似乎每个框架都比 go 里存在框架复杂, 本来 Rust 的学习曲线就够陡峭的了, 又何苦把 Web 框架整得那么复杂?</p>
<h2>如何做到足够简单</h2>
<p>很多底层的实现 Hyper 都已经实现，所以，一般需求，基于 Hyper 实现应该没有错。Salvo 也是一样。 核心功能是提供还用简单的API，以及一个功能强大并且灵活的路由系统。</p>
<p>Salvo 里统一了 Handler 和 Middleware. Middleware 就是 Handler. 通过路由的 before 或者 after 添加到 Router 上。本质上, Middleware 和 Handler 都是处理 Request 请求，并且可能向 Response 写入数据。而 Handler 接收的参数是 Request, Depot, Response 三个, 其中 Depot 用于存储请求处理过程中的临时数据. 为方便书写, 在用不着的情况下可以省略掉某些参数.</p>
<pre><code>use Salvo::prelude::*;

#[fn_handler]
async fn hello_world(_req: &amp;mut Request, _depot: &amp;mut Depot, res: &amp;mut Response) {
    res.render_plain_text("Hello World");
}
#[fn_handler]
async fn hello_world2(res: &amp;mut Response) {
    res.render_plain_text("Hello World");
}
</code></pre>
<p>另外路由系统提供的 API 也是极其简单的, 但是, 功能却是强大的. 正常使用需求下, 基本上就是只关注 Router 一个类型即可.</p>
<h3>路由系统</h3>
<p>我自己感觉路由系统是跟其他的框架不太一样的. Router 可以写平，也可以写成树状。这里区业务逻辑树与访问目录树。业务逻辑树是根据业务逻辑需求，划分 router 结构，形成 router 树，它不一定与访问目录树一致。</p>
<p>正常情况下我们是这样写路由的：</p>
<pre><code>Router::new().path("articles").get(list_articles).post(create_article);
Router::new()
    .path("articles/&lt;id&gt;")
    .get(show_article)
    .patch(edit_article)
    .delete(delete_article);
</code></pre>
<p>往往查看文章和文章列表是不需要用户登录的, 但是创建, 编辑, 删除文章等需要用户登录认证权限才可以. Salvo 中支持嵌套的路由系统可以很好地满足这种需求. 我们可以把不需要用户登录的路由写到一起：</p>
<pre><code>Router::new()
    .path("articles")
    .get(list_articles)
    .push(Router::new().path("&lt;id&gt;").get(show_article));
</code></pre>
<p>然后把需要用户登录的路由写到一起， 并且使用相应的中间件验证用户是否登录：</p>
<pre><code>Router::new()
    .path("articles")
    .before(auth_check)
    .post(list_articles)
    .push(Router::new().path("&lt;id&gt;").patch(edit_article).delete(delete_article));
</code></pre>
<p>虽然这两个路由都有这同样的 <code>path("articles")</code>, 然而它们依然可以被同时添加到同一个父路由, 所以最后的路由长成了这个样子:</p>
<pre><code>Router::new()
    .push(
        Router::new()
            .path("articles")
            .get(list_articles)
            .push(Router::new().path("&lt;id&gt;").get(show_article)),
    )
    .push(
        Router::new()
            .path("articles")
            .before(auth_check)
            .post(list_articles)
            .push(Router::new().path("&lt;id&gt;").patch(edit_article).delete(delete_article)),
    );
</code></pre>
<p><code>&lt;id&gt;</code>匹配了路径中的一个片段, 正常情况下文章的 <code>id</code> 只是一个数字, 这是我们可以使用正则表达式限制 <code>id</code> 的匹配规则, <code>r"&lt;id:/\d+/&gt;"</code>.</p>
<p>更多信息可以查看网站 https://salvo.rs</p>
<p>源码地址: https://github.com/salvo-rs/salvo</p>
<p>非常欢迎大家为项目贡献力量，可以通过以下方法为项目作出贡献:</p>
<ul>
<li>在 issue 中提交功能需求和 bug report;</li>
<li>在 issues 或者 require feedback 下留下自己的意见;</li>
<li>通过 pull requests 提交代码;</li>
<li>在博客或者技术平台发表 Salvo 相关的技术文章。</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">[已解决]println! 严重拖延效能，仅列印一行</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ab0d06cb-d33d-4e18-b7b3-0b3e889f7b11">
<div class="article-summary-box-inner">
<span><p>当把call函数注解后，或是注解println! 都可以快速运行。</p>
<p>在 https://play.rust-lang.org/ 上有时候可以 ""使用println! "" 而且依然编译的很快，有时候则不行，我自己本地电脑都不行。</p>
<p>这效能差了十万八千里，请大家帮忙，新手总是在 println! 跌坑。</p>
<p>这边使用 <code>cargo run --release</code> 编译</p>
<pre><code>use std::time::{Duration, Instant};

struct Struct {
    a: String,
    b: bool,
}
trait Dyn {}
impl Dyn for Struct {}

fn main() {
    let start = Instant::now();
    let mut count = 0;
    let count_end = 100_000_000i64;

    while count &lt;= count_end {
        let m: Box&lt;Struct&gt; = Box::new(Struct {
            b: false,
            a: "str".to_string(),
        });
        if count == count_end {
            call();               // ---- 这儿
            m.b;
            m.a;
        }
        count += 1;
    }

    let duration = start.elapsed();
    println!("Time: {:?}", duration);
}

fn call(){
    println!("run call()\n");     // ---- 重点在这儿，注解后变超快
}
</code></pre>
<p>Time:</p>
<table>
<thead>
<tr>
<th align="right">😫使用println!</th>
<th align="right">😄注解//println!</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">12.863911s</td>
<td align="right">2.8486ms</td>
</tr>
<tr>
<td align="right">13.2101748s</td>
<td align="right">2.4661ms</td>
</tr>
<tr>
<td align="right">13.5353751s</td>
<td align="right">2.0433ms</td>
</tr>
<tr>
<td align="right">13.4852107s</td>
<td align="right">1.7869ms</td>
</tr>
<tr>
<td align="right">————————</td>
<td align="right">————————</td>
</tr>
</tbody>
</table>
<hr>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课：《 Rust 异步编程入门 Future 》|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>
<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是 Rust 异步编程的核心基础。</p>
<h3>课程大纲</h3>
<p>1、为什么需要异步.</p>
<p>2、理解异步编程模型.</p>
<p>3、Future 编程模型讲解.</p>
<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>
<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>异步引擎 C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D 游戏引擎</h3>
<ul>
<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>
<li><strong>延迟着色</strong></li>
<li><strong>内置保存/加载</strong></li>
<li><strong>独立场景编辑器</strong></li>
<li><strong>高级物理模型</strong></li>
<li><strong>分层模型资源</strong></li>
<li><strong>几何实例化</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>
<p><strong>课程时间：</strong> 2021年8月22日 20:30-21:30</p>
<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>
<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>
<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>
<h3>课程大纲</h3>
<ol>
<li>
<p>什么是分布式追踪系统OpenTracing及应用场景</p>
</li>
<li>
<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>
</li>
<li>
<p>为什么需要tokio-rs/tracing库</p>
</li>
<li>
<p>演示Datafuse项目中tokio-rs/tracing的使用</p>
</li>
</ol>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">论坛github账户无法登录解决笔记</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>有反映这两天github账户无法登录了。</p>
<p>报这个错：</p>
<pre><code>get github user info err
</code></pre>
<p>查了几个地方：</p>
<ol>
<li>代码是否运行正常：Ok</li>
<li>https代理是否正常：Ok</li>
<li>检查了github返回日志，发现是：</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>进入这个地址一看：<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>
<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>
<p>特此记录。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 的 Future 与 Javascript 的 Promise 功能对照参考</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>
<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>
<blockquote>
<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* 什么都不做 */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被 （1）跨线程传递 （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：《通过实战理解 Rust 宏》| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>
<p><strong>课程时间：</strong> 2021年8月15日 20:30-21:30</p>
<p><strong>课程介绍：</strong></p>
<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>这就是通过宏实现配置的统一行为，代码参考：
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>
<h3>课程大纲</h3>
<ul>
<li>什么是 Rust 宏</li>
<li>什么是宏运行原理</li>
<li>如何创建 Rust 宏过程</li>
<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>
</ul>
<p><strong>讲师介绍</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：理解Rust的所有权| Vol 2</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c107b830-9fe1-43dd-94a3-9efcd5544205">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：《理解Rust所有权》</strong></p>
<p><strong>课程时间：2021年8月8日 20:30-21:30</strong></p>
<p><strong>嘉宾讲师： 苏林</strong></p>
<p><strong>嘉宾介绍：</strong></p>
<p>Rust中文社区成员，多点Dmall技术Leader，前折800互联网研发团队负责人、10余年一线研发经验。具有多年的软件开发经验, 熟练Ruby、Java、Rust等开发语言, 同时也参与过Rust中文社区日报维护工作。</p>
<p><strong>课程介绍</strong></p>
<p>本次课程通过10个左右的小例子，带大家理解一下Rust的所有权，Rust引用和借用，Rust变量克隆和复制的理念。</p>
<p><strong>参加课程</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/Rust-pbc-1.jpg" alt></p>
<p><strong>课程规划</strong></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">数据表 Timestamp 日期 Serialize</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2ff8a69e-59bb-4502-87c0-c3416ffae8a0">
<div class="article-summary-box-inner">
<span><p>主要参考：<a href="https://github.com/rustcc/forustm" rel="noopener noreferrer">Rustcc网站源码库</a></p>
<p>在处理数据表中日期相关数据时，Seralize序列化相关操作会报错，提示 DateTime 字段不识别，
查了 rustcc 源码才发现依赖中需要开启相应的feature。特此记录。</p>
<h2>1.依赖的库：</h2>
<pre><code>[dependencies]
# 日期时间处理 需要开启 serde 特征 支持序列化
chrono = { version = "0.4.19", features = ["serde"] }

# 数据库ORM
diesel = { version = "1.4.4", features = ["postgres", "chrono", "uuid", "r2d2"] }
dotenv = "0.15.0"
serde = { version = "1.0.127", features = ["derive"] }
serde_json = "1.0.66"
uuid = { version = "0.8.2", features = ["serde", "v4"] }
</code></pre>
<h2>2.创建数据表</h2>
<pre><code>CREATE TABLE characters (
    id SERIAL PRIMARY KEY,
    name VARCHAR(128) UNIQUE NOT NULL,
    age INTEGER NOT NULL DEFAULT 0,
    friends VARCHAR NOT NULL DEFAULT '',
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
)
</code></pre>
<h2>3.数据表对应的 model</h2>
<pre><code>use chrono::{NaiveDateTime};
use serde::{Deserialize, Serialize};

#[derive(Queryable, Serialize, Deserialize, Debug)]
pub struct Characters {
    pub id: i32,
    pub name: String,
    pub age: i32,
    pub friends: String,
    // 这里的 NaiveDateTime 日期格式序列化需要开启相关 features
    pub created_at: NaiveDateTime,
}
</code></pre>
<h2>4.获取数据</h2>
<pre><code>use db::schema::characters;
use db::{get_connection};
use db::models::{Characters, NewCharacter};
use db::schema::characters::dsl::*;
use diesel::QueryDsl;
use diesel::prelude::*;

fn main() {
    let conn = get_connection();

    // 查询年龄大于30的10条数据
    let arr: Vec&lt;Characters&gt; = characters.filter(characters::age.gt(30))
        .limit(10)
        .load::&lt;Characters&gt;(&amp;conn)
        .expect("Loading Error");

    let date_arr = arr.iter()
        .map(|item| {
	    // 数据格式化
            let t = item.created_at.format("%Y-%m-%d %H:%M:%S").to_string();
            println!("{} {}", item.name, t);
            t
        })
        .collect::&lt;Vec&lt;String&gt;&gt;();
}
</code></pre>
<p>输出结果类似：</p>
<pre><code>Box 2021-08-05 09:39:34
Bobe 2021-08-05 09:39:34
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cargo workspace config</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c3dcce30-1fc0-4819-8992-142365c7e21c">
<div class="article-summary-box-inner">
<span><p><a href="https://kaisery.github.io/trpl-zh-cn/ch14-03-cargo-workspaces.html" rel="noopener noreferrer">Workspace 文档链接</a></p>
<h2>目录结构</h2>
<pre><code>workspace-test/
    Cargo.toml
    db/
        src/
            bin/
                init.rs
        Cargo.tml
</code></pre>
<h2>workspace</h2>
<p>workspace-test/Cargo.toml</p>
<pre><code>[workspace]
members = ["db"]
default-member = "db"
</code></pre>
<h2>子项目</h2>
<p>workspace-test/db/Cargo.toml</p>
<pre><code>[package]
name = "db"
version = "0.1.0"
edition = "2018"

[dependencies]

# 可选的可执行文件配置
# [[bin]]
# name = "init"
# path = "src/bin/init.rs"
</code></pre>
<h2>操作</h2>
<pre><code># 运行 init
cargo run --bin init
# -p 指定项目
cargo run -p db --bin init
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 异步编程浅悟（一）</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=120035c3-944d-4a79-9b3a-8390697a6e13">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>异步编程浅悟（一）</h1>
<p>不同于<code>javascript</code>的<code>new Promise((resolve, reject) =&gt; {...})</code>构造即运行，<code>Rust</code>中的<code>Future</code>是·惰性·状态机。这体现为：</p>
<ol>
<li>【调用异步函数】或【执行异步块】仅只构造一个<code>Future trait object</code>。</li>
<li>因为<code>Future</code>是惰性状态机，所以它不会自动执行【异步函数】或【异步块】内的任何一行代码 --- 此点与<code>javascript</code>的·活性·状态机完全不同。相反，需要人工激活触发。</li>
<li>人工启动<code>Future</code>运行，又分为两个场景的两种情况：
<ol>
<li>
<p>已经在<code>async fn</code>内，<code>Future.await</code>激活。但，同时<strong>阻塞</strong>当前异步程序执行流。</p>
</li>
<li>
<p>在<code>async fn</code>外，需要借助由【运行时】提供的【执行器】。就<code>async-std</code>库而言，有两个选择：</p>
<ol>
<li><code>task::block_on(Future)</code> 执行<code>Future</code>且阻塞当前线程直到<code>Future</code>被完成。</li>
<li><code>task::spawn(Future)</code>仅执行<code>Future</code>和不阻塞当前线程。</li>
</ol>
<p>无论选择上面哪种方式，若在<code>Future</code>执行期间出现了<code>panic</code>，其都会终止（<code>abort</code>）正在共享同一个执行线程（<code>thread</code>）的所有<code>task</code>（·无栈·协程）的运行。</p>
</li>
</ol>
</li>
</ol>
<p>题外话，</p>
<ol>
<li>绿色线程是·有栈·协程；异步函数与异步块是·无栈·协程。</li>
<li>在<code>async-std</code>库的词汇表内，协程被称作<code>task</code>而不是惯例的<code>coroutine</code>。</li>
<li><code>task::spawn(Future)</code>也能被使用于<code>async fn</code>或<code>async {...}</code>内。它被用来代替<code>.await</code>指令，以<strong>非阻塞</strong><code>async fn</code>或<code>async {...}</code>的方式，激活与执行一个<code>Future</code>实例。</li>
</ol>
<h2>例程</h2>
<pre><code>async fn accept_loop(addr: impl ToSocketAddrs) -&gt; Result&lt;()&gt; {
    // 1. TcpListener::bind(addr) 返回 Future
    // 2. .await 于 Future 取得 Result&lt;T, E&gt;
    // 3. Result&lt;T, E&gt;? 再拿得 Ok&lt;T&gt; 中的 T
    let listener = TcpListener::bind(addr).await?; // 异步函数内的人工启动 Future
    let mut incoming = listener.incoming();
    // 因为没有从语言层面支持 async for loop，所以 while loop + Iterator&lt;Item = T&gt; 来模拟之。
    while let Some(stream) = incoming.next().await {
        // TODO
    }
    Ok(())
}
fn main() {
    let fut = accept_loop("127.0.0.1:8080");
    task::block_on(fut); // 异步函数外的人工启动 Future
}
</code></pre>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-30T01:30:00Z">08-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Seq2Seq Autoencoder via Contrastive Learning for Abstractive Text Summarization. (arXiv:2108.11992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11992">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a denoising sequence-to-sequence (seq2seq)
autoencoder via contrastive learning for abstractive text summarization. Our
model adopts a standard Transformer-based architecture with a multi-layer
bi-directional encoder and an auto-regressive decoder. To enhance its denoising
ability, we incorporate self-supervised contrastive learning along with various
sentence-level document augmentation. These two components, seq2seq autoencoder
and contrastive learning, are jointly trained through fine-tuning, which
improves the performance of text summarization with regard to ROUGE scores and
human evaluation. We conduct experiments on two datasets and demonstrate that
our model outperforms many existing benchmarks and even achieves comparable
performance to the state-of-the-art abstractive systems trained with more
complex architecture and extensive computation resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Sentence Ordering Method Using BERT Pretrained Model. (arXiv:2108.11994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11994">
<div class="article-summary-box-inner">
<span><p>Building systems with capability of natural language understanding (NLU) has
been one of the oldest areas of AI. An essential component of NLU is to detect
logical succession of events contained in a text. The task of sentence ordering
is proposed to learn succession of events with applications in AI tasks. The
performance of previous works employing statistical methods is poor, while the
neural networks-based approaches are in serious need of large corpora for model
learning. In this paper, we propose a method for sentence ordering which does
not need a training phase and consequently a large corpus for learning. To this
end, we generate sentence embedding using BERT pre-trained model and measure
sentence similarity using cosine similarity score. We suggest this score as an
indicator of sequential events' level of coherence. We finally sort the
sentences through brute-force search to maximize overall similarities of the
sequenced sentences. Our proposed method outperformed other baselines on
ROCStories, a corpus of 5-sentence human-made stories. The method is
specifically more efficient than neural network-based methods when no huge
corpus is available. Among other advantages of this method are its
interpretability and needlessness to linguistic knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa. (arXiv:2108.12009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12009">
<div class="article-summary-box-inner">
<span><p>We present EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with
RoBERTa, a simple yet expressive scheme of solving the ERC (emotion recognition
in conversation) task. By simply prepending speaker names to utterances and
inserting separation tokens between the utterances in a dialogue, EmoBERTa can
learn intra- and inter- speaker states and context to predict the emotion of a
current speaker, in an end-to-end manner. Our experiments show that we reach a
new state of the art on the two popular ERC datasets using a basic and
straight-forward approach. We've open sourced our code and models at
https://github.com/tae898/erc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-based Self-Critical Training For Question Generation. (arXiv:2108.12026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12026">
<div class="article-summary-box-inner">
<span><p>We present in this work a fully Transformer-based reinforcement learning
generator-evaluator architecture for neural question generation. Question
generation is a task that consists in generating questions given a context and
answer. To improve the quality of the generated question, we came up with a
semantic-based self-critical training layout in generator-evaluator
architecture, which goes beyond typical maximum likelihood training. Evaluation
metrics for language modeling only based on n-gram overlapping do not consider
semantic relations between reference and candidate strings. To improve the
evaluation step, we assess our model for both n-gram overlap using BLEU and
semantically using BERTScore and NUBIA, a novel state-of-the-art evaluation
metric for text generation. Question generation could be used in many
downstream applications, including in extending question answering datasets,
conversational systems, and educational assessment systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using GAN-based models to sentimental analysis on imbalanced datasets in education domain. (arXiv:2108.12061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12061">
<div class="article-summary-box-inner">
<span><p>While the whole world is still struggling with the COVID-19 pandemic, online
learning and home office become more common. Many schools transfer their
courses teaching to the online classroom. Therefore, it is significant to mine
the students' feedback and opinions from their reviews towards studies so that
both schools and teachers can know where they need to improve. This paper
trains machine learning and deep learning models using both balanced and
imbalanced datasets for sentiment classification. Two SOTA category-aware text
generation GAN models: CatGAN and SentiGAN, are utilized to synthesize text
used to balance the highly imbalanced dataset. Results on three datasets with
different imbalance degree from distinct domains show that when using generated
text to balance the dataset, the F1-score of machine learning and deep learning
model on sentiment classification increases 2.79% ~ 9.28%. Also, the results
indicate that the average growth degree for CR100k is higher than CR23k, the
average growth degree for deep learning is more increased than machine learning
algorithms, and the average growth degree for more complex deep learning models
is more increased than simpler deep learning models in experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">4-bit Quantization of LSTM-based Speech Recognition Models. (arXiv:2108.12074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12074">
<div class="article-summary-box-inner">
<span><p>We investigate the impact of aggressive low-precision representations of
weights and activations in two families of large LSTM-based architectures for
Automatic Speech Recognition (ASR): hybrid Deep Bidirectional LSTM - Hidden
Markov Models (DBLSTM-HMMs) and Recurrent Neural Network - Transducers
(RNN-Ts). Using a 4-bit integer representation, a na\"ive quantization approach
applied to the LSTM portion of these models results in significant Word Error
Rate (WER) degradation. On the other hand, we show that minimal accuracy loss
is achievable with an appropriate choice of quantizers and initializations. In
particular, we customize quantization schemes depending on the local properties
of the network, improving recognition performance while limiting computational
time. We demonstrate our solution on the Switchboard (SWB) and CallHome (CH)
test sets of the NIST Hub5-2000 evaluation. DBLSTM-HMMs trained with 300 or
2000 hours of SWB data achieves $&lt;$0.5% and $&lt;$1% average WER degradation,
respectively. On the more challenging RNN-T models, our quantization strategy
limits degradation in 4-bit inference to 1.3%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12084">
<div class="article-summary-box-inner">
<span><p>Gender is widely discussed in the context of language tasks and when
examining the stereotypes propagated by language models. However, current
discussions primarily treat gender as binary, which can perpetuate harms such
as the cyclical erasure of non-binary gender identities. These harms are driven
by model and dataset biases, which are consequences of the non-recognition and
lack of understanding of non-binary genders in society. In this paper, we
explain the complexity of gender and language around it, and survey non-binary
persons to understand harms associated with the treatment of gender as binary
in English language technologies. We also detail how current language
representations (e.g., GloVe, BERT) capture and perpetuate these harms and
related challenges that need to be acknowledged and addressed for
representations to equitably encode gender information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lingxi: A Diversity-aware Chinese Modern Poetry Generation System. (arXiv:2108.12108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12108">
<div class="article-summary-box-inner">
<span><p>Poetry generation has been a difficult task in natural language processing.
Unlike plain neural text generation tasks, poetry has a high requirement for
novelty, since an easily-understood sentence with too many high frequency words
might not be considered as poetic, while adequately ambiguous sentences with
low frequency words can possibly be novel and creative. Inspired by this, we
present Lingxi, a diversity-aware Chinese modern poetry generation system. We
propose nucleus sampling with randomized head (NS-RH) algorithm, which
randomizes the high frequency part ("head") of the predicted distribution, in
order to emphasize on the "comparatively low frequency" words. The proposed
algorithm can significantly increase the novelty of generated poetry compared
with traditional sampling methods. The permutation of distribution is
controllable by tuning the filtering parameter that determines the "head" to
permutate, achieving diversity-aware sampling. We find that even when a large
portion of filtered vocabulary is randomized, it can actually generate fluent
poetry but with notably higher novelty. We also propose a
semantic-similarity-based rejection sampling algorithm, which creates longer
and more informative context on the basis of the short input poetry title while
maintaining high semantic similarity to the title, alleviating the off-topic
issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Generation of Accurate \& Fluent Medical X-ray Reports. (arXiv:2108.12126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12126">
<div class="article-summary-box-inner">
<span><p>Our paper focuses on automating the generation of medical reports from chest
X-ray image inputs, a critical yet time-consuming task for radiologists. Unlike
existing medical re-port generation efforts that tend to produce human-readable
reports, we aim to generate medical reports that are both fluent and clinically
accurate. This is achieved by our fully differentiable and end-to-end paradigm
containing three complementary modules: taking the chest X-ray images and
clinical his-tory document of patients as inputs, our classification module
produces an internal check-list of disease-related topics, referred to as
enriched disease embedding; the embedding representation is then passed to our
transformer-based generator, giving rise to the medical reports; meanwhile, our
generator also pro-duces the weighted embedding representation, which is fed to
our interpreter to ensure consistency with respect to disease-related
topics.Our approach achieved promising results on commonly-used metrics
concerning language fluency and clinical accuracy. Moreover, noticeable
performance gains are consistently ob-served when additional input information
is available, such as the clinical document and extra scans of different views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Secoco: Self-Correcting Encoding for Neural Machine Translation. (arXiv:2108.12137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12137">
<div class="article-summary-box-inner">
<span><p>This paper presents Self-correcting Encoding (Secoco), a framework that
effectively deals with input noise for robust neural machine translation by
introducing self-correcting predictors. Different from previous robust
approaches, Secoco enables NMT to explicitly correct noisy inputs and delete
specific errors simultaneously with the translation decoding process. Secoco is
able to achieve significant improvements over strong baselines on two
real-world test sets and a benchmark WMT dataset with good interpretability. We
will make our code and dataset publicly available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving callsign recognition with air-surveillance data in air-traffic communication. (arXiv:2108.12156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12156">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) can be used as the assistance of speech
communication between pilots and air-traffic controllers. Its application can
significantly reduce the complexity of the task and increase the reliability of
transmitted information. Evidently, high accuracy predictions are needed to
minimize the risk of errors. Especially, high accuracy is required in
recognition of key information, such as commands and callsigns, used to
navigate pilots. Our results prove that the surveillance data containing
callsigns can help to considerably improve the recognition of a callsign in an
utterance when the weights of probable callsign n-grams are reduced per
utterance. In this paper, we investigate two approaches: (1) G-boosting, when
callsigns weights are adjusted at language model level (G) and followed by the
dynamic decoder with an on-the-fly composition, and (2) lattice rescoring when
callsign information is introduced on top of lattices generated using a
conventional decoder. Boosting callsign n-grams with the combination of two
methods allowed us to gain 28.4% of absolute improvement in callsign
recognition accuracy and up to 74.2% of relative improvement in WER of callsign
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar Based Identification Of Speaker Role For Improving ATCO And Pilot ASR. (arXiv:2108.12175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12175">
<div class="article-summary-box-inner">
<span><p>Assistant Based Speech Recognition (ABSR) for air traffic control is
generally trained by pooling both Air Traffic Controller (ATCO) and pilot data.
In practice, this is motivated by the fact that the proportion of pilot data is
lesser compared to ATCO while their standard language of communication is
similar. However, due to data imbalance of ATCO and pilot and their varying
acoustic conditions, the ASR performance is usually significantly better for
ATCOs than pilots. In this paper, we propose to (1) split the ATCO and pilot
data using an automatic approach exploiting ASR transcripts, and (2) consider
ATCO and pilot ASR as two separate tasks for Acoustic Model (AM) training. For
speaker role classification of ATCO and pilot data, a hypothesized ASR
transcript is generated with a seed model, subsequently used to classify the
speaker role based on the knowledge extracted from grammar defined by
International Civil Aviation Organization (ICAO). This approach provides an
average speaker role identification accuracy of 83% for ATCO and pilot.
Finally, we show that training AMs separately for each task, or using a
multitask approach is well suited for this data compared to AM trained by
pooling all data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling. (arXiv:2108.12177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12177">
<div class="article-summary-box-inner">
<span><p>Social media has effectively become the prime hub of communication and
digital marketing. As these platforms enable the free manifestation of thoughts
and facts in text, images and video, there is an extensive need to screen them
to protect individuals and groups from offensive content targeted at them. Our
work intends to classify codemixed social media comments/posts in the Dravidian
languages of Tamil, Kannada, and Malayalam. We intend to improve offensive
language identification by generating pseudo-labels on the dataset. A custom
dataset is constructed by transliterating all the code-mixed texts into the
respective Dravidian language, either Kannada, Malayalam, or Tamil and then
generating pseudo-labels for the transliterated dataset. The two datasets are
combined using the generated pseudo-labels to create a custom dataset called
CMTRA. As Dravidian languages are under-resourced, our approach increases the
amount of training data for the language models. We fine-tune several recent
pretrained language models on the newly constructed dataset. We extract the
pretrained language embeddings and pass them onto recurrent neural networks. We
observe that fine-tuning ULMFiT on the custom dataset yields the best results
on the code-mixed test sets of all three languages. Our approach yields the
best results among the benchmarked models on Tamil-English, achieving a
weighted F1-Score of 0.7934 while scoring competitive weighted F1-Scores of
0.9624 and 0.7306 on the code-mixed test sets of Malayalam-English and
Kannada-English, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions. (arXiv:2108.12189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12189">
<div class="article-summary-box-inner">
<span><p>This paper presents Macquarie University's participation to the BioASQ
Synergy Task, and BioASQ9b Phase B. In each of these tasks, our participation
focused on the use of query-focused extractive summarisation to obtain the
ideal answers to medical questions. The Synergy Task is an end-to-end question
answering task on COVID-19 where systems are required to return relevant
documents, snippets, and answers to a given question. Given the absence of
training data, we used a query-focused summarisation system that was trained
with the BioASQ8b training data set and we experimented with methods to
retrieve the documents and snippets. Considering the poor quality of the
documents and snippets retrieved by our system, we observed reasonably good
quality in the answers returned. For phase B of the BioASQ9b task, the relevant
documents and snippets were already included in the test data. Our system split
the snippets into candidate sentences and used BERT variants under a sentence
classification setup. The system used the question and candidate sentence as
input and was trained to predict the likelihood of the candidate sentence being
part of the ideal answer. The runs obtained either the best or second best
ROUGE-F1 results of all participants to all batches of BioASQ9b. This shows
that using BERT in a classification setup is a very strong baseline for the
identification of ideal answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translation Error Detection as Rationale Extraction. (arXiv:2108.12197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12197">
<div class="article-summary-box-inner">
<span><p>Recent Quality Estimation (QE) models based on multilingual pre-trained
representations have achieved very competitive results when predicting the
overall quality of translated sentences. Predicting translation errors, i.e.
detecting specifically which words are incorrect, is a more challenging task,
especially with limited amounts of training data. We hypothesize that, not
unlike humans, successful QE models rely on translation errors to predict
overall sentence quality. By exploring a set of feature attribution methods
that assign relevance scores to the inputs to explain model predictions, we
study the behaviour of state-of-the-art sentence-level QE models and show that
explanations (i.e. rationales) extracted from these models can indeed be used
to detect translation errors. We therefore (i) introduce a novel
semi-supervised method for word-level QE and (ii) propose to use the QE task as
a new benchmark for evaluating the plausibility of feature attribution, i.e.
how interpretable model explanations are to humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features are dependent
upon each other. Experiment results on five public datasets show that our model
performs significantly better than previous approaches. The source code can be
found in https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors. (arXiv:2108.12216v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12216">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the capacity of a language model-based method for
grammatical error detection in detail. We first show that 5 to 10% of training
data are enough for a BERT-based error detection method to achieve performance
equivalent to a non-language model-based method can achieve with the full
training data; recall improves much faster with respect to training data size
in the BERT-based method than in the non-language model method while precision
behaves similarly. These suggest that (i) the BERT-based method should have a
good knowledge of grammar required to recognize certain types of error and that
(ii) it can transform the knowledge into error detection rules by fine-tuning
with a few training samples, which explains its high generalization ability in
grammatical error detection. We further show with pseudo error data that it
actually exhibits such nice properties in learning rules for recognizing
various types of error. Finally, based on these findings, we explore a
cost-effective method for detecting grammatical errors with feedback comments
explaining relevant grammatical rules to learners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Text in Self-Supervised Speech Pretraining. (arXiv:2108.12226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12226">
<div class="article-summary-box-inner">
<span><p>Self-supervised pretraining for Automated Speech Recognition (ASR) has shown
varied degrees of success. In this paper, we propose to jointly learn
representations during pretraining from two different modalities: speech and
text. The proposed method, tts4pretrain complements the power of contrastive
learning in self-supervision with linguistic/lexical representations derived
from synthesized speech, effectively learning from untranscribed speech and
unspoken text. Lexical learning in the speech encoder is enforced through an
additional sequence loss term that is coupled with contrastive loss during
pretraining. We demonstrate that this novel pretraining method yields Word
Error Rate (WER) reductions of 10% relative on the well-benchmarked,
Librispeech task over a state-of-the-art baseline pretrained with wav2vec2.0
only. The proposed method also serves as an effective strategy to compensate
for the lack of transcribed speech, effectively matching the performance of
5000 hours of transcribed speech with just 100 hours of transcribed speech on
the AMI meeting transcription task. Finally, we demonstrate WER reductions of
up to 15% on an in-house Voice Search task over traditional pretraining.
Incorporating text into encoder pretraining is complimentary to rescoring with
a larger or in-domain language model, resulting in additional 6% relative
reduction in WER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications since the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current algorithms can tackle such
problem reliably in a realistic scenario where zero OOD training data is
available. In this study, we propose ProtoInfoMax, a new architecture that
extends Prototypical Networks to simultaneously process In-Domain (ID) and OOD
sentences via Mutual Information Maximization (InfoMax) objective. Experimental
results show that our proposed method can substantially improve performance up
to 20% for OOD detection in low resource settings of text classification. We
also show that ProtoInfoMax is less prone to typical over-confidence Error of
Neural Networks, leading to more reliable ID and OOD prediction outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Robustness of Neural Language Models to Input Perturbations. (arXiv:2108.12237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12237">
<div class="article-summary-box-inner">
<span><p>High-performance neural language models have obtained state-of-the-art
results on a wide range of Natural Language Processing (NLP) tasks. However,
results for common benchmark datasets often do not reflect model reliability
and robustness when applied to noisy, real-world data. In this study, we design
and implement various types of character-level and word-level perturbation
methods to simulate realistic scenarios in which input texts may be slightly
noisy or different from the data distribution on which NLP systems were
trained. Conducting comprehensive experiments on different NLP tasks, we
investigate the ability of high-performance language models such as BERT,
XLNet, RoBERTa, and ELMo in handling different types of input perturbations.
The results suggest that language models are sensitive to input perturbations
and their performance can decrease even when small changes are introduced. We
highlight that models need to be further improved and that current benchmarks
are not reflecting model robustness well. We argue that evaluations on
perturbed inputs should routinely complement widely-used benchmarks in order to
yield a more realistic understanding of NLP systems robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning models are not robust against noise in clinical text. (arXiv:2108.12242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12242">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI) systems are attracting increasing interest in
the medical domain due to their ability to learn complicated tasks that require
human intelligence and expert knowledge. AI systems that utilize
high-performance Natural Language Processing (NLP) models have achieved
state-of-the-art results on a wide variety of clinical text processing
benchmarks. They have even outperformed human accuracy on some tasks. However,
performance evaluation of such AI systems have been limited to accuracy
measures on curated and clean benchmark datasets that may not properly reflect
how robustly these systems can operate in real-world situations. In order to
address this challenge, we introduce and implement a wide variety of
perturbation methods that simulate different types of noise and variability in
clinical text data. While noisy samples produced by these perturbation methods
can often be understood by humans, they may cause AI systems to make erroneous
decisions. Conducting extensive experiments on several clinical text processing
tasks, we evaluated the robustness of high-performance NLP models against
various types of character-level and word-level noise. The results revealed
that the NLP models performance degrades when the input contains small amounts
of noise. This study is a significant step towards exposing vulnerabilities of
AI models utilized in clinical text processing systems. The proposed
perturbation methods can be used in performance evaluation tests to assess how
robustly clinical NLP models can operate on noisy data, in real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Propaganda on the Sentence Level during the COVID-19 Pandemic. (arXiv:2108.12269v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12269">
<div class="article-summary-box-inner">
<span><p>The spread of misinformation, conspiracy, and questionable content and
information manipulation by foreign adversaries on social media has surged
along with the COVID-19 pandemic. Such malicious cyber-enabled actions may
cause increasing social polarization, health crises, and property loss. In this
paper, using fine-tuned contextualized embedding trained on Reddit, we tackle
the detection of the propaganda of such user accounts and their targeted issues
on Twitter during March 2020 when the COVID-19 epidemic became recognized as a
pandemic. Our result shows that the pro-China group appeared to be tweeting 35
to 115 times more than the neutral group. At the same time, neutral groups were
tweeting more positive-attitude content and voicing alarm for the COVID-19
situation. The pro-China group was also using more call-for-action words on
political issues not necessarily China-related.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can the Transformer Be Used as a Drop-in Replacement for RNNs in Text-Generating GANs?. (arXiv:2108.12275v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12275">
<div class="article-summary-box-inner">
<span><p>In this paper we address the problem of fine-tuned text generation with a
limited computational budget. For that, we use a well-performing text
generative adversarial network (GAN) architecture - Diversity-Promoting GAN
(DPGAN), and attempted a drop-in replacement of the LSTM layer with a
self-attention-based Transformer layer in order to leverage their efficiency.
The resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance,
quality and diversity of generated text and stability. Computational
experiments suggested that a transformer architecture is unable to drop-in
replace the LSTM layer, under-performing during the pre-training phase and
undergoing a complete mode collapse during the GAN tuning phase. Our results
suggest that the transformer architecture need to be adapted before it can be
used as a replacement for RNNs in text-generating GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree Decomposition Attention for AMR-to-Text Generation. (arXiv:2108.12300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12300">
<div class="article-summary-box-inner">
<span><p>Text generation from AMR requires mapping a semantic graph to a string that
it annotates. Transformer-based graph encoders, however, poorly capture vertex
dependencies that may benefit sequence prediction. To impose order on an
encoder, we locally constrain vertex self-attention using a graph's tree
decomposition. Instead of forming a full query-key bipartite graph, we restrict
attention to vertices in parent, subtree, and same-depth bags of a vertex. This
hierarchical context lends both sparsity and structure to vertex state updates.
We apply dynamic programming to derive a forest of tree decompositions,
choosing the most structurally similar tree to the AMR. Our system outperforms
a self-attentive baseline by 1.6 BLEU and 1.8 chrF++.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Tree Decomposition Parsers for AMR-to-Text Generation. (arXiv:2108.12304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12304">
<div class="article-summary-box-inner">
<span><p>Graph encoders in AMR-to-text generation models often rely on neighborhood
convolutions or global vertex attention. While these approaches apply to
general graphs, AMRs may be amenable to encoders that target their tree-like
structure. By clustering edges into a hierarchy, a tree decomposition
summarizes graph structure. Our model encodes a derivation forest of tree
decompositions and extracts an expected tree. From tree node embeddings, it
builds graph edge features used in vertex attention of the graph encoder.
Encoding TD forests instead of shortest-pairwise paths in a self-attentive
baseline raises BLEU by 0.7 and chrF++ by 0.3. The forest encoder also
surpasses a convolutional baseline for molecular property prediction by 1.92%
ROC-AUC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAPE: Context-Aware Private Embeddings for Private Language Learning. (arXiv:2108.12318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12318">
<div class="article-summary-box-inner">
<span><p>Deep learning-based language models have achieved state-of-the-art results in
a number of applications including sentiment analysis, topic labelling, intent
classification and others. Obtaining text representations or embeddings using
these models presents the possibility of encoding personally identifiable
information learned from language and context cues that may present a risk to
reputation or privacy. To ameliorate these issues, we propose Context-Aware
Private Embeddings (CAPE), a novel approach which preserves privacy during
training of embeddings. To maintain the privacy of text representations, CAPE
applies calibrated noise through differential privacy, preserving the encoded
semantic links while obscuring sensitive information. In addition, CAPE employs
an adversarial training regime that obscures identified private variables.
Experimental results demonstrate that the proposed approach reduces private
information leakage better than either single intervention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning. (arXiv:2108.12370v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12370">
<div class="article-summary-box-inner">
<span><p>We demonstrate a library for the integration of domain knowledge in deep
learning architectures. Using this library, the structure of the data is
expressed symbolically via graph declarations and the logical constraints over
outputs or latent variables can be seamlessly added to the deep models. The
domain knowledge can be defined explicitly, which improves the models'
explainability in addition to the performance and generalizability in the
low-data regime. Several approaches for such an integration of symbolic and
sub-symbolic models have been introduced; however, there is no library to
facilitate the programming for such an integration in a generic way while
various underlying algorithms can be used. Our library aims to simplify
programming for such an integration in both training and inference phases while
separating the knowledge representation from learning algorithms. We showcase
various NLP benchmark tasks and beyond. The framework is publicly available at
Github(https://github.com/HLR/DomiKnowS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. (arXiv:2108.12409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12409">
<div class="article-summary-box-inner">
<span><p>Since the introduction of the transformer model by Vaswani et al. (2017), a
fundamental question remains open: how to achieve extrapolation at inference
time to longer sequences than seen during training? We first show that
extrapolation can be improved by changing the position representation method,
though we find that existing proposals do not allow efficient extrapolation. We
introduce a simple and efficient method, Attention with Linear Biases (ALiBi),
that allows for extrapolation. ALiBi does not add positional embeddings to the
word embeddings; instead, it biases the query-key attention scores with a term
that is proportional to their distance. We show that this method allows
training a 1.3 billion parameter model on input sequences of length 1024 that
extrapolates to input sequences of length 2048, achieving the same perplexity
as a sinusoidal position embedding model trained on inputs of length 2048, 11%
faster and using 11% less memory. ALiBi's inductive bias towards recency allows
it to outperform multiple strong position methods on the WikiText-103
benchmark. Finally, we provide analysis of ALiBi to understand why it leads to
better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach. (arXiv:2003.10715v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.10715">
<div class="article-summary-box-inner">
<span><p>Knowledge about the software used in scientific investigations is necessary
for different reasons, including provenance of the results, measuring software
impact to attribute developers, and bibliometric software citation analysis in
general. Additionally, providing information about whether and how the software
and the source code are available allows an assessment about the state and role
of open source software in science in general. While such analyses can be done
manually, large scale analyses require the application of automated methods of
information extraction and linking. In this paper, we present SoftwareKG - a
knowledge graph that contains information about software mentions from more
than 51,000 scientific articles from the social sciences. A silver standard
corpus, created by a distant and weak supervision approach, and a gold standard
corpus, created by manual annotation, were used to train an LSTM based neural
network to identify software mentions in scientific articles. The model
achieves a recognition rate of .82 F-score in exact matches. As a result, we
identified more than 133,000 software mentions. For entity disambiguation, we
used the public domain knowledge base DBpedia. Furthermore, we linked the
entities of the knowledge graph to other knowledge bases such as the Microsoft
Academic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we
illustrate, how SoftwareKG can be used to assess the role of software in the
social sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A review of on-device fully neural end-to-end automatic speech recognition algorithms. (arXiv:2012.07974v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07974">
<div class="article-summary-box-inner">
<span><p>In this paper, we review various end-to-end automatic speech recognition
algorithms and their optimization techniques for on-device applications.
Conventional speech recognition systems comprise a large number of discrete
components such as an acoustic model, a language model, a pronunciation model,
a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted
Finite State Transducer (WFST), and so on. To obtain sufficiently high speech
recognition accuracy with such conventional speech recognition systems, a very
large language model (up to 100 GB) is usually needed. Hence, the corresponding
WFST size becomes enormous, which prohibits their on-device implementation.
Recently, fully neural network end-to-end speech recognition algorithms have
been proposed. Examples include speech recognition systems based on
Connectionist Temporal Classification (CTC), Recurrent Neural Network
Transducer (RNN-T), Attention-based Encoder-Decoder models (AED), Monotonic
Chunk-wise Attention (MoChA), transformer-based speech recognition systems, and
so on. These fully neural network-based systems require much smaller memory
footprints compared to conventional algorithms, therefore their on-device
implementation has become feasible. In this paper, we review such end-to-end
speech recognition models. We extensively discuss their structures,
performance, and advantages compared to conventional algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00433">
<div class="article-summary-box-inner">
<span><p>Broader disclosive transparency$-$truth and clarity in communication
regarding the function of AI systems$-$is widely considered desirable.
Unfortunately, it is a nebulous concept, difficult to both define and quantify.
This is problematic, as previous work has demonstrated possible trade-offs and
negative consequences to disclosive transparency, such as a confusion effect,
where 'too much information' clouds a reader's understanding of what a system
description means. Disclosive transparency's subjective nature has rendered
deep study into these problems and their remedies difficult. To improve this
state of affairs, We introduce neural language model-based probabilistic
metrics to directly model disclosive transparency, and demonstrate that they
correlate with user and expert opinions of system transparency, making them a
valid objective proxy. Finally, we demonstrate the use of these metrics in a
pilot study quantifying the relationships between transparency, confusion, and
user perceptions in a corpus of real NLP system descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Semi-Supervised Learning: An Approach To Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems. (arXiv:2104.03643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03643">
<div class="article-summary-box-inner">
<span><p>Air traffic management and specifically air-traffic control (ATC) rely mostly
on voice communications between Air Traffic Controllers (ATCos) and pilots. In
most cases, these voice communications follow a well-defined grammar that could
be leveraged in Automatic Speech Recognition (ASR) technologies. The callsign
used to address an airplane is an essential part of all ATCo-pilot
communications. We propose a two-steps approach to add contextual knowledge
during semi-supervised training to reduce the ASR system error rates at
recognizing the part of the utterance that contains the callsign. Initially, we
represent in a WFST the contextual knowledge (i.e. air-surveillance data) of an
ATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the
contextual knowledge is added by second-pass decoding (i.e. lattice
re-scoring). Results show that `unseen domains' (e.g. data from airports not
present in the supervised training data) are further aided by contextual SSL
when compared to standalone SSL. For this task, we introduce the Callsign Word
Error Rate (CA-WER) as an evaluation metric, which only assesses ASR
performance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER
relative improvement applying SSL with an additional 17.5% CA-WER improvement
by adding contextual knowledge during SSL on a challenging ATC-based test set
gathered from LiveATC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Remove: Towards Isotropic Pre-trained BERT Embedding. (arXiv:2104.05274v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05274">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models such as BERT have become a more common choice of
natural language processing (NLP) tasks. Research in word representation shows
that isotropic embeddings can significantly improve performance on downstream
tasks. However, we measure and analyze the geometry of pre-trained BERT
embedding and find that it is far from isotropic. We find that the word vectors
are not centered around the origin, and the average cosine similarity between
two random words is much higher than zero, which indicates that the word
vectors are distributed in a narrow cone and deteriorate the representation
capacity of word embedding. We propose a simple, and yet effective method to
fix this problem: remove several dominant directions of BERT embedding with a
set of learnable weights. We train the weights on word similarity tasks and
show that processed embedding is more isotropic. Our method is evaluated on
three standardized tasks: word similarity, word analogy, and semantic textual
similarity. In all tasks, the word embedding processed by our method
consistently outperforms the original embedding (with average improvement of
13% on word analogy and 16% on semantic textual similarity) and two baseline
methods. Our method is also proven to be more robust to changes of
hyperparameter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06669">
<div class="article-summary-box-inner">
<span><p>We propose the task of Narrative Reordering (NAREOR) which involves rewriting
a given story in a different narrative order while preserving its plot. We
present a dataset, NAREORC, with human rewritings of stories within ROCStories
in non-linear orders, and conduct a detailed analysis of it. Further, we
propose novel task-specific training methods with suitable evaluation metrics.
We perform experiments on NAREORC using state-of-the-art models such as BART
and T5 and conduct extensive automatic and human evaluations. We demonstrate
that NAREOR is a challenging task with potential for further exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08315">
<div class="article-summary-box-inner">
<span><p>Large language models have shown promising results in zero-shot settings
(Brown et al.,2020; Radford et al., 2019). For example, they can perform
multiple choice tasks simply by conditioning on a question and selecting the
answer with the highest probability.
</p>
<p>However, ranking by string probability can be problematic due to surface form
competition-wherein different surface forms compete for probability mass, even
if they represent the same underlying concept, e.g. "computer" and "PC." Since
probability mass is finite, this lowers the probability of the correct answer,
due to competition from other strings that are valid answers (but not one of
the multiple choice options).
</p>
<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative
scoring function that directly compensates for surface form competition by
simply reweighing each option according to a term that is proportional to its a
priori likelihood within the context of the specific zero-shot task. It
achieves consistent gains in zero-shot performance over both calibrated (Zhao
et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models
over a variety of multiple choice datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. (arXiv:2104.08808v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08808">
<div class="article-summary-box-inner">
<span><p>The ability to continuously expand knowledge over time and utilize it to
rapidly generalize to new tasks is a key feature of human linguistic
intelligence. Existing models that pursue rapid generalization to new tasks
(e.g., few-shot learning methods), however, are mostly trained in a single shot
on fixed datasets, unable to dynamically expand their knowledge; while
continual learning algorithms are not specifically designed for rapid
generalization. We present a new learning setup, Continual Learning of Few-Shot
Learners (CLIF), to address the challenges of both learning settings in a
unified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks
arriving sequentially, accumulating knowledge for improved generalization to
new tasks, while also retaining performance on the tasks learned earlier. We
examine how the generalization ability is affected in the continual learning
setup, evaluate a number of continual learning algorithms, and propose a novel
regularized adapter generation approach. We find that catastrophic forgetting
affects generalization ability to a less degree than performance on seen tasks;
while continual learning algorithms can still bring considerable benefit to the
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts. (arXiv:2104.08809v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08809">
<div class="article-summary-box-inner">
<span><p>Determining coreference of concept mentions across multiple documents is a
fundamental task in natural language understanding. Work on cross-document
coreference resolution (CDCR) typically considers mentions of events in the
news, which seldom involve abstract technical concepts that are prevalent in
science and technology. These complex concepts take diverse or ambiguous forms
and have many hierarchical levels of granularity (e.g., tasks and subtasks),
posing challenges for CDCR. We present a new task of Hierarchical CDCR (H-CDCR)
with the goal of jointly inferring coreference clusters and hierarchy between
them. We create SciCo, an expert-annotated dataset for H-CDCR in scientific
papers, 3X larger than the prominent ECB+ resource. We study strong baseline
models that we customize for H-CDCR, and highlight challenges for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. (arXiv:2104.08836v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08836">
<div class="article-summary-box-inner">
<span><p>Multimodal pre-training with text, layout, and image has achieved SOTA
performance for visually-rich document understanding tasks recently, which
demonstrates the great potential for joint learning across different
modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model
for multilingual document understanding, which aims to bridge the language
barriers for visually-rich document understanding. To accurately evaluate
LayoutXLM, we also introduce a multilingual form understanding benchmark
dataset named XFUND, which includes form understanding samples in 7 languages
(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and
key-value pairs are manually labeled for each language. Experiment results show
that the LayoutXLM model has significantly outperformed the existing SOTA
cross-lingual pre-trained models on the XFUND dataset. The pre-trained
LayoutXLM model and the XFUND dataset are publicly available at
https://aka.ms/layoutxlm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09106">
<div class="article-summary-box-inner">
<span><p>Subword units are commonly used for end-to-end automatic speech recognition
(ASR), while a fully acoustic-oriented subword modeling approach is somewhat
missing. We propose an acoustic data-driven subword modeling (ADSM) approach
that adapts the advantages of several text-based and acoustic-based subword
methods into one pipeline. With a fully acoustic-oriented label design and
learning process, ADSM produces acoustic-structured subword units and
acoustic-matched target sequence for further ASR training. The obtained ADSM
labels are evaluated with different end-to-end ASR approaches including CTC,
RNN-Transducer and attention models. Experiments on the LibriSpeech corpus show
that ADSM clearly outperforms both byte pair encoding (BPE) and
pronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis
shows that ADSM achieves acoustically more logical word segmentation and more
balanced sequence length, and thus, is suitable for both time-synchronous and
label-synchronous models. We also briefly describe how to apply acoustic-based
subword regularization and unseen text segmentation using ADSM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Guided Curriculum Learning for Neural Machine Translation. (arXiv:2105.04475v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04475">
<div class="article-summary-box-inner">
<span><p>In the field of machine learning, the well-trained model is assumed to be
able to recover the training labels, i.e. the synthetic labels predicted by the
model should be as close to the ground-truth labels as possible. Inspired by
this, we propose a self-guided curriculum strategy to encourage the learning of
neural machine translation (NMT) models to follow the above recovery criterion,
where we cast the recovery degree of each training example as its learning
difficulty. Specifically, we adopt the sentence level BLEU score as the proxy
of recovery degree. Different from existing curricula relying on linguistic
prior knowledge or third-party language models, our chosen learning difficulty
is more suitable to measure the degree of knowledge mastery of the NMT models.
Experiments on translation benchmarks, including WMT14
English$\Rightarrow$German and WMT17 Chinese$\Rightarrow$English, demonstrate
that our approach can consistently improve translation performance against
strong baseline Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08988">
<div class="article-summary-box-inner">
<span><p>Social media platforms provide convenient means for users to participate in
multiple online activities on various contents and create fast widespread
interactions. However, this rapidly growing access has also increased the
diverse information, and characterizing user types to understand people's
lifestyle decisions shared in social media is challenging. In this paper, we
propose a weakly supervised graph embedding based framework for understanding
user types. We evaluate the user embedding learned using weak supervision over
well-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.
Experiments on real-world datasets demonstrate that the proposed framework
outperforms the baselines for detecting user types. Finally, we illustrate data
analysis on different types of users (e.g., practitioner vs. promotional) from
our dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our
method for constructing user representation readily generalizes to other
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Communication with Adaptive Universal Transformer. (arXiv:2108.09119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09119">
<div class="article-summary-box-inner">
<span><p>With the development of deep learning (DL), natural language processing (NLP)
makes it possible for us to analyze and understand a large amount of language
texts. Accordingly, we can achieve a semantic communication in terms of joint
semantic source and channel coding over a noisy channel with the help of NLP.
However, the existing method to realize this goal is to use a fixed transformer
of NLP while ignoring the difference of semantic information contained in each
sentence. To solve this problem, we propose a new semantic communication system
based on Universal Transformer. Compared with the traditional transformer, an
adaptive circulation mechanism is introduced in the Universal Transformer.
Through the introduction of the circulation mechanism, the new semantic
communication system can be more flexible to transmit sentences with different
semantic information, and achieve better end-to-end performance under various
channel conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10750">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) from tables is the task of identifying relations
between pairs of columns of a table. Generally, RE models for this task require
labelled tables for training. These labelled tables can also be generated
artificially from a Knowledge Graph (KG), which makes the cost to acquire them
much lower in comparison to manual annotations. However, unlike real tables,
these synthetic tables lack associated metadata, such as, column-headers,
captions, etc; this is because synthetic tables are created out of KGs that do
not store such metadata. Meanwhile, previous works have shown that metadata is
important for accurate RE from tables. To address this issue, we propose
methods to artificially create some of this metadata for synthetic tables.
Afterward, we experiment with a BERT-based model, in line with recently
published works, that takes as input a combination of proposed artificial
metadata and table content. Our empirical results show that this leads to an
improvement of 9\%-45\% in F1 score, in absolute terms, over 2 tabular
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Offensive Language Identification for Tamil Code-Mixed YouTube Comments and Posts. (arXiv:2108.10939v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10939">
<div class="article-summary-box-inner">
<span><p>Offensive Language detection in social media platforms has been an active
field of research over the past years. In non-native English spoken countries,
social media users mostly use a code-mixed form of text in their
posts/comments. This poses several challenges in the offensive content
identification tasks, and considering the low resources available for Tamil,
the task becomes much harder. The current study presents extensive experiments
using multiple deep learning, and transfer learning models to detect offensive
content on YouTube. We propose a novel and flexible approach of selective
translation and transliteration techniques to reap better results from
fine-tuning and ensembling multilingual transformer networks like BERT, Distil-
BERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best
model for this task. The best performing models were ULMFiT and mBERTBiLSTM for
this Tamil code-mix dataset instead of more popular transfer learning models
such as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The
proposed model ULMFiT and mBERTBiLSTM yielded good results and are promising
for effective offensive speech identification in low-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutReader: Pre-training of Text and Layout for Reading Order Detection. (arXiv:2108.11591v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11591">
<div class="article-summary-box-inner">
<span><p>Reading order detection is the cornerstone to understanding visually-rich
documents (e.g., receipts and forms). Unfortunately, no existing work took
advantage of advanced deep learning models because it is too laborious to
annotate a large enough dataset. We observe that the reading order of WORD
documents is embedded in their XML metadata; meanwhile, it is easy to convert
WORD documents to PDFs or images. Therefore, in an automated manner, we
construct ReadingBank, a benchmark dataset that contains reading order, text,
and layout information for 500,000 document images covering a wide spectrum of
document types. This first-ever large-scale dataset unleashes the power of deep
neural networks for reading order detection. Specifically, our proposed
LayoutReader captures the text and layout information for reading order
prediction using the seq2seq model. It performs almost perfectly in reading
order detection and significantly improves both open-source and commercial OCR
engines in ordering text lines in their results in our experiments. We will
release the dataset and model at \url{https://aka.ms/layoutreader}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Negative Sampling for Unlabeled Entity Problem in Named Entity Recognition. (arXiv:2108.11607v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11607">
<div class="article-summary-box-inner">
<span><p>In many situations (e.g., distant supervision), unlabeled entity problem
seriously degrades the performances of named entity recognition (NER) models.
Recently, this issue has been well addressed by a notable approach based on
negative sampling. In this work, we perform two studies along this direction.
Firstly, we analyze why negative sampling succeeds both theoretically and
empirically. Based on the observation that named entities are highly sparse in
datasets, we show a theoretical guarantee that, for a long sentence, the
probability of containing no unlabeled entities in sampled negatives is high.
Missampling tests on synthetic datasets have verified our guarantee in
practice. Secondly, to mine hard negatives and further reduce missampling
rates, we propose a weighted and adaptive sampling distribution for negative
sampling. Experiments on synthetic datasets and well-annotated datasets show
that our method significantly improves negative sampling in robustness and
effectiveness. We also have achieved new state-of-the-art results on real-world
datasets.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-08-30 01:49:53.305542600 UTC">2021-08-30 01:49:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>