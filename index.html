<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-01T01:56:19.980087076Z">09-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">【全职远程】15K-30K/硅谷无码AI团队招/后端开发工程师/Java or Scala</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=6c2bed71-6adf-45ac-bd60-904427c5c51c">
<div class="article-summary-box-inner">
<span><p>公司简介
我们是一个刚刚成立在上海的创业公司。创始人在硅谷，全员远程，致力于打造下一代无码AI数据产品，技术氛围浓厚。目前和国内一家知名CDP平台厂商合理推出了第一版无代码机器学习预测平台，帮助企业实现运营的智能化。</p>
<p>工作职责
1、负责机器学习平台的后端技术架构;</p>
<p>2、负责机器学习平台的后端代码实现及单元测试；</p>
<p>3、负责编写技术设计文档、API文档。</p>
<p>任职要求
1、计算机相关专业，本科及以上学历；</p>
<p>2、 熟练使用Java 或 Scala 开发，5年以上的开发经验；</p>
<p>3、熟练使用SprintBoot，熟悉相关的生态和使用方法；</p>
<p>4、有大数据处理经验优先。</p>
<p>关于沟通
1、使用飞书作为沟通和文档工具；</p>
<p>2、每天上午 9 点（冬令时，夏令时是 上午 8:30 ）会有简单的同步；</p>
<p>3、每天写日报，说明任务的进度，以及发现哪些问题和需要什么帮助；</p>
<p>4、每个 Sprint 会花时间拆解 Story 和分配任务，需要各自分析出各个需求点和关键点，发现风险和不确定的地方及早确认。</p>
<p>薪资待遇
15K-30K，全职（不接受兼职），提供五险一金。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mike Tang 张汉东 老油条各位大佬请进，素数多线程问题</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=b10b7a68-e2bf-42b6-a583-ef99478e50d3">
<div class="article-summary-box-inner">
<span><p>各位大佬：</p>
<p>昨天腾讯视频聊的比较开心，留了一个尾巴。我现在把我的困惑放出来，大家尽情拍砖。</p>
<p>这个是我写的文章，文章分了两章 ：</p>
<p><a href="https://github.com/sunnyrust/rustBible/blob/master/books/6.2.md" rel="noopener noreferrer">6.2 多线程——channel</a></p>
<p><a href="https://github.com/sunnyrust/rustBible/blob/master/books/6.3.md" rel="noopener noreferrer">6.3 多线程——future</a></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-31 Rust edition</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d6a7f8d7-4d99-4488-a933-1d70cefcd2cd">
<div class="article-summary-box-inner">
<span><h2><code>delicate</code> 分布式调度系统</h2>
<p>周二的唠嗑室主题
主持人：槟橙炮炮
简介：Rust社区之前没有活跃的分布式调度系统项目，为了填补这个空白我开始调研实现项目，目前已经快要发布V1.1了。</p>
<p>在项目设计与底层库的实现从smol套件中获得了很多灵感，也会简单跟大家介绍下 smol &amp; tokio 一些各自的设计哲学，async-process async-io async-task 一些漂亮的代码片段。</p>
<ul>
<li>文档地址：https://delicate-rs.github.io/</li>
<li>源码：https://github.com/BinChengZhao/delicate</li>
</ul>
<h2>datafusion-5.0.0 发布</h2>
<p>datafusion 是基于 Apache Arrow 列格式、使用 Rust 实现的可扩展查询执行框架，支持SQL 和 DataFrame API，也可以通过 ballista crate(也发布了 v0.5.0) 支持分布式查询</p>
<ul>
<li>发布页：https://arrow.apache.org/blog/2021/08/18/datafusion-5.0.0/</li>
<li>仓库： https://github.com/apache/arrow-datafusion</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【原创】Rust tokio 如何以异步非阻塞方式运行大量任务</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ba4f86c6-667d-4acb-89a1-e2fb0617f524">
<div class="article-summary-box-inner">
<span><p>tokio 官方给了一个完整的<a href="https://tokio.rs/tokio/topics/bridging#spawning-things-on-a-runtime" rel="noopener noreferrer">例子</a>：手动构建 runtime ，利用 block_on 来运行多个任务。
tokio 的任务是由 <code>tokio::spawn</code> 之类的函数产生的 <code>JoinHandle</code> 类型，而且是个 <code>Future</code> 。</p>
<p>而下面利用 <code>#[tokio::main]</code> 和 await 编写了等价的版本（为了直观对比任务完成的实际顺序和总耗时，我对 sleep 的时间做了一些简化）：</p>
<pre><code>use std::time::Instant;
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    let now = Instant::now();

    let mut handles = Vec::with_capacity(10);
    for i in 0..10 {
        handles.push(tokio::spawn(my_bg_task(i)));
    }

    // Do something time-consuming while the background tasks execute.
    std::thread::sleep(Duration::from_millis(120));
    println!("Finished time-consuming task.");

    // Wait for all of them to complete.
    for handle in handles {
        handle.await?;
    }

    println!("总耗时：{} ms", now.elapsed().as_millis());
    Ok(())
}

async fn my_bg_task(i: u64) {
    let millis = 100;
    println!("Task {} sleeping for {} ms.", i, millis);
    sleep(Duration::from_millis(millis)).await;
    println!("Task {} stopping.", i);
}
</code></pre>
<p>输出结果：</p>
<pre><code>Task 0 sleeping for 100 ms.
Task 1 sleeping for 100 ms.
Task 2 sleeping for 100 ms.
Task 3 sleeping for 100 ms.
Task 4 sleeping for 100 ms.
Task 5 sleeping for 100 ms.
Task 6 sleeping for 100 ms.
Task 7 sleeping for 100 ms.
Task 8 sleeping for 100 ms.
Task 9 sleeping for 100 ms.
Task 9 stopping.
Task 0 stopping.
Task 1 stopping.
Task 2 stopping.
Task 3 stopping.
Task 4 stopping.
Task 5 stopping.
Task 6 stopping.
Task 7 stopping.
Task 8 stopping.
Finished time-consuming task.
总耗时：120 ms
</code></pre>
<p>如果把主线程的的 sleep 时间改成 100 ms：<code>std::thread::sleep(Duration::from_millis(100));</code>
则产生下面的结果：</p>
<pre><code>Task 0 sleeping for 100 ms.
Task 1 sleeping for 100 ms.
Task 2 sleeping for 100 ms.
Task 3 sleeping for 100 ms.
Task 4 sleeping for 100 ms.
Task 5 sleeping for 100 ms.
Task 6 sleeping for 100 ms.
Task 7 sleeping for 100 ms.
Task 8 sleeping for 100 ms.
Task 9 sleeping for 100 ms.
Finished time-consuming task.
Task 3 stopping.
Task 0 stopping.
Task 1 stopping.
Task 2 stopping.
Task 9 stopping.
Task 4 stopping.
Task 5 stopping.
Task 6 stopping.
Task 7 stopping.
Task 8 stopping.
总耗时：103 ms
</code></pre>
<p>可以看到，<code>my_bg_task</code> 实际是异步非阻塞执行的 👍 ：</p>
<ul>
<li>异步：因为每个任务不必等待其结果就可以开始下一个任务，即；</li>
</ul>
<pre><code>// 异步
Task 0 sleeping for 100 ms.
Task 1 sleeping for 100 ms.
...

// 同步
Task 0 sleeping for 100 ms.
Task 0 stopping.
Task 1 sleeping for 100 ms.
Task 1 stopping.
...
</code></pre>
<ul>
<li>非阻塞：每个任务之间可以快速切换，不必等待其他任务完成才切换，这个例子表现在：
<ul>
<li>任务 0-9 以乱序方式 stop</li>
<li><code>Finished time-consuming task.</code> 与 <code>Task x stopping.</code> 的打印顺序只与任务各自的运行 (sleep) 时间有关，与源代码的声明执行顺序无关。只有任务之间快速切换才能做到这一点。回顾官网的例子：10 个任务的 sleep 时间线性递减 （<code>let millis = 1000 - 50 * i;</code>），从 6 个任务开始小于主线程 sleep 任务的时间（750 ms），而等待 10 个任务执行的语句 <code>for handle in handles { ... }</code> 显然位于 <code>std::thread::sleep</code> 之后，所以任务之间非阻塞执行的话，打印结果为 sleep 时间越短的任务先完成，时间越长的任务后完成，总耗时为任务中的最长耗时：</li>
</ul>
</li>
</ul>
<pre><code>Task 0 sleeping for 1000 ms.
Task 1 sleeping for 950 ms.
Task 2 sleeping for 900 ms.
Task 3 sleeping for 850 ms.
Task 4 sleeping for 800 ms.
Task 5 sleeping for 750 ms.
Task 6 sleeping for 700 ms.
Task 7 sleeping for 650 ms.
Task 8 sleeping for 600 ms.
Task 9 sleeping for 550 ms.
Task 9 stopping.
Task 8 stopping.
Task 7 stopping.
Task 6 stopping.
Finished time-consuming task.
Task 5 stopping.
Task 4 stopping.
Task 3 stopping.
Task 2 stopping.
Task 1 stopping.
Task 0 stopping.
总耗时：1001 ms // 非常完美
</code></pre>
<p>一般情况下，对于 async block/fn 你至少有以下一些做法：</p>
<ol>
<li>对 async block/fn 调用 <code>.await</code> 来等待结果；</li>
<li>对可列举的少数 Future 调用 <code>join!</code> 或者 <code>select!</code> 来同时等待多个结果 或者 等待多个分支的第一个结果；</li>
<li>对大量 Future 调用 <a href="https://docs.rs/futures/0.3.17/futures/?search=join" rel="noopener noreferrer">join</a> 或者 <a href="https://docs.rs/futures/0.3.17/futures/?search=select" rel="noopener noreferrer">select</a> 一类支持传入 Vec / iter 参数类型的函数，比如这个例子中的 <code>for handle in handles { ... }</code> 部分就可以改写成 <code>futures::future::join_all(handles).await;</code> ；</li>
<li>把 async block/fn 变成任务，然后调用 <code>Runtime::block_on</code> （等价地，对任务 await）来执行许多任务。</li>
</ol>
<p>容易犯的错误是，希望异步非阻塞时，对所有 async block/fn 进行了 await，而没有进行任务化处理（即 把 Future 通过 spwan 函数转化成任务）：</p>
<pre><code>use std::time::Instant;
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() {
    let now = Instant::now();

    let mut handles = Vec::with_capacity(10);
    for i in 0..10 {
        handles.push(my_bg_task(i)); // 没有把 Future 变成任务
    }

    std::thread::sleep(Duration::from_millis(120));
    println!("Finished time-consuming task.");

    for handle in handles {
        handle.await; // 而且每个 handle 必须执行完才能执行下一个 handle
    }
    println!("总耗时：{} ms", now.elapsed().as_millis());
}

async fn my_bg_task(i: u64) {
    let millis = 100;
    println!("Task {} sleeping for {} ms.", i, millis);
    sleep(Duration::from_millis(millis)).await;
    println!("Task {} stopping.", i);
}
</code></pre>
<p>运行结果：同步阻塞</p>
<pre><code>Finished time-consuming task.
Task 0 sleeping for 100 ms.
Task 0 stopping.
Task 1 sleeping for 100 ms.
Task 1 stopping.
Task 2 sleeping for 100 ms.
Task 2 stopping.
Task 3 sleeping for 100 ms.
Task 3 stopping.
Task 4 sleeping for 100 ms.
Task 4 stopping.
Task 5 sleeping for 100 ms.
Task 5 stopping.
Task 6 sleeping for 100 ms.
Task 6 stopping.
Task 7 sleeping for 100 ms.
Task 7 stopping.
Task 8 sleeping for 100 ms.
Task 8 stopping.
Task 9 sleeping for 100 ms.
Task 9 stopping.
总耗时：1130 ms
</code></pre>
<hr>
<p>或者像这样：</p>
<pre><code>use std::time::Instant;
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() {
    let now = Instant::now();

    let mut handles = Vec::with_capacity(10);
    for i in 0..10 {
        handles.push(my_bg_task(i)); // 没有把 Future 变成任务
    }

    std::thread::sleep(Duration::from_millis(120));
    println!("Finished time-consuming task.");

    futures::future::join_all(handles).await; // 但是 join_all 会等待所有 Future 并发执行完
    println!("总耗时：{} ms", now.elapsed().as_millis());
}

async fn my_bg_task(i: u64) {
    let millis = 100;
    println!("Task {} sleeping for {} ms.", i, millis);
    sleep(Duration::from_millis(millis)).await;
    println!("Task {} stopping.", i);
}
</code></pre>
<p>运行结果：异步阻塞</p>
<pre><code>Finished time-consuming task.
Task 0 sleeping for 100 ms.
Task 1 sleeping for 100 ms.
Task 2 sleeping for 100 ms.
Task 3 sleeping for 100 ms.
Task 4 sleeping for 100 ms.
Task 5 sleeping for 100 ms.
Task 6 sleeping for 100 ms.
Task 7 sleeping for 100 ms.
Task 8 sleeping for 100 ms.
Task 9 sleeping for 100 ms.
Task 0 stopping.
Task 1 stopping.
Task 2 stopping.
Task 3 stopping.
Task 4 stopping.
Task 5 stopping.
Task 6 stopping.
Task 7 stopping.
Task 8 stopping.
Task 9 stopping.
总耗时：221 ms
</code></pre>
<p>​</p>
<p><em>P.S. 关于代码中 <code>std::thread::sleep</code> 和 <code>tokio::time::sleep</code> 的区别，参考这篇文章 <a href="https://ryhl.io/blog/async-what-is-blocking/" rel="noopener noreferrer">Async: What is blocking? (by Alice Ryhl)</a> 。</em></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">[问题已解决，非常适合新手看]新手求助指针的使用问题，写了个前缀树算法一直不能编译有谁帮忙看一下。请高手赐教。</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=56e27ff0-860c-41c8-81d4-4756fffd5abc">
<div class="article-summary-box-inner">
<span><pre><code>fn insert(mut self, url_rule: &amp;str) {
        let mut current = self.clone(); // 作为游标指针使用，好像没有达到游标的效果
        let list = parse_path(url_rule);
        for word in &amp;list {
            let mut is_exist = false;
            for n in current.child() {
                if n.name == word.to_string() {
                    is_exist = true;
                    current = n.clone();
                    break;
                }
            }

            if is_exist {
                continue;
            }
            let mut node = Tree::new(word);
            if is_variable(word) {
                node.is_variable = true
            };
            current.append_child(&amp;node);
            current = node.clone()
        }

        current.rule = url_rule.to_string();
        current.is_end = true;
    }

</code></pre>
<p>上面的current游标我应该用什么类型指针，因为是自定义类型Tree，绑定只能用Clone。怎么用引用指针修改子节点的数据
golang的实现在https://github.com/obity/pretree/blob/main/pretree.go这个是没有问题的。rust实在是不会写。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust进程关闭时的回调钩子</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=77a67a30-75d9-49f5-bf60-7a650477f552">
<div class="article-summary-box-inner">
<span><p>请教一下进程关闭时的回调函数怎么实现？</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">如何把rust编译的可执行文件中的众多文件名去除</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=fdd79d8b-5b4f-471a-84ea-47f832d1f4c1">
<div class="article-summary-box-inner">
<span><p>下面是从可执行文件中截取的一段。里面包含大量的文件名和字符串。这些文件名占用很多空间。请问如何不让这个文件名编译到可执行文件中？</p>
<pre><code>library/core/src/fmt/mod.rs^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^D^D^D^D^D^@^@^@^@^@^@^@^@^@^@^@attempted to index str up to maximum usizelibrary/core/src/str/pattern.rs^@library/core/src/str/lossy.rsassertion failed: broken.is_empty()EmptyParseIntErrorInvalidDigitPosOverflowNegOverflowUtf8Errorvalid_up_toerror_len.debug_str_offsets.debug_str.debug_rnglists.debug_ranges.debug_line_str.debug_line.debug_info.debug_addr.debug_abbrevassertion failed: src.len() == dst.len()assertion failed: edge.height == self.height - 1assertion failed: idx &lt; CAPACITY/rustc/a15f484b918a4533ad633ea903ccce82910af342/library/alloc/src/collections/btree/node.rs/rustc/a15f484b918a4533ad633ea903ccce82910af342/library/alloc/src/collections/btree/map/entry.rs/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/gimli-0.23.0/src/read/abbrev.rsJimI am hello world
^@^@^@^@^@^@^@^@^@^@^@^@^@Invalid archive member headerInvalid archive terminatorInvalid archive member sizeArchive member size is too large^@^@^@Invalid archive extended name offsetInvalid archive extended name length::@*&amp;&lt;&gt;(,/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/rustc-demangle-0.1.18/src/legacy.rs?[]::{closure#}, _-false...!f64f32usizeu64u32u16u8isizei64i32i16i8()str/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/rustc-demangle-0.1.18/src/v0.rs0x' const ; &gt;  +  = Cunsafe " fn(punycode{.llvm.^@^@^@^@^@^@^@^@^@^@/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/rustc-demangle-0.1.18/src/lib.rsAccessErroruse of std::thread::current() is not possible after the thread's local data has been destroyedalready mutably borrowedcalled `Option::unwrap()` on a `None` valuelibrary/std/src/sys_common/thread_info.rsthread name may not contain interior null bytesfailed to generate unique thread ID: bitspace exhausted^@^@^@»±°&lt;^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@§«ª2^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@called `Result::unwrap()` on an `Err` valuelibrary/std/src/sys/unix/mutex.rsOsmessageCustomerrorUnexpectedEofConnectionRefusedConnectionResetConnectionAbortedNotConnectedAddrInUseBrokenPipeAlreadyExistsWouldBlockInvalidInputInvalidDataWriteZeroInterruptedOtherstrerror_r failurelibrary/std/src/sys/unix/os.rslibrary/std/src/ffi/c_str.rslibrary/std/src/thread/mod.rscannot access a Thread Local Storage value during or after destruction^@^@^@^@^@^@rwlock maximum reader count exceededrwlock read lock would result in deadlockfatal runtime error:
thread panicked while panicking. aborting.
RUST_BACKTRACE&lt;unnamed&gt;formatter errorfailed to write whole bufferlibrary/std/src/io/mod.rsnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-30 如何来看待 unwrap</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=59dad850-933e-49dd-9ab8-5370d5c77857">
<div class="article-summary-box-inner">
<span><h1>如何来看待 unwrap</h1>
<p><code>unwrap</code> 方法可能会让新手感到困惑。一些建议:</p>
<ul>
<li>可以使用 Expect (&amp;str) 而不是 unwrap() 为 panic 提供上下文。</li>
<li>使用 unwrap 和 expect 类似于断言。如果他们 panic，那只有在不可挽回的情况下才会发生。</li>
<li>避免在库代码中使用。</li>
</ul>
<p><a href="https://owengage.com/writing/2021-08-30-how-to-think-of-unwrap/" rel="noopener noreferrer">原文链接</a></p>
<h1>singleton-cell: 一个更强大的 ghost cell 扩展</h1>
<p>这个库提供了一个安全的、零开销的接口，用于通过访问另一个单例令牌来保护对共享数据的访问。它是 GhostCell的扩展，除了品牌令牌外，它还允许更多普通的单例，使数据成为“静态的”</p>
<p>这个库本身也提供了两个单例实现:</p>
<ul>
<li>通过with_token将限定范围的标记令牌作为 GhostCell</li>
<li>通过new_singleton简单地创建一次单例结构</li>
</ul>
<p><a href="https://crates.io/crates/singleton-cell" rel="noopener noreferrer">crate 地址</a></p>
<h1>Learning Rust: Interfacing with C</h1>
<p>通过本文学习如何使用 Rust 调用 C 方法以及如何在 C 中调用 Rust 方法.</p>
<p><a href="https://piware.de/post/2021-08-27-rust-and-c/" rel="noopener noreferrer">原文链接</a></p>
<h1>RefineDB: Rust编写的强类型文档数据库</h1>
<p>运行在任何事务性 键值存储上的 强类型 文档数据库。</p>
<p>目前支持的 backends 有:</p>
<ul>
<li>FoundationDB</li>
<li>单机部署的 SQLite。</li>
<li>一个简单的内存键值存储。</li>
</ul>
<p><a href="https://github.com/losfair/RefineDB" rel="noopener noreferrer">github 地址</a></p>
<p>--</p>
<p>From 日报小组 BobQin，FBI小白</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">远程办公，不限地域，缴纳社保公积金，周末双休，告别 996，拒绝 007，Nervina Labs 欢迎你！</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=a90b0635-b332-4e23-9a44-eb9282f519ef">
<div class="article-summary-box-inner">
<span><p>rust开发工程师
岗位职责：1、负责智能合约的开发及设计；2、负责区块链业务系统分析与设计工作；3、负责智能合约代码测试、运行和维护。任职要求：1、计算机相关专业本科及以上学历，3年以上工作经验；2、熟练掌握 C/C++、Rust 等系统开发语言至少一种，至少有过两年相关开发经验；3、对数据结构和算法，对密码学，安全协议和加密算法有研究者优先；4、优秀的英语文档撰写与阅读能力者优先；5、了解区块链，有合约开发经验更佳。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">构建安全易用的链表</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=273831e7-932d-476f-9d31-323151afb123">
<div class="article-summary-box-inner">
<span><p>写了一个链表的Crate，愿景是构建安全且易用的链表。</p>
<p>欢迎大家来找茬（Bug）或提需求 :)</p>
<p>Crate IO链接：<a href="https://crates.io/crates/cyclic_list" rel="noopener noreferrer">https://crates.io/crates/cyclic_list</a>;</p>
<p>GitHub链接：<a href="https://github.com/whjpji/cyclic_list" rel="noopener noreferrer">https://github.com/whjpji/cyclic_list</a></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课：《 Rust 异步编程入门 Future 》|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>
<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是 Rust 异步编程的核心基础。</p>
<h3>课程大纲</h3>
<p>1、为什么需要异步.</p>
<p>2、理解异步编程模型.</p>
<p>3、Future 编程模型讲解.</p>
<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>
<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>异步引擎 C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D 游戏引擎</h3>
<ul>
<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>
<li><strong>延迟着色</strong></li>
<li><strong>内置保存/加载</strong></li>
<li><strong>独立场景编辑器</strong></li>
<li><strong>高级物理模型</strong></li>
<li><strong>分层模型资源</strong></li>
<li><strong>几何实例化</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>
<p><strong>课程时间：</strong> 2021年8月22日 20:30-21:30</p>
<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>
<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>
<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>
<h3>课程大纲</h3>
<ol>
<li>
<p>什么是分布式追踪系统OpenTracing及应用场景</p>
</li>
<li>
<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>
</li>
<li>
<p>为什么需要tokio-rs/tracing库</p>
</li>
<li>
<p>演示Datafuse项目中tokio-rs/tracing的使用</p>
</li>
</ol>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">论坛github账户无法登录解决笔记</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>有反映这两天github账户无法登录了。</p>
<p>报这个错：</p>
<pre><code>get github user info err
</code></pre>
<p>查了几个地方：</p>
<ol>
<li>代码是否运行正常：Ok</li>
<li>https代理是否正常：Ok</li>
<li>检查了github返回日志，发现是：</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>进入这个地址一看：<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>
<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>
<p>特此记录。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 的 Future 与 Javascript 的 Promise 功能对照参考</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>
<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>
<blockquote>
<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* 什么都不做 */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被 （1）跨线程传递 （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：《通过实战理解 Rust 宏》| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>
<p><strong>课程时间：</strong> 2021年8月15日 20:30-21:30</p>
<p><strong>课程介绍：</strong></p>
<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>这就是通过宏实现配置的统一行为，代码参考：
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>
<h3>课程大纲</h3>
<ul>
<li>什么是 Rust 宏</li>
<li>什么是宏运行原理</li>
<li>如何创建 Rust 宏过程</li>
<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>
</ul>
<p><strong>讲师介绍</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：理解Rust的所有权| Vol 2</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c107b830-9fe1-43dd-94a3-9efcd5544205">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：《理解Rust所有权》</strong></p>
<p><strong>课程时间：2021年8月8日 20:30-21:30</strong></p>
<p><strong>嘉宾讲师： 苏林</strong></p>
<p><strong>嘉宾介绍：</strong></p>
<p>Rust中文社区成员，多点Dmall技术Leader，前折800互联网研发团队负责人、10余年一线研发经验。具有多年的软件开发经验, 熟练Ruby、Java、Rust等开发语言, 同时也参与过Rust中文社区日报维护工作。</p>
<p><strong>课程介绍</strong></p>
<p>本次课程通过10个左右的小例子，带大家理解一下Rust的所有权，Rust引用和借用，Rust变量克隆和复制的理念。</p>
<p><strong>参加课程</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/Rust-pbc-1.jpg" alt></p>
<p><strong>课程规划</strong></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">数据表 Timestamp 日期 Serialize</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2ff8a69e-59bb-4502-87c0-c3416ffae8a0">
<div class="article-summary-box-inner">
<span><p>主要参考：<a href="https://github.com/rustcc/forustm" rel="noopener noreferrer">Rustcc网站源码库</a></p>
<p>在处理数据表中日期相关数据时，Seralize序列化相关操作会报错，提示 DateTime 字段不识别，
查了 rustcc 源码才发现依赖中需要开启相应的feature。特此记录。</p>
<h2>1.依赖的库：</h2>
<pre><code>[dependencies]
# 日期时间处理 需要开启 serde 特征 支持序列化
chrono = { version = "0.4.19", features = ["serde"] }

# 数据库ORM
diesel = { version = "1.4.4", features = ["postgres", "chrono", "uuid", "r2d2"] }
dotenv = "0.15.0"
serde = { version = "1.0.127", features = ["derive"] }
serde_json = "1.0.66"
uuid = { version = "0.8.2", features = ["serde", "v4"] }
</code></pre>
<h2>2.创建数据表</h2>
<pre><code>CREATE TABLE characters (
    id SERIAL PRIMARY KEY,
    name VARCHAR(128) UNIQUE NOT NULL,
    age INTEGER NOT NULL DEFAULT 0,
    friends VARCHAR NOT NULL DEFAULT '',
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
)
</code></pre>
<h2>3.数据表对应的 model</h2>
<pre><code>use chrono::{NaiveDateTime};
use serde::{Deserialize, Serialize};

#[derive(Queryable, Serialize, Deserialize, Debug)]
pub struct Characters {
    pub id: i32,
    pub name: String,
    pub age: i32,
    pub friends: String,
    // 这里的 NaiveDateTime 日期格式序列化需要开启相关 features
    pub created_at: NaiveDateTime,
}
</code></pre>
<h2>4.获取数据</h2>
<pre><code>use db::schema::characters;
use db::{get_connection};
use db::models::{Characters, NewCharacter};
use db::schema::characters::dsl::*;
use diesel::QueryDsl;
use diesel::prelude::*;

fn main() {
    let conn = get_connection();

    // 查询年龄大于30的10条数据
    let arr: Vec&lt;Characters&gt; = characters.filter(characters::age.gt(30))
        .limit(10)
        .load::&lt;Characters&gt;(&amp;conn)
        .expect("Loading Error");

    let date_arr = arr.iter()
        .map(|item| {
	    // 数据格式化
            let t = item.created_at.format("%Y-%m-%d %H:%M:%S").to_string();
            println!("{} {}", item.name, t);
            t
        })
        .collect::&lt;Vec&lt;String&gt;&gt;();
}
</code></pre>
<p>输出结果类似：</p>
<pre><code>Box 2021-08-05 09:39:34
Bobe 2021-08-05 09:39:34
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cargo workspace config</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c3dcce30-1fc0-4819-8992-142365c7e21c">
<div class="article-summary-box-inner">
<span><p><a href="https://kaisery.github.io/trpl-zh-cn/ch14-03-cargo-workspaces.html" rel="noopener noreferrer">Workspace 文档链接</a></p>
<h2>目录结构</h2>
<pre><code>workspace-test/
    Cargo.toml
    db/
        src/
            bin/
                init.rs
        Cargo.tml
</code></pre>
<h2>workspace</h2>
<p>workspace-test/Cargo.toml</p>
<pre><code>[workspace]
members = ["db"]
default-member = "db"
</code></pre>
<h2>子项目</h2>
<p>workspace-test/db/Cargo.toml</p>
<pre><code>[package]
name = "db"
version = "0.1.0"
edition = "2018"

[dependencies]

# 可选的可执行文件配置
# [[bin]]
# name = "init"
# path = "src/bin/init.rs"
</code></pre>
<h2>操作</h2>
<pre><code># 运行 init
cargo run --bin init
# -p 指定项目
cargo run -p db --bin init
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 异步编程浅悟（一）</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=120035c3-944d-4a79-9b3a-8390697a6e13">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>异步编程浅悟（一）</h1>
<p>不同于<code>javascript</code>的<code>new Promise((resolve, reject) =&gt; {...})</code>构造即运行，<code>Rust</code>中的<code>Future</code>是·惰性·状态机。这体现为：</p>
<ol>
<li>【调用异步函数】或【执行异步块】仅只构造一个<code>Future trait object</code>。</li>
<li>因为<code>Future</code>是惰性状态机，所以它不会自动执行【异步函数】或【异步块】内的任何一行代码 --- 此点与<code>javascript</code>的·活性·状态机完全不同。相反，需要人工激活触发。</li>
<li>人工启动<code>Future</code>运行，又分为两个场景的两种情况：
<ol>
<li>
<p>已经在<code>async fn</code>内，<code>Future.await</code>激活。但，同时<strong>阻塞</strong>当前异步程序执行流。</p>
</li>
<li>
<p>在<code>async fn</code>外，需要借助由【运行时】提供的【执行器】。就<code>async-std</code>库而言，有两个选择：</p>
<ol>
<li><code>task::block_on(Future)</code> 执行<code>Future</code>且阻塞当前线程直到<code>Future</code>被完成。</li>
<li><code>task::spawn(Future)</code>仅执行<code>Future</code>和不阻塞当前线程。</li>
</ol>
<p>无论选择上面哪种方式，若在<code>Future</code>执行期间出现了<code>panic</code>，其都会终止（<code>abort</code>）正在共享同一个执行线程（<code>thread</code>）的所有<code>task</code>（·无栈·协程）的运行。</p>
</li>
</ol>
</li>
</ol>
<p>题外话，</p>
<ol>
<li>绿色线程是·有栈·协程；异步函数与异步块是·无栈·协程。</li>
<li>在<code>async-std</code>库的词汇表内，协程被称作<code>task</code>而不是惯例的<code>coroutine</code>。</li>
<li><code>task::spawn(Future)</code>也能被使用于<code>async fn</code>或<code>async {...}</code>内。它被用来代替<code>.await</code>指令，以<strong>非阻塞</strong><code>async fn</code>或<code>async {...}</code>的方式，激活与执行一个<code>Future</code>实例。</li>
</ol>
<h2>例程</h2>
<pre><code>async fn accept_loop(addr: impl ToSocketAddrs) -&gt; Result&lt;()&gt; {
    // 1. TcpListener::bind(addr) 返回 Future
    // 2. .await 于 Future 取得 Result&lt;T, E&gt;
    // 3. Result&lt;T, E&gt;? 再拿得 Ok&lt;T&gt; 中的 T
    let listener = TcpListener::bind(addr).await?; // 异步函数内的人工启动 Future
    let mut incoming = listener.incoming();
    // 因为没有从语言层面支持 async for loop，所以 while loop + Iterator&lt;Item = T&gt; 来模拟之。
    while let Some(stream) = incoming.next().await {
        // TODO
    }
    Ok(())
}
fn main() {
    let fut = accept_loop("127.0.0.1:8080");
    task::block_on(fut); // 异步函数外的人工启动 Future
}
</code></pre>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-01T01:30:00Z">09-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Want To Reduce Labeling Cost? GPT-3 Can Help. (arXiv:2108.13487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13487">
<div class="article-summary-box-inner">
<span><p>Data annotation is a time-consuming and labor-intensive process for many NLP
tasks. Although there exist various methods to produce pseudo data labels, they
are often task-specific and require a decent amount of labeled data to start
with. Recently, the immense language model GPT-3 with 175 billion parameters
has achieved tremendous improvement across many few-shot learning tasks. In
this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to
train other models. We find that, to make the downstream model achieve the same
performance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use
labels from GPT-3 than using labels from humans. Furthermore, we propose a
novel framework of combining pseudo labels from GPT-3 with human labels, which
leads to even better performance with limited labeling budget. These results
present a cost-effective data labeling methodology that is generalizable to
many practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Exaggeration Detection of Health Science Press Releases. (arXiv:2108.13493v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13493">
<div class="article-summary-box-inner">
<span><p>Public trust in science depends on honest and factual communication of
scientific papers. However, recent studies have demonstrated a tendency of news
media to misrepresent scientific papers by exaggerating their findings. Given
this, we present a formalization of and study into the problem of exaggeration
detection in science communication. While there are an abundance of scientific
papers and popular media articles written about them, very rarely do the
articles include a direct link to the original paper, making data collection
challenging. We address this by curating a set of labeled press
release/abstract pairs from existing expert annotated studies on exaggeration
in press releases of scientific papers suitable for benchmarking the
performance of machine learning models on the task. Using limited data from
this and previous studies on exaggeration detection in science, we introduce
MT-PET, a multi-task version of Pattern Exploiting Training (PET), which
leverages knowledge from complementary cloze-style QA tasks to improve few-shot
learning. We demonstrate that MT-PET outperforms PET and supervised learning
both when data is limited, as well as when there is an abundance of data for
the main task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConVIScope: Visual Analytics for Exploring Patient Conversations. (arXiv:2108.13514v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13514">
<div class="article-summary-box-inner">
<span><p>The proliferation of text messaging for mobile health is generating a large
amount of patient-doctor conversations that can be extremely valuable to health
care professionals. We present ConVIScope, a visual text analytic system that
tightly integrates interactive visualization with natural language processing
in analyzing patient-doctor conversations. ConVIScope was developed in
collaboration with healthcare professionals following a user-centered iterative
design. Case studies with six domain experts suggest the potential utility of
ConVIScope and reveal lessons for further developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution. (arXiv:2108.13530v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13530">
<div class="article-summary-box-inner">
<span><p>We consider the task of document-level entity linking (EL), where it is
important to make consistent decisions for entity mentions over the full
document jointly. We aim to leverage explicit "connections" among mentions
within the document itself: we propose to join the EL task with that of
coreference resolution (coref). This is complementary to related works that
exploit either (i) implicit document information (e.g., latent relations among
entity mentions, or general language models) or (ii) connections between the
candidate links (e.g, as inferred from the external knowledge base).
Specifically, we cluster mentions that are linked via coreference, and enforce
a single EL for all of the clustered mentions together. The latter constraint
has the added benefit of increased coverage by joining EL candidate lists for
the thus clustered mentions. We formulate the coref+EL problem as a structured
prediction task over directed trees and use a globally normalized model to
solve it. Experimental results on two datasets show a boost of up to +5%
F1-score on both coref and EL tasks, compared to their standalone counterparts.
For a subset of hard cases, with individual mentions lacking the correct EL in
their candidate entity list, we obtain a +50% increase in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Characterization of Divisive Topics Online: Case Studies on Contentiousness in Abortion, Climate Change, and Gun Control. (arXiv:2108.13556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13556">
<div class="article-summary-box-inner">
<span><p>As public discourse continues to move and grow online, conversations about
divisive topics on social media platforms have also increased. These divisive
topics prompt both contentious and non-contentious conversations. Although what
distinguishes these conversations, often framed as what makes these
conversations contentious, is known in broad strokes, much less is known about
the linguistic signature of these conversations. Prior work has shown that
contentious content and structure can be a predictor for this task, however,
most of them have been focused on conversation in general, very specific
events, or complex structural analysis. Additionally, many models used in prior
work have lacked interpret-ability, a key factor in online moderation. Our work
fills these gaps by focusing on conversations from highly divisive topics
(abortion, climate change, and gun control), operationalizing a set of novel
linguistic and conversational characteristics and user factors, and
incorporating them to build interpretable models. We demonstrate that such
characteristics can largely improve the performance of prediction on this task,
and also enable nuanced interpretability. Our case studies on these three
contentious topics suggest that certain generic linguistic characteristics are
highly correlated with contentiousness in conversations while others
demonstrate significant contextual influences on specific divisive topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP. (arXiv:2108.13587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13587">
<div class="article-summary-box-inner">
<span><p>Transformers are the dominant architecture in NLP, but their training and
fine-tuning is still very challenging. In this paper, we present the design and
implementation of a visual analytic framework for assisting researchers in such
process, by providing them with valuable insights about the model's intrinsic
properties and behaviours. Our framework offers an intuitive overview that
allows the user to explore different facets of the model (e.g., hidden states,
attention) through interactive visualization, and allows a suite of built-in
algorithms that compute the importance of model components and different parts
of the input sequence. Case studies and feedback from a user focus group
indicate that the framework is useful, and suggest several improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Adversarial Fine-Tuning Benefit BERT?. (arXiv:2108.13602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13602">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) is one of the most reliable methods for defending
against adversarial attacks in machine learning. Variants of this method have
been used as regularization mechanisms to achieve SOTA results on NLP
benchmarks, and they have been found to be useful for transfer learning and
continual learning. We search for the reasons for the effectiveness of AT by
contrasting vanilla and adversarially fine-tuned BERT models. We identify
partial preservation of BERT's syntactic abilities during fine-tuning as the
key to the success of AT. We observe that adversarially fine-tuned models
remain more faithful to BERT's language modeling behavior and are more
sensitive to the word order. As concrete examples of syntactic abilities, an
adversarially fine-tuned model could have an advantage of up to 38% on anaphora
agreement and up to 11% on dependency parsing. Our analysis demonstrates that
vanilla fine-tuning oversimplifies the sentence representation by focusing
heavily on one or a few label-indicative words. AT, however, moderates the
effect of these influential words and encourages representational diversity.
This allows for a more hierarchical representation of a sentence and leads to
the mitigation of BERT's loss of syntactic abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Text Classification of Transliterated Hindi and Malayalam. (arXiv:2108.13620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13620">
<div class="article-summary-box-inner">
<span><p>Transliteration is very common on social media, but transliterated text is
not adequately handled by modern neural models for various NLP tasks. In this
work, we combine data augmentation approaches with a Teacher-Student training
scheme to address this issue in a cross-lingual transfer setting for
fine-tuning state-of-the-art pre-trained multilingual language models such as
mBERT and XLM-R. We evaluate our method on transliterated Hindi and Malayalam,
also introducing new datasets for benchmarking on real-world scenarios: one on
sentiment classification in transliterated Malayalam, and another on crisis
tweet classification in transliterated Hindi and Malayalam (related to the 2013
North India and 2018 Kerala floods). Our method yielded an average improvement
of +5.6% on mBERT and +4.7% on XLM-R in F1 scores over their strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Sliding Window for Meeting Summarization. (arXiv:2108.13629v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13629">
<div class="article-summary-box-inner">
<span><p>Recently abstractive spoken language summarization raises emerging research
interest, and neural sequence-to-sequence approaches have brought significant
performance improvement. However, summarizing long meeting transcripts remains
challenging. Due to the large length of source contents and targeted summaries,
neural models are prone to be distracted on the context, and produce summaries
with degraded quality. Moreover, pre-trained language models with input length
limitations cannot be readily applied to long sequences. In this work, we first
analyze the linguistic characteristics of meeting transcripts on a
representative corpus, and find that the sentences comprising the summary
correlate with the meeting agenda. Based on this observation, we propose a
dynamic sliding window strategy for meeting summarization. Experimental results
show that performance benefit from the proposed method, and outputs obtain
higher factual consistency than the base model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory. (arXiv:2108.13630v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13630">
<div class="article-summary-box-inner">
<span><p>Lip reading, aiming to recognize spoken sentences according to the given
video of lip movements without relying on the audio stream, has attracted great
interest due to its application in many scenarios. Although prior works that
explore lip reading have obtained salient achievements, they are all trained in
a non-simultaneous manner where the predictions are generated requiring access
to the full video. To breakthrough this constraint, we study the task of
simultaneous lip reading and devise SimulLR, a simultaneous lip Reading
transducer with attention-guided adaptive memory from three aspects: (1) To
address the challenge of monotonic alignments while considering the syntactic
structure of the generated sentences under simultaneous setting, we build a
transducer-based model and design several effective training strategies
including CTC pre-training, model warm-up and curriculum learning to promote
the training of the lip reading transducer. (2) To learn better spatio-temporal
representations for simultaneous encoder, we construct a truncated 3D
convolution and time-restricted self-attention layer to perform the
frame-to-frame interaction within a video segment containing fixed number of
frames. (3) The history information is always limited due to the storage in
real-time scenarios, especially for massive video data. Therefore, we devise a
novel attention-guided adaptive memory to organize semantic information of
history segments and enhance the visual representations with acceptable
computation-aware latency. The experiments show that the SimulLR achieves the
translation speedup 9.10$\times$ compared with the state-of-the-art
non-simultaneous methods, and also obtains competitive results, which indicates
the effectiveness of our proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Classes through Word Attribution. (arXiv:2108.13653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13653">
<div class="article-summary-box-inner">
<span><p>In recent years, several methods have been proposed for explaining individual
predictions of deep learning models, yet there has been little study of how to
aggregate these predictions to explain how such models view classes as a whole
in text classification tasks. In this work, we propose a method for explaining
classes using deep learning models and the Integrated Gradients feature
attribution technique by aggregating explanations of individual examples in
text classification to general descriptions of the classes. We demonstrate the
approach on Web register (genre) classification using the XML-R model and the
Corpus of Online Registers of English (CORE), finding that the method
identifies plausible and discriminative keywords characterizing all but the
smallest class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discretized Integrated Gradients for Explaining Language Models. (arXiv:2108.13654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13654">
<div class="article-summary-box-inner">
<span><p>As a prominent attribution-based explanation algorithm, Integrated Gradients
(IG) is widely adopted due to its desirable explanation axioms and the ease of
gradient computation. It measures feature importance by averaging the model's
output gradient interpolated along a straight-line path in the input data
space. However, such straight-line interpolated points are not representative
of text data due to the inherent discreteness of the word embedding space. This
questions the faithfulness of the gradients computed at the interpolated points
and consequently, the quality of the generated explanations. Here we propose
Discretized Integrated Gradients (DIG), which allows effective attribution
along non-linear interpolation paths. We develop two interpolation strategies
for the discrete word embedding space that generates interpolation points that
lie close to actual words in the embedding space, yielding more faithful
gradient computation. We demonstrate the effectiveness of DIG over IG through
experimental and human evaluations on multiple sentiment classification
datasets. We provide the source code of DIG to encourage reproducible research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MELM: Data Augmentation with Masked Entity Language Modeling for Cross-lingual NER. (arXiv:2108.13655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13655">
<div class="article-summary-box-inner">
<span><p>Data augmentation for cross-lingual NER requires fine-grained control over
token labels of the augmented text. Existing augmentation approach based on
masked language modeling may replace a labeled entity with words of a different
class, which makes the augmented sentence incompatible with the original label
sequence, and thus hurts the performance.We propose a data augmentation
framework with Masked-Entity Language Modeling (MELM) which effectively ensures
the replacing entities fit the original labels. Specifically, MELM linearizes
NER labels into sentence context, and thus the fine-tuned MELM is able to
predict masked tokens by explicitly conditioning on their labels. Our MELM is
agnostic to the source of data to be augmented. Specifically, when MELM is
applied to augment training data of the source language, it achieves up to 3.5%
F1 score improvement for cross-lingual NER. When unlabeled target data is
available and MELM can be further applied to augment pseudo-labeled target
data, the performance gain reaches 5.7%. Moreover, MELM consistently
outperforms multiple baseline methods for data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13658">
<div class="article-summary-box-inner">
<span><p>The understanding of time expressions includes two sub-tasks: recognition and
normalization. In recent years, significant progress has been made in the
recognition of time expressions while research on normalization has lagged
behind. Existing SOTA normalization methods highly rely on rules or grammars
designed by experts, which limits their performance on emerging corpora, such
as social media texts. In this paper, we model time expression normalization as
a sequence of operations to construct the normalized temporal value, and we
present a novel method called ARTime, which can automatically generate
normalization rules from training data without expert interventions.
Specifically, ARTime automatically captures possible operation sequences from
annotated data and generates normalization rules on time expressions with
common surface forms. The experimental results show that ARTime can
significantly surpass SOTA methods on the Tweets benchmark, and achieves
competitive results with existing expert-engineered rule methods on the
TempEval-3 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gray Cycles of Maximum Length Related to k-Character Substitutions. (arXiv:2108.13659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13659">
<div class="article-summary-box-inner">
<span><p>Given a word binary relation $\tau$ we define a $\tau$-Gray cycle over a
finite language $X$ to be a permutation $\left(w_{[i]}\right)_{0\le i\le
|X|-1}$ of $X$ such that each word $w_i$ is an image of the previous word
$w_{i-1}$ by $\tau$. In that framework, we introduce the complexity measure
$\lambda(n)$, equal to the largest cardinality of a language $X$ having words
of length at most $n$, and such that a $\tau$-Gray cycle over $X$ exists. The
present paper is concerned with the relation $\tau=\sigma_k$, the so-called
$k$-character substitution, where $(u,v)$ belongs to $\sigma_k$ if, and only
if, the Hamming distance of $u$ and $v$ is $k$. We compute the bound
$\lambda(n)$ for all cases of the alphabet cardinality and the argument $n$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13679">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to formulate the task-oriented dialogue system as
the purely natural language generation task, so as to fully leverage the
large-scale pre-trained models like GPT-2 and simplify complicated
delexicalization prepossessing. However, directly applying this method heavily
suffers from the dialogue entity inconsistency caused by the removal of
delexicalized tokens, as well as the catastrophic forgetting problem of the
pre-trained model during fine-tuning, leading to unsatisfactory performance. To
alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which
incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve
better performance on transfer learning and dialogue entity generation.
Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ
dataset demonstrate that our proposed approach significantly outperforms
baseline models with a remarkable performance on automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization. (arXiv:2108.13684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13684">
<div class="article-summary-box-inner">
<span><p>Despite recent progress in abstractive summarization, systems still suffer
from faithfulness errors. While prior work has proposed models that improve
faithfulness, it is unclear whether the improvement comes from an increased
level of extractiveness of the model outputs as one naive way to improve
faithfulness is to make summarization models more extractive. In this work, we
present a framework for evaluating the effective faithfulness of summarization
systems, by generating a faithfulnessabstractiveness trade-off curve that
serves as a control at different operating points on the abstractiveness
spectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as
well as a recently proposed method for improving faithfulness, are both worse
than the control at the same level of abstractiveness. Finally, we learn a
selector to identify the most faithful and abstractive summary for a given
document, and show that this system can attain higher faithfulness scores in
human evaluations while being more abstractive than the baseline system on two
datasets. Moreover, we show that our system is able to achieve a better
faithfulness-abstractiveness trade-off than the control at the same level of
abstractiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Grounded Dialogue with Reward-Driven Knowledge Selection. (arXiv:2108.13686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13686">
<div class="article-summary-box-inner">
<span><p>Knowledge-grounded dialogue is a task of generating a fluent and informative
response based on both conversation context and a collection of external
knowledge, in which knowledge selection plays an important role and attracts
more and more research interest. However, most existing models either select
only one knowledge or use all knowledge for responses generation. The former
may lose valuable information in discarded knowledge, while the latter may
bring a lot of noise. At the same time, many approaches need to train the
knowledge selector with knowledge labels that indicate ground-truth knowledge,
but these labels are difficult to obtain and require a large number of manual
annotations. Motivated by these issues, we propose Knoformer, a dialogue
response generation model based on reinforcement learning, which can
automatically select one or more related knowledge from the knowledge pool and
does not need knowledge labels during training. Knoformer is evaluated on two
knowledge-guided conversation datasets, and achieves state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TNNT: The Named Entity Recognition Toolkit. (arXiv:2108.13700v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13700">
<div class="article-summary-box-inner">
<span><p>Extraction of categorised named entities from text is a complex task given
the availability of a variety of Named Entity Recognition (NER) models and the
unstructured information encoded in different source document formats.
Processing the documents to extract text, identifying suitable NER models for a
task, and obtaining statistical information is important in data analysis to
make informed decisions. This paper presents TNNT, a toolkit that automates the
extraction of categorised named entities from unstructured information encoded
in source documents, using diverse state-of-the-art Natural Language Processing
(NLP) tools and NER models. TNNT integrates 21 different NER models as part of
a Knowledge Graph Construction Pipeline (KGCP) that takes a document set as
input and processes it based on the defined settings, applying the selected
blocks of NER models to output the results. The toolkit generates all results
with an integrated summary of the extracted entities, enabling enhanced data
analysis to support the KGCP, and also, to aid further NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plan-then-Generate: Controlled Data-to-Text Generation via Planning. (arXiv:2108.13740v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13740">
<div class="article-summary-box-inner">
<span><p>Recent developments in neural networks have led to the advance in
data-to-text generation. However, the lack of ability of neural models to
control the structure of generated output can be limiting in certain real-world
applications. In this study, we propose a novel Plan-then-Generate (PlanGen)
framework to improve the controllability of neural data-to-text models.
Extensive experiments and analyses are conducted on two benchmark datasets,
ToTTo and WebNLG. The results show that our model is able to control both the
intra-sentence and inter-sentence structure of the generated output.
Furthermore, empirical comparisons against previous state-of-the-art methods
show that our model improves the generation quality as well as the output
diversity as judged by human and automatic evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13741">
<div class="article-summary-box-inner">
<span><p>Recent researches have demonstrated that BERT shows potential in a wide range
of natural language processing tasks. It is adopted as an encoder for many
state-of-the-art automatic summarizing systems, which achieve excellent
performance. However, so far, there is not much work done for Vietnamese. In
this paper, we showcase how BERT can be implemented for extractive text
summarization in Vietnamese. We introduce a novel comparison between different
multilingual and monolingual BERT models. The experiment results indicate that
monolingual models produce promising results compared to other multilingual
models and previous text summarizing models for Vietnamese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Search Engine for Discovery of Biomedical Challenges and Directions. (arXiv:2108.13751v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13751">
<div class="article-summary-box-inner">
<span><p>The ability to keep track of scientific challenges, advances and emerging
directions is a fundamental part of research. However, researchers face a flood
of papers that hinders discovery of important knowledge. In biomedicine, this
directly impacts human lives. To address this problem, we present a novel task
of extraction and search of scientific challenges and directions, to facilitate
rapid knowledge discovery. We construct and release an expert-annotated corpus
of texts sampled from full-length papers, labeled with novel semantic
categories that generalize across many types of challenges and directions. We
focus on a large corpus of interdisciplinary work relating to the COVID-19
pandemic, ranging from biomedicine to areas such as AI and economics. We apply
a model trained on our data to identify challenges and directions across the
corpus and build a dedicated search engine for this information. In studies
with researchers, including those working directly on COVID-19, we outperform a
popular scientific search engine in assisting knowledge discovery. Finally, we
show that models trained on our resource generalize to the wider biomedical
domain, highlighting its broad utility. We make our data, model and search
engine publicly available. https://challenges.apps.allenai.org
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience. (arXiv:2108.13759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13759">
<div class="article-summary-box-inner">
<span><p>Pretrained transformer-based models such as BERT have demonstrated
state-of-the-art predictive performance when adapted into a range of natural
language processing tasks. An open problem is how to improve the faithfulness
of explanations (rationales) for the predictions of these models. In this
paper, we hypothesize that salient information extracted a priori from the
training data can complement the task-specific information learned by the model
during fine-tuning on a downstream task. In this way, we aim to help BERT not
to forget assigning importance to informative input tokens when making
predictions by proposing SaLoss; an auxiliary loss function for guiding the
multi-head attention mechanism during training to be close to salient
information extracted a priori using TextRank. Experiments for explanation
faithfulness across five datasets, show that models trained with SaLoss
consistently provide more faithful explanations across four different feature
attribution methods compared to vanilla BERT. Using the rationales extracted
from vanilla BERT and SaLoss models to train inherently faithful classifiers,
we further show that the latter result in higher predictive performance in
downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The five Is: Key principles for interpretable and safe conversational AI. (arXiv:2108.13766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13766">
<div class="article-summary-box-inner">
<span><p>In this position paper, we present five key principles, namely
interpretability, inherent capability to explain, independent data, interactive
learning, and inquisitiveness, for the development of conversational AI that,
unlike the currently popular black box approaches, is transparent and
accountable. At present, there is a growing concern with the use of black box
statistical language models: While displaying impressive average performance,
such systems are also prone to occasional spectacular failures, for which there
is no clear remedy. In an effort to initiate a discussion on possible
alternatives, we outline and exemplify how our five principles enable the
development of conversational AI systems that are transparent and thus safer
for use. We also present some of the challenges inherent in the implementation
of those principles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Network psychometrics and cognitive network science open new ways for detecting, understanding and tackling the complexity of math anxiety: A review. (arXiv:2108.13800v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13800">
<div class="article-summary-box-inner">
<span><p>Math anxiety is a clinical pathology impairing cognitive processing in
math-related contexts. Originally thought to affect only inexperienced,
low-achieving students, recent investigations show how math anxiety is vastly
diffused even among high-performing learners. This review of data-informed
studies outlines math anxiety as a complex system that: (i) cripples
well-being, self-confidence and information processing on both conscious and
subconscious levels, (ii) can be transmitted by social interactions, like a
pathogen, and worsened by distorted perceptions, (iii) affects roughly 20% of
students in 63 out of 64 worldwide educational systems but correlates weakly
with academic performance, and (iv) poses a concrete threat to students'
well-being, computational literacy and career prospects in science. These
patterns underline the crucial need to go beyond performance for estimating
math anxiety. Recent advances with network psychometrics and cognitive network
science provide ideal frameworks for detecting, interpreting and intervening
upon such clinical condition. Merging education research, psychology and data
science, the approaches reviewed here reconstruct psychological constructs as
complex systems, represented either as multivariate correlation models (e.g.
graph exploratory analysis) or as cognitive networks of semantic/emotional
associations (e.g. free association networks or forma mentis networks). Not
only can these interconnected networks detect otherwise hidden levels of math
anxiety but - more crucially - they can unveil the specific layout of
interacting factors, e.g. key sources and targets, behind math anxiety in a
given cohort. As discussed here, these network approaches open concrete ways
for unveiling students' perceptions, emotions and mental well-being, and can
enable future powerful data-informed interventions untangling math anxiety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TREND: Trigger-Enhanced Relation-Extraction Network for Dialogues. (arXiv:2108.13811v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13811">
<div class="article-summary-box-inner">
<span><p>The goal of dialogue relation extraction (DRE) is to identify the relation
between two entities in a given dialogue. During conversations, speakers may
expose their relations to certain entities by some clues, such evidences called
"triggers". However, none of the existing work on DRE tried to detect triggers
and leverage the information for enhancing the performance. This paper proposes
TREND, a multi-tasking BERT-based model which learns to identify triggers for
improving relation extraction. The experimental results show that the proposed
method achieves the state-of-the-art on the benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Open-Domain Question Answering. (arXiv:2108.13817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13817">
<div class="article-summary-box-inner">
<span><p>Open-domain Question Answering (ODQA) has achieved significant results in
terms of supervised learning manner. However, data annotation cannot also be
irresistible for its huge demand in an open domain. Though unsupervised QA or
unsupervised Machine Reading Comprehension (MRC) has been tried more or less,
unsupervised ODQA has not been touched according to our best knowledge. This
paper thus pioneers the work of unsupervised ODQA by formally introducing the
task and proposing a series of key data construction methods. Our exploration
in this work inspiringly shows unsupervised ODQA can reach up to 86%
performance of supervised ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Domain Adaptation for Question Answering using Limited Text Corpora. (arXiv:2108.13854v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13854">
<div class="article-summary-box-inner">
<span><p>Question generation has recently shown impressive results in customizing
question answering (QA) systems to new domains. These approaches circumvent the
need for manually annotated training data from the new domain and, instead,
generate synthetic question-answer pairs that are used for training. However,
existing methods for question generation rely on large amounts of synthetically
generated datasets and costly computational resources, which render these
techniques widely inaccessible when the text corpora is of limited size. This
is problematic as many niche domains rely on small text corpora, which
naturally restricts the amount of synthetic data that can be generated. In this
paper, we propose a novel framework for domain adaptation called contrastive
domain adaptation for QA (CAQA). Specifically, CAQA combines techniques from
question generation and domain-invariant learning to answer out-of-domain
questions in settings with limited text corpora. Here, we train a QA system on
both source data and generated data from the target domain with a contrastive
adaptation loss that is incorporated in the training objective. By combining
techniques from question generation and domain-invariant learning, our model
achieved considerable improvements compared to state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs. (arXiv:2108.13873v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13873">
<div class="article-summary-box-inner">
<span><p>Machine-learning-as-a-service (MLaaS) has attracted millions of users to
their outperforming sophisticated models. Although published as black-box APIs,
the valuable models behind these services are still vulnerable to imitation
attacks. Recently, a series of works have demonstrated that attackers manage to
steal or extract the victim models. Nonetheless, none of the previous stolen
models can outperform the original black-box APIs. In this work, we take the
first step of showing that attackers could potentially surpass victims via
unsupervised domain adaptation and multi-victim ensemble. Extensive experiments
on benchmark datasets and real-world APIs validate that the imitators can
succeed in outperforming the original black-box models. We consider this as a
milestone in the research of imitation attack, especially on NLP APIs, as the
superior performance could influence the defense or even publishing strategy of
API providers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13875">
<div class="article-summary-box-inner">
<span><p>Scenario-based question answering (SQA) requires retrieving and reading
paragraphs from a large corpus to answer a question which is contextualized by
a long scenario description. Since a scenario contains both keyphrases for
retrieval and much noise, retrieval for SQA is extremely difficult. Moreover,
it can hardly be supervised due to the lack of relevance labels of paragraphs
for SQA. To meet the challenge, in this paper we propose a joint
retriever-reader model called JEEVES where the retriever is implicitly
supervised only using QA labels via a novel word weighting mechanism. JEEVES
significantly outperforms a variety of strong baselines on multiple-choice
questions in three SQA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning. (arXiv:2108.13888v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13888">
<div class="article-summary-box-inner">
<span><p>\textbf{P}re-\textbf{T}rained \textbf{M}odel\textbf{s} have been widely
applied and recently proved vulnerable under backdoor attacks: the released
pre-trained weights can be maliciously poisoned with certain triggers. When the
triggers are activated, even the fine-tuned model will predict pre-defined
labels, causing a security threat. These backdoors generated by the poisoning
methods can be erased by changing hyper-parameters during fine-tuning or
detected by finding the triggers. In this paper, we propose a stronger
weight-poisoning attack method that introduces a layerwise weight poisoning
strategy to plant deeper backdoors; we also introduce a combinatorial trigger
that cannot be easily detected. The experiments on text classification tasks
show that previous defense methods cannot resist our weight-poisoning method,
which indicates that our method can be widely applied and may provide hints for
future model robustness studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Like Article, Like Audience: Enforcing Multimodal Correlations for Disinformation Detection. (arXiv:2108.13892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13892">
<div class="article-summary-box-inner">
<span><p>User-generated content (e.g., tweets and profile descriptions) and shared
content between users (e.g., news articles) reflect a user's online identity.
This paper investigates whether correlations between user-generated and
user-shared content can be leveraged for detecting disinformation in online
news articles. We develop a multimodal learning algorithm for disinformation
detection. The latent representations of news articles and user-generated
content allow that during training the model is guided by the profile of users
who prefer content similar to the news article that is evaluated, and this
effect is reinforced if that content is shared among different users. By only
leveraging user information during model optimization, the model does not rely
on user profiling when predicting an article's veracity. The algorithm is
successfully applied to three widely used neural classifiers, and results are
obtained on different datasets. Visualization techniques show that the proposed
model learns feature representations of unseen news articles that better
discriminate between fake and real news texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13897">
<div class="article-summary-box-inner">
<span><p>The MS MARCO ranking dataset has been widely used for training deep learning
models for IR tasks, achieving considerable effectiveness on diverse zero-shot
scenarios. However, this type of resource is scarce in other languages than
English. In this work we present mMARCO, a multilingual version of the MS MARCO
passage ranking dataset comprising 8 languages that was created using machine
translation. We evaluated mMARCO by fine-tuning mono and multilingual
re-ranking models on it. Experimental results demonstrate that multilingual
models fine-tuned on our translated dataset achieve superior effectiveness than
models fine-tuned on the original English version alone. Also, our distilled
multilingual re-ranker is competitive with non-distilled models while having
5.4 times fewer parameters. The translated datasets as well as fine-tuned
models are available at https://github.com/unicamp-dl/mMARCO.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The emojification of sentiment on social media: Collection and analysis of a longitudinal Twitter sentiment dataset. (arXiv:2108.13898v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13898">
<div class="article-summary-box-inner">
<span><p>Social media, as a means for computer-mediated communication, has been
extensively used to study the sentiment expressed by users around events or
topics. There is however a gap in the longitudinal study of how sentiment
evolved in social media over the years. To fill this gap, we develop TM-Senti,
a new large-scale, distantly supervised Twitter sentiment dataset with over 184
million tweets and covering a time period of over seven years. We describe and
assess our methodology to put together a large-scale, emoticon- and emoji-based
labelled sentiment analysis dataset, along with an analysis of the resulting
dataset. Our analysis highlights interesting temporal changes, among others in
the increasing use of emojis over emoticons. We publicly release the dataset
for further research in tasks including sentiment analysis and text
classification of tweets. The dataset can be fully rehydrated including tweet
metadata and without missing tweets thanks to the archive of tweets publicly
available on the Internet Archive, which the dataset is based on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Retrieval Augmented Generation for Zero-shot Slot Filling. (arXiv:2108.13934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13934">
<div class="article-summary-box-inner">
<span><p>Automatically inducing high quality knowledge graphs from a given collection
of documents still remains a challenging problem in AI. One way to make headway
for this problem is through advancements in a related task known as slot
filling. In this task, given an entity query in form of [Entity, Slot, ?], a
system is asked to fill the slot by generating or extracting the missing value
exploiting evidence extracted from relevant passage(s) in the given document
collection. The recent works in the field try to solve this task in an
end-to-end fashion using retrieval-based language models. In this paper, we
present a novel approach to zero-shot slot filling that extends dense passage
retrieval with hard negatives and robust training procedures for retrieval
augmented generation models. Our model reports large improvements on both T-REx
and zsRE slot filling datasets, improving both passage retrieval and slot value
generation, and ranking at the top-1 position in the KILT leaderboard.
Moreover, we demonstrate the robustness of our system showing its domain
adaptation capability on a new variant of the TACRED dataset for slot filling,
through a combination of zero/few-shot learning. We release the source code and
pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools. (arXiv:2108.13961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13961">
<div class="article-summary-box-inner">
<span><p>In the language domain, as in other domains, neural explainability takes an
ever more important role, with feature attribution methods on the forefront.
Many such methods require considerable computational resources and expert
knowledge about implementation details and parameter choices. To facilitate
research, we present Thermostat which consists of a large collection of model
explanations and accompanying analysis tools. Thermostat allows easy access to
over 200k explanations for the decisions of prominent state-of-the-art models
spanning across different NLP tasks, generated with multiple explainers. The
dataset took over 10k GPU hours (&gt; one year) to compile; compute time that the
community now saves. The accompanying software tools allow to analyse
explanations instance-wise but also accumulatively on corpus level. Users can
investigate and compare models, datasets and explainers without the need to
orchestrate implementation details. Thermostat is fully open source,
democratizes explainability research in the language domain, circumvents
redundant computations and increases comparability and replicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13990">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,
but how to properly use them for dialogue state tracking has not been
systematically investigated. In this paper, we study this problem from the
perspectives of pre-training objectives as well as the formats of context
representations. We demonstrate that the choice of pre-training objective makes
a significant difference to the state tracking quality. In particular, we find
that masked span prediction is more effective than auto-regressive language
modeling. We also explore using Pegasus, a span prediction-based pre-training
objective for text summarization, for the state tracking model. We found that
pre-training for the seemingly distant summarization task works surprisingly
well for dialogue state tracking. In addition, we found that while recurrent
state context representation works also reasonably well, the model may have a
hard time recovering from earlier mistakes. We conducted experiments on the
MultiWOZ 2.1-2.4 data sets with consistent observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generative Approach for Mitigating Structural Biases in Natural Language Inference. (arXiv:2108.14006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.14006">
<div class="article-summary-box-inner">
<span><p>Many natural language inference (NLI) datasets contain biases that allow
models to perform well by only using a biased subset of the input, without
considering the remainder features. For instance, models are able to make a
classification decision by only using the hypothesis, without learning the true
relationship between it and the premise. These structural biases lead
discriminative models to learn unintended superficial features and to
generalize poorly out of the training distribution. In this work, we
reformulate the NLI task as a generative task, where a model is conditioned on
the biased subset of the input and the label and generates the remaining subset
of the input. We show that by imposing a uniform prior, we obtain a provably
unbiased model. Through synthetic experiments, we find that this approach is
highly robust to large amounts of bias. We then demonstrate empirically on two
types of natural bias that this approach leads to fully unbiased models in
practice. However, we find that generative models are difficult to train and
they generally perform worse than discriminative baselines. We highlight the
difficulty of the generative modeling task in the context of NLI as a cause for
this worse performance. Finally, by fine-tuning the generative model with a
discriminative objective, we reduce the performance gap between the generative
model and the discriminative baseline, while allowing for a small amount of
bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HUMBO: Bridging Response Generation and Facial Expression Synthesis. (arXiv:1905.11240v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.11240">
<div class="article-summary-box-inner">
<span><p>Spoken dialogue systems that assist users to solve complex tasks such as
movie ticket booking have become an emerging research topic in artificial
intelligence and natural language processing areas. With a well-designed
dialogue system as an intelligent personal assistant, people can accomplish
certain tasks more easily via natural language interactions. Today there are
several virtual intelligent assistants in the market; however, most systems
only focus on textual or vocal interaction. In this paper, we present HUMBO, a
system aiming at generating dialogue responses and simultaneously synthesize
corresponding visual expressions on faces for better multimodal interaction.
HUMBO can (1) let users determine the appearances of virtual assistants by a
single image, and (2) generate coherent emotional utterances and facial
expressions on the user-provided image. This is not only a brand new research
direction but more importantly, an ultimate step toward more human-like virtual
assistants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer. (arXiv:2004.06698v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06698">
<div class="article-summary-box-inner">
<span><p>Visual dialog is a task of answering a sequence of questions grounded in an
image using the previous dialog history as context. In this paper, we study how
to address two fundamental challenges for this task: (1) reasoning over
underlying semantic structures among dialog rounds and (2) identifying several
appropriate answers to the given question. To address these challenges, we
propose a Sparse Graph Learning (SGL) method to formulate visual dialog as a
graph structure learning task. SGL infers inherently sparse dialog structures
by incorporating binary and score edges and leveraging a new structural loss
function. Next, we introduce a Knowledge Transfer (KT) method that extracts the
answer predictions from the teacher model and uses them as pseudo labels. We
propose KT to remedy the shortcomings of single ground-truth labels, which
severely limit the ability of a model to obtain multiple reasonable answers. As
a result, our proposed model significantly improves reasoning capability
compared to baseline methods and outperforms the state-of-the-art approaches on
the VisDial v1.0 dataset. The source code is available at
https://github.com/gicheonkang/SGLKT-VisDial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural CRF Model for Sentence Alignment in Text Simplification. (arXiv:2005.02324v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02324">
<div class="article-summary-box-inner">
<span><p>The success of a text simplification system heavily depends on the quality
and quantity of complex-simple sentence pairs in the training corpus, which are
extracted by aligning sentences between parallel articles. To evaluate and
improve sentence alignment quality, we create two manually annotated
sentence-aligned datasets from two commonly used text simplification corpora,
Newsela and Wikipedia. We propose a novel neural CRF alignment model which not
only leverages the sequential nature of sentences in parallel documents but
also utilizes a neural sentence pair model to capture semantic similarity.
Experiments demonstrate that our proposed approach outperforms all the previous
work on monolingual sentence alignment task by more than 5 points in F1. We
apply our CRF aligner to construct two new text simplification datasets,
Newsela-Auto and Wiki-Auto, which are much larger and of better quality
compared to the existing datasets. A Transformer-based seq2seq model trained on
our datasets establishes a new state-of-the-art for text simplification in both
automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Response Selection with Hierarchical Curriculum Learning. (arXiv:2012.14756v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14756">
<div class="article-summary-box-inner">
<span><p>We study the learning of a matching model for dialogue response selection.
Motivated by the recent finding that models trained with random negative
samples are not ideal in real-world scenarios, we propose a hierarchical
curriculum learning framework that trains the matching model in an
"easy-to-difficult" scheme. Our learning framework consists of two
complementary curricula: (1) corpus-level curriculum (CC); and (2)
instance-level curriculum (IC). In CC, the model gradually increases its
ability in finding the matching clues between the dialogue context and a
response candidate. As for IC, it progressively strengthens the model's ability
in identifying the mismatching information between the dialogue context and a
response candidate. Empirical studies on three benchmark datasets with three
state-of-the-art matching models demonstrate that the proposed learning
framework significantly improves the model performance across various
evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Transformer-Based Generation of Radiology Reports. (arXiv:2102.09777v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09777">
<div class="article-summary-box-inner">
<span><p>Inspired by Curriculum Learning, we propose a consecutive (i.e.,
image-to-text-to-text) generation framework where we divide the problem of
radiology report generation into two steps. Contrary to generating the full
radiology report from the image at once, the model generates global concepts
from the image in the first step and then reforms them into finer and coherent
texts using a transformer architecture. We follow the transformer-based
sequence-to-sequence paradigm at each step. We improve upon the
state-of-the-art on two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06979">
<div class="article-summary-box-inner">
<span><p>Learning sentence embeddings often requires a large amount of labeled data.
However, for most tasks and domains, labeled data is seldom available and
creating it is expensive. In this work, we present a new state-of-the-art
unsupervised method based on pre-trained Transformers and Sequential Denoising
Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.
It can achieve up to 93.1% of the performance of in-domain supervised
approaches. Further, we show that TSDAE is a strong domain adaptation and
pre-training method for sentence embeddings, significantly outperforming other
approaches like Masked Language Model.
</p>
<p>A crucial shortcoming of previous studies is the narrow evaluation: Most work
mainly evaluates on the single task of Semantic Textual Similarity (STS), which
does not require any domain knowledge. It is unclear if these proposed methods
generalize to other domains and tasks. We fill this gap and evaluate TSDAE and
other recent approaches on four different datasets from heterogeneous domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08315">
<div class="article-summary-box-inner">
<span><p>Large language models have shown promising results in zero-shot settings
(Brown et al.,2020; Radford et al., 2019). For example, they can perform
multiple choice tasks simply by conditioning on a question and selecting the
answer with the highest probability.
</p>
<p>However, ranking by string probability can be problematic due to surface form
competition-wherein different surface forms compete for probability mass, even
if they represent the same underlying concept, e.g. "computer" and "PC." Since
probability mass is finite, this lowers the probability of the correct answer,
due to competition from other strings that are valid answers (but not one of
the multiple choice options).
</p>
<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative
scoring function that directly compensates for surface form competition by
simply reweighing each option according to a term that is proportional to its a
priori likelihood within the context of the specific zero-shot task. It
achieves consistent gains in zero-shot performance over both calibrated (Zhao
et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models
over a variety of multiple choice datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08821">
<div class="article-summary-box-inner">
<span><p>This paper presents SimCSE, a simple contrastive learning framework that
greatly advances the state-of-the-art sentence embeddings. We first describe an
unsupervised approach, which takes an input sentence and predicts itself in a
contrastive objective, with only standard dropout used as noise. This simple
method works surprisingly well, performing on par with previous supervised
counterparts. We hypothesize that dropout acts as minimal data augmentation and
removing it leads to a representation collapse. Then, we incorporate annotated
pairs from natural language inference datasets into our contrastive learning
framework, by using "entailment" pairs as positives and "contradiction" pairs
as hard negatives. We evaluate SimCSE on standard semantic textual similarity
(STS) tasks, and our unsupervised and supervised models using BERT-base achieve
an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2 and
2.2 points improvement compared to previous best results. We also show -- both
theoretically and empirically -- that contrastive learning objective
regularizes pre-trained embeddings' anisotropic space to be more uniform, and
it better aligns positive pairs when supervised signals are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction. (arXiv:2106.04847v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04847">
<div class="article-summary-box-inner">
<span><p>Keyphrase Prediction (KP) task aims at predicting several keyphrases that can
summarize the main idea of the given document. Mainstream KP methods can be
categorized into purely generative approaches and integrated models with
extraction and generation. However, these methods either ignore the diversity
among keyphrases or only weakly capture the relation across tasks implicitly.
In this paper, we propose UniKeyphrase, a novel end-to-end learning framework
that jointly learns to extract and generate keyphrases. In UniKeyphrase,
stacked relation layer and bag-of-words constraint are proposed to fully
exploit the latent semantic relation between extraction and generation in the
view of model structure and training process, respectively. Experiments on KP
benchmarks demonstrate that our joint approach outperforms mainstream methods
by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03158">
<div class="article-summary-box-inner">
<span><p>Data augmentation, the artificial creation of training data for machine
learning by transformations, is a widely studied research field across machine
learning disciplines. While it is useful for increasing the generalization
capabilities of a model, it can also address many other challenges and
problems, from overcoming a limited amount of training data over regularizing
the objective to limiting the amount data used to protect privacy. Based on a
precise description of the goals and applications of data augmentation (C1) and
a taxonomy for existing works (C2), this survey is concerned with data
augmentation methods for textual classification and aims to achieve a concise
and comprehensive overview for researchers and practitioners (C3). Derived from
the taxonomy, we divided more than 100 methods into 12 different groupings and
provide state-of-the-art references expounding which methods are highly
promising (C4). Finally, research perspectives that may constitute a building
block for future work are given (C5).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12079">
<div class="article-summary-box-inner">
<span><p>Dialogue systems are widely used in AI to support timely and interactive
communication with users. We propose a general-purpose dialogue system
architecture that leverages computational argumentation to perform reasoning
and provide consistent and explainable answers. We illustrate the system using
a COVID-19 vaccine information case study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Goal-Oriented Script Construction. (arXiv:2107.13189v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13189">
<div class="article-summary-box-inner">
<span><p>The knowledge of scripts, common chains of events in stereotypical scenarios,
is a valuable asset for task-oriented natural language understanding systems.
We propose the Goal-Oriented Script Construction task, where a model produces a
sequence of steps to accomplish a given goal. We pilot our task on the first
multilingual script learning dataset supporting 18 languages collected from
wikiHow, a website containing half a million how-to articles. For baselines, we
consider both a generation-based approach using a language model and a
retrieval-based approach by first retrieving the relevant steps from a large
candidate pool and then ordering them. We show that our task is practical,
feasible but challenging for state-of-the-art Transformer models, and that our
methods can be readily deployed for various other datasets and domains with
decent zero-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02854">
<div class="article-summary-box-inner">
<span><p>Languages differ in terms of the absence or presence of gender features, the
number of gender classes and whether and where gender features are explicitly
marked. These cross-linguistic differences can lead to ambiguities that are
difficult to resolve, especially for sentence-level MT systems. The
identification of ambiguity and its subsequent resolution is a challenging task
for which currently there aren't any specific resources or challenge sets
available. In this paper, we introduce gENder-IT, an English--Italian challenge
set focusing on the resolution of natural gender phenomena by providing
word-level gender tags on the English source side and multiple gender
alternative translations, where needed, on the Italian target side.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (arXiv:2108.05921v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05921">
<div class="article-summary-box-inner">
<span><p>Detecting online hate is a complex task, and low-performing models have
harmful consequences when used for sensitive applications such as content
moderation. Emoji-based hate is a key emerging challenge for automated
detection. We present HatemojiCheck, a test suite of 3,930 short-form
statements that allows us to evaluate performance on hateful language expressed
with emoji. Using the test suite, we expose weaknesses in existing hate
detection models. To address these weaknesses, we create the HatemojiTrain
dataset using a human-and-model-in-the-loop approach. Models trained on these
5,912 adversarial examples perform substantially better at detecting
emoji-based hate, while retaining strong performance on text-only hate. Both
HatemojiCheck and HatemojiTrain are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CushLEPOR: Customised hLEPOR Metric Using LABSE Distilled Knowledge Model to Improve Agreement with Human Judgements. (arXiv:2108.09484v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09484">
<div class="article-summary-box-inner">
<span><p>Human evaluation has always been expensive while researchers struggle to
trust the automatic metrics. To address this, we propose to customise
traditional metrics by taking advantages of the pre-trained language models
(PLMs) and the limited available human labelled scores. We first re-introduce
the hLEPOR metric factors, followed by the Python portable version we developed
which achieved the automatic tuning of the weighting parameters in hLEPOR
metric. Then we present the customised hLEPOR (cushLEPOR) which uses LABSE
distilled knowledge model to improve the metric agreement with human judgements
by automatically optimised factor weights regarding the exact MT language pairs
that cushLEPOR is deployed to. We also optimise cushLEPOR towards human
evaluation data based on MQM and pSQM framework on English-German and
Chinese-English language pairs. The experimental investigations show cushLEPOR
boosts hLEPOR performances towards better agreements to PLMs like LABSE with
much lower cost, and better agreements to human evaluations including MQM and
pSQM scores, and yields much better performances than BLEU (data available at
\url{https://github.com/poethan/cushLEPOR}).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sarcasm Detection in Twitter -- Performance Impact while using Data Augmentation: Word Embeddings. (arXiv:2108.09924v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09924">
<div class="article-summary-box-inner">
<span><p>Sarcasm is the use of words usually used to either mock or annoy someone, or
for humorous purposes. Sarcasm is largely used in social networks and
microblogging websites, where people mock or censure in a way that makes it
difficult even for humans to tell if what is said is what is meant. Failure to
identify sarcastic utterances in Natural Language Processing applications such
as sentiment analysis and opinion mining will confuse classification algorithms
and generate false results. Several studies on sarcasm detection have utilized
different learning algorithms. However, most of these learning models have
always focused on the contents of expression only, leaving the contextual
information in isolation. As a result, they failed to capture the contextual
information in the sarcastic expression. Moreover, some datasets used in
several studies have an unbalanced dataset which impacting the model result. In
this paper, we propose a contextual model for sarcasm identification in twitter
using RoBERTa, and augmenting the dataset by applying Global Vector
representation (GloVe) for the construction of word embedding and context
learning to generate more data and balancing the dataset. The effectiveness of
this technique is tested with various datasets and data augmentation settings.
In particular, we achieve performance gain by 3.2% in the iSarcasm dataset when
using data augmentation to increase 20% of data labeled as sarcastic, resulting
F-score of 40.4% compared to 37.2% without data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions. (arXiv:2108.12189v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12189">
<div class="article-summary-box-inner">
<span><p>This paper presents Macquarie University's participation to the BioASQ
Synergy Task, and BioASQ9b Phase B. In each of these tasks, our participation
focused on the use of query-focused extractive summarisation to obtain the
ideal answers to medical questions. The Synergy Task is an end-to-end question
answering task on COVID-19 where systems are required to return relevant
documents, snippets, and answers to a given question. Given the absence of
training data, we used a query-focused summarisation system that was trained
with the BioASQ8b training data set and we experimented with methods to
retrieve the documents and snippets. Considering the poor quality of the
documents and snippets retrieved by our system, we observed reasonably good
quality in the answers returned. For phase B of the BioASQ9b task, the relevant
documents and snippets were already included in the test data. Our system split
the snippets into candidate sentences and used BERT variants under a sentence
classification setup. The system used the question and candidate sentence as
input and was trained to predict the likelihood of the candidate sentence being
part of the ideal answer. The runs obtained either the best or second best
ROUGE-F1 results of all participants to all batches of BioASQ9b. This shows
that using BERT in a classification setup is a very strong baseline for the
identification of ideal answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Table-to-Text Generation with Prototype Memory. (arXiv:2108.12516v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12516">
<div class="article-summary-box-inner">
<span><p>Neural table-to-text generation models have achieved remarkable progress on
an array of tasks. However, due to the data-hungry nature of neural models,
their performances strongly rely on large-scale training examples, limiting
their applicability in real-world applications. To address this, we propose a
new framework: Prototype-to-Generate (P2G), for table-to-text generation under
the few-shot scenario. The proposed framework utilizes the retrieved
prototypes, which are jointly selected by an IR system and a novel prototype
selector to help the model bridging the structural gap between tables and
texts. Experimental results on three benchmark datasets with three
state-of-the-art models demonstrate that the proposed framework significantly
improves the model performance across various evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation. (arXiv:2108.12582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12582">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable performance of large-scale generative models in
open-domain conversation, they are known to be less practical for building
real-time conversation systems due to high latency. On the other hand,
retrieval models could return responses with much lower latency but show
inferior performance to the large-scale generative models since the
conversation quality is bounded by the pre-defined response set. To take
advantage of both approaches, we propose a new training method called G2R
(Generative-to-Retrieval distillation) that preserves the efficiency of a
retrieval model while leveraging the conversational ability of a large-scale
generative model by infusing the knowledge of the generative model into the
retrieval model. G2R consists of two distinct techniques of distillation: the
data-level G2R augments the dialogue dataset with additional responses
generated by the large-scale generative model, and the model-level G2R
transfers the response quality score assessed by the generative model to the
score of the retrieval model by the knowledge distillation loss. Through
extensive experiments including human evaluation, we demonstrate that our
retrieval-based conversation system trained with G2R shows a substantially
improved performance compared to the baseline retrieval model while showing
significantly lower inference latency than the large-scale generative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scheduled Sampling Based on Decoding Steps for Neural Machine Translation. (arXiv:2108.12963v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12963">
<div class="article-summary-box-inner">
<span><p>Scheduled sampling is widely used to mitigate the exposure bias problem for
neural machine translation. Its core motivation is to simulate the inference
scene during training by replacing ground-truth tokens with predicted tokens,
thus bridging the gap between training and inference. However, vanilla
scheduled sampling is merely based on training steps and equally treats all
decoding steps. Namely, it simulates an inference scene with uniform error
rates, which disobeys the real inference scene, where larger decoding steps
usually have higher error rates due to error accumulations. To alleviate the
above discrepancy, we propose scheduled sampling methods based on decoding
steps, increasing the selection chance of predicted tokens with the growth of
decoding steps. Consequently, we can more realistically simulate the inference
scene during training, thus better bridging the gap between training and
inference. Moreover, we investigate scheduled sampling based on both training
steps and decoding steps for further improvements. Experimentally, our
approaches significantly outperform the Transformer baseline and vanilla
scheduled sampling on three large-scale WMT tasks. Additionally, our approaches
also generalize well to the text summarization task on two popular benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13161">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have contributed significantly to
natural language processing by demonstrating remarkable abilities as few-shot
learners. However, their effectiveness depends mainly on scaling the model
parameters and prompt design, hindering their implementation in most real-world
applications. This study proposes a novel pluggable, extensible, and efficient
approach named DifferentiAble pRompT (DART), which can convert small language
models into better few-shot learners without any prompt engineering. The main
principle behind this approach involves reformulating potential natural
language processing tasks into the task of a pre-trained language model and
differentially optimizing the prompt template as well as the target label with
backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any
pre-trained language models; (ii) Extended to widespread classification tasks.
A comprehensive evaluation of standard NLP tasks demonstrates that the proposed
approach achieves a better few-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12971">
<div class="article-summary-box-inner">
<span><p>A smart contract is a program executed on a blockchain, based on which many
cryptocurrencies are implemented, and is being used for automating
transactions. Due to the large amount of money that smart contracts deal with,
there is a surging demand for a method that can statically and formally verify
them.
</p>
<p>This article describes our type-based static verification tool HELMHOLTZ for
Michelson, which is a statically typed stack-based language for writing smart
contracts that are executed on the blockchain platform Tezos. HELMHOLTZ is
designed on top of our extension of Michelson's type system with refinement
types. HELMHOLTZ takes a Michelson program annotated with a user-defined
specification written in the form of a refinement type as input; it then
typechecks the program against the specification based on the refinement type
system, discharging the generated verification conditions with the SMT solver
Z3. We briefly introduce our refinement type system for the core calculus
Mini-Michelson of Michelson, which incorporates the characteristic features
such as compound datatypes (e.g., lists and pairs), higher-order functions, and
invocation of another contract. \HELMHOLTZ{} successfully verifies several
practical Michelson programs, including one that transfers money to an account
and that checks a digital signature.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-01 01:56:20.021897409 UTC">2021-09-01 01:56:20 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>