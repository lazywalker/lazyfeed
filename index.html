<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-30T01:49:53.272297370Z">08-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">æ„å»ºå®‰å…¨æ˜“ç”¨çš„é“¾è¡¨</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=273831e7-932d-476f-9d31-323151afb123">
<div class="article-summary-box-inner">
<span><p>å†™äº†ä¸€ä¸ªé“¾è¡¨çš„Crateï¼Œæ„¿æ™¯æ˜¯æ„å»ºå®‰å…¨ä¸”æ˜“ç”¨çš„é“¾è¡¨ã€‚</p>
<p>æ¬¢è¿å¤§å®¶æ¥æ‰¾èŒ¬ï¼ˆBugï¼‰æˆ–æéœ€æ±‚ :)</p>
<p>Crate IOé“¾æ¥ï¼š<a href="https://crates.io/crates/cyclic_list" rel="noopener noreferrer">https://crates.io/crates/cyclic_list</a>;</p>
<p>GitHubé“¾æ¥ï¼š<a href="https://github.com/whjpji/cyclic_list" rel="noopener noreferrer">https://github.com/whjpji/cyclic_list</a></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rust æ—¥æŠ¥ã€‘2021-08-29 Tangramï¼šè®­ç»ƒã€éƒ¨ç½²å’Œç›‘æ§æœºå™¨å­¦ä¹ æ¨¡å‹</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=4a218f6c-3c77-4aa0-84d6-90ac2bf1fc7c">
<div class="article-summary-box-inner">
<span><h3>Embedded Rust ç¬¬ä¸€æ­¥ï¼šé€‰æ‹©ä¸€å—æ¿å­</h3>
<p>å†…å®¹æ•´ç†è‡ª <a href="https://github.com/robyoung" rel="noopener noreferrer">robyoung (Rob Young)</a> çš„æ–‡ç« ï¼šFirst steps with Embedded Rust: Selecting a board</p>
<p>æœ‰è¿™ä¹ˆå¤šä»¤äººçœ¼èŠ±ç¼­ä¹±çš„å¾®æ§åˆ¶å™¨å’Œé¡¹ç›®ï¼Œå¯¹äºåµŒå…¥å¼ç»éªŒå¾ˆå°‘çš„äººæ¥è¯´åº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ</p>
<p><strong>æˆ‘ä»¬åœ¨å¼€å‘æ¿ä¸­æƒ³è¦ä»€ä¹ˆï¼Ÿ</strong></p>
<ul>
<li>è‰¯å¥½çš„æ¶æ„æ”¯æŒ</li>
<li>è‰¯å¥½çš„èŠ¯ç‰‡æ”¯æŒ</li>
<li>æ´»è·ƒçš„ç¤¾åŒº</li>
<li>å†…ç½®è°ƒè¯•å™¨</li>
</ul>
<p><strong>æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ¶æ„ï¼Ÿ</strong></p>
<p>æ‹¥æœ‰æœ€å®Œæ•´åº“ã€æœ€è¯¦å°½æŒ‡å—å’Œæœ€å¤§ç¤¾åŒºçš„æ¶æ„æ˜¯ ARM Cortex-Mã€‚ ARM Cortex-M æ˜¯é¢å‘å¾®æ§åˆ¶å™¨åº”ç”¨çš„ä½åŠŸè€—ã€ä½æˆæœ¬å¤„ç†å™¨ã€‚ æŸ¥çœ‹ crates.io ä¸Šçš„ä¸‹è½½é‡è™½è¯´ä¸æ˜¯ä¸€ä¸ªå®Œç¾çš„æŒ‡æ ‡ï¼Œä½†å¯ä»¥è®©æˆ‘ä»¬äº†è§£è§„æ¨¡ä¸Šçš„å·®å¼‚ã€‚åœ¨è¿‡å»çš„ 90 å¤©å†…ï¼Œcortex-m çš„ä¸‹è½½é‡è¶…è¿‡ 250kã€‚ RISC-Vã€AVR æˆ– Xtensa æœ€å¤šæœ‰ 3k æ¬¡ä¸‹è½½ï¼Œcortex-a æœ‰å¤§çº¦ 18k æ¬¡ä¸‹è½½ã€‚ARM Cortex-M ç‹¬æ ‘ä¸€å¸œã€‚</p>
<ul>
<li>AVRï¼šAVR æ˜¯ç”¨äºåµŒå…¥å¼ç³»ç»Ÿçš„ 8 ä½å¾®æ§åˆ¶å™¨ç³»åˆ—ã€‚åœ¨ Rust ç”Ÿæ€ç³»ç»Ÿä¸­ï¼Œå®ƒä»¬å¹¶æ²¡æœ‰å¾—åˆ°å¾ˆå¥½çš„æ”¯æŒã€‚ç›´åˆ°æœ€è¿‘ï¼Œè¿˜éœ€è¦ä½¿ç”¨ rustc çš„ä¸€ä¸ªåˆ†æ”¯æ¥æ„å»º AVRã€‚ ç°åœ¨æœ‰å‡ ä¸ªä¸åŒçš„é€‰æ‹©ï¼Œawesome-avr-rust æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚</li>
<li>ARM Cortex-Aï¼šæ›´å¼ºå¤§çš„å¤šæ ¸ ARM å¤„ç†å™¨ï¼Œä¸“ä¸ºè¿è¡Œæ›´å¤§çš„ä¸œè¥¿è€Œè®¾è®¡ã€‚ é€šå¸¸ä¼šåœ¨å®ƒä»¬ä¸Šè¿è¡Œå®Œæ•´çš„æ“ä½œç³»ç»Ÿã€‚ ä¾‹å¦‚è¿™æ˜¯å¤§å¤šæ•°æ™ºèƒ½æ‰‹æœºå’ŒæŒä¸Šæ¸¸æˆæœºä¸­ä½¿ç”¨çš„æ¶æ„ã€‚æŸ¥çœ‹ <a href="https://crates.io/crates/cortex-a" rel="noopener noreferrer">cortex-a - crates.io: Rust Package Registry</a> äº†è§£æ›´å¤šã€‚</li>
<li>RISC-Vï¼šä¼¼ä¹æ˜¯æœºå™¨æ¶æ„çš„æ–°çƒ­ç‚¹ï¼Œå®ƒæ˜¯ä¸€ç§å…è´¹ä¸”å¼€æ”¾çš„æŒ‡ä»¤é›†æ¶æ„ (ISA)ã€‚ å®ƒä¹Ÿä»ä¸€å¼€å§‹å°±è¢«è®¾è®¡æˆæ¨¡å—åŒ–çš„ï¼Œè¿™æ„å‘³ç€èŠ¯ç‰‡è®¾è®¡äººå‘˜å¯ä»¥åˆ›å»ºå„ç§å„æ ·çš„ä¸“ç”¨èŠ¯ç‰‡ï¼Œè™½ç„¶ç›®å‰å¼€å‘æ¿çš„èŒƒå›´å¾ˆå°ã€‚æœ‰ä¸€ä¸ªæ´»è·ƒçš„ Rust RISC-V ç¤¾åŒºï¼ŒSiFive æˆ– www.riscv.org éƒ½æ˜¯ä¸é”™çš„èµ·ç‚¹ï¼ŒRust æ–¹é¢ï¼Œå¯ä»¥æŸ¥çœ‹ riscv crateã€‚</li>
<li>Xtensaï¼šæœ€å—æ¬¢è¿çš„ä¸»æ¿ç»„æ˜¯æ¥è‡ª Espressif çš„ ESP32 ç³»åˆ—èŠ¯ç‰‡ã€‚å®ƒä»¬æ˜¯å°å‹ã€å»‰ä»·ã€æ”¯æŒ WiFi çš„ç”µè·¯æ¿ã€‚ éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¹¶éæ‰€æœ‰ ESP32 å¼€å‘æ¿éƒ½ä½¿ç”¨ Xtensa èŠ¯ç‰‡ï¼Œæ–°çš„ ESP32-C3 æ˜¯åŸºäº RISC-V çš„ã€‚åœ¨ Xtensa èŠ¯ç‰‡ä¸Šä½¿ç”¨ Rust çš„æœ€å¤§éšœç¢å¯èƒ½æ˜¯ llvm ä¸æ”¯æŒå®ƒï¼Œå› æ­¤éœ€è¦æ„å»º Rust çš„ forkï¼š<a href="https://github.com/esp-rs/rust" rel="noopener noreferrer">esp-rs/rust</a>ã€‚</li>
</ul>
<p><strong>æˆ‘ä»¬éœ€è¦ä»€ä¹ˆèŠ¯ç‰‡ï¼Ÿ</strong></p>
<p>å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ARM Cortex-Mã€‚ è¿™ç¼©å°äº†æœç´¢èŒƒå›´ï¼Œä½†ä»æœ‰å¾ˆå¤šé€‰æ‹©ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹ cortex-m <a href="https://crates.io/crates/cortex-m/reverse_dependencies" rel="noopener noreferrer">crate</a> çš„ä¾èµ–é¡¹ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æœ‰ä¸¤ç»„èŠ¯ç‰‡æ¯”å…¶ä»–ä»»ä½•ä¸€ç»„éƒ½ä½¿ç”¨å¾—æ›´å¤šï¼› <a href="https://www.st.com/content/st_com/en/products/microcontrollers-microprocessors/stm32-32-bit-arm-cortex-mcus.html" rel="noopener noreferrer">STM32</a> ç³»åˆ—èŠ¯ç‰‡å’Œ <a href="https://www.nordicsemi.com/Products/Bluetooth-Low-Energy" rel="noopener noreferrer">nRF5</a> ç³»åˆ—ï¼Œè¿™æ˜¯æˆ‘ä»¬è¦é‡ç‚¹æœç´¢çš„åœ°æ–¹ã€‚</p>
<ul>
<li>STM32ï¼šSTM32 ç³»åˆ—èŠ¯ç‰‡å¯èƒ½æ˜¯åº”ç”¨æœ€å¹¿æ³›çš„åµŒå…¥å¼ Rust ARM Cortex-M èŠ¯ç‰‡ã€‚ä¸¤ç§æœ€å—æ¬¢è¿çš„ STM32 æ¿æ˜¯ Blue Pill å’Œ Black Pillã€‚ä¸»è¦çš„ç¼ºç‚¹æ˜¯æ²¡æœ‰æ¿è½½è°ƒè¯•å™¨ã€‚å¦‚æœæƒ³è¦å¸¦æœ‰è°ƒè¯•å™¨çš„åŸºäº STM32 çš„ç”µè·¯æ¿ï¼Œé‚£ä¹ˆè·å¾— STMicroelectronics <a href="https://www.st.com/en/evaluation-tools/stm32-discovery-kits.html#overview" rel="noopener noreferrer">å®˜æ–¹å¥—ä»¶</a>æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼ˆSTM32F3 æˆ– STM32F4 æ˜¯ä¸é”™çš„é€‰æ‹©ï¼‰ã€‚Rust Embedded Discovery ä¹¦çš„åŸå§‹ç‰ˆæœ¬æ˜¯é’ˆå¯¹ STM32F3 æ¿ç¼–å†™çš„ï¼Œå› æ­¤æœ‰éå¸¸é«˜è´¨é‡çš„åˆå­¦è€…æ–‡æ¡£ï¼Œå¯ä»¥ä»é‚£é‡Œå¼€å§‹ã€‚</li>
<li>nRF5ï¼šç”¨äºåµŒå…¥å¼ Rust çš„ç¬¬äºŒä¸ªæœ€å¹¿æ³›ä½¿ç”¨çš„ ARM Cortex-M èŠ¯ç‰‡ç³»åˆ—æ˜¯ Nordic Semiconductor çš„ <a href="https://www.nordicsemi.com/Products/Bluetooth-Low-Energy" rel="noopener noreferrer">nRF5 ç³»åˆ—</a>ã€‚å®˜æ–¹å¼€å‘<a href="https://www.nordicsemi.com/Products/Bluetooth-Low-Energy/Development-hardware" rel="noopener noreferrer">å¥—ä»¶</a> (DK) æ˜¯å¾ˆæ£’çš„å…¥é—¨æ¿ã€‚ Ferrous Systems çš„ Knurling-rs ä¼šè®®ä½¿ç”¨ nRF52840 <a href="https://www.nordicsemi.com/Products/Development-hardware/nRF52840-DK" rel="noopener noreferrer">å¼€å‘å¥—ä»¶</a>ã€‚Knurling è¯¾ç¨‹è´¨é‡éå¸¸é«˜ï¼Œæ‰‹æŠŠæ‰‹æŒ‡å¯¼ï¼Œé€šè¿‡æœ‰è¶£å¥½ç©çš„é¡¹ç›®æ•™æˆåµŒå…¥ Rustï¼Œæ˜¯ä½¿ç”¨ Rust è¿›è¡ŒåµŒå…¥å¼å¼€å‘çš„æœ€ä½³åˆ‡å…¥ç‚¹ã€‚å¦ä¸€ä¸ªå¾ˆæ£’çš„åŸºäº nRF çš„å¼€å‘æ¿æ˜¯ <a href="https://www.microbit.org/" rel="noopener noreferrer">BBC micro:bit</a>ã€‚å®ƒé…å¤‡äº†æ¿è½½è°ƒè¯•å™¨å’Œä¸€ç³»åˆ—æœ‰è¶£çš„æ¿è½½å¤–å›´è®¾å¤‡ï¼Œå¦‚æ¿ä¸Šçš„ LED æ˜¾ç¤ºå±ã€æŒ‰é’®å’Œä¼ æ„Ÿå™¨ã€‚BBC micro:bit è¢«è®¾è®¡ä¸ºä¸€ä¸ªæ•™è‚²å¹³å°ï¼Œå› æ­¤ç¡¬ä»¶åœ¨ä»–ä»¬çš„<a href="https://tech.microbit.org/" rel="noopener noreferrer">å¼€å‘è€…ç¤¾åŒº</a>ä¸­ä»¥éå¸¸é€‚åˆåˆå­¦è€…çš„æ–¹å¼è¿›è¡Œè®°å½•ï¼Œå¹¶ä¸”äº’è”ç½‘ä¸Šæœ‰å¤§é‡é¡¹ç›®åˆ›æ„ã€‚</li>
<li>RP2040ï¼š<a href="https://www.raspberrypi.org/documentation/rp2040/getting-started/" rel="noopener noreferrer">RP2040</a> äº 2020 å¹´åº•å‘å¸ƒï¼Œæ˜¯ Raspberry Pi åŸºé‡‘ä¼šé¦–æ¬¡å°è¯•è®¾è®¡è‡ªå·±çš„èŠ¯ç‰‡ã€‚ç”±äºå¦‚æ­¤æ–°ï¼ŒRust å¯¹å®ƒçš„æ”¯æŒä»åœ¨å¼€å‘ä¸­ã€‚ä¸ BBC micro:bit ä¸€æ ·ï¼ŒRP2040 æ—¨åœ¨æˆä¸ºä¸€ä¸ªæ•™è‚²å¹³å°ï¼Œå› æ­¤ç¡¬ä»¶æ–‡æ¡£æ˜¯ä¸€æµçš„ï¼Œå¹¶ä¸”æœ‰å¤§é‡åˆå­¦è€…å‹å¥½çš„ä»£ç ç¤ºä¾‹å’Œå…¶ä»–ç¼–ç¨‹è¯­è¨€çš„åº“ï¼ˆæ²¡æœ‰å¤šå°‘é€‚åˆåˆå­¦è€…çš„åµŒå…¥å¼ Rust æ–‡æ¡£ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ä»¤äººå…´å¥‹çš„å¹³å°ï¼Œå¹¶ä¸”åœ¨ Embedded Rust ç¤¾åŒºä¸­å›´ç»•å®ƒè¿›è¡Œäº†å¤§é‡æ´»åŠ¨ï¼Œæ‰€ä»¥ä¸€å®šè¦å¯†åˆ‡å…³æ³¨ï¼Œä½†å®ƒå¯èƒ½ä¸é€‚åˆä½œä¸ºå…¥é—¨ç¬¬ä¸€å—æ¿ã€‚</li>
</ul>
<p><strong>æ¿è½½è°ƒè¯•å™¨ï¼Ÿ</strong></p>
<p>åœ¨ä¸»æœºä¸Šè¿è¡Œç¨‹åºæ—¶ï¼Œå¯ä»¥åœ¨ shell ä¸­è¿è¡Œå®ƒå¹¶æŸ¥çœ‹æ‰“å°è¾“å‡ºã€‚è¿™åœ¨åµŒå…¥å¼ç›®æ ‡ä¸Šæ›´åŠ å›°éš¾ï¼Œè°ƒè¯•å™¨å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚é™¤äº†å…è®¸å•æ­¥è°ƒè¯•ã€æ–­ç‚¹è°ƒè¯•å¤–ï¼Œå®ƒè¿˜å…è®¸å°†ç¨‹åºåŠ è½½åˆ°è®¾å¤‡ä¸Šå¹¶è½»æ¾æŸ¥çœ‹è¾“å‡ºã€‚ä¸è¿‡æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå®ƒé€šå¸¸æ˜¯è¿æ¥åˆ°ä¸»æœºç„¶åè¿æ¥åˆ°ç›®æ ‡è®¾å¤‡çš„å•ç‹¬è®¾å¤‡ã€‚ç¬¬ä¸€æ¬¡å¼€å§‹æ—¶ï¼Œè¿™æ˜¯ä¸€ç¬”ä¸å¯å¿½è§†çš„è´¹ç”¨ï¼Œä¹Ÿæ˜¯å¿…é¡»æ­£ç¡®è®¾ç½®çš„å¦ä¸€ä»¶äº‹ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰äº›è®¾å¤‡å¸¦æœ‰å†…ç½®è°ƒè¯•å™¨ï¼Œå°†å®ƒä»¬ç›´æ¥æ’å…¥ä¸»æœºå¹¶åœ¨ç¬é—´æ¢æµ‹è¿è¡Œçš„ä»£ç ï¼ˆé€šå¸¸éœ€è¦åœ¨ä¸»æœºä¸Šè¿›è¡Œä¸€äº›è®¾ç½®æ‰èƒ½ä½¿è°ƒè¯•å™¨æ­£å¸¸å·¥ä½œï¼Œferrous æœ‰ä¸€ä¸ªå¾ˆå¥½çš„è®¾ç½®<a href="https://session20q4.ferrous-systems.com/sessions/installation.html" rel="noopener noreferrer">æŒ‡å—</a>ï¼‰ã€‚</p>
<p><strong>ç»“è®º</strong></p>
<p>ä»¥ä¸‹è¿™äº›æ¿éƒ½æœ‰å¾ˆæ£’çš„ HAL å’Œ BSP crateã€æ´»è·ƒå‹å¥½çš„ç¤¾åŒºå’Œæ¿è½½è°ƒè¯•å™¨ã€‚</p>
<ul>
<li><a href="https://www.microbit.org/" rel="noopener noreferrer">BBC micro:bit</a>ï¼ˆçº¦ 13 è‹±é•‘ï¼‰ï¼šå®ƒæ˜¯æ–°ç‰ˆ Rust Embedded Discovery ä¹¦ä¸­ä½¿ç”¨çš„æ¿ã€‚</li>
<li><a href="https://www.nordicsemi.com/Products/Development-hardware/nRF52840-DK" rel="noopener noreferrer">nRF52840 å¼€å‘å¥—ä»¶</a>ï¼ˆçº¦ 35 è‹±é•‘ï¼‰ï¼› å®ƒæ˜¯ Ferrous Systems åœ¨ Kunrling ä¼šè®®å’ŒåŸ¹è®­ä¸­ä½¿ç”¨çš„æ¿ã€‚</li>
<li><a href="https://www.st.com/en/evaluation-tools/stm32f3discovery.html" rel="noopener noreferrer">STM32F3 æ¢ç´¢å¥—ä»¶</a>ï¼ˆçº¦ 14 è‹±é•‘ï¼‰ï¼› å®ƒæ˜¯ Rust Embedded Discovery ä¹¦çš„ç¬¬ä¸€ç‰ˆä¸­ä½¿ç”¨çš„æ¿ã€‚</li>
</ul>
<p>å¯†åˆ‡å…³æ³¨ï¼š</p>
<ul>
<li><a href="https://www.raspberrypi.org/products/raspberry-pi-pico/" rel="noopener noreferrer">Raspberry Pi Pico</a>ï¼ˆçº¦ 6 è‹±é•‘ï¼Œå¸¦é¢„ç„Šå¼•è„šï¼‰ï¼› ARM Cortex-M ä½†æ²¡æœ‰å†…ç½®è°ƒè¯•å™¨ï¼ŒHAL ä»åœ¨å¼€å‘ä¸­ã€‚ä¸è¿‡ç›®å‰æœ‰å¾ˆå¤šæ´»åŠ¨ï¼Œè¿›å±•å¾ˆå¿«ã€‚</li>
<li><a href="https://www.sifive.com/boards/hifive1-rev-b" rel="noopener noreferrer">HiFive1 Rev B</a>ï¼ˆçº¦ 50 è‹±é•‘ï¼‰ï¼› RISC-V æ˜¯æ–°çš„çƒ­ç‚¹ã€‚ Rust ä¸­ä¼¼ä¹æœ‰å¾ˆå¤šå›´ç»•å®ƒçš„æ´»åŠ¨ï¼Œä½†å®ƒç›®å‰è¿˜æ²¡æœ‰ ARM Cortex-M çš„æ”¯æŒã€‚ å…¶ä»–éœ€è¦å…³æ³¨çš„å¼€å‘æ¿æ˜¯ <a href="https://longan.sipeed.com/en/" rel="noopener noreferrer">Logan Nano</a> å’Œ <a href="https://hackaday.com/2021/02/08/hands-on-the-risc-v-esp32-c3-will-be-your-new-esp8266/" rel="noopener noreferrer">ESP32-C3</a>ã€‚</li>
</ul>
<p>éƒ¨åˆ†å†…å®¹ç•¥æœ‰è½»å¾®è°ƒæ•´ï¼Œæ›´å¤šå¯é˜…è¯»åŸæ–‡ï¼š<a href="https://robyoung.digital/blog/embedded-rust-selecting-a-board/" rel="noopener noreferrer">Rob Young | digital</a></p>
<h3>Tangramï¼šè®­ç»ƒã€éƒ¨ç½²å’Œç›‘æ§æœºå™¨å­¦ä¹ æ¨¡å‹</h3>
<p>ä¸€ä¸ªæœºå™¨å­¦ä¹ å¥—ä»¶ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š</p>
<pre><code># è®­ç»ƒ
$ tangram train --file heart_disease.csv --target diagnosis --output heart_disease.tangram
</code></pre>
<p>æ¨ç†æ”¯æŒå¤šç§è¯­è¨€ï¼š<a href="https://hex.pm/packages/tangram" rel="noopener noreferrer">Elixir</a>, <a href="https://pkg.go.dev/github.com/tangramdotdev/tangram-go" rel="noopener noreferrer">Go</a>, <a href="https://www.npmjs.com/package/@tangramdotdev/tangram" rel="noopener noreferrer">JavaScript</a>, <a href="https://pypi.org/project/tangram" rel="noopener noreferrer">Python</a>, <a href="https://rubygems.org/gems/tangram" rel="noopener noreferrer">Ruby</a> å’Œ <a href="https://lib.rs/tangram" rel="noopener noreferrer">Rust</a>ï¼Œä»¥ Rust ä¸ºä¾‹ï¼š</p>
<pre><code>let model: tangram::Model = tangram::Model::from_path("heart_disease.tangram", None).unwrap();

let input = tangram::predict_input! {
  "age": 63.0,
  "gender": "male",
  // ...
};

let output = model.predict_one(input, None);
# { className: 'Negative', probability: 0.9381780624389648 }
</code></pre>
<p>å¾ˆå¥½å¥‡è®­ç»ƒçš„æ—¶å€™å±…ç„¶æ²¡æœ‰è¦æŒ‡å®šæ¨¡å‹ï¼Œå‘ç°å…¶å°†æ¨¡å‹å…±åˆ†ä¸ºä¸‰ç±»ï¼šå›å½’ã€äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»ï¼Œè®­ç»ƒæ—¶ä¼šæ ¹æ®æ•°æ®è‡ªåŠ¨é€‰æ‹©åˆé€‚ï¼ˆä½¿ç”¨è¯„ä¼°æ–¹æ³•ï¼‰çš„æ¨¡å‹ï¼Œæ¯ç§æ¨¡å‹åˆæœ‰ä¸¤ç§ä¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼šçº¿æ€§æ–¹æ³•å’Œæ ‘æ–¹æ³•ã€‚</p>
<p>è‡ªå¸¦çš„ç›‘æ§åŠŸèƒ½çœ‹èµ·æ¥è¿˜ä¸é”™ï¼Œæ¯”å¦‚ä¸‹é¢è¿™å¼ å¯ä»¥å±•ç¤ºç‰¹å¾å¯¹è¾“å‡ºçš„è´¡çŒ®ï¼š</p>
<p><img src="https://github.com/tangramdotdev/tangram/raw/main/readme/predictions.png" alt></p>
<p>é¡¹ç›®ç†è®ºä¸Šå¯ä»¥ç”¨åœ¨ç®€å•æœºå™¨å­¦ä¹ åœºæ™¯ä¸‹ï¼Œå°¤å…¶æ˜¯é‚£äº›è¿˜æ²¡æœ‰æ”¯æŒæœºå™¨å­¦ä¹ çš„è¯­è¨€ï¼Œä¸è¿‡æ¨ç†å¹¶æ²¡æœ‰ Benchmarkï¼Œç”Ÿäº§ä¸­ä½¿ç”¨éœ€è¦åšå¥½æ€§èƒ½æµ‹è¯•ã€‚</p>
<p>GitHubï¼š<a href="https://github.com/tangramdotdev/tangram" rel="noopener noreferrer">tangramdotdev/tangram: Tangram makes it easy for programmers to train, deploy, and monitor machine learning models.</a></p>
<p>æ–‡æ¡£ï¼š<a href="https://www.tangram.dev/docs/" rel="noopener noreferrer">Tangram</a></p>
<h3>lateralï¼šä¸€ä¸ªåœ¨ x86_64 ä¸Šå¯åŠ¨çš„æ¨¡å—åŒ–å†…æ ¸</h3>
<p>åœ¨æœ¬åœ°æ‰§è¡Œï¼š</p>
<pre><code>$ make run-release ARCH=x86_64
</code></pre>
<p>å¯ä»¥æ ¹æ®è‡ªå·±çš„æƒ…å†µè°ƒæ•´ Makefile ç¬¬ä¸€è¡Œ Bash çš„é…ç½®ã€‚æ‰§è¡Œåå¦‚æœæœ‰å®‰è£… QEMU çš„è¯ä¼šè‡ªåŠ¨åŠ è½½ï¼š</p>
<p><img src="http://qnimg.lovevivian.cn/tmp-os-1.jpg" alt></p>
<p>æ¯ä¸ªç»„ä»¶éƒ½å»ºç«‹åœ¨çª—å£ç®¡ç†å™¨ä¹‹ä¸Šï¼Œè€Œä¸æ˜¯åƒå¤§å¤šæ•°æ“ä½œç³»ç»Ÿé‚£æ ·å»ºç«‹åœ¨ç»ˆç«¯ä¹‹ä¸Šã€‚</p>
<p>GitHubï¼š<a href="https://github.com/carterisonline/lateral" rel="noopener noreferrer">carterisonline/lateral: A clean, custom-built modular kernel ready to boot on x86_64.</a></p>
<h3>tvï¼šæ˜¾ç¤ºè¡¨æ ¼çš„ cli å·¥å…·</h3>
<p>å°±æ˜¯æŠŠ json æˆ– csv æ˜¾ç¤ºæˆè¡¨æ ¼ï¼Œçœ‹èµ·æ¥å¾ˆä¸é”™ï¼š</p>
<pre><code>$ cat test.json
[
  {
    "name": "test",
    "age": 10,
    "lang": "ja"
  },
  {
    "name": "uzimaru",
    "age": 23,
    "lang": "ja"
  },
  {
    "name": "hogehoge",
    "age": 21,
    "lang": "en"
  },
  {
    "name": "hugehuge",
    "age": 32,
    "lang": "en"
  }
]

$ tv test.json
|age|lang|    name|
|---|----|--------|
| 10|  ja|    test|
| 23|  ja| uzimaru|
| 21|  en|hogehoge|
| 32|  en|hugehuge|

$ cat test.csv
name,age,lang
test,10,ja
uzimaru,23,ja
hogehoge,21,en
hugehuge,32,en

$ tv test.csv
|age|lang|    name|
|---|----|--------|
| 10|  ja|    test|
| 23|  ja| uzimaru|
| 21|  en|hogehoge|
| 32|  en|hugehuge|
</code></pre>
<p>Mac ç”¨æˆ· brew å®‰è£…ï¼š</p>
<pre><code>$ brew install uzimaru0000/tap/tv
</code></pre>
<p>GitHubï¼š<a href="https://github.com/uzimaru0000/tv" rel="noopener noreferrer">uzimaru0000/tv: CLI tool for displaying table</a></p>
<h3>minesweeperï¼šä½¿ç”¨ Rustï¼ŒWebAssembly å’Œ Canvas çš„æ‰«é›·æ¸¸æˆ</h3>
<p>ç•Œé¢é•¿è¿™æ ·ï¼š</p>
<p><img src="https://github.com/KarthikNedunchezhiyan/minesweeper/raw/main/www/assets/stage_bomb_triggered.png" alt></p>
<p>æ˜¯å¾ˆå¥½çš„å­¦ä¹ èµ„æ–™ã€‚åœ¨è¿™é‡Œç©å„¿ï¼š<a href="https://karthiknedunchezhiyan.me/minesweeper/" rel="noopener noreferrer">Minesweeper</a></p>
<p>GitHubï¼š<a href="https://github.com/karthikNedunchezhiyan/minesweeper" rel="noopener noreferrer">KarthikNedunchezhiyan/minesweeper: Minesweeper game developed with Rust, WebAssembly (Wasm), and Canvas</a></p>
<h3>copy-translatorï¼šåˆ’è¯ç¿»è¯‘</h3>
<p>å¤åˆ¶åç¿»è¯‘ï¼Œä½¿ç”¨ DeepL çš„ APIï¼Œä¸è¿‡ç›®å‰åªæœ‰ Local ç‰ˆæœ¬å¥½ç”¨ï¼š</p>
<p><img src="http://qnimg.lovevivian.cn/tmp-rust-1.jpg" alt></p>
<p>å½“ç„¶ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ Eudicï¼ˆæ¬§è·¯è¯å…¸ï¼‰ã€‚</p>
<p>GitHubï¼š<a href="https://github.com/zu1k/copy-translator" rel="noopener noreferrer">zu1k/copy-translator: Copy Translator, using DeepL api</a></p>
<h3>veccentricï¼šå°å·§çš„ 2-D å‘é‡ Library</h3>
<p>é¡¹ç›®å— <a href="https://p5js.org/reference/#/p5.Vector" rel="noopener noreferrer">p5.Vector</a> å¯å‘ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š</p>
<pre><code>use veccentric::Vecc;

let a = Vecc::new(3_i32, 4);
let b = a * 5;
let c = Vecc::new(-10, -8);
let d = b - c;
let e = -d;
</code></pre>
<p>GitHubï¼š<a href="https://github.com/micouy/veccentric" rel="noopener noreferrer">micouy/veccentric: Tiny 2D vector library. Inspired by p5.js's p5.Vector.</a></p>
<hr>
<p>From æ—¥æŠ¥å°ç»„ é•¿ç´</p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc è®ºå›ï¼šæ”¯æŒ rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRust è¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">axumå¦‚ä½•ä½¿ç”¨é™æ€æ–‡ä»¶ç›®å½•</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=f3fa9c8e-004b-4d95-8d5f-bdf6609c2e8e">
<div class="article-summary-box-inner">
<span><p>å†é‡æ„ä¸€ä¸ªç®€å•çš„å•é¡µé¢å°ç¨‹åºçš„æ—¶å€™æ‰“ç®—ç”¨<code>axum</code>ä»£æ›¿<code>warp</code>çš„æ—¶å€™é‡åˆ°äº†ä¸ªé—®é¢˜ã€‚</p>
<pre><code>    let post = warp::post()
        .and(warp::body::bytes())
        .map(move |content: Bytes| {
            Response::builder().body(server::handle_post_request(content))
        });

    let routers = warp::get().and(warp::fs::dir("./wwwroot")).or(post);

    warp::serve(routers).run(([127, 0, 0, 1], 3030)).await;
</code></pre>
<p>æœ‰å¦‚ä¸Šçš„ç®€å•ä»£ç ï¼Œä½¿ç”¨<code>wwwroot</code>æ–‡ä»¶å¤¹ç›®å½•æ¥ç”Ÿæˆé¡µé¢ï¼Œæ–‡ä»¶å¤¹é‡ŒåŒ…å«æœ‰<code>index.html</code>,JSå’ŒCSSæ–‡ä»¶ï¼Œæ€ä¹ˆä½¿ç”¨<code>axum</code>æ”¹å†™å‘¢ï¼Ÿçœ‹äº†ä¸‹docï¼Œåªçœ‹åˆ°</p>
<pre><code>let app = Router::new()
    // this route cannot fail
    .route("/foo", get(|| async {}))
    // this route can fail with io::Error
    .route(
        "/",
        service::get(service_fn(|_req: Request&lt;Body&gt;| async {
            let contents = tokio::fs::read_to_string("some_file").await?;
            Ok::&lt;_, io::Error&gt;(Response::new(Body::from(contents)))
        }))
        .handle_error(handle_io_error),
    );

fn handle_io_error(error: io::Error) -&gt; Result&lt;impl IntoResponse, Infallible&gt; {
    // ...
}
</code></pre>
<p>è¿™ç§å†™æ³•ã€‚çœ‹ç€å¤´å¤§ä¸è¯´ï¼Œé‚£ä¸ª<code>some_file</code>åªæ˜¯ç®€å•è¯»å–æ–‡ä»¶ï¼Œå®Œæˆä¸äº†æˆ‘çš„è¦æ±‚ã€‚</p>
<p>æœ‰å¤§ç¥è¯´è¯´axumå®Œæˆäº†è¿™ä¸ªéƒ¨åˆ†äº†å—ï¼Ÿè¿™æ¡†æ¶çš„ä»£ç çœ‹ç€æ„Ÿè§‰æœ‰ç‚¹è¿‡äºå¤æ‚äº†ã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">è‡ªå·±ç®¡ç†å†…å­˜çš„æµ‹è¯•æ–¹æ³•</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d4e8f317-f43f-44ea-a041-f39dd3ce1578">
<div class="article-summary-box-inner">
<span><p>å¾ˆæ¼‚äº®çš„ä¸€æ®µcaseï¼Œæ¥è‡ªstd</p>
<p>library/alloc/tests/linked_list.rs</p>
<pre><code>#[test]
fn test_drop() {
    static mut DROPS: i32 = 0;
    struct Elem;
    impl Drop for Elem {
        fn drop(&amp;mut self) {
            unsafe {
                DROPS += 1;
            }
        }
    }

    let mut ring = LinkedList::new();
    ring.push_back(Elem);
    ring.push_front(Elem);
    ring.push_back(Elem);
    ring.push_front(Elem);
    drop(ring);

    assert_eq!(unsafe { DROPS }, 4);
}

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">é—®ä¸€ä¸ªDisplay traitçš„é—®é¢˜</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=6b59dfb1-87d8-4b79-8820-e9d5397f178a">
<div class="article-summary-box-inner">
<span><p>è¯·é—®&amp;str, &amp;&amp;str, &amp;&amp;&amp;str å¹¶æ²¡æœ‰å®ç°Display çš„trait, ä¸ºä»€ä¹ˆè¿™ä¸ªå‡½æ•°è°ƒç”¨æ²¡é—®é¢˜?</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rustæ—¥æŠ¥ã€‘2021-08-28 å¼€æºæ“ä½œç³»ç»Ÿå¤ä»¤è¥æœ€ç»ˆæŠ¥å‘Šä¼šå®‰æ’</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ef3dd4e8-a8e8-4fec-bc7e-75703e1117ff">
<div class="article-summary-box-inner">
<span><h3>å¼€æºæ“ä½œç³»ç»Ÿå¤ä»¤è¥æœ€ç»ˆæŠ¥å‘Šä¼šå®‰æ’</h3>
<p>ä¼šè®®ä¸»é¢˜ï¼šå¼€æºæ“ä½œç³»ç»Ÿå¤ä»¤è¥æœ€ç»ˆæŠ¥å‘Šä¼š
ä¼šè®®æ—¶é—´ï¼š2021/08/29 09:00-11:30 (GMT+08:00) ä¸­å›½æ ‡å‡†æ—¶é—´ - åŒ—äº¬
ç‚¹å‡»é“¾æ¥å…¥ä¼šï¼Œæˆ–æ·»åŠ è‡³ä¼šè®®åˆ—è¡¨ï¼š https://meeting.tencent.com/dm/Mp7T1h5zeQOk?rs=25
ä¼šè®® IDï¼š635 194 989</p>
<p>ä¸‹é¢æ˜¯9ä½å…¨ç¨‹å‚ä¸å¤ä»¤è¥æ´»åŠ¨åŒå­¦çš„æŠ¥å‘Šé¡ºåºã€‚æ¯äººæŠ¥å‘Šæ—¶é—´æœ€é•¿15åˆ†é’Ÿã€‚</p>
<ol>
<li>æ¨äº‘æ« ç‹æ¶› Rustsbiçš„å“ªå’å¼€å‘ç‰ˆç§»æ¤</li>
<li>å…°é™ˆæ˜• zCoreå›¾å½¢æ”¯æŒ</li>
<li>éƒ½ç§‰ç”² å®¹å™¨æŠ€æœ¯å­¦ä¹ </li>
<li>è–›æ½‡å· RVM çš„ RISC-V æ”¯æŒ</li>
<li>é™ˆä¹ å…±äº«è°ƒåº¦å™¨</li>
<li>å´éå‡¡ åŸºäºç”¨æˆ·æ€ä¸­æ–­çš„å¼‚æ­¥ç³»ç»Ÿè°ƒç”¨è®¾è®¡ä¸å®ç°</li>
<li>å½­æ·³æ¯… é™ˆå¿—æ‰¬ åŸºäºrCore-Tutorialçš„æ€§èƒ½åˆ†æè½¯ä»¶å®ç°</li>
</ol>
<h3>crates.liveï¼šå¯è§†åŒ– Rust crates ä¾èµ–é¡¹</h3>
<p>crates.live æ˜¯æ¥è‡ª crates.io çš„ Rust crates çš„ä¾èµ–å¯è§†åŒ–å·¥å…·ã€‚ å®ƒæ˜¾ç¤ºäº† Rust cratesï¼ˆåŒ…ï¼‰çš„ä¾èµ–æ ‘ã€‚åŠŸèƒ½åŒ…æ‹¬ï¼š</p>
<ul>
<li>ä¾èµ–è§£æï¼Œ crates.live å¼•æ“é€šè¿‡åŒ¹é…ä¾èµ–ç‰ˆæœ¬æ¥å®Œæˆå®Œæ•´çš„ä¾èµ–è§£æã€‚</li>
<li>äº¤äº’å¼å›¾è¡¨ï¼Œå¸¦æœ‰æ ‡è®°çš„æ¿æ¡ç®±çš„å¯ç¼©æ”¾äº¤äº’å¼å›¾è¡¨ã€‚</li>
<li>å›¾åƒå¯¼å‡ºï¼Œ å°†å›¾å½¢å¯¼å‡ºä¸º PNGã€‚</li>
<li>å¼€æ”¾ APIï¼šï¼ˆå³å°†æ¨å‡ºï¼‰GraphQL APIã€‚</li>
</ul>
<p>crates.live ä½¿ç”¨äº†ä¸€å †æŠ€æœ¯æ¡†æ¶ï¼ŒæŠ€æœ¯æ ˆåŒ…æ‹¬ï¼š</p>
<ul>
<li>Rustï¼Œ crates.live åç«¯å’Œçˆ¬è™«æ˜¯ç”¨ Rust å’Œå¼€æº Rust åº“å¼€å‘çš„ã€‚</li>
<li>GraphQlï¼Œ WASM é©±åŠ¨çš„ GraphQL æœåŠ¡å™¨ã€‚</li>
<li>React/Bulmaï¼Œ å‰ç«¯åº“ã€‚</li>
<li>Terraformï¼Œ å¸®åŠ©å¯åŠ¨å’Œç»´æŠ¤æˆ‘ä»¬çš„åŸºç¡€è®¾æ–½ã€‚</li>
<li>Cloudflareï¼Œ Cloudflare å·¥ä½œäººå‘˜è¿è¡Œ WASM åç«¯ã€‚</li>
</ul>
<p>å¦‚æœåœ¨ä½¿ç”¨æ­¤åº”ç”¨ç¨‹åºæ—¶æœ‰ä»»ä½•ç–‘é—®ã€å»ºè®®æˆ–é—®é¢˜ï¼› å¯ä»¥é€šè¿‡ contact@crates.live è”ç³»ã€‚ crates.live ç”± Abid Omar å¼€å‘ï¼Œå¯é€šè¿‡ contact@omarabid.com è”ç³»ã€‚</p>
<p><a href="https://crates.live/" rel="noopener noreferrer">é“¾æ¥</a>ï¼šhttps://crates.live/</p>
<h3>Obakeï¼Œç‰ˆæœ¬åŒ–æ•°æ®ç»“æ„</h3>
<p>Obake æ˜¯ä¸€ä¸ªç”¨äºå£°æ˜å’Œç»´æŠ¤ç‰ˆæœ¬åŒ–æ•°æ®ç»“æ„çš„è¿‡ç¨‹å®ã€‚ â€œobakeâ€è¿™ä¸ªåå­—å–è‡ªæ—¥è¯­â€œãŠåŒ–ã‘ï¼ˆãŠã°ã‘ï¼‰â€ï¼Œè¿™æ˜¯æ—¥æœ¬æ°‘é—´ä¼ è¯´ä¸­ä¸€ç±»ä¼šå˜å½¢çš„è¶…è‡ªç„¶ç”Ÿç‰©ã€‚</p>
<p>åœ¨å¼€å‘åº”ç”¨ç¨‹åºæ—¶ï¼Œé…ç½®æ ¼å¼å’Œå†…éƒ¨æ•°æ®ç»“æ„é€šå¸¸ä¼šåœ¨ç‰ˆæœ¬ä¹‹é—´æ¼”å˜ã€‚ ç„¶è€Œï¼Œä¿æŒè¿™äº›ç‰ˆæœ¬ä¹‹é—´çš„å‘åå…¼å®¹æ€§éœ€è¦å£°æ˜å’Œç»´æŠ¤é—ç•™æ ¼å¼çš„æ•°æ®ç»“æ„å’Œç”¨äºåœ¨å®ƒä»¬ä¹‹é—´è¿ç§»çš„ä»£ç ã€‚ Obake çš„ç›®æ ‡æ˜¯è®©è¿™ä¸ªè¿‡ç¨‹å˜å¾—è½»æ¾ã€‚</p>
<pre><code>#[obake::versioned]                 // create a versioned data-structure
#[obake(version("0.1.0"))]          // declare some versions
#[obake(version("0.2.0"))]
#[derive(PartialEq, Eq, Hash)]      // additional attributes are applied to all versions
struct Foo {
    #[obake(cfg("0.1.0"))]          // enable fields for specific versions with
    foo: String,                    // semantic version constraints
   
    #[obake(cfg("&gt;=0.2, &lt;=0.3.0"))] // any semantic version constraint can appear in
    bar: u32,                       // a `cfg` attribute 
   
    #[obake(cfg("0.1.0"))]          // multiple `cfg` attributes are treated as a
    #[obake(cfg("&gt;=0.3"))]          // disjunction over version constraints
    baz: char,
}

// describe migrations between versions using the `From` trait
// and an automatically generated type-level macro for referring to
// specific versions of `Foo`
impl From&lt;Foo!["0.1.0"]&gt; for Foo!["0.2.0"] {
    fn from(foo: Foo!["0.1.0"]) -&gt; Self {
        Self { bar: 0 }
    }
}

// an enumeration of all versions of `Foo` is accessed using the
// `obake::Versioned` trait:
let versioned_example: &lt;Foo as obake::Versioned&gt;::Versioned = unimplemented!();

// this enumeration implements `Into&lt;Foo&gt;`, where `Foo` is the latest declared
// version of `Foo` (in this case, `Foo!["0.2.0"]`)
let example: Foo = versioned_example.into();
</code></pre>
<p>Github<a href="https://github.com/doctorn/obake" rel="noopener noreferrer">é“¾æ¥</a>ï¼šhttps://github.com/doctorn/obake</p>
<h3>icedï¼Œè·¨å¹³å° GUI åº“</h3>
<p>icedï¼ŒRust çš„è·¨å¹³å° GUI åº“ï¼Œä¸“æ³¨äºç®€å•æ€§å’Œç±»å‹å®‰å…¨ã€‚ çµæ„Ÿæ¥è‡ª<a href="https://elm-lang.org/" rel="noopener noreferrer">Elm</a>ã€‚</p>
<p><img src="https://raw.githubusercontent.com/hecrj/iced/master/docs/graphs/ecosystem.png" alt="eco"></p>
<p>Github<a href="https://github.com/hecrj/iced/" rel="noopener noreferrer">é“¾æ¥</a>ï¼šhttps://github.com/hecrj/iced/</p>
<p>ç¤ºä¾‹ï¼šhttps://github.com/hecrj/iced/tree/master/examples</p>
<hr>
<p>From æ—¥æŠ¥å°ç»„ <a href="https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929" rel="noopener noreferrer">æ´‹èŠ‹</a></p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustccè®ºå›: æ”¯æŒrss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rust æ—¥æŠ¥ã€‘2021-8-27 Rudra Rust çš„å†…å­˜å®‰å…¨å’Œæœªå®šä¹‰è¡Œä¸ºæ£€æµ‹å·¥å…·</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ce7eb559-fdda-45d7-a53e-293af787a813">
<div class="article-summary-box-inner">
<span><h4>Rudra Rust çš„å†…å­˜å®‰å…¨å’Œæœªå®šä¹‰è¡Œä¸ºæ£€æµ‹å·¥å…·</h4>
<p>Rudra æ˜¯ä¸€ä¸ªé™æ€åˆ†æå™¨ï¼Œç”¨äºæ£€æµ‹ Rust ç¨‹åºä¸­å¸¸è§çš„æœªå®šä¹‰è¡Œä¸ºã€‚å®ƒèƒ½å¤Ÿåˆ†æå•ä¸ª Rust åŒ…ä»¥åŠ crates.io ä¸Šçš„æ‰€æœ‰åŒ…ã€‚Rudra åŠå…¶ç›¸å…³è®ºæ–‡å°†åœ¨ Proceedings of the 28th ACM Symposium on Operating Systems Principles 2021 (SOSP '21) ä¸Šå‘è¡¨ã€‚</p>
<ul>
<li>https://github.com/sslab-gatech/Rudra#readme</li>
</ul>
<h4>nom 7.0 ç‰ˆæœ¬å‘å¸ƒ</h4>
<p>nom æ˜¯ä¸€ä¸ªç”¨ Rust ç¼–å†™çš„è§£æå™¨ç»„åˆåº“ã€‚å®ƒçš„ç›®æ ‡æ˜¯æä¾›å·¥å…·æ¥æ„å»ºå®‰å…¨çš„è§£æå™¨ï¼Œè€Œä¸ä¼šå½±å“é€Ÿåº¦æˆ–å†…å­˜æ¶ˆè€—ã€‚ä¸ºæ­¤ï¼Œå®ƒå¹¿æ³›ä½¿ç”¨ Rust çš„å¼ºç±»å‹å’Œå†…å­˜å®‰å…¨æ¥ç”Ÿæˆå¿«é€Ÿä¸”æ­£ç¡®çš„è§£æå™¨ï¼Œå¹¶æä¾›å‡½æ•°ã€å®å’Œç‰¹å¾æ¥æŠ½è±¡å¤§éƒ¨åˆ†å®¹æ˜“å‡ºé”™çš„ç®¡é“ã€‚ç›®å‰7.0å·²ç»å‘å¸ƒ</p>
<ul>
<li>https://crates.io/crates/nom</li>
</ul>
<h4>egui 0.14 ç‰ˆæœ¬å‘å¸ƒ</h4>
<p>egui æ˜¯ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„çº¯ Rust å›¾å½¢ç”¨æˆ·ç•Œé¢ã€‚egui å¯ä»¥åœ¨ Web ä¸Šã€æœ¬æœºä¸Šä»¥åŠæ‚¨æœ€å–œæ¬¢çš„æ¸¸æˆå¼•æ“ä¸­è¿è¡Œã€‚egui æ—¨åœ¨æˆä¸ºæœ€å®¹æ˜“ä½¿ç”¨çš„ Rust GUI åº“ï¼Œä»¥åŠåœ¨ Rust ä¸­åˆ¶ä½œ Web åº”ç”¨ç¨‹åºçš„æœ€ç®€å•æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨ä»»ä½•å¯ä»¥ç»˜åˆ¶çº¹ç†ä¸‰è§’å½¢çš„åœ°æ–¹ä½¿ç”¨ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥è½»æ¾åœ°å°†å…¶é›†æˆåˆ°æ‚¨é€‰æ‹©çš„æ¸¸æˆå¼•æ“ä¸­ã€‚</p>
<ul>
<li>æ¼”ç¤ºæ–‡æ¡£ï¼šhttps://emilk.github.io/egui/</li>
<li>https://github.com/emilk/egui</li>
</ul>
<hr>
<p>From æ—¥æŠ¥å°ç»„ åŒ—çº¬27åº¦ï¼Œä¾¯ç››é‘«</p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustccè®ºå›: æ”¯æŒrss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">å¼€æºé¡¹ç›®xiuç™»ä¸Šäº†GitHub rust trendingæ¦œ</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=86c83d9a-8370-42cf-8993-ef15af6932c4">
<div class="article-summary-box-inner">
<span><p><a href="https://github.com/harlanc/xiu" rel="noopener noreferrer">https://github.com/harlanc/xiu</a></p>
<p><a href="https://github.com/trending/rust?since=daily" rel="noopener noreferrer">https://github.com/trending/rust?since=daily</a></p>
<p>æ„Ÿè°¢å¤§å®¶çš„æ”¯æŒï¼ï¼</p>
<p>PSï¼š</p>
<p>å‰ä¸‰åæœ‰ä¸¤ä¸ªéƒ½åœ¨è®ºå›é‡Œå‘è¿‡ï¼Œè¿™ä¸ªè®ºå›æœ‰ç‚¹ç‹ ï¼Œå“ˆå“ˆ</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Salvo - ä¸€ä¸ªç®€å•çš„ Web åç«¯æ¡†æ¶</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=e5dc5be9-b1ab-488f-8944-dd7cd97b0128">
<div class="article-summary-box-inner">
<span><h2>ä¸ºä»€ä¹ˆè¦å†™è¿™ä¸ªæ¡†æ¶</h2>
<p>å› ä¸ºæˆ‘ç¬¨ï¼Œæ— æ³•å­¦ä¼šä½¿ç”¨ actix-web ç­‰ç°å­˜çš„æ¡†æ¶ã€‚å½“æˆ‘æƒ³æŠŠä»¥å‰çš„ go çš„ web æœåŠ¡ä½¿ç”¨ rust å®ç°æ—¶ï¼Œä¸€çœ¼çœ‹å»ï¼Œä¼¼ä¹æ¯ä¸ªæ¡†æ¶éƒ½æ¯” go é‡Œå­˜åœ¨æ¡†æ¶å¤æ‚, æœ¬æ¥ Rust çš„å­¦ä¹ æ›²çº¿å°±å¤Ÿé™¡å³­çš„äº†, åˆä½•è‹¦æŠŠ Web æ¡†æ¶æ•´å¾—é‚£ä¹ˆå¤æ‚?</p>
<h2>å¦‚ä½•åšåˆ°è¶³å¤Ÿç®€å•</h2>
<p>å¾ˆå¤šåº•å±‚çš„å®ç° Hyper éƒ½å·²ç»å®ç°ï¼Œæ‰€ä»¥ï¼Œä¸€èˆ¬éœ€æ±‚ï¼ŒåŸºäº Hyper å®ç°åº”è¯¥æ²¡æœ‰é”™ã€‚Salvo ä¹Ÿæ˜¯ä¸€æ ·ã€‚ æ ¸å¿ƒåŠŸèƒ½æ˜¯æä¾›è¿˜ç”¨ç®€å•çš„APIï¼Œä»¥åŠä¸€ä¸ªåŠŸèƒ½å¼ºå¤§å¹¶ä¸”çµæ´»çš„è·¯ç”±ç³»ç»Ÿã€‚</p>
<p>Salvo é‡Œç»Ÿä¸€äº† Handler å’Œ Middleware. Middleware å°±æ˜¯ Handler. é€šè¿‡è·¯ç”±çš„ before æˆ–è€… after æ·»åŠ åˆ° Router ä¸Šã€‚æœ¬è´¨ä¸Š, Middleware å’Œ Handler éƒ½æ˜¯å¤„ç† Request è¯·æ±‚ï¼Œå¹¶ä¸”å¯èƒ½å‘ Response å†™å…¥æ•°æ®ã€‚è€Œ Handler æ¥æ”¶çš„å‚æ•°æ˜¯ Request, Depot, Response ä¸‰ä¸ª, å…¶ä¸­ Depot ç”¨äºå­˜å‚¨è¯·æ±‚å¤„ç†è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ•°æ®. ä¸ºæ–¹ä¾¿ä¹¦å†™, åœ¨ç”¨ä¸ç€çš„æƒ…å†µä¸‹å¯ä»¥çœç•¥æ‰æŸäº›å‚æ•°.</p>
<pre><code>use Salvo::prelude::*;

#[fn_handler]
async fn hello_world(_req: &amp;mut Request, _depot: &amp;mut Depot, res: &amp;mut Response) {
    res.render_plain_text("Hello World");
}
#[fn_handler]
async fn hello_world2(res: &amp;mut Response) {
    res.render_plain_text("Hello World");
}
</code></pre>
<p>å¦å¤–è·¯ç”±ç³»ç»Ÿæä¾›çš„ API ä¹Ÿæ˜¯æå…¶ç®€å•çš„, ä½†æ˜¯, åŠŸèƒ½å´æ˜¯å¼ºå¤§çš„. æ­£å¸¸ä½¿ç”¨éœ€æ±‚ä¸‹, åŸºæœ¬ä¸Šå°±æ˜¯åªå…³æ³¨ Router ä¸€ä¸ªç±»å‹å³å¯.</p>
<h3>è·¯ç”±ç³»ç»Ÿ</h3>
<p>æˆ‘è‡ªå·±æ„Ÿè§‰è·¯ç”±ç³»ç»Ÿæ˜¯è·Ÿå…¶ä»–çš„æ¡†æ¶ä¸å¤ªä¸€æ ·çš„. Router å¯ä»¥å†™å¹³ï¼Œä¹Ÿå¯ä»¥å†™æˆæ ‘çŠ¶ã€‚è¿™é‡ŒåŒºä¸šåŠ¡é€»è¾‘æ ‘ä¸è®¿é—®ç›®å½•æ ‘ã€‚ä¸šåŠ¡é€»è¾‘æ ‘æ˜¯æ ¹æ®ä¸šåŠ¡é€»è¾‘éœ€æ±‚ï¼Œåˆ’åˆ† router ç»“æ„ï¼Œå½¢æˆ router æ ‘ï¼Œå®ƒä¸ä¸€å®šä¸è®¿é—®ç›®å½•æ ‘ä¸€è‡´ã€‚</p>
<p>æ­£å¸¸æƒ…å†µä¸‹æˆ‘ä»¬æ˜¯è¿™æ ·å†™è·¯ç”±çš„ï¼š</p>
<pre><code>Router::new().path("articles").get(list_articles).post(create_article);
Router::new()
    .path("articles/&lt;id&gt;")
    .get(show_article)
    .patch(edit_article)
    .delete(delete_article);
</code></pre>
<p>å¾€å¾€æŸ¥çœ‹æ–‡ç« å’Œæ–‡ç« åˆ—è¡¨æ˜¯ä¸éœ€è¦ç”¨æˆ·ç™»å½•çš„, ä½†æ˜¯åˆ›å»º, ç¼–è¾‘, åˆ é™¤æ–‡ç« ç­‰éœ€è¦ç”¨æˆ·ç™»å½•è®¤è¯æƒé™æ‰å¯ä»¥. Salvo ä¸­æ”¯æŒåµŒå¥—çš„è·¯ç”±ç³»ç»Ÿå¯ä»¥å¾ˆå¥½åœ°æ»¡è¶³è¿™ç§éœ€æ±‚. æˆ‘ä»¬å¯ä»¥æŠŠä¸éœ€è¦ç”¨æˆ·ç™»å½•çš„è·¯ç”±å†™åˆ°ä¸€èµ·ï¼š</p>
<pre><code>Router::new()
    .path("articles")
    .get(list_articles)
    .push(Router::new().path("&lt;id&gt;").get(show_article));
</code></pre>
<p>ç„¶åæŠŠéœ€è¦ç”¨æˆ·ç™»å½•çš„è·¯ç”±å†™åˆ°ä¸€èµ·ï¼Œ å¹¶ä¸”ä½¿ç”¨ç›¸åº”çš„ä¸­é—´ä»¶éªŒè¯ç”¨æˆ·æ˜¯å¦ç™»å½•ï¼š</p>
<pre><code>Router::new()
    .path("articles")
    .before(auth_check)
    .post(list_articles)
    .push(Router::new().path("&lt;id&gt;").patch(edit_article).delete(delete_article));
</code></pre>
<p>è™½ç„¶è¿™ä¸¤ä¸ªè·¯ç”±éƒ½æœ‰è¿™åŒæ ·çš„ <code>path("articles")</code>, ç„¶è€Œå®ƒä»¬ä¾ç„¶å¯ä»¥è¢«åŒæ—¶æ·»åŠ åˆ°åŒä¸€ä¸ªçˆ¶è·¯ç”±, æ‰€ä»¥æœ€åçš„è·¯ç”±é•¿æˆäº†è¿™ä¸ªæ ·å­:</p>
<pre><code>Router::new()
    .push(
        Router::new()
            .path("articles")
            .get(list_articles)
            .push(Router::new().path("&lt;id&gt;").get(show_article)),
    )
    .push(
        Router::new()
            .path("articles")
            .before(auth_check)
            .post(list_articles)
            .push(Router::new().path("&lt;id&gt;").patch(edit_article).delete(delete_article)),
    );
</code></pre>
<p><code>&lt;id&gt;</code>åŒ¹é…äº†è·¯å¾„ä¸­çš„ä¸€ä¸ªç‰‡æ®µ, æ­£å¸¸æƒ…å†µä¸‹æ–‡ç« çš„ <code>id</code> åªæ˜¯ä¸€ä¸ªæ•°å­—, è¿™æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é™åˆ¶ <code>id</code> çš„åŒ¹é…è§„åˆ™, <code>r"&lt;id:/\d+/&gt;"</code>.</p>
<p>æ›´å¤šä¿¡æ¯å¯ä»¥æŸ¥çœ‹ç½‘ç«™ https://salvo.rs</p>
<p>æºç åœ°å€: https://github.com/salvo-rs/salvo</p>
<p>éå¸¸æ¬¢è¿å¤§å®¶ä¸ºé¡¹ç›®è´¡çŒ®åŠ›é‡ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•ä¸ºé¡¹ç›®ä½œå‡ºè´¡çŒ®:</p>
<ul>
<li>åœ¨ issue ä¸­æäº¤åŠŸèƒ½éœ€æ±‚å’Œ bug report;</li>
<li>åœ¨ issues æˆ–è€… require feedback ä¸‹ç•™ä¸‹è‡ªå·±çš„æ„è§;</li>
<li>é€šè¿‡ pull requests æäº¤ä»£ç ;</li>
<li>åœ¨åšå®¢æˆ–è€…æŠ€æœ¯å¹³å°å‘è¡¨ Salvo ç›¸å…³çš„æŠ€æœ¯æ–‡ç« ã€‚</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">[å·²è§£å†³]println! ä¸¥é‡æ‹–å»¶æ•ˆèƒ½ï¼Œä»…åˆ—å°ä¸€è¡Œ</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ab0d06cb-d33d-4e18-b7b3-0b3e889f7b11">
<div class="article-summary-box-inner">
<span><p>å½“æŠŠcallå‡½æ•°æ³¨è§£åï¼Œæˆ–æ˜¯æ³¨è§£println! éƒ½å¯ä»¥å¿«é€Ÿè¿è¡Œã€‚</p>
<p>åœ¨ https://play.rust-lang.org/ ä¸Šæœ‰æ—¶å€™å¯ä»¥ ""ä½¿ç”¨println! "" è€Œä¸”ä¾ç„¶ç¼–è¯‘çš„å¾ˆå¿«ï¼Œæœ‰æ—¶å€™åˆ™ä¸è¡Œï¼Œæˆ‘è‡ªå·±æœ¬åœ°ç”µè„‘éƒ½ä¸è¡Œã€‚</p>
<p>è¿™æ•ˆèƒ½å·®äº†åä¸‡å…«åƒé‡Œï¼Œè¯·å¤§å®¶å¸®å¿™ï¼Œæ–°æ‰‹æ€»æ˜¯åœ¨ println! è·Œå‘ã€‚</p>
<p>è¿™è¾¹ä½¿ç”¨ <code>cargo run --release</code> ç¼–è¯‘</p>
<pre><code>use std::time::{Duration, Instant};

struct Struct {
    a: String,
    b: bool,
}
trait Dyn {}
impl Dyn for Struct {}

fn main() {
    let start = Instant::now();
    let mut count = 0;
    let count_end = 100_000_000i64;

    while count &lt;= count_end {
        let m: Box&lt;Struct&gt; = Box::new(Struct {
            b: false,
            a: "str".to_string(),
        });
        if count == count_end {
            call();               // ---- è¿™å„¿
            m.b;
            m.a;
        }
        count += 1;
    }

    let duration = start.elapsed();
    println!("Time: {:?}", duration);
}

fn call(){
    println!("run call()\n");     // ---- é‡ç‚¹åœ¨è¿™å„¿ï¼Œæ³¨è§£åå˜è¶…å¿«
}
</code></pre>
<p>Time:</p>
<table>
<thead>
<tr>
<th align="right">ğŸ˜«ä½¿ç”¨println!</th>
<th align="right">ğŸ˜„æ³¨è§£//println!</th>
</tr>
</thead>
<tbody>
<tr>
<td align="right">12.863911s</td>
<td align="right">2.8486ms</td>
</tr>
<tr>
<td align="right">13.2101748s</td>
<td align="right">2.4661ms</td>
</tr>
<tr>
<td align="right">13.5353751s</td>
<td align="right">2.0433ms</td>
</tr>
<tr>
<td align="right">13.4852107s</td>
<td align="right">1.7869ms</td>
</tr>
<tr>
<td align="right">â€”â€”â€”â€”â€”â€”â€”â€”</td>
<td align="right">â€”â€”â€”â€”â€”â€”â€”â€”</td>
</tr>
</tbody>
</table>
<hr>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</h3>
<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´8æœˆ29æ—¥ 20:00-21:00</p>
<p><strong>è¯¾ç¨‹ä»‹ç»:</strong> è®²åˆ° Rust ä½¿ç”¨ Future å¼‚æ­¥ç¼–ç¨‹ï¼Œå°±ä¸å¾—ä¸è¯´ futures å’Œ tokio è¿™ä¸¤ä¸ª crateï¼Œå…¶å®æ ‡å‡†åº“ä¸­çš„ futureï¼Œä»¥åŠ async/await å°±æ˜¯ä» futures åº“ä¸­æ•´åˆè¿›æ ‡å‡†åº“çš„, Tokio æ‹¥æœ‰æå¿«çš„æ€§èƒ½ï¼Œæ˜¯å¤§éƒ¨åˆ†ç³»ç»Ÿå¼‚æ­¥å¤„ç†çš„é€‰æ‹©ï¼Œå…¶æ„å»ºäº future ä¹‹ä¸Šã€‚Future æ˜¯ Rust å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒåŸºç¡€ã€‚</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<p>1ã€ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥.</p>
<p>2ã€ç†è§£å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>
<p>3ã€Future ç¼–ç¨‹æ¨¡å‹è®²è§£.</p>
<p>4ã€å¸¦é¢†å¤§å®¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ future , å†æ¬¡å¸®å¿™å¤§å®¶ç†è§£</p>
<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>
<ol>
<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>
<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>
<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>
<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>
</ol>
<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>
<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rustæ—¥æŠ¥ã€‘2021-08-19 -- Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</h3>
<p>å·²ç»åœ¨ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªcrate ä¸Šæµ‹è¯•äº†ç‰ˆæœ¬è¿ç§»,å¹¶ä¸”å°†æµ‹è¯•æ‰€æœ‰å…¬å…±çš„ crateã€‚</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>å¼‚æ­¥å¼•æ“ C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D æ¸¸æˆå¼•æ“</h3>
<ul>
<li><strong>PCï¼ˆWindowsã€Linuxã€macOSï¼‰å’Œ Web (WebAssembly)</strong> æ”¯æŒã€‚</li>
<li><strong>å»¶è¿Ÿç€è‰²</strong></li>
<li><strong>å†…ç½®ä¿å­˜/åŠ è½½</strong></li>
<li><strong>ç‹¬ç«‹åœºæ™¯ç¼–è¾‘å™¨</strong></li>
<li><strong>é«˜çº§ç‰©ç†æ¨¡å‹</strong></li>
<li><strong>åˆ†å±‚æ¨¡å‹èµ„æº</strong></li>
<li><strong>å‡ ä½•å®ä¾‹åŒ–</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From æ—¥æŠ¥å°ç»„ å†°å±±ä¸Šçš„ mook &amp;&amp; æŒºè‚¥</p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustccè®ºå›: æ”¯æŒrss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">å…¬å¼€è¯¾: é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Šé€šè¿‡Datafuseç†è§£å…¨é“¾è·¯è·Ÿè¸ªã€‹| Vol. 4</strong></p>
<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong> 2021å¹´8æœˆ22æ—¥ 20:30-21:30</p>
<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong> æ•°æ®åº“ç³»ç»Ÿä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚ï¼Œåºå¤§çš„ç³»ç»Ÿã€‚ç‰¹åˆ«æ˜¯åœ¨è°ƒè¯•å’Œè§‚å¯ŸSQLæ‰§è¡Œï¼Œå¤šçº¿ç¨‹ä»»åŠ¡åˆ‡æ¢ï¼Œå› ä¸ºæ²¡æœ‰å†…å­˜è°ƒç”¨æˆ–å †æ ˆè·Ÿè¸ªï¼Œè¿™ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¿½è¸ªçš„ç”±æ¥ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°å¤šè¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ªä¸ºæè¿°å’Œåˆ†æè·¨è¿›ç¨‹äº‹åŠ¡æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚Google Dapper(Dapper: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿé“¾è·¯è¿½è¸ªåŸºç¡€è®¾æ–½)è®ºæ–‡(å„tracerçš„åŸºç¡€)ä¸­æè¿°äº†åˆ†å¸ƒå¼è¿½è¸ªçš„ä¸€äº›ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­ç¨³æ€é—®é¢˜ã€åˆ†å¸ƒå¼åˆ†æã€èµ„æºå±æ€§å’Œå¾®æœåŠ¡çš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ã€‚</p>
<p>æœ¬æ¬¡å…¬å¼€è¯¾é€š Google çš„ OpenTraceing ä»‹ç»ï¼Œç»“åˆRustçš„ tokio-rs/tracing ä½¿ç”¨ï¼Œæœ€ç»ˆç»“åˆ Datafuse é¡¹ç›®ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹å¤§å‹åº”ç”¨çš„å…¨é“¾è·¯è·Ÿè¸ªåˆ†æè¿‡ç¨‹ã€‚</p>
<p>å…³äºDatafuse : https://github.com/datafuselabs/datafuse</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<ol>
<li>
<p>ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»ŸOpenTracingåŠåº”ç”¨åœºæ™¯</p>
</li>
<li>
<p>ä»‹ç» tokio-rs/tracing åŠåœ¨ç¨‹åºå¼€å‘ä¸­çš„ä½œç”¨</p>
</li>
<li>
<p>ä¸ºä»€ä¹ˆéœ€è¦tokio-rs/tracingåº“</p>
</li>
<li>
<p>æ¼”ç¤ºDatafuseé¡¹ç›®ä¸­tokio-rs/tracingçš„ä½¿ç”¨</p>
</li>
</ol>
<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>
<ol>
<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>
<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>
<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>
<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>
</ol>
<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>
<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">è®ºå›githubè´¦æˆ·æ— æ³•ç™»å½•è§£å†³ç¬”è®°</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>æœ‰åæ˜ è¿™ä¸¤å¤©githubè´¦æˆ·æ— æ³•ç™»å½•äº†ã€‚</p>
<p>æŠ¥è¿™ä¸ªé”™ï¼š</p>
<pre><code>get github user info err
</code></pre>
<p>æŸ¥äº†å‡ ä¸ªåœ°æ–¹ï¼š</p>
<ol>
<li>ä»£ç æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼šOk</li>
<li>httpsä»£ç†æ˜¯å¦æ­£å¸¸ï¼šOk</li>
<li>æ£€æŸ¥äº†githubè¿”å›æ—¥å¿—ï¼Œå‘ç°æ˜¯ï¼š</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>è¿›å…¥è¿™ä¸ªåœ°å€ä¸€çœ‹ï¼š<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>åŸæ¥2020å¹´2æœˆå°±å·²ç»è¯´äº†ï¼Œè¦æ”¹è¦æ”¹ã€‚ä¸è¿‡æˆ‘ç¡®å®æ²¡ç•™æ„åˆ°è¿™ä¸ªä¿¡æ¯ã€‚ï¼šï¼ˆ</p>
<p>æ„æ€å°±æ˜¯è¯´access_tokenä¸è¦æ”¾åœ¨queryå‚æ•°ä¸­ï¼Œè€Œæ˜¯è¦æ”¾åœ¨headeré‡Œé¢ã€‚ç…§å®ƒè¯´çš„ï¼Œæ”¹äº†åå°±å¥½äº†ã€‚</p>
<p>ç‰¹æ­¤è®°å½•ã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust çš„ Future ä¸ Javascript çš„ Promise åŠŸèƒ½å¯¹ç…§å‚è€ƒ</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>çš„<code>Future</code>ä¸<code>Javascript</code>çš„<code>Promise</code>åŠŸèƒ½å¯¹ç…§å‚è€ƒ</h1>
<p>å­¦ä¹ æ–°é²œæŠ€æœ¯æ—¶ï¼Œæˆ‘æ€»æ˜¯ä¼šä¹ æƒ¯æ€§å‘æ›¾ç»ç†Ÿæ‚‰çš„å†…å®¹ä¸Šé ï¼Œç”šè‡³å¥—ç”¨ç°æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¿™æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œå¯¹ç…§<code>Javascript - Promise/A+ API</code>æ¥è®°å¿†ä¸€éƒ¨åˆ†<code>Rust Future</code>å¸¸ç”¨<code>API</code>ã€‚</p>
<blockquote>
<p>æ³¨æ„ï¼šæ‰€æœ‰çš„<code>Rust - Future</code>æ“ä½œéƒ½æ˜¯ä»¥<code>.await</code>ç»“å°¾çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œä¸åŒäº<code>Javascript - Promise/A+</code>ï¼Œ<code>Rust - Future</code>æ˜¯æƒ°æ€§çš„ã€‚åªæœ‰è¢«<code>.await</code>æŒ‡ä»¤æ¿€æ´»åï¼Œåœ¨<code>Rust - Future</code>å†…å°è£…çš„æ“ä½œæ‰ä¼šè¢«çœŸæ­£åœ°æ‰§è¡Œã€‚</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">æè¿°</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* ä»€ä¹ˆéƒ½ä¸åš */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll ä¸èƒ½è¢«ç”¨æ¥æ„é€ åŒ…å«äº†å¼‚æ­¥æ“ä½œçš„ Future å®ä¾‹ï¼Œå› ä¸ºã€å›è°ƒé—­åŒ…ã€‘å†…çš„ã€å¯ä¿®æ”¹å¼•ç”¨ã€‘&amp;mut Context&lt;'_&gt; ä¸èƒ½è¢« ï¼ˆ1ï¼‰è·¨çº¿ç¨‹ä¼ é€’ ï¼ˆ2ï¼‰ä¼ é€’å‡ºé—­åŒ…ä½œç”¨åŸŸ2. task::spawn_blocking() ã€å›è°ƒé—­åŒ…ã€‘è¾“å…¥å‚æ•°å†…çš„ thread::sleep() ä¸æ˜¯é˜»å¡è¿è¡Œ task::spawn_blocking() çš„ä¸»çº¿ç¨‹ï¼Œè€Œæ˜¯é˜»å¡ä»ã€é˜»å¡ä»»åŠ¡çº¿ç¨‹æ± ã€‘ä¸­åˆ†é…æ¥è¿è¡Œé˜»å¡ä»»åŠ¡çš„ã€å·¥ä½œçº¿ç¨‹ã€‘ã€‚</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. æœ‰ä¸€ä¸ª promise/future å¤±è´¥å°±æ•´ä½“æ€§åœ°å¤±è´¥ã€‚2. try_join æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future çš„æˆåŠŸä¸å¤±è´¥ç»“æœéƒ½æ”¶é›†2. è¿”å›ç»“æœï¼š(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. ä»…åªæ”¶é›†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ promise/future2. try_race æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. æ”¶é›†ç¬¬ä¸€ä¸ªç»“æŸçš„ promise/futureï¼Œæ— è®ºå®ƒæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯å¤±è´¥æ”¶åœºã€‚2. è¿”å›ç»“æœï¼šT</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rustå…¬å¼€è¯¾ï¼šã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼š</strong>ã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹</p>
<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong> 2021å¹´8æœˆ15æ—¥ 20:30-21:30</p>
<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong></p>
<p>å¦‚æœæƒ³ç”¨ Rust å¼€å‘å¤§å‹ç›®ï¼Œæˆ–è€…å­¦ä¹ å¤§å‹é¡¹ç›®ä»£ç ï¼Œç‰¹åˆ«æ˜¯æ¡†æ¶çº§åˆ«çš„é¡¹ç›®ï¼Œé‚£ä¹ˆ Rust çš„å®æœºåˆ¶è‚¯å®šæ˜¯ä¸€ä¸ªå¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚ ä¾‹å¦‚ datafuse ä¸­çš„ä¸€äº›é…ç½®ç®¡ç†ï¼š
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>è¿™å°±æ˜¯é€šè¿‡å®å®ç°é…ç½®çš„ç»Ÿä¸€è¡Œä¸ºï¼Œä»£ç å‚è€ƒï¼š
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust è¯­è¨€å¼ºå¤§çš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¯ä»¥åˆ›å»ºå’Œåˆ©ç”¨å®ï¼Œä¸è¿‡åˆ›å»ºå®çœ‹èµ·æ¥æŒºå¤æ‚ï¼Œå¸¸å¸¸ä»¤åˆšæ¥è§¦ Rust çš„å¼€å‘è€…ç”Ÿç•æƒ§ã€‚ åœ¨æœ¬æ¬¡å…¬å¼€è¯¾ä¸­å¸®åŠ©ä½ ç†è§£ Rust Macro çš„åŸºæœ¬åŸç†ï¼Œå­¦ä¹ å¦‚ä½•åˆ›è‡ªå·²çš„ Rust å®ï¼Œä»¥åŠæŸ¥çœ‹æºç å­¦ä¹ å®çš„å®ç°ã€‚</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<ul>
<li>ä»€ä¹ˆæ˜¯ Rust å®</li>
<li>ä»€ä¹ˆæ˜¯å®è¿è¡ŒåŸç†</li>
<li>å¦‚ä½•åˆ›å»º Rust å®è¿‡ç¨‹</li>
<li>é˜…è¯» datafuse é¡¹ç›®æºç ï¼Œ å­¦ä¹ é¡¹ç›®ä¸­å®çš„å®ç°</li>
</ul>
<p><strong>è®²å¸ˆä»‹ç»</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rustå…¬å¼€è¯¾ï¼šç†è§£Rustçš„æ‰€æœ‰æƒ| Vol 2</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c107b830-9fe1-43dd-94a3-9efcd5544205">
<div class="article-summary-box-inner">
<span><p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼šã€Šç†è§£Rustæ‰€æœ‰æƒã€‹</strong></p>
<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š2021å¹´8æœˆ8æ—¥ 20:30-21:30</strong></p>
<p><strong>å˜‰å®¾è®²å¸ˆï¼š è‹æ—</strong></p>
<p><strong>å˜‰å®¾ä»‹ç»ï¼š</strong></p>
<p>Rustä¸­æ–‡ç¤¾åŒºæˆå‘˜ï¼Œå¤šç‚¹DmallæŠ€æœ¯Leaderï¼Œå‰æŠ˜800äº’è”ç½‘ç ”å‘å›¢é˜Ÿè´Ÿè´£äººã€10ä½™å¹´ä¸€çº¿ç ”å‘ç»éªŒã€‚å…·æœ‰å¤šå¹´çš„è½¯ä»¶å¼€å‘ç»éªŒ, ç†Ÿç»ƒRubyã€Javaã€Rustç­‰å¼€å‘è¯­è¨€, åŒæ—¶ä¹Ÿå‚ä¸è¿‡Rustä¸­æ–‡ç¤¾åŒºæ—¥æŠ¥ç»´æŠ¤å·¥ä½œã€‚</p>
<p><strong>è¯¾ç¨‹ä»‹ç»</strong></p>
<p>æœ¬æ¬¡è¯¾ç¨‹é€šè¿‡10ä¸ªå·¦å³çš„å°ä¾‹å­ï¼Œå¸¦å¤§å®¶ç†è§£ä¸€ä¸‹Rustçš„æ‰€æœ‰æƒï¼ŒRustå¼•ç”¨å’Œå€Ÿç”¨ï¼ŒRustå˜é‡å…‹éš†å’Œå¤åˆ¶çš„ç†å¿µã€‚</p>
<p><strong>å‚åŠ è¯¾ç¨‹</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/Rust-pbc-1.jpg" alt></p>
<p><strong>è¯¾ç¨‹è§„åˆ’</strong></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloudé¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">æ•°æ®è¡¨ Timestamp æ—¥æœŸ Serialize</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2ff8a69e-59bb-4502-87c0-c3416ffae8a0">
<div class="article-summary-box-inner">
<span><p>ä¸»è¦å‚è€ƒï¼š<a href="https://github.com/rustcc/forustm" rel="noopener noreferrer">Rustccç½‘ç«™æºç åº“</a></p>
<p>åœ¨å¤„ç†æ•°æ®è¡¨ä¸­æ—¥æœŸç›¸å…³æ•°æ®æ—¶ï¼ŒSeralizeåºåˆ—åŒ–ç›¸å…³æ“ä½œä¼šæŠ¥é”™ï¼Œæç¤º DateTime å­—æ®µä¸è¯†åˆ«ï¼Œ
æŸ¥äº† rustcc æºç æ‰å‘ç°ä¾èµ–ä¸­éœ€è¦å¼€å¯ç›¸åº”çš„featureã€‚ç‰¹æ­¤è®°å½•ã€‚</p>
<h2>1.ä¾èµ–çš„åº“ï¼š</h2>
<pre><code>[dependencies]
# æ—¥æœŸæ—¶é—´å¤„ç† éœ€è¦å¼€å¯ serde ç‰¹å¾ æ”¯æŒåºåˆ—åŒ–
chrono = { version = "0.4.19", features = ["serde"] }

# æ•°æ®åº“ORM
diesel = { version = "1.4.4", features = ["postgres", "chrono", "uuid", "r2d2"] }
dotenv = "0.15.0"
serde = { version = "1.0.127", features = ["derive"] }
serde_json = "1.0.66"
uuid = { version = "0.8.2", features = ["serde", "v4"] }
</code></pre>
<h2>2.åˆ›å»ºæ•°æ®è¡¨</h2>
<pre><code>CREATE TABLE characters (
    id SERIAL PRIMARY KEY,
    name VARCHAR(128) UNIQUE NOT NULL,
    age INTEGER NOT NULL DEFAULT 0,
    friends VARCHAR NOT NULL DEFAULT '',
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
)
</code></pre>
<h2>3.æ•°æ®è¡¨å¯¹åº”çš„ model</h2>
<pre><code>use chrono::{NaiveDateTime};
use serde::{Deserialize, Serialize};

#[derive(Queryable, Serialize, Deserialize, Debug)]
pub struct Characters {
    pub id: i32,
    pub name: String,
    pub age: i32,
    pub friends: String,
    // è¿™é‡Œçš„ NaiveDateTime æ—¥æœŸæ ¼å¼åºåˆ—åŒ–éœ€è¦å¼€å¯ç›¸å…³ features
    pub created_at: NaiveDateTime,
}
</code></pre>
<h2>4.è·å–æ•°æ®</h2>
<pre><code>use db::schema::characters;
use db::{get_connection};
use db::models::{Characters, NewCharacter};
use db::schema::characters::dsl::*;
use diesel::QueryDsl;
use diesel::prelude::*;

fn main() {
    let conn = get_connection();

    // æŸ¥è¯¢å¹´é¾„å¤§äº30çš„10æ¡æ•°æ®
    let arr: Vec&lt;Characters&gt; = characters.filter(characters::age.gt(30))
        .limit(10)
        .load::&lt;Characters&gt;(&amp;conn)
        .expect("Loading Error");

    let date_arr = arr.iter()
        .map(|item| {
	    // æ•°æ®æ ¼å¼åŒ–
            let t = item.created_at.format("%Y-%m-%d %H:%M:%S").to_string();
            println!("{} {}", item.name, t);
            t
        })
        .collect::&lt;Vec&lt;String&gt;&gt;();
}
</code></pre>
<p>è¾“å‡ºç»“æœç±»ä¼¼ï¼š</p>
<pre><code>Box 2021-08-05 09:39:34
Bobe 2021-08-05 09:39:34
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cargo workspace config</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c3dcce30-1fc0-4819-8992-142365c7e21c">
<div class="article-summary-box-inner">
<span><p><a href="https://kaisery.github.io/trpl-zh-cn/ch14-03-cargo-workspaces.html" rel="noopener noreferrer">Workspace æ–‡æ¡£é“¾æ¥</a></p>
<h2>ç›®å½•ç»“æ„</h2>
<pre><code>workspace-test/
    Cargo.toml
    db/
        src/
            bin/
                init.rs
        Cargo.tml
</code></pre>
<h2>workspace</h2>
<p>workspace-test/Cargo.toml</p>
<pre><code>[workspace]
members = ["db"]
default-member = "db"
</code></pre>
<h2>å­é¡¹ç›®</h2>
<p>workspace-test/db/Cargo.toml</p>
<pre><code>[package]
name = "db"
version = "0.1.0"
edition = "2018"

[dependencies]

# å¯é€‰çš„å¯æ‰§è¡Œæ–‡ä»¶é…ç½®
# [[bin]]
# name = "init"
# path = "src/bin/init.rs"
</code></pre>
<h2>æ“ä½œ</h2>
<pre><code># è¿è¡Œ init
cargo run --bin init
# -p æŒ‡å®šé¡¹ç›®
cargo run -p db --bin init
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=120035c3-944d-4a79-9b3a-8390697a6e13">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰</h1>
<p>ä¸åŒäº<code>javascript</code>çš„<code>new Promise((resolve, reject) =&gt; {...})</code>æ„é€ å³è¿è¡Œï¼Œ<code>Rust</code>ä¸­çš„<code>Future</code>æ˜¯Â·æƒ°æ€§Â·çŠ¶æ€æœºã€‚è¿™ä½“ç°ä¸ºï¼š</p>
<ol>
<li>ã€è°ƒç”¨å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€æ‰§è¡Œå¼‚æ­¥å—ã€‘ä»…åªæ„é€ ä¸€ä¸ª<code>Future trait object</code>ã€‚</li>
<li>å› ä¸º<code>Future</code>æ˜¯æƒ°æ€§çŠ¶æ€æœºï¼Œæ‰€ä»¥å®ƒä¸ä¼šè‡ªåŠ¨æ‰§è¡Œã€å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€å¼‚æ­¥å—ã€‘å†…çš„ä»»ä½•ä¸€è¡Œä»£ç  --- æ­¤ç‚¹ä¸<code>javascript</code>çš„Â·æ´»æ€§Â·çŠ¶æ€æœºå®Œå…¨ä¸åŒã€‚ç›¸åï¼Œéœ€è¦äººå·¥æ¿€æ´»è§¦å‘ã€‚</li>
<li>äººå·¥å¯åŠ¨<code>Future</code>è¿è¡Œï¼Œåˆåˆ†ä¸ºä¸¤ä¸ªåœºæ™¯çš„ä¸¤ç§æƒ…å†µï¼š
<ol>
<li>
<p>å·²ç»åœ¨<code>async fn</code>å†…ï¼Œ<code>Future.await</code>æ¿€æ´»ã€‚ä½†ï¼ŒåŒæ—¶<strong>é˜»å¡</strong>å½“å‰å¼‚æ­¥ç¨‹åºæ‰§è¡Œæµã€‚</p>
</li>
<li>
<p>åœ¨<code>async fn</code>å¤–ï¼Œéœ€è¦å€ŸåŠ©ç”±ã€è¿è¡Œæ—¶ã€‘æä¾›çš„ã€æ‰§è¡Œå™¨ã€‘ã€‚å°±<code>async-std</code>åº“è€Œè¨€ï¼Œæœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š</p>
<ol>
<li><code>task::block_on(Future)</code> æ‰§è¡Œ<code>Future</code>ä¸”é˜»å¡å½“å‰çº¿ç¨‹ç›´åˆ°<code>Future</code>è¢«å®Œæˆã€‚</li>
<li><code>task::spawn(Future)</code>ä»…æ‰§è¡Œ<code>Future</code>å’Œä¸é˜»å¡å½“å‰çº¿ç¨‹ã€‚</li>
</ol>
<p>æ— è®ºé€‰æ‹©ä¸Šé¢å“ªç§æ–¹å¼ï¼Œè‹¥åœ¨<code>Future</code>æ‰§è¡ŒæœŸé—´å‡ºç°äº†<code>panic</code>ï¼Œå…¶éƒ½ä¼šç»ˆæ­¢ï¼ˆ<code>abort</code>ï¼‰æ­£åœ¨å…±äº«åŒä¸€ä¸ªæ‰§è¡Œçº¿ç¨‹ï¼ˆ<code>thread</code>ï¼‰çš„æ‰€æœ‰<code>task</code>ï¼ˆÂ·æ— æ ˆÂ·åç¨‹ï¼‰çš„è¿è¡Œã€‚</p>
</li>
</ol>
</li>
</ol>
<p>é¢˜å¤–è¯ï¼Œ</p>
<ol>
<li>ç»¿è‰²çº¿ç¨‹æ˜¯Â·æœ‰æ ˆÂ·åç¨‹ï¼›å¼‚æ­¥å‡½æ•°ä¸å¼‚æ­¥å—æ˜¯Â·æ— æ ˆÂ·åç¨‹ã€‚</li>
<li>åœ¨<code>async-std</code>åº“çš„è¯æ±‡è¡¨å†…ï¼Œåç¨‹è¢«ç§°ä½œ<code>task</code>è€Œä¸æ˜¯æƒ¯ä¾‹çš„<code>coroutine</code>ã€‚</li>
<li><code>task::spawn(Future)</code>ä¹Ÿèƒ½è¢«ä½¿ç”¨äº<code>async fn</code>æˆ–<code>async {...}</code>å†…ã€‚å®ƒè¢«ç”¨æ¥ä»£æ›¿<code>.await</code>æŒ‡ä»¤ï¼Œä»¥<strong>éé˜»å¡</strong><code>async fn</code>æˆ–<code>async {...}</code>çš„æ–¹å¼ï¼Œæ¿€æ´»ä¸æ‰§è¡Œä¸€ä¸ª<code>Future</code>å®ä¾‹ã€‚</li>
</ol>
<h2>ä¾‹ç¨‹</h2>
<pre><code>async fn accept_loop(addr: impl ToSocketAddrs) -&gt; Result&lt;()&gt; {
    // 1. TcpListener::bind(addr) è¿”å› Future
    // 2. .await äº Future å–å¾— Result&lt;T, E&gt;
    // 3. Result&lt;T, E&gt;? å†æ‹¿å¾— Ok&lt;T&gt; ä¸­çš„ T
    let listener = TcpListener::bind(addr).await?; // å¼‚æ­¥å‡½æ•°å†…çš„äººå·¥å¯åŠ¨ Future
    let mut incoming = listener.incoming();
    // å› ä¸ºæ²¡æœ‰ä»è¯­è¨€å±‚é¢æ”¯æŒ async for loopï¼Œæ‰€ä»¥ while loop + Iterator&lt;Item = T&gt; æ¥æ¨¡æ‹Ÿä¹‹ã€‚
    while let Some(stream) = incoming.next().await {
        // TODO
    }
    Ok(())
}
fn main() {
    let fut = accept_loop("127.0.0.1:8080");
    task::block_on(fut); // å¼‚æ­¥å‡½æ•°å¤–çš„äººå·¥å¯åŠ¨ Future
}
</code></pre>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-08-30T01:30:00Z">08-30</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Seq2Seq Autoencoder via Contrastive Learning for Abstractive Text Summarization. (arXiv:2108.11992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11992">
<div class="article-summary-box-inner">
<span><p>In this paper, we present a denoising sequence-to-sequence (seq2seq)
autoencoder via contrastive learning for abstractive text summarization. Our
model adopts a standard Transformer-based architecture with a multi-layer
bi-directional encoder and an auto-regressive decoder. To enhance its denoising
ability, we incorporate self-supervised contrastive learning along with various
sentence-level document augmentation. These two components, seq2seq autoencoder
and contrastive learning, are jointly trained through fine-tuning, which
improves the performance of text summarization with regard to ROUGE scores and
human evaluation. We conduct experiments on two datasets and demonstrate that
our model outperforms many existing benchmarks and even achieves comparable
performance to the state-of-the-art abstractive systems trained with more
complex architecture and extensive computation resources.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A New Sentence Ordering Method Using BERT Pretrained Model. (arXiv:2108.11994v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11994">
<div class="article-summary-box-inner">
<span><p>Building systems with capability of natural language understanding (NLU) has
been one of the oldest areas of AI. An essential component of NLU is to detect
logical succession of events contained in a text. The task of sentence ordering
is proposed to learn succession of events with applications in AI tasks. The
performance of previous works employing statistical methods is poor, while the
neural networks-based approaches are in serious need of large corpora for model
learning. In this paper, we propose a method for sentence ordering which does
not need a training phase and consequently a large corpus for learning. To this
end, we generate sentence embedding using BERT pre-trained model and measure
sentence similarity using cosine similarity score. We suggest this score as an
indicator of sequential events' level of coherence. We finally sort the
sentences through brute-force search to maximize overall similarities of the
sequenced sentences. Our proposed method outperformed other baselines on
ROCStories, a corpus of 5-sentence human-made stories. The method is
specifically more efficient than neural network-based methods when no huge
corpus is available. Among other advantages of this method are its
interpretability and needlessness to linguistic knowledge.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa. (arXiv:2108.12009v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12009">
<div class="article-summary-box-inner">
<span><p>We present EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with
RoBERTa, a simple yet expressive scheme of solving the ERC (emotion recognition
in conversation) task. By simply prepending speaker names to utterances and
inserting separation tokens between the utterances in a dialogue, EmoBERTa can
learn intra- and inter- speaker states and context to predict the emotion of a
current speaker, in an end-to-end manner. Our experiments show that we reach a
new state of the art on the two popular ERC datasets using a basic and
straight-forward approach. We've open sourced our code and models at
https://github.com/tae898/erc.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic-based Self-Critical Training For Question Generation. (arXiv:2108.12026v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12026">
<div class="article-summary-box-inner">
<span><p>We present in this work a fully Transformer-based reinforcement learning
generator-evaluator architecture for neural question generation. Question
generation is a task that consists in generating questions given a context and
answer. To improve the quality of the generated question, we came up with a
semantic-based self-critical training layout in generator-evaluator
architecture, which goes beyond typical maximum likelihood training. Evaluation
metrics for language modeling only based on n-gram overlapping do not consider
semantic relations between reference and candidate strings. To improve the
evaluation step, we assess our model for both n-gram overlap using BLEU and
semantically using BERTScore and NUBIA, a novel state-of-the-art evaluation
metric for text generation. Question generation could be used in many
downstream applications, including in extending question answering datasets,
conversational systems, and educational assessment systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using GAN-based models to sentimental analysis on imbalanced datasets in education domain. (arXiv:2108.12061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12061">
<div class="article-summary-box-inner">
<span><p>While the whole world is still struggling with the COVID-19 pandemic, online
learning and home office become more common. Many schools transfer their
courses teaching to the online classroom. Therefore, it is significant to mine
the students' feedback and opinions from their reviews towards studies so that
both schools and teachers can know where they need to improve. This paper
trains machine learning and deep learning models using both balanced and
imbalanced datasets for sentiment classification. Two SOTA category-aware text
generation GAN models: CatGAN and SentiGAN, are utilized to synthesize text
used to balance the highly imbalanced dataset. Results on three datasets with
different imbalance degree from distinct domains show that when using generated
text to balance the dataset, the F1-score of machine learning and deep learning
model on sentiment classification increases 2.79% ~ 9.28%. Also, the results
indicate that the average growth degree for CR100k is higher than CR23k, the
average growth degree for deep learning is more increased than machine learning
algorithms, and the average growth degree for more complex deep learning models
is more increased than simpler deep learning models in experiments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">4-bit Quantization of LSTM-based Speech Recognition Models. (arXiv:2108.12074v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12074">
<div class="article-summary-box-inner">
<span><p>We investigate the impact of aggressive low-precision representations of
weights and activations in two families of large LSTM-based architectures for
Automatic Speech Recognition (ASR): hybrid Deep Bidirectional LSTM - Hidden
Markov Models (DBLSTM-HMMs) and Recurrent Neural Network - Transducers
(RNN-Ts). Using a 4-bit integer representation, a na\"ive quantization approach
applied to the LSTM portion of these models results in significant Word Error
Rate (WER) degradation. On the other hand, we show that minimal accuracy loss
is achievable with an appropriate choice of quantizers and initializations. In
particular, we customize quantization schemes depending on the local properties
of the network, improving recognition performance while limiting computational
time. We demonstrate our solution on the Switchboard (SWB) and CallHome (CH)
test sets of the NIST Hub5-2000 evaluation. DBLSTM-HMMs trained with 300 or
2000 hours of SWB data achieves $&lt;$0.5% and $&lt;$1% average WER degradation,
respectively. On the more challenging RNN-T models, our quantization strategy
limits degradation in 4-bit inference to 1.3%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12084">
<div class="article-summary-box-inner">
<span><p>Gender is widely discussed in the context of language tasks and when
examining the stereotypes propagated by language models. However, current
discussions primarily treat gender as binary, which can perpetuate harms such
as the cyclical erasure of non-binary gender identities. These harms are driven
by model and dataset biases, which are consequences of the non-recognition and
lack of understanding of non-binary genders in society. In this paper, we
explain the complexity of gender and language around it, and survey non-binary
persons to understand harms associated with the treatment of gender as binary
in English language technologies. We also detail how current language
representations (e.g., GloVe, BERT) capture and perpetuate these harms and
related challenges that need to be acknowledged and addressed for
representations to equitably encode gender information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lingxi: A Diversity-aware Chinese Modern Poetry Generation System. (arXiv:2108.12108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12108">
<div class="article-summary-box-inner">
<span><p>Poetry generation has been a difficult task in natural language processing.
Unlike plain neural text generation tasks, poetry has a high requirement for
novelty, since an easily-understood sentence with too many high frequency words
might not be considered as poetic, while adequately ambiguous sentences with
low frequency words can possibly be novel and creative. Inspired by this, we
present Lingxi, a diversity-aware Chinese modern poetry generation system. We
propose nucleus sampling with randomized head (NS-RH) algorithm, which
randomizes the high frequency part ("head") of the predicted distribution, in
order to emphasize on the "comparatively low frequency" words. The proposed
algorithm can significantly increase the novelty of generated poetry compared
with traditional sampling methods. The permutation of distribution is
controllable by tuning the filtering parameter that determines the "head" to
permutate, achieving diversity-aware sampling. We find that even when a large
portion of filtered vocabulary is randomized, it can actually generate fluent
poetry but with notably higher novelty. We also propose a
semantic-similarity-based rejection sampling algorithm, which creates longer
and more informative context on the basis of the short input poetry title while
maintaining high semantic similarity to the title, alleviating the off-topic
issue.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Generation of Accurate \& Fluent Medical X-ray Reports. (arXiv:2108.12126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12126">
<div class="article-summary-box-inner">
<span><p>Our paper focuses on automating the generation of medical reports from chest
X-ray image inputs, a critical yet time-consuming task for radiologists. Unlike
existing medical re-port generation efforts that tend to produce human-readable
reports, we aim to generate medical reports that are both fluent and clinically
accurate. This is achieved by our fully differentiable and end-to-end paradigm
containing three complementary modules: taking the chest X-ray images and
clinical his-tory document of patients as inputs, our classification module
produces an internal check-list of disease-related topics, referred to as
enriched disease embedding; the embedding representation is then passed to our
transformer-based generator, giving rise to the medical reports; meanwhile, our
generator also pro-duces the weighted embedding representation, which is fed to
our interpreter to ensure consistency with respect to disease-related
topics.Our approach achieved promising results on commonly-used metrics
concerning language fluency and clinical accuracy. Moreover, noticeable
performance gains are consistently ob-served when additional input information
is available, such as the clinical document and extra scans of different views.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Secoco: Self-Correcting Encoding for Neural Machine Translation. (arXiv:2108.12137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12137">
<div class="article-summary-box-inner">
<span><p>This paper presents Self-correcting Encoding (Secoco), a framework that
effectively deals with input noise for robust neural machine translation by
introducing self-correcting predictors. Different from previous robust
approaches, Secoco enables NMT to explicitly correct noisy inputs and delete
specific errors simultaneously with the translation decoding process. Secoco is
able to achieve significant improvements over strong baselines on two
real-world test sets and a benchmark WMT dataset with good interpretability. We
will make our code and dataset publicly available soon.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving callsign recognition with air-surveillance data in air-traffic communication. (arXiv:2108.12156v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12156">
<div class="article-summary-box-inner">
<span><p>Automatic Speech Recognition (ASR) can be used as the assistance of speech
communication between pilots and air-traffic controllers. Its application can
significantly reduce the complexity of the task and increase the reliability of
transmitted information. Evidently, high accuracy predictions are needed to
minimize the risk of errors. Especially, high accuracy is required in
recognition of key information, such as commands and callsigns, used to
navigate pilots. Our results prove that the surveillance data containing
callsigns can help to considerably improve the recognition of a callsign in an
utterance when the weights of probable callsign n-grams are reduced per
utterance. In this paper, we investigate two approaches: (1) G-boosting, when
callsigns weights are adjusted at language model level (G) and followed by the
dynamic decoder with an on-the-fly composition, and (2) lattice rescoring when
callsign information is introduced on top of lattices generated using a
conventional decoder. Boosting callsign n-grams with the combination of two
methods allowed us to gain 28.4% of absolute improvement in callsign
recognition accuracy and up to 74.2% of relative improvement in WER of callsign
recognition.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Grammar Based Identification Of Speaker Role For Improving ATCO And Pilot ASR. (arXiv:2108.12175v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12175">
<div class="article-summary-box-inner">
<span><p>Assistant Based Speech Recognition (ABSR) for air traffic control is
generally trained by pooling both Air Traffic Controller (ATCO) and pilot data.
In practice, this is motivated by the fact that the proportion of pilot data is
lesser compared to ATCO while their standard language of communication is
similar. However, due to data imbalance of ATCO and pilot and their varying
acoustic conditions, the ASR performance is usually significantly better for
ATCOs than pilots. In this paper, we propose to (1) split the ATCO and pilot
data using an automatic approach exploiting ASR transcripts, and (2) consider
ATCO and pilot ASR as two separate tasks for Acoustic Model (AM) training. For
speaker role classification of ATCO and pilot data, a hypothesized ASR
transcript is generated with a seed model, subsequently used to classify the
speaker role based on the knowledge extracted from grammar defined by
International Civil Aviation Organization (ICAO). This approach provides an
average speaker role identification accuracy of 83% for ATCO and pilot.
Finally, we show that training AMs separately for each task, or using a
multitask approach is well suited for this data compared to AM trained by
pooling all data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling. (arXiv:2108.12177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12177">
<div class="article-summary-box-inner">
<span><p>Social media has effectively become the prime hub of communication and
digital marketing. As these platforms enable the free manifestation of thoughts
and facts in text, images and video, there is an extensive need to screen them
to protect individuals and groups from offensive content targeted at them. Our
work intends to classify codemixed social media comments/posts in the Dravidian
languages of Tamil, Kannada, and Malayalam. We intend to improve offensive
language identification by generating pseudo-labels on the dataset. A custom
dataset is constructed by transliterating all the code-mixed texts into the
respective Dravidian language, either Kannada, Malayalam, or Tamil and then
generating pseudo-labels for the transliterated dataset. The two datasets are
combined using the generated pseudo-labels to create a custom dataset called
CMTRA. As Dravidian languages are under-resourced, our approach increases the
amount of training data for the language models. We fine-tune several recent
pretrained language models on the newly constructed dataset. We extract the
pretrained language embeddings and pass them onto recurrent neural networks. We
observe that fine-tuning ULMFiT on the custom dataset yields the best results
on the code-mixed test sets of all three languages. Our approach yields the
best results among the benchmarked models on Tamil-English, achieving a
weighted F1-Score of 0.7934 while scoring competitive weighted F1-Scores of
0.9624 and 0.7306 on the code-mixed test sets of Malayalam-English and
Kannada-English, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions. (arXiv:2108.12189v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12189">
<div class="article-summary-box-inner">
<span><p>This paper presents Macquarie University's participation to the BioASQ
Synergy Task, and BioASQ9b Phase B. In each of these tasks, our participation
focused on the use of query-focused extractive summarisation to obtain the
ideal answers to medical questions. The Synergy Task is an end-to-end question
answering task on COVID-19 where systems are required to return relevant
documents, snippets, and answers to a given question. Given the absence of
training data, we used a query-focused summarisation system that was trained
with the BioASQ8b training data set and we experimented with methods to
retrieve the documents and snippets. Considering the poor quality of the
documents and snippets retrieved by our system, we observed reasonably good
quality in the answers returned. For phase B of the BioASQ9b task, the relevant
documents and snippets were already included in the test data. Our system split
the snippets into candidate sentences and used BERT variants under a sentence
classification setup. The system used the question and candidate sentence as
input and was trained to predict the likelihood of the candidate sentence being
part of the ideal answer. The runs obtained either the best or second best
ROUGE-F1 results of all participants to all batches of BioASQ9b. This shows
that using BERT in a classification setup is a very strong baseline for the
identification of ideal answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translation Error Detection as Rationale Extraction. (arXiv:2108.12197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12197">
<div class="article-summary-box-inner">
<span><p>Recent Quality Estimation (QE) models based on multilingual pre-trained
representations have achieved very competitive results when predicting the
overall quality of translated sentences. Predicting translation errors, i.e.
detecting specifically which words are incorrect, is a more challenging task,
especially with limited amounts of training data. We hypothesize that, not
unlike humans, successful QE models rely on translation errors to predict
overall sentence quality. By exploring a set of feature attribution methods
that assign relevance scores to the inputs to explain model predictions, we
study the behaviour of state-of-the-art sentence-level QE models and show that
explanations (i.e. rationales) extracted from these models can indeed be used
to detect translation errors. We therefore (i) introduce a novel
semi-supervised method for word-level QE and (ii) propose to use the QE task as
a new benchmark for evaluating the plausibility of feature attribution, i.e.
how interpretable model explanations are to humans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features are dependent
upon each other. Experiment results on five public datasets show that our model
performs significantly better than previous approaches. The source code can be
found in https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors. (arXiv:2108.12216v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12216">
<div class="article-summary-box-inner">
<span><p>In this paper, we explore the capacity of a language model-based method for
grammatical error detection in detail. We first show that 5 to 10% of training
data are enough for a BERT-based error detection method to achieve performance
equivalent to a non-language model-based method can achieve with the full
training data; recall improves much faster with respect to training data size
in the BERT-based method than in the non-language model method while precision
behaves similarly. These suggest that (i) the BERT-based method should have a
good knowledge of grammar required to recognize certain types of error and that
(ii) it can transform the knowledge into error detection rules by fine-tuning
with a few training samples, which explains its high generalization ability in
grammatical error detection. We further show with pseudo error data that it
actually exhibits such nice properties in learning rules for recognizing
various types of error. Finally, based on these findings, we explore a
cost-effective method for detecting grammatical errors with feedback comments
explaining relevant grammatical rules to learners.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Injecting Text in Self-Supervised Speech Pretraining. (arXiv:2108.12226v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12226">
<div class="article-summary-box-inner">
<span><p>Self-supervised pretraining for Automated Speech Recognition (ASR) has shown
varied degrees of success. In this paper, we propose to jointly learn
representations during pretraining from two different modalities: speech and
text. The proposed method, tts4pretrain complements the power of contrastive
learning in self-supervision with linguistic/lexical representations derived
from synthesized speech, effectively learning from untranscribed speech and
unspoken text. Lexical learning in the speech encoder is enforced through an
additional sequence loss term that is coupled with contrastive loss during
pretraining. We demonstrate that this novel pretraining method yields Word
Error Rate (WER) reductions of 10% relative on the well-benchmarked,
Librispeech task over a state-of-the-art baseline pretrained with wav2vec2.0
only. The proposed method also serves as an effective strategy to compensate
for the lack of transcribed speech, effectively matching the performance of
5000 hours of transcribed speech with just 100 hours of transcribed speech on
the AMI meeting transcription task. Finally, we demonstrate WER reductions of
up to 15% on an in-house Voice Search task over traditional pretraining.
Incorporating text into encoder pretraining is complimentary to rescoring with
a larger or in-domain language model, resulting in additional 6% relative
reduction in WER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12229">
<div class="article-summary-box-inner">
<span><p>The ability to detect Out-of-Domain (OOD) inputs has been a critical
requirement in many real-world NLP applications since the inclusion of
unsupported OOD inputs may lead to catastrophic failure of systems. However, it
remains an empirical question whether current algorithms can tackle such
problem reliably in a realistic scenario where zero OOD training data is
available. In this study, we propose ProtoInfoMax, a new architecture that
extends Prototypical Networks to simultaneously process In-Domain (ID) and OOD
sentences via Mutual Information Maximization (InfoMax) objective. Experimental
results show that our proposed method can substantially improve performance up
to 20% for OOD detection in low resource settings of text classification. We
also show that ProtoInfoMax is less prone to typical over-confidence Error of
Neural Networks, leading to more reliable ID and OOD prediction outcomes.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Evaluating the Robustness of Neural Language Models to Input Perturbations. (arXiv:2108.12237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12237">
<div class="article-summary-box-inner">
<span><p>High-performance neural language models have obtained state-of-the-art
results on a wide range of Natural Language Processing (NLP) tasks. However,
results for common benchmark datasets often do not reflect model reliability
and robustness when applied to noisy, real-world data. In this study, we design
and implement various types of character-level and word-level perturbation
methods to simulate realistic scenarios in which input texts may be slightly
noisy or different from the data distribution on which NLP systems were
trained. Conducting comprehensive experiments on different NLP tasks, we
investigate the ability of high-performance language models such as BERT,
XLNet, RoBERTa, and ELMo in handling different types of input perturbations.
The results suggest that language models are sensitive to input perturbations
and their performance can decrease even when small changes are introduced. We
highlight that models need to be further improved and that current benchmarks
are not reflecting model robustness well. We argue that evaluations on
perturbed inputs should routinely complement widely-used benchmarks in order to
yield a more realistic understanding of NLP systems robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Deep learning models are not robust against noise in clinical text. (arXiv:2108.12242v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12242">
<div class="article-summary-box-inner">
<span><p>Artificial Intelligence (AI) systems are attracting increasing interest in
the medical domain due to their ability to learn complicated tasks that require
human intelligence and expert knowledge. AI systems that utilize
high-performance Natural Language Processing (NLP) models have achieved
state-of-the-art results on a wide variety of clinical text processing
benchmarks. They have even outperformed human accuracy on some tasks. However,
performance evaluation of such AI systems have been limited to accuracy
measures on curated and clean benchmark datasets that may not properly reflect
how robustly these systems can operate in real-world situations. In order to
address this challenge, we introduce and implement a wide variety of
perturbation methods that simulate different types of noise and variability in
clinical text data. While noisy samples produced by these perturbation methods
can often be understood by humans, they may cause AI systems to make erroneous
decisions. Conducting extensive experiments on several clinical text processing
tasks, we evaluated the robustness of high-performance NLP models against
various types of character-level and word-level noise. The results revealed
that the NLP models performance degrades when the input contains small amounts
of noise. This study is a significant step towards exposing vulnerabilities of
AI models utilized in clinical text processing systems. The proposed
perturbation methods can be used in performance evaluation tests to assess how
robustly clinical NLP models can operate on noisy data, in real-world settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Detecting Propaganda on the Sentence Level during the COVID-19 Pandemic. (arXiv:2108.12269v1 [cs.CY])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12269">
<div class="article-summary-box-inner">
<span><p>The spread of misinformation, conspiracy, and questionable content and
information manipulation by foreign adversaries on social media has surged
along with the COVID-19 pandemic. Such malicious cyber-enabled actions may
cause increasing social polarization, health crises, and property loss. In this
paper, using fine-tuned contextualized embedding trained on Reddit, we tackle
the detection of the propaganda of such user accounts and their targeted issues
on Twitter during March 2020 when the COVID-19 epidemic became recognized as a
pandemic. Our result shows that the pro-China group appeared to be tweeting 35
to 115 times more than the neutral group. At the same time, neutral groups were
tweeting more positive-attitude content and voicing alarm for the COVID-19
situation. The pro-China group was also using more call-for-action words on
political issues not necessarily China-related.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can the Transformer Be Used as a Drop-in Replacement for RNNs in Text-Generating GANs?. (arXiv:2108.12275v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12275">
<div class="article-summary-box-inner">
<span><p>In this paper we address the problem of fine-tuned text generation with a
limited computational budget. For that, we use a well-performing text
generative adversarial network (GAN) architecture - Diversity-Promoting GAN
(DPGAN), and attempted a drop-in replacement of the LSTM layer with a
self-attention-based Transformer layer in order to leverage their efficiency.
The resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance,
quality and diversity of generated text and stability. Computational
experiments suggested that a transformer architecture is unable to drop-in
replace the LSTM layer, under-performing during the pre-training phase and
undergoing a complete mode collapse during the GAN tuning phase. Our results
suggest that the transformer architecture need to be adapted before it can be
used as a replacement for RNNs in text-generating GANs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tree Decomposition Attention for AMR-to-Text Generation. (arXiv:2108.12300v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12300">
<div class="article-summary-box-inner">
<span><p>Text generation from AMR requires mapping a semantic graph to a string that
it annotates. Transformer-based graph encoders, however, poorly capture vertex
dependencies that may benefit sequence prediction. To impose order on an
encoder, we locally constrain vertex self-attention using a graph's tree
decomposition. Instead of forming a full query-key bipartite graph, we restrict
attention to vertices in parent, subtree, and same-depth bags of a vertex. This
hierarchical context lends both sparsity and structure to vertex state updates.
We apply dynamic programming to derive a forest of tree decompositions,
choosing the most structurally similar tree to the AMR. Our system outperforms
a self-attentive baseline by 1.6 BLEU and 1.8 chrF++.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Latent Tree Decomposition Parsers for AMR-to-Text Generation. (arXiv:2108.12304v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12304">
<div class="article-summary-box-inner">
<span><p>Graph encoders in AMR-to-text generation models often rely on neighborhood
convolutions or global vertex attention. While these approaches apply to
general graphs, AMRs may be amenable to encoders that target their tree-like
structure. By clustering edges into a hierarchy, a tree decomposition
summarizes graph structure. Our model encodes a derivation forest of tree
decompositions and extracts an expected tree. From tree node embeddings, it
builds graph edge features used in vertex attention of the graph encoder.
Encoding TD forests instead of shortest-pairwise paths in a self-attentive
baseline raises BLEU by 0.7 and chrF++ by 0.3. The forest encoder also
surpasses a convolutional baseline for molecular property prediction by 1.92%
ROC-AUC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CAPE: Context-Aware Private Embeddings for Private Language Learning. (arXiv:2108.12318v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12318">
<div class="article-summary-box-inner">
<span><p>Deep learning-based language models have achieved state-of-the-art results in
a number of applications including sentiment analysis, topic labelling, intent
classification and others. Obtaining text representations or embeddings using
these models presents the possibility of encoding personally identifiable
information learned from language and context cues that may present a risk to
reputation or privacy. To ameliorate these issues, we propose Context-Aware
Private Embeddings (CAPE), a novel approach which preserves privacy during
training of embeddings. To maintain the privacy of text representations, CAPE
applies calibrated noise through differential privacy, preserving the encoded
semantic links while obscuring sensitive information. In addition, CAPE employs
an adversarial training regime that obscures identified private variables.
Experimental results demonstrate that the proposed approach reduces private
information leakage better than either single intervention.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning. (arXiv:2108.12370v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12370">
<div class="article-summary-box-inner">
<span><p>We demonstrate a library for the integration of domain knowledge in deep
learning architectures. Using this library, the structure of the data is
expressed symbolically via graph declarations and the logical constraints over
outputs or latent variables can be seamlessly added to the deep models. The
domain knowledge can be defined explicitly, which improves the models'
explainability in addition to the performance and generalizability in the
low-data regime. Several approaches for such an integration of symbolic and
sub-symbolic models have been introduced; however, there is no library to
facilitate the programming for such an integration in a generic way while
various underlying algorithms can be used. Our library aims to simplify
programming for such an integration in both training and inference phases while
separating the knowledge representation from learning algorithms. We showcase
various NLP benchmark tasks and beyond. The framework is publicly available at
Github(https://github.com/HLR/DomiKnowS).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. (arXiv:2108.12409v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12409">
<div class="article-summary-box-inner">
<span><p>Since the introduction of the transformer model by Vaswani et al. (2017), a
fundamental question remains open: how to achieve extrapolation at inference
time to longer sequences than seen during training? We first show that
extrapolation can be improved by changing the position representation method,
though we find that existing proposals do not allow efficient extrapolation. We
introduce a simple and efficient method, Attention with Linear Biases (ALiBi),
that allows for extrapolation. ALiBi does not add positional embeddings to the
word embeddings; instead, it biases the query-key attention scores with a term
that is proportional to their distance. We show that this method allows
training a 1.3 billion parameter model on input sequences of length 1024 that
extrapolates to input sequences of length 2048, achieving the same perplexity
as a sinusoidal position embedding model trained on inputs of length 2048, 11%
faster and using 11% less memory. ALiBi's inductive bias towards recency allows
it to outperform multiple strong position methods on the WikiText-103
benchmark. Finally, we provide analysis of ALiBi to understand why it leads to
better performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach. (arXiv:2003.10715v2 [cs.IR] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2003.10715">
<div class="article-summary-box-inner">
<span><p>Knowledge about the software used in scientific investigations is necessary
for different reasons, including provenance of the results, measuring software
impact to attribute developers, and bibliometric software citation analysis in
general. Additionally, providing information about whether and how the software
and the source code are available allows an assessment about the state and role
of open source software in science in general. While such analyses can be done
manually, large scale analyses require the application of automated methods of
information extraction and linking. In this paper, we present SoftwareKG - a
knowledge graph that contains information about software mentions from more
than 51,000 scientific articles from the social sciences. A silver standard
corpus, created by a distant and weak supervision approach, and a gold standard
corpus, created by manual annotation, were used to train an LSTM based neural
network to identify software mentions in scientific articles. The model
achieves a recognition rate of .82 F-score in exact matches. As a result, we
identified more than 133,000 software mentions. For entity disambiguation, we
used the public domain knowledge base DBpedia. Furthermore, we linked the
entities of the knowledge graph to other knowledge bases such as the Microsoft
Academic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we
illustrate, how SoftwareKG can be used to assess the role of software in the
social sciences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A review of on-device fully neural end-to-end automatic speech recognition algorithms. (arXiv:2012.07974v3 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.07974">
<div class="article-summary-box-inner">
<span><p>In this paper, we review various end-to-end automatic speech recognition
algorithms and their optimization techniques for on-device applications.
Conventional speech recognition systems comprise a large number of discrete
components such as an acoustic model, a language model, a pronunciation model,
a text-normalizer, an inverse-text normalizer, a decoder based on a Weighted
Finite State Transducer (WFST), and so on. To obtain sufficiently high speech
recognition accuracy with such conventional speech recognition systems, a very
large language model (up to 100 GB) is usually needed. Hence, the corresponding
WFST size becomes enormous, which prohibits their on-device implementation.
Recently, fully neural network end-to-end speech recognition algorithms have
been proposed. Examples include speech recognition systems based on
Connectionist Temporal Classification (CTC), Recurrent Neural Network
Transducer (RNN-T), Attention-based Encoder-Decoder models (AED), Monotonic
Chunk-wise Attention (MoChA), transformer-based speech recognition systems, and
so on. These fully neural network-based systems require much smaller memory
footprints compared to conventional algorithms, therefore their on-device
implementation has become feasible. In this paper, we review such end-to-end
speech recognition models. We extensively discuss their structures,
performance, and advantages compared to conventional algorithms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00433">
<div class="article-summary-box-inner">
<span><p>Broader disclosive transparency$-$truth and clarity in communication
regarding the function of AI systems$-$is widely considered desirable.
Unfortunately, it is a nebulous concept, difficult to both define and quantify.
This is problematic, as previous work has demonstrated possible trade-offs and
negative consequences to disclosive transparency, such as a confusion effect,
where 'too much information' clouds a reader's understanding of what a system
description means. Disclosive transparency's subjective nature has rendered
deep study into these problems and their remedies difficult. To improve this
state of affairs, We introduce neural language model-based probabilistic
metrics to directly model disclosive transparency, and demonstrate that they
correlate with user and expert opinions of system transparency, making them a
valid objective proxy. Finally, we demonstrate the use of these metrics in a
pilot study quantifying the relationships between transparency, confusion, and
user perceptions in a corpus of real NLP system descriptions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Semi-Supervised Learning: An Approach To Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems. (arXiv:2104.03643v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03643">
<div class="article-summary-box-inner">
<span><p>Air traffic management and specifically air-traffic control (ATC) rely mostly
on voice communications between Air Traffic Controllers (ATCos) and pilots. In
most cases, these voice communications follow a well-defined grammar that could
be leveraged in Automatic Speech Recognition (ASR) technologies. The callsign
used to address an airplane is an essential part of all ATCo-pilot
communications. We propose a two-steps approach to add contextual knowledge
during semi-supervised training to reduce the ASR system error rates at
recognizing the part of the utterance that contains the callsign. Initially, we
represent in a WFST the contextual knowledge (i.e. air-surveillance data) of an
ATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the
contextual knowledge is added by second-pass decoding (i.e. lattice
re-scoring). Results show that `unseen domains' (e.g. data from airports not
present in the supervised training data) are further aided by contextual SSL
when compared to standalone SSL. For this task, we introduce the Callsign Word
Error Rate (CA-WER) as an evaluation metric, which only assesses ASR
performance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER
relative improvement applying SSL with an additional 17.5% CA-WER improvement
by adding contextual knowledge during SSL on a challenging ATC-based test set
gathered from LiveATC.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning to Remove: Towards Isotropic Pre-trained BERT Embedding. (arXiv:2104.05274v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.05274">
<div class="article-summary-box-inner">
<span><p>Pre-trained language models such as BERT have become a more common choice of
natural language processing (NLP) tasks. Research in word representation shows
that isotropic embeddings can significantly improve performance on downstream
tasks. However, we measure and analyze the geometry of pre-trained BERT
embedding and find that it is far from isotropic. We find that the word vectors
are not centered around the origin, and the average cosine similarity between
two random words is much higher than zero, which indicates that the word
vectors are distributed in a narrow cone and deteriorate the representation
capacity of word embedding. We propose a simple, and yet effective method to
fix this problem: remove several dominant directions of BERT embedding with a
set of learnable weights. We train the weights on word similarity tasks and
show that processed embedding is more isotropic. Our method is evaluated on
three standardized tasks: word similarity, word analogy, and semantic textual
similarity. In all tasks, the word embedding processed by our method
consistently outperforms the original embedding (with average improvement of
13% on word analogy and 16% on semantic textual similarity) and two baseline
methods. Our method is also proven to be more robust to changes of
hyperparameter.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06669">
<div class="article-summary-box-inner">
<span><p>We propose the task of Narrative Reordering (NAREOR) which involves rewriting
a given story in a different narrative order while preserving its plot. We
present a dataset, NAREORC, with human rewritings of stories within ROCStories
in non-linear orders, and conduct a detailed analysis of it. Further, we
propose novel task-specific training methods with suitable evaluation metrics.
We perform experiments on NAREORC using state-of-the-art models such as BART
and T5 and conduct extensive automatic and human evaluations. We demonstrate
that NAREOR is a challenging task with potential for further exploration.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08315">
<div class="article-summary-box-inner">
<span><p>Large language models have shown promising results in zero-shot settings
(Brown et al.,2020; Radford et al., 2019). For example, they can perform
multiple choice tasks simply by conditioning on a question and selecting the
answer with the highest probability.
</p>
<p>However, ranking by string probability can be problematic due to surface form
competition-wherein different surface forms compete for probability mass, even
if they represent the same underlying concept, e.g. "computer" and "PC." Since
probability mass is finite, this lowers the probability of the correct answer,
due to competition from other strings that are valid answers (but not one of
the multiple choice options).
</p>
<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative
scoring function that directly compensates for surface form competition by
simply reweighing each option according to a term that is proportional to its a
priori likelihood within the context of the specific zero-shot task. It
achieves consistent gains in zero-shot performance over both calibrated (Zhao
et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models
over a variety of multiple choice datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. (arXiv:2104.08808v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08808">
<div class="article-summary-box-inner">
<span><p>The ability to continuously expand knowledge over time and utilize it to
rapidly generalize to new tasks is a key feature of human linguistic
intelligence. Existing models that pursue rapid generalization to new tasks
(e.g., few-shot learning methods), however, are mostly trained in a single shot
on fixed datasets, unable to dynamically expand their knowledge; while
continual learning algorithms are not specifically designed for rapid
generalization. We present a new learning setup, Continual Learning of Few-Shot
Learners (CLIF), to address the challenges of both learning settings in a
unified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks
arriving sequentially, accumulating knowledge for improved generalization to
new tasks, while also retaining performance on the tasks learned earlier. We
examine how the generalization ability is affected in the continual learning
setup, evaluate a number of continual learning algorithms, and propose a novel
regularized adapter generation approach. We find that catastrophic forgetting
affects generalization ability to a less degree than performance on seen tasks;
while continual learning algorithms can still bring considerable benefit to the
generalization ability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts. (arXiv:2104.08809v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08809">
<div class="article-summary-box-inner">
<span><p>Determining coreference of concept mentions across multiple documents is a
fundamental task in natural language understanding. Work on cross-document
coreference resolution (CDCR) typically considers mentions of events in the
news, which seldom involve abstract technical concepts that are prevalent in
science and technology. These complex concepts take diverse or ambiguous forms
and have many hierarchical levels of granularity (e.g., tasks and subtasks),
posing challenges for CDCR. We present a new task of Hierarchical CDCR (H-CDCR)
with the goal of jointly inferring coreference clusters and hierarchy between
them. We create SciCo, an expert-annotated dataset for H-CDCR in scientific
papers, 3X larger than the prominent ECB+ resource. We study strong baseline
models that we customize for H-CDCR, and highlight challenges for future work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. (arXiv:2104.08836v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08836">
<div class="article-summary-box-inner">
<span><p>Multimodal pre-training with text, layout, and image has achieved SOTA
performance for visually-rich document understanding tasks recently, which
demonstrates the great potential for joint learning across different
modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model
for multilingual document understanding, which aims to bridge the language
barriers for visually-rich document understanding. To accurately evaluate
LayoutXLM, we also introduce a multilingual form understanding benchmark
dataset named XFUND, which includes form understanding samples in 7 languages
(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and
key-value pairs are manually labeled for each language. Experiment results show
that the LayoutXLM model has significantly outperformed the existing SOTA
cross-lingual pre-trained models on the XFUND dataset. The pre-trained
LayoutXLM model and the XFUND dataset are publicly available at
https://aka.ms/layoutxlm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.09106">
<div class="article-summary-box-inner">
<span><p>Subword units are commonly used for end-to-end automatic speech recognition
(ASR), while a fully acoustic-oriented subword modeling approach is somewhat
missing. We propose an acoustic data-driven subword modeling (ADSM) approach
that adapts the advantages of several text-based and acoustic-based subword
methods into one pipeline. With a fully acoustic-oriented label design and
learning process, ADSM produces acoustic-structured subword units and
acoustic-matched target sequence for further ASR training. The obtained ADSM
labels are evaluated with different end-to-end ASR approaches including CTC,
RNN-Transducer and attention models. Experiments on the LibriSpeech corpus show
that ADSM clearly outperforms both byte pair encoding (BPE) and
pronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis
shows that ADSM achieves acoustically more logical word segmentation and more
balanced sequence length, and thus, is suitable for both time-synchronous and
label-synchronous models. We also briefly describe how to apply acoustic-based
subword regularization and unseen text segmentation using ADSM.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Guided Curriculum Learning for Neural Machine Translation. (arXiv:2105.04475v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.04475">
<div class="article-summary-box-inner">
<span><p>In the field of machine learning, the well-trained model is assumed to be
able to recover the training labels, i.e. the synthetic labels predicted by the
model should be as close to the ground-truth labels as possible. Inspired by
this, we propose a self-guided curriculum strategy to encourage the learning of
neural machine translation (NMT) models to follow the above recovery criterion,
where we cast the recovery degree of each training example as its learning
difficulty. Specifically, we adopt the sentence level BLEU score as the proxy
of recovery degree. Different from existing curricula relying on linguistic
prior knowledge or third-party language models, our chosen learning difficulty
is more suitable to measure the degree of knowledge mastery of the NMT models.
Experiments on translation benchmarks, including WMT14
English$\Rightarrow$German and WMT17 Chinese$\Rightarrow$English, demonstrate
that our approach can consistently improve translation performance against
strong baseline Transformer.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.08988">
<div class="article-summary-box-inner">
<span><p>Social media platforms provide convenient means for users to participate in
multiple online activities on various contents and create fast widespread
interactions. However, this rapidly growing access has also increased the
diverse information, and characterizing user types to understand people's
lifestyle decisions shared in social media is challenging. In this paper, we
propose a weakly supervised graph embedding based framework for understanding
user types. We evaluate the user embedding learned using weak supervision over
well-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.
Experiments on real-world datasets demonstrate that the proposed framework
outperforms the baselines for detecting user types. Finally, we illustrate data
analysis on different types of users (e.g., practitioner vs. promotional) from
our dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our
method for constructing user representation readily generalizes to other
domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semantic Communication with Adaptive Universal Transformer. (arXiv:2108.09119v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09119">
<div class="article-summary-box-inner">
<span><p>With the development of deep learning (DL), natural language processing (NLP)
makes it possible for us to analyze and understand a large amount of language
texts. Accordingly, we can achieve a semantic communication in terms of joint
semantic source and channel coding over a noisy channel with the help of NLP.
However, the existing method to realize this goal is to use a fixed transformer
of NLP while ignoring the difference of semantic information contained in each
sentence. To solve this problem, we propose a new semantic communication system
based on Universal Transformer. Compared with the traditional transformer, an
adaptive circulation mechanism is introduced in the Universal Transformer.
Through the introduction of the circulation mechanism, the new semantic
communication system can be more flexible to transmit sentences with different
semantic information, and achieve better end-to-end performance under various
channel conditions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10750">
<div class="article-summary-box-inner">
<span><p>Relation Extraction (RE) from tables is the task of identifying relations
between pairs of columns of a table. Generally, RE models for this task require
labelled tables for training. These labelled tables can also be generated
artificially from a Knowledge Graph (KG), which makes the cost to acquire them
much lower in comparison to manual annotations. However, unlike real tables,
these synthetic tables lack associated metadata, such as, column-headers,
captions, etc; this is because synthetic tables are created out of KGs that do
not store such metadata. Meanwhile, previous works have shown that metadata is
important for accurate RE from tables. To address this issue, we propose
methods to artificially create some of this metadata for synthetic tables.
Afterward, we experiment with a BERT-based model, in line with recently
published works, that takes as input a combination of proposed artificial
metadata and table content. Our empirical results show that this leads to an
improvement of 9\%-45\% in F1 score, in absolute terms, over 2 tabular
datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Offensive Language Identification for Tamil Code-Mixed YouTube Comments and Posts. (arXiv:2108.10939v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.10939">
<div class="article-summary-box-inner">
<span><p>Offensive Language detection in social media platforms has been an active
field of research over the past years. In non-native English spoken countries,
social media users mostly use a code-mixed form of text in their
posts/comments. This poses several challenges in the offensive content
identification tasks, and considering the low resources available for Tamil,
the task becomes much harder. The current study presents extensive experiments
using multiple deep learning, and transfer learning models to detect offensive
content on YouTube. We propose a novel and flexible approach of selective
translation and transliteration techniques to reap better results from
fine-tuning and ensembling multilingual transformer networks like BERT, Distil-
BERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best
model for this task. The best performing models were ULMFiT and mBERTBiLSTM for
this Tamil code-mix dataset instead of more popular transfer learning models
such as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The
proposed model ULMFiT and mBERTBiLSTM yielded good results and are promising
for effective offensive speech identification in low-resourced languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutReader: Pre-training of Text and Layout for Reading Order Detection. (arXiv:2108.11591v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11591">
<div class="article-summary-box-inner">
<span><p>Reading order detection is the cornerstone to understanding visually-rich
documents (e.g., receipts and forms). Unfortunately, no existing work took
advantage of advanced deep learning models because it is too laborious to
annotate a large enough dataset. We observe that the reading order of WORD
documents is embedded in their XML metadata; meanwhile, it is easy to convert
WORD documents to PDFs or images. Therefore, in an automated manner, we
construct ReadingBank, a benchmark dataset that contains reading order, text,
and layout information for 500,000 document images covering a wide spectrum of
document types. This first-ever large-scale dataset unleashes the power of deep
neural networks for reading order detection. Specifically, our proposed
LayoutReader captures the text and layout information for reading order
prediction using the seq2seq model. It performs almost perfectly in reading
order detection and significantly improves both open-source and commercial OCR
engines in ordering text lines in their results in our experiments. We will
release the dataset and model at \url{https://aka.ms/layoutreader}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rethinking Negative Sampling for Unlabeled Entity Problem in Named Entity Recognition. (arXiv:2108.11607v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.11607">
<div class="article-summary-box-inner">
<span><p>In many situations (e.g., distant supervision), unlabeled entity problem
seriously degrades the performances of named entity recognition (NER) models.
Recently, this issue has been well addressed by a notable approach based on
negative sampling. In this work, we perform two studies along this direction.
Firstly, we analyze why negative sampling succeeds both theoretically and
empirically. Based on the observation that named entities are highly sparse in
datasets, we show a theoretical guarantee that, for a long sentence, the
probability of containing no unlabeled entities in sampled negatives is high.
Missampling tests on synthetic datasets have verified our guarantee in
practice. Secondly, to mine hard negatives and further reduce missampling
rates, we propose a weighted and adaptive sampling distribution for negative
sampling. Experiments on synthetic datasets and well-annotated datasets show
that our method significantly improves negative sampling in robustness and
effectiveness. We also have achieved new state-of-the-art results on real-world
datasets.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-08-30 01:49:53.305542600 UTC">2021-08-30 01:49:53 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>