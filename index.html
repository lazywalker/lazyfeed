<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-24T04:27:42.750435403Z">09-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">一句话Rust接龙</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=86409c66-5622-46be-a4bf-6416897c5fc4">
<div class="article-summary-box-inner">
<span><p>我先来几个：</p>
<ol>
<li>Rust，让一切皆有可能。</li>
<li>Rust，看不到天花板。</li>
</ol>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-23 - 协作编曲工具 Composing Studio</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=fe8af943-9a57-4d6c-b77b-6ee73a9c9152">
<div class="article-summary-box-inner">
<span><h3>Composing Studio - 协作编曲工具</h3>
<p>Composing Studio 是一款支持在线实时协作的音乐编辑器，使用 Rust、WebAssembly 和 TypeScript 构建，允许任何人创建简单的音乐作品。</p>
<p>Composing Studio 使用一种名为 ABC 的文本格式来完成对音乐的编辑，可以用于转录简单的歌曲 + 吉他和弦，以及其他一些作品，如合唱和民间音乐。同时提供一个友好直观的 Web 界面，具有语法突出显示、实时预览、音频播放和实时协作等功能。</p>
<p><img src="https://s3.bmp.ovh/imgs/2021/09/fcef25acd954e36e.png" alt="Composing Studio"></p>
<p><a href="https://github.com/ekzhang/composing.studio" rel="noopener noreferrer">GitHub - ekzhang/composing.studio</a>: https://github.com/ekzhang/composing.studio</p>
<p><a href="https://composing.studio/productive-animal-5688" rel="noopener noreferrer">online demo</a>: https://composing.studio/productive-animal-5688</p>
<h3>termusic - 终端音乐播放器</h3>
<p>termusic 是一款用 Rust 开发的终端音乐播放器，目前支持 mp3, m4a, flac 和 ogg/vorbis 多种格式。作者曾经是 GOMU 的贡献者，由于在开发时遇到像数据竞争这样的严重问题，所以使用 Rust 进行了重写。</p>
<p><img src="https://s3.bmp.ovh/imgs/2021/09/5a9b980383719ec7.png" alt="termusic"></p>
<p><a href="https://github.com/tramhao/termusic" rel="noopener noreferrer">GitHub - tramhao/termusic</a>: https://github.com/tramhao/termusic</p>
<p><a href="https://crates.io/crates/termusic" rel="noopener noreferrer">Crates.io - termusic</a>: https://crates.io/crates/termusic</p>
<h3>This Week in Rust 409</h3>
<p>新一期的 Rust 周报速递发布，快来看看有哪些内容你曾经关注过 :)</p>
<p><a href="https://this-week-in-rust.org/blog/2021/09/22/this-week-in-rust-409/" rel="noopener noreferrer">This Week in Rust 409</a>: https://this-week-in-rust.org/blog/2021/09/22/this-week-in-rust-409/</p>
<hr>
<p>From 日报小组 <a href="https://github.com/PsiACE" rel="noopener noreferrer">PsiACE</a></p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rust.cc 论坛: 支持 rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust 语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Rustonomicon 的中文翻译</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=108c18ae-f5d5-4799-a457-7604149f4972">
<div class="article-summary-box-inner">
<span><p>最近在学习 Rust，发现 Rust 社区真的是有非常丰富的资源，从社区中学到了很多有用的东西。与此同时，也一直想着能够为社区做点什么。</p>
<p>正好发现<a href="https://doc.rust-lang.org/nomicon/" rel="noopener noreferrer">《The Rustonomicon》</a>（也称为 Rust 秘典、死灵书）之前的一版中文翻译（感谢@tjxing）是更新到了 2018 年，之后就再也没再更新维护过了；而这三年官方也对于这本书进行了大量的迭代升级，于是想着重新翻译一版，并尽可能持续跟进迭代，贡献给社区，也算是尽一份绵薄之力。</p>
<p>在线阅读地址：<a href="https://nomicon.purewhite.io/" rel="noopener noreferrer">https://nomicon.purewhite.io/</a></p>
<p>github 地址：https://github.com/PureWhiteWu/nomicon-zh-Hans</p>
<h1>一些想说的话</h1>
<p>首先，限于译者自身姿势水平，翻译有可能无法做到完全信达雅，并且有一些专业术语不知道如何翻译到中文，在这里先向大家道歉，请多包涵。</p>
<p>不过，译者保证所有翻译的内容都是译者阅读并调整过多次的，并且译者会努力将内容调整到满足<strong>能看懂</strong>的要求，并且做到不遗漏原文内容。</p>
<p>如果大家对于翻译有更好的建议或者想法，欢迎直接 PR~</p>
<p>目前翻译基于 commit：2747c4bb2cbc0639b733793ddb0bf4e9daa2634e，基于时间：2021/9/19</p>
<p>Q：为什么不基于之前已有的中文版进行改进？</p>
<p>A：因为翻译成中文版后，很难再回过头去看和现在的英文版原文到底差了啥，所以还不如完全重新翻译一遍。</p>
<p>Q：那会不会有一天你的这个版本也过期了？</p>
<p>A：希望没有那一天。我 watch 了英文原版的所有 PR，如果有变更（希望）能及时更新。当然，也欢迎大家一起贡献 PR。</p>
<h1>TODO</h1>
<ul>
<li> github 增加 action，合入 master 后自动更新线上版本</li>
<li> 思考是否把英文原版和中文翻译按段落放在一起，方便查阅原版</li>
</ul>
<p>也欢迎大家集思广益，一起建设 Rust 社区。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">一个Rust学习和编写困难的有偿调研</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8c7ceb65-0c99-494f-a213-913ac55a01c7">
<div class="article-summary-box-inner">
<span><p>美国宾州州立大学的科研团队设计了一项针对Rust开发人员的用户调研，调研的目标是理解Rust学习和编码的难点。要求调研参与者有一定的Rust编程经验，年龄在18岁以上，现在不在欧洲，和具有熟练的英语读写能力。预计的调研时间是20分钟。调研完成后，参与者会获得10美元的亚马逊购物劵。</p>
<p>宾州州立大学的这个科研团队，有两年的Rust研究经验。之前关于Rust程序缺陷的研究发表在了<a href="https://songlh.github.io/paper/rust-study.pdf" rel="noopener noreferrer">PLDI‘2020</a>上，给Rust设计的开发工具发表成了<a href="https://songlh.github.io/paper/vr.pdf" rel="noopener noreferrer">CCS’2020的demo论文</a>。</p>
<p>调研的英文说明如下：
Paid Online Research: Rust Programmers’ On-board Programming Experience and Challenges</p>
<p>Researchers at the Pennsylvania State University are conducting a study to understand Rust programmers’ on-board programming experience and the challenges they face when programming with Rust. Throughout a survey link, you will answer questions about Rust programming and your programming experience. The survey will take about 20 minutes. You will earn a $10 Amazon gift card for completing the study.</p>
<p>You are eligible to participate in this study if you are:</p>
<ul>
<li>At least 18 years old</li>
<li>An active user of Rust</li>
<li>Fluent in English</li>
<li>Not currently be residing in the EEA</li>
</ul>
<p>To participate, please click the following link:
<a href="https://personal.psu.edu/suz305/rust-survey-aug/recruitment.html?s=rust_chinese" rel="noopener noreferrer">https://personal.psu.edu/suz305/rust-survey-aug/recruitment.html?s=rust_chinese</a>
Please finish the survey on PC or Mac, and mobile devices are not supported.</p>
<p>Many research projects in human-centered computing depend on participation by individuals like yourself. We are very grateful for your help. Also, please DO NOT discuss this survey with others because this might jeopardize our research.</p>
<p>Thank you again for your participation in this study.
Any questions, please feel free to contact us.
Email: rust.pennstate@gmail.com
Phone Number: +1 814-865-2370
You can also find the contact information of <a href="https://songlh.github.io/" rel="noopener noreferrer">Dr. Linhai Song</a> and <a href="https://ist.psu.edu/directory/axx29" rel="noopener noreferrer">Dr. Aiping Xiong</a> from their homepages.</p>
<p><em>Due to the nature of the work, you may only complete the survey once. Participants will not be compensated if they are suspected of fraudulent behavior.</em></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">如何快速深入了解crate库</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=83ab42f4-b4ec-46ed-b574-409781a813c0">
<div class="article-summary-box-inner">
<span><p>最近因为工作需要，要了解ntex-mqtt、ntex库，看了几天代码，外加断点查看调用栈，依旧没有整体认知，感觉有几个原因：</p>
<ol>
<li>大量用到了泛型、异步future。这导致光看代码无法整理串联。</li>
<li>抽象出的service和servicefactory,感觉把一些简单的逻辑变复杂了。</li>
<li>其他，诸如对mqtt不够熟悉、代码没有注释、没有文档等原因</li>
</ol>
<p>挫败之余（求大家别喷），想请教大家，有没有合适的方式方法，能够比较快速、完整、深入的了解crate库？</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-22 -- kbio基于io_uring的异步 IO 框架</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=e208a573-4fab-4dd0-a10a-c3ddcce40ace">
<div class="article-summary-box-inner">
<span><h3>kbio基于io_uring的异步 IO 框架</h3>
<ul>
<li>支持多线程并发任务提交。</li>
<li>很快。</li>
<li>实现在 tokio 中引入的 AsyncRead/AsyncWrite trait。</li>
</ul>
<p>ReadMore:<a href="https://github.com/KuiBaDB/kbio" rel="noopener noreferrer">https://github.com/KuiBaDB/kbio</a></p>
<p>Blog:<a href="https://blog.hidva.com/2021/09/14/kbio/" rel="noopener noreferrer">https://blog.hidva.com/2021/09/14/kbio/</a></p>
<h3>KuiBaDB</h3>
<p>KuiBaDB是另一个用Asynchronous Rust重写的PostgreSQL，KuiBaDB专注于 OLAP 分析。</p>
<p>KuiBaDB建立在kbio和tokio之上。只用 tokio 的“rt-multi-thread”、“rt”和“io-util”功能。所有 IO，包括文件 IO 和网络 IO，以及异步系统调用都由kbio提供支持。</p>
<p>KuiBaDB使用矢量化引擎，也是目录驱动的。KuiBaDB使用了Hologres 中引入的列式存储。但是我删除了Delete Map并为每行添加了xmin，xmax，xmin/xmax保存在行存储中。</p>
<p>ReadMore:<a href="https://github.com/KuiBaDB/KuiBaDB" rel="noopener noreferrer">https://github.com/KuiBaDB/KuiBaDB</a></p>
<h3>Robyn</h3>
<p>Robyn 是一个由用 Rust 编写的异步 Python 后端HTTP服务运行时。</p>
<p>在 Rust 异步运行时之上运行的 Python 服务。</p>
<h4>安装</h4>
<pre><code>pip install robyn
</code></pre>
<h4>用法</h4>
<pre><code>from robyn import Robyn

app = Robyn(__file__)

@app.get("/")
async def h():
    return "Hello, world!"

app.start(port=5000)
</code></pre>
<h4>GET 请求</h4>
<pre><code>```python3
@app.get("/")
async def h(request):
    return "Hello World"
```
</code></pre>
<h4>POST 请求</h4>
<pre><code>```python3
@app.post("/post")
async def postreq(request):
    return bytearray(request["body"]).decode("utf-8")
```
</code></pre>
<p>ReadMore:<a href="https://sansyrox.github.io/robyn" rel="noopener noreferrer">https://sansyrox.github.io/robyn</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; Mike</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">actix_web</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=6413ef06-e6be-4998-aaab-5f9268b2250b">
<div class="article-summary-box-inner">
<span><p>、</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-21 Rustacean 中秋节快乐</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=f83ce9ba-7c21-4673-87b2-a97a9c639764">
<div class="article-summary-box-inner">
<span><p>所有 Rustaceans，中秋节快乐。</p>
<h3>组合 Axum, Hyper, Tonic 和 Tower 一起，开发一个混合的 web/gRPC 应用：第四部分</h3>
<p>本系列已经更新到第四部分了，也是终结篇。欢迎跟进。</p>
<p>https://www.fpcomplete.com/blog/axum-hyper-tonic-tower-part4/</p>
<h3>【播客】使用 Tarpaulin 进行 Rust 工程测试率覆盖</h3>
<p>Allen Wyma 与软件工程师 Daniel McKenna，也是 Tarpaulin 覆盖测试工具的作者的访谈节目。欢迎收听。</p>
<p>https://rustacean-station.org/episode/037-daniel-mckenna/</p>
<h3>Trunk - 一个 Rust 的 WASM web 应用打包器</h3>
<p>Trunk 会打包 WASM，JS 代码片断，静态资源（images, css, scss 等）。它的配置使用 HTML 文件。</p>
<p>Trunk 支持所有基于 wasm-bindgen 的框架，包括但不仅限于 Yew 和 Seed。</p>
<p>官网：https://trunkrs.dev/</p>
<p>代码仓库：https://github.com/thedodd/trunk</p>
<h3>Perseus - 另一个前端集成 Web UI 框架</h3>
<p>perseus 采用 No-VDOM 技术实现页面渲染。实现纯 Rust 前端 Web UI 开发。</p>
<ul>
<li>支持服务端静态页面生成</li>
<li>支持服务端动态渲染</li>
<li>支持增量生成</li>
<li>各种定制渲染策略</li>
<li>命令行工具</li>
<li>基于 <a href="https://projectfluent.org/" rel="noopener noreferrer">Fluent</a> 的 i18n 支持</li>
</ul>
<p>它基于强大的 <a href="https://github.com/sycamore-rs/sycamore" rel="noopener noreferrer">sycamore</a> 实现。实际上，Perseus 与 Yew, Seed 等算竞争对手，但是所采用的技术思路实际是不一样的。</p>
<p>https://github.com/arctic-hen7/perseus</p>
<p>--</p>
<p>From 日报小组 Mike Tang</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li>Rustcc论坛: 支持rss</li>
<li>微信公众号：Rust语言中文社区</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">在rust的async-std中怎么获取当前时间</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=603210f7-0575-44f9-b925-579e3e13a9ba">
<div class="article-summary-box-inner">
<span><p>请问在async_std包裹的区域内怎么获取当前时间， 我使用了chrono获取时间，但似乎因为chrono没有实现futures，所以在代码中有问题</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【武汉 or 远程】来个接地气的招聘</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=76aac56f-ea37-4695-9eb3-7937fe0bd389">
<div class="article-summary-box-inner">
<span><h2>你们做什么</h2>
<p><a href="https://rustdesk.com/" rel="noopener noreferrer">RustDesk</a>是一款远程桌面软件，目前桌面客户端开源（<a href="https://github.com/rustdesk/rustdesk" rel="noopener noreferrer">项目地址</a>），起源于一个 Rust 练手项目，主要使用 Rust 开发，移动端 UI 采用 Flutter，定位于更开放、更安全、更注重隐私保护，不断完善用户体验。</p>
<h2>你们团队怎么样</h2>
<p>最近才拿到两笔投资，一笔国内，一笔国外，目前团队里只有原作者一人，没有硅谷、华尔街亦或者常青藤背景，也没有经历过 996，只是一名华科的普通老毕业生，选择武汉是为了能够更方便照顾老父老母，摸一摸母校的老梧桐。深知这是一个竞争相当激烈的市场，前路布满荆棘，所以更加期待你的加入，大家一起努力，做好产品，接受市场的考验。</p>
<h2>你们的技术栈是什么？</h2>
<p>Rust 、Flutter 、React/Javascript</p>
<h2>我能从这份工作中得到什么？</h2>
<p>你将是团队的第一批员工，见证一个产品的完整成长过程，团队文化也将由你们来定义。如果你喜欢 Rust，并且不断学习，不甘愿做螺丝钉，追求成就感，欣赏积极主动的工作态度，请考虑加入 RustDesk，我们一起探索国内新的 IT 职业生态。</p>
<h2>岗位</h2>
<h3>全栈开发工程师 [15K-35K + 期权（如果你有兴趣）]</h3>
<p>也许你不喜欢全栈这个词汇，但是 RustDesk 的确在目前阶段还是一款重客户端，轻服务端的跨平台产品。根据你的经验或者喜好，你可以选择你的侧重点。</p>
<h4>岗位要求：</h4>
<ul>
<li>写过 Rust</li>
<li>了解基础数据结构和算法</li>
<li>喜欢学习新东西，主动思考，提问前先 Google</li>
<li>能够接受他人意见，不要对自己的代码迷之自信，也不要轻易吐槽他人的代码</li>
<li>加分项：不错的 GitHub 项目、能够写出漂亮的 UI 、视频编解码开发经验、网络通信安全开发经验、后端高并发开发经验、网络协议栈开发经验</li>
</ul>
<h2>面试方式</h2>
<p>你不需要准备 LeetCode，也无需通读算法导论，但是请熟悉 GNU STL 里的基础数据结构和算法。我希望你花点时间了解 RustDesk，然后自行选择 GitHub 上的 3 个 issues，我们一起在面试中讨论分享。</p>
<h2>投递简历</h2>
<p>Email：info [at] rustdesk.com</p>
<p>请投递 PDF 版本完整简历（教育+工作经历），包括籍贯</p>
<p>我会在第一时间回复你，如果你在两天内没有收到回复，请包涵，你依然很优秀，只是我眼拙没能找到彼此的契合点。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b">
<div class="article-summary-box-inner">
<span><p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>
<ol>
<li>认识面向基础架构语言</li>
<li>理解 Rust 所有权</li>
<li>通过实战理解 Rust 宏</li>
<li>通过 Datafuse 理解全链路跟踪</li>
<li>Rust 异步编程入门 Future Part 1</li>
<li>Rust 异步编程入门 Future Part 2</li>
</ol>
<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。
本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>
<hr>
<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>
<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>
<p>分享讲师： 董泽润</p>
<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现
讲师介绍：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<hr>
<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>
<p>分享时间： 周日晚上2021-09-12 20:00-21:00</p>
<p>分享讲师： 苏林</p>
<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>
<ol>
<li>利用 Tokio 实现一个 Mini Http server</li>
<li>基于 Http server提供内容动态的 API 网关</li>
<li>利用 Redis 实现对 API 网关加速</li>
<li>学习 Rust RPC 调用，实现微服务调用</li>
</ol>
<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<p>Rust 异步编程入门 Future Part 1 | Vol. 5
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<p>Rust 异步编程入门 Future Part 2 | Vol. 6
https://www.bilibili.com/video/bv1oy4y1G7jC</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust 学习随笔</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8">
<div class="article-summary-box-inner">
<span><h1>切换镜像源</h1>
<p>crm =&gt; https://github.com/wtklbm/crm</p>
<p>常用命令就是 <code>crm best</code></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">pretree 补全文档发布了,再次谢谢大神的指点终于入门了。</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827">
<div class="article-summary-box-inner">
<span><h1>Pretree</h1>
<p>pretree is a package for storing and querying routing rules with prefix tree .</p>
<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>
<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>
<p>Inspired by <a href="https://github.com/obity/pretree" rel="noopener noreferrer">obity/pretree</a> (golang)</p>
<h1>Doc</h1>
<p>See this document at <a href="https://docs.rs/pretree" rel="noopener noreferrer">API documentation</a></p>
<h1>Install</h1>
<p>Add the following line to your Cargo.toml file:</p>
<pre><code>pretree = "1.0.0"
</code></pre>
<h1>Example</h1>
<pre><code>use pretree::Pretree;
let mut p = Pretree::new();
p.store("GET","account/{id}/info/:name");
p.store("GET","account/:id/login");
p.store("GET","account/{id}");
p.store("GET","bacteria/count_number_by_month");
let (ok,rule,vars) = p.query("GET","account/929239");
println!("ok:{} rule:{} vars:{:#?}",ok,rule,vars);

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>
<p><strong>课程时间:</strong> 2021年9月5日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识, Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>
<h3>课程大纲</h3>
<p>1、回顾 Rust 异步编程模型.</p>
<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>
<p>3、Tokio 介绍.</p>
<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>
<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/
Rust 异步编程入门 Future Part 1 回放地址：
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课：《 Rust 异步编程入门 Future 》|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>
<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是 Rust 异步编程的核心基础。</p>
<h3>课程大纲</h3>
<p>1、为什么需要异步.</p>
<p>2、理解异步编程模型.</p>
<p>3、Future 编程模型讲解.</p>
<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>
<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>异步引擎 C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D 游戏引擎</h3>
<ul>
<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>
<li><strong>延迟着色</strong></li>
<li><strong>内置保存/加载</strong></li>
<li><strong>独立场景编辑器</strong></li>
<li><strong>高级物理模型</strong></li>
<li><strong>分层模型资源</strong></li>
<li><strong>几何实例化</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>
<p><strong>课程时间：</strong> 2021年8月22日 20:30-21:30</p>
<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>
<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>
<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>
<h3>课程大纲</h3>
<ol>
<li>
<p>什么是分布式追踪系统OpenTracing及应用场景</p>
</li>
<li>
<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>
</li>
<li>
<p>为什么需要tokio-rs/tracing库</p>
</li>
<li>
<p>演示Datafuse项目中tokio-rs/tracing的使用</p>
</li>
</ol>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">论坛github账户无法登录解决笔记</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>有反映这两天github账户无法登录了。</p>
<p>报这个错：</p>
<pre><code>get github user info err
</code></pre>
<p>查了几个地方：</p>
<ol>
<li>代码是否运行正常：Ok</li>
<li>https代理是否正常：Ok</li>
<li>检查了github返回日志，发现是：</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>进入这个地址一看：<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>
<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>
<p>特此记录。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 的 Future 与 Javascript 的 Promise 功能对照参考</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>
<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>
<blockquote>
<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* 什么都不做 */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被 （1）跨线程传递 （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：《通过实战理解 Rust 宏》| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>
<p><strong>课程时间：</strong> 2021年8月15日 20:30-21:30</p>
<p><strong>课程介绍：</strong></p>
<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>这就是通过宏实现配置的统一行为，代码参考：
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>
<h3>课程大纲</h3>
<ul>
<li>什么是 Rust 宏</li>
<li>什么是宏运行原理</li>
<li>如何创建 Rust 宏过程</li>
<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>
</ul>
<p><strong>讲师介绍</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-24T01:30:00Z">09-24</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-linguistically Consistent Semantic and Syntactic Annotation of Child-directed Speech. (arXiv:2109.10952v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10952">
<div class="article-summary-box-inner">
<span><p>While corpora of child speech and child-directed speech (CDS) have enabled
major contributions to the study of child language acquisition, semantic
annotation for such corpora is still scarce and lacks a uniform standard. We
compile two CDS corpora with sentential logical forms, one in English and the
other in Hebrew. In compiling the corpora we employ a methodology that enforces
a cross-linguistically consistent representation, building on recent advances
in dependency representation and semantic parsing. The corpora are based on a
sizable portion of Brown's Adam corpus from CHILDES (about 80% of its
child-directed utterances), and to all child-directed utterances from Berman's
Hebrew CHILDES corpus Hagar.
</p>
<p>We begin by annotating the corpora with the Universal Dependencies (UD)
scheme for syntactic annotation, motivated by its applicability to a wide
variety of domains and languages. We then proceed by applying an automatic
method for transducing sentential logical forms (LFs) from UD structures. The
two representations have complementary strengths: UD structures are
language-neutral and support direct annotation, whereas LFs are neutral as to
the interface between syntax and semantics, and transparently encode semantic
distinctions. We verify the quality of the annotated UD annotation using an
inter-annotator agreement study. We then demonstrate the utility of the
compiled corpora through a longitudinal corpus study of the prevalence of
different syntactic and semantic phenomena.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scalable Fact-checking with Human-in-the-Loop. (arXiv:2109.10992v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10992">
<div class="article-summary-box-inner">
<span><p>Researchers have been investigating automated solutions for fact-checking in
a variety of fronts. However, current approaches often overlook the fact that
the amount of information released every day is escalating, and a large amount
of them overlap. Intending to accelerate fact-checking, we bridge this gap by
grouping similar messages and summarizing them into aggregated claims.
Specifically, we first clean a set of social media posts (e.g., tweets) and
build a graph of all posts based on their semantics; Then, we perform two
clustering methods to group the messages for further claim summarization. We
evaluate the summaries both quantitatively with ROUGE scores and qualitatively
with human evaluation. We also generate a graph of summaries to verify that
there is no significant overlap among them. The results reduced 28,818 original
messages to 700 summary claims, showing the potential to speed up the
fact-checking process by organizing and selecting representative claims from
massive disorganized and redundant messages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Alzheimers Dementia Detection using Acoustic & Linguistic features and Pre-Trained BERT. (arXiv:2109.11010v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11010">
<div class="article-summary-box-inner">
<span><p>Alzheimers disease is a fatal progressive brain disorder that worsens with
time. It is high time we have inexpensive and quick clinical diagnostic
techniques for early detection and care. In previous studies, various Machine
Learning techniques and Pre-trained Deep Learning models have been used in
conjunction with the extraction of various acoustic and linguistic features.
Our study focuses on three models for the classification task in the ADReSS
(The Alzheimers Dementia Recognition through Spontaneous Speech) 2021
Challenge. We use the well-balanced dataset provided by the ADReSS Challenge
for training and validating our models. Model 1 uses various acoustic features
from the eGeMAPs feature-set, Model 2 uses various linguistic features that we
generated from auto-generated transcripts and Model 3 uses the auto-generated
transcripts directly to extract features using a Pre-trained BERT and TF-IDF.
These models are described in detail in the models section.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploring Decomposition for Table-based Fact Verification. (arXiv:2109.11020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11020">
<div class="article-summary-box-inner">
<span><p>Fact verification based on structured data is challenging as it requires
models to understand both natural language and symbolic operations performed
over tables. Although pre-trained language models have demonstrated a strong
capability in verifying simple statements, they struggle with complex
statements that involve multiple operations. In this paper, we improve fact
verification by decomposing complex statements into simpler subproblems.
Leveraging the programs synthesized by a weakly supervised semantic parser, we
propose a program-guided approach to constructing a pseudo dataset for
decomposition model training. The subproblems, together with their predicted
answers, serve as the intermediate evidence to enhance our fact verification
model. Experiments show that our proposed approach achieves the new
state-of-the-art performance, an 82.7\% accuracy, on the TabFact benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Conditional Poisson Stochastic Beam Search. (arXiv:2109.11034v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11034">
<div class="article-summary-box-inner">
<span><p>Beam search is the default decoding strategy for many sequence generation
tasks in NLP. The set of approximate K-best items returned by the algorithm is
a useful summary of the distribution for many applications; however, the
candidates typically exhibit high overlap and may give a highly biased estimate
for expectations under our model. These problems can be addressed by instead
using stochastic decoding strategies. In this work, we propose a new method for
turning beam search into a stochastic process: Conditional Poisson stochastic
beam search. Rather than taking the maximizing set at each iteration, we sample
K candidates without replacement according to the conditional Poisson sampling
design. We view this as a more natural alternative to Kool et. al. 2019's
stochastic beam search (SBS). Furthermore, we show how samples generated under
the CPSBS design can be used to build consistent estimators and sample diverse
sets from sequence models. In our experiments, we observe CPSBS produces lower
variance and more efficient estimators than SBS, even showing improvements in
high entropy settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Controlled Evaluation of Grammatical Knowledge in Mandarin Chinese Language Models. (arXiv:2109.11058v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11058">
<div class="article-summary-box-inner">
<span><p>Prior work has shown that structural supervision helps English language
models learn generalizations about syntactic phenomena such as subject-verb
agreement. However, it remains unclear if such an inductive bias would also
improve language models' ability to learn grammatical dependencies in
typologically different languages. Here we investigate this question in
Mandarin Chinese, which has a logographic, largely syllable-based writing
system; different word order; and sparser morphology than English. We train
LSTMs, Recurrent Neural Network Grammars, Transformer language models, and
Transformer-parameterized generative parsing models on two Mandarin Chinese
datasets of different sizes. We evaluate the models' ability to learn different
aspects of Mandarin grammar that assess syntactic and semantic relationships.
We find suggestive evidence that structural supervision helps with representing
syntactic state across intervening content and improves performance in low-data
settings, suggesting that the benefits of hierarchical inductive biases in
acquiring dependency relationships may extend beyond English.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Using Sociolinguistic Variables to Reveal Changing Attitudes Towards Sexuality and Gender. (arXiv:2109.11061v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11061">
<div class="article-summary-box-inner">
<span><p>Individuals signal aspects of their identity and beliefs through linguistic
choices. Studying these choices in aggregate allows us to examine large-scale
attitude shifts within a population. Here, we develop computational methods to
study word choice within a sociolinguistic lexical variable -- alternate words
used to express the same concept -- in order to test for change in the United
States towards sexuality and gender. We examine two variables: i) referents to
significant others, such as the word "partner" and ii) referents to an
indefinite person, both of which could optionally be marked with gender. The
linguistic choices in each variable allow us to study increased rates of
acceptances of gay marriage and gender equality, respectively. In longitudinal
analyses across Twitter and Reddit over 87M messages, we demonstrate that
attitudes are changing but that these changes are driven by specific
demographics within the United States. Further, in a quasi-causal analysis, we
show that passages of Marriage Equality Acts in different states are drivers of
linguistic change.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Actionable Conversational Quality Indicators for Improving Task-Oriented Dialog Systems. (arXiv:2109.11064v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11064">
<div class="article-summary-box-inner">
<span><p>Automatic dialog systems have become a mainstream part of online customer
service. Many such systems are built, maintained, and improved by customer
service specialists, rather than dialog systems engineers and computer
programmers. As conversations between people and machines become commonplace,
it is critical to understand what is working, what is not, and what actions can
be taken to reduce the frequency of inappropriate system responses. These
analyses and recommendations need to be presented in terms that directly
reflect the user experience rather than the internal dialog processing.
</p>
<p>This paper introduces and explains the use of Actionable Conversational
Quality Indicators (ACQIs), which are used both to recognize parts of dialogs
that can be improved, and to recommend how to improve them. This combines
benefits of previous approaches, some of which have focused on producing dialog
quality scoring while others have sought to categorize the types of errors the
dialog system is making.
</p>
<p>We demonstrate the effectiveness of using ACQIs on LivePerson internal dialog
systems used in commercial customer service applications, and on the publicly
available CMU LEGOv2 conversational dataset (Raux et al. 2005). We report on
the annotation and analysis of conversational datasets showing which ACQIs are
important to fix in various situations.
</p>
<p>The annotated datasets are then used to build a predictive model which uses a
turn-based vector embedding of the message texts and achieves an 79% weighted
average f1-measure at the task of finding the correct ACQI for a given
conversation. We predict that if such a model worked perfectly, the range of
potential improvement actions a bot-builder must consider at each turn could be
reduced by an average of 81%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Universal Dense Retrieval for Open-domain Question Answering. (arXiv:2109.11085v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11085">
<div class="article-summary-box-inner">
<span><p>In open-domain question answering, a model receives a text question as input
and searches for the correct answer using a large evidence corpus. The
retrieval step is especially difficult as typical evidence corpora have
\textit{millions} of documents, each of which may or may not have the correct
answer to the question. Very recently, dense models have replaced sparse
methods as the de facto retrieval method. Rather than focusing on lexical
overlap to determine similarity, dense methods build an encoding function that
captures semantic similarity by learning from a small collection of
question-answer or question-context pairs. In this paper, we investigate dense
retrieval models in the context of open-domain question answering across
different input distributions. To do this, first we introduce an entity-rich
question answering dataset constructed from Wikidata facts and demonstrate
dense models are unable to generalize to unseen input question distributions.
Second, we perform analyses aimed at better understanding the source of the
problem and propose new training techniques to improve out-of-domain
performance on a wide variety of datasets. We encourage the field to further
investigate the creation of a single, universal dense retrieval model that
generalizes well across all input distributions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BiRdQA: A Bilingual Dataset for Question Answering on Tricky Riddles. (arXiv:2109.11087v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11087">
<div class="article-summary-box-inner">
<span><p>A riddle is a question or statement with double or veiled meanings, followed
by an unexpected answer. Solving riddle is a challenging task for both machine
and human, testing the capability of understanding figurative, creative natural
language and reasoning with commonsense knowledge. We introduce BiRdQA, a
bilingual multiple-choice question answering dataset with 6614 English riddles
and 8751 Chinese riddles. For each riddle-answer pair, we provide four
distractors with additional information from Wikipedia. The distractors are
automatically generated at scale with minimal bias. Existing monolingual and
multilingual QA models fail to perform well on our dataset, indicating that
there is a long way to go before machine can beat human on solving tricky
riddles. The dataset has been released to the community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distiller: A Systematic Study of Model Distillation Methods in Natural Language Processing. (arXiv:2109.11105v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11105">
<div class="article-summary-box-inner">
<span><p>We aim to identify how different components in the KD pipeline affect the
resulting performance and how much the optimal KD pipeline varies across
different datasets/tasks, such as the data augmentation policy, the loss
function, and the intermediate representation for transferring the knowledge
between teacher and student. To tease apart their effects, we propose
Distiller, a meta KD framework that systematically combines a broad range of
techniques across different stages of the KD pipeline, which enables us to
quantify each component's contribution. Within Distiller, we unify commonly
used objectives for distillation of intermediate representations under a
universal mutual information (MI) objective and propose a class of MI-$\alpha$
objective functions with better bias/variance trade-off for estimating the MI
between the teacher and the student. On a diverse set of NLP datasets, the best
Distiller configurations are identified via large-scale hyperparameter
optimization. Our experiments reveal the following: 1) the approach used to
distill the intermediate representations is the most important factor in KD
performance, 2) among different objectives for intermediate distillation,
MI-$\alpha$ performs the best, and 3) data augmentation provides a large boost
for small training datasets or small student networks. Moreover, we find that
different datasets/tasks prefer different KD algorithms, and thus propose a
simple AutoDistiller algorithm that can recommend a good KD pipeline for a new
dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Language Model Meta-Pretraining. (arXiv:2109.11129v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11129">
<div class="article-summary-box-inner">
<span><p>The success of pretrained cross-lingual language models relies on two
essential abilities, i.e., generalization ability for learning downstream tasks
in a source language, and cross-lingual transferability for transferring the
task knowledge to other languages. However, current methods jointly learn the
two abilities in a single-phase cross-lingual pretraining process, resulting in
a trade-off between generalization and cross-lingual transfer. In this paper,
we propose cross-lingual language model meta-pretraining, which learns the two
abilities in different training phases. Our method introduces an additional
meta-pretraining phase before cross-lingual pretraining, where the model learns
generalization ability on a large-scale monolingual corpus. Then, the model
focuses on learning cross-lingual transfer on a multilingual corpus.
Experimental results show that our method improves both generalization and
cross-lingual transfer, and produces better-aligned representations across
different languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-Parametric Online Learning from Human Feedback for Neural Machine Translation. (arXiv:2109.11136v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11136">
<div class="article-summary-box-inner">
<span><p>We study the problem of online learning with human feedback in the
human-in-the-loop machine translation, in which the human translators revise
the machine-generated translations and then the corrected translations are used
to improve the neural machine translation (NMT) system. However, previous
methods require online model updating or additional translation memory networks
to achieve high-quality performance, making them inflexible and inefficient in
practice. In this paper, we propose a novel non-parametric online learning
method without changing the model structure. This approach introduces two
k-nearest-neighbor (KNN) modules: one module memorizes the human feedback,
which is the correct sentences provided by human translators, while the other
balances the usage of the history human feedback and original NMT models
adaptively. Experiments conducted on EMEA and JRC-Acquis benchmarks demonstrate
that our proposed method obtains substantial improvements on translation
accuracy and achieves better adaptation performance with less repeating human
correction operations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Joint speaker diarisation and tracking in switching state-space model. (arXiv:2109.11140v1 [cs.SD])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11140">
<div class="article-summary-box-inner">
<span><p>Speakers may move around while diarisation is being performed. When a
microphone array is used, the instantaneous locations of where the sounds
originated from can be estimated, and previous investigations have shown that
such information can be complementary to speaker embeddings in the diarisation
task. However, these approaches often assume that speakers are fairly
stationary throughout a meeting. This paper relaxes this assumption, by
proposing to explicitly track the movements of speakers while jointly
performing diarisation within a unified model. A state-space model is proposed,
where the hidden state expresses the identity of the current active speaker and
the predicted locations of all speakers. The model is implemented as a particle
filter. Experiments on a Microsoft rich meeting transcription task show that
the proposed joint location tracking and diarisation approach is able to
perform comparably with other methods that use location information.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Zero-Shot Information Extraction as a Unified Text-to-Triple Translation. (arXiv:2109.11171v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11171">
<div class="article-summary-box-inner">
<span><p>We cast a suite of information extraction tasks into a text-to-triple
translation framework. Instead of solving each task relying on task-specific
datasets and models, we formalize the task as a translation between
task-specific input text and output triples. By taking the task-specific input,
we enable a task-agnostic translation by leveraging the latent knowledge that a
pre-trained language model has about the task. We further demonstrate that a
simple pre-training task of predicting which relational information corresponds
to which input text is an effective way to produce task-specific outputs. This
enables the zero-shot transfer of our framework to downstream tasks. We study
the zero-shot performance of this framework on open information extraction
(OIE2016, NYT, WEB, PENN), relation classification (FewRel and TACRED), and
factual probe (Google-RE and T-REx). The model transfers non-trivially to most
tasks and is often competitive with a fully supervised method without the need
for any task-specific training. For instance, we significantly outperform the
F1 score of the supervised open information extraction without needing to use
its training set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Exploiting Curriculum Learning in Unsupervised Neural Machine Translation. (arXiv:2109.11177v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11177">
<div class="article-summary-box-inner">
<span><p>Back-translation (BT) has become one of the de facto components in
unsupervised neural machine translation (UNMT), and it explicitly makes UNMT
have translation ability. However, all the pseudo bi-texts generated by BT are
treated equally as clean data during optimization without considering the
quality diversity, leading to slow convergence and limited translation
performance. To address this problem, we propose a curriculum learning method
to gradually utilize pseudo bi-texts based on their quality from multiple
granularities. Specifically, we first apply cross-lingual word embedding to
calculate the potential translation difficulty (quality) for the monolingual
sentences. Then, the sentences are fed into UNMT from easy to hard batch by
batch. Furthermore, considering the quality of sentences/tokens in a particular
batch are also diverse, we further adopt the model itself to calculate the
fine-grained quality scores, which are served as learning factors to balance
the contributions of different parts when computing loss and encourage the UNMT
model to focus on pseudo data with higher quality. Experimental results on WMT
14 En-Fr, WMT 16 En-De, WMT 16 En-Ro, and LDC En-Zh translation tasks
demonstrate that the proposed method achieves consistent improvements with
faster convergence speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Incorporating Linguistic Knowledge for Abstractive Multi-document Summarization. (arXiv:2109.11199v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11199">
<div class="article-summary-box-inner">
<span><p>Within natural language processing tasks, linguistic knowledge can always
serve an important role in assisting the model to learn excel representations
and better guide the natural language generation. In this work, we develop a
neural network based abstractive multi-document summarization (MDS) model which
leverages dependency parsing to capture cross-positional dependencies and
grammatical structures. More concretely, we process the dependency information
into the linguistic-guided attention mechanism and further fuse it with the
multi-head attention for better feature representation. With the help of
linguistic signals, sentence-level relations can be correctly captured, thus
improving MDS performance. Our model has two versions based on Flat-Transformer
and Hierarchical Transformer respectively. Empirical studies on both versions
demonstrate that this simple but effective method outperforms existing works on
the benchmark dataset. Extensive analyses examine different settings and
configurations of the proposed model which provide a good reference to the
community.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fuzzy Generalised Quantifiers for Natural Language in Categorical Compositional Distributional Semantics. (arXiv:2109.11227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11227">
<div class="article-summary-box-inner">
<span><p>Recent work on compositional distributional models shows that bialgebras over
finite dimensional vector spaces can be applied to treat generalised
quantifiers for natural language. That technique requires one to construct the
vector space over powersets, and therefore is computationally costly. In this
paper, we overcome this problem by considering fuzzy versions of quantifiers
along the lines of Zadeh, within the category of many valued relations. We show
that this category is a concrete instantiation of the compositional
distributional model. We show that the semantics obtained in this model is
equivalent to the semantics of the fuzzy quantifiers of Zadeh. As a result, we
are now able to treat fuzzy quantification without requiring a powerset
construction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Pregroup Grammars, their Syntax and Semantics. (arXiv:2109.11237v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11237">
<div class="article-summary-box-inner">
<span><p>Pregroup grammars were developed in 1999 and stayed Lambek's preferred
algebraic model of grammar. The set-theoretic semantics of pregroups, however,
faces an ambiguity problem. In his latest book, Lambek suggests that this
problem might be overcome using finite dimensional vector spaces rather than
sets. What is the right notion of composition in this setting, direct sum or
tensor product of spaces?
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Volctrans GLAT System: Non-autoregressive Translation Meets WMT21. (arXiv:2109.11247v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11247">
<div class="article-summary-box-inner">
<span><p>This paper describes the Volctrans' submission to the WMT21 news translation
shared task for German-&gt;English translation. We build a parallel (i.e.,
non-autoregressive) translation system using the Glancing Transformer, which
enables fast and accurate parallel decoding in contrast to the currently
prevailing autoregressive models. To the best of our knowledge, this is the
first parallel translation system that can be scaled to such a practical
scenario like WMT competition. More importantly, our parallel translation
system achieves the best BLEU score (35.0) on German-&gt;English translation task,
outperforming all strong autoregressive counterparts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Can Question Generation Debias Question Answering Models? A Case Study on Question-Context Lexical Overlap. (arXiv:2109.11256v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11256">
<div class="article-summary-box-inner">
<span><p>Question answering (QA) models for reading comprehension have been
demonstrated to exploit unintended dataset biases such as question-context
lexical overlap. This hinders QA models from generalizing to under-represented
samples such as questions with low lexical overlap. Question generation (QG), a
method for augmenting QA datasets, can be a solution for such performance
degradation if QG can properly debias QA datasets. However, we discover that
recent neural QG models are biased towards generating questions with high
lexical overlap, which can amplify the dataset bias. Moreover, our analysis
reveals that data augmentation with these QG models frequently impairs the
performance on questions with low lexical overlap, while improving that on
questions with high lexical overlap. To address this problem, we use a synonym
replacement-based approach to augment questions with low lexical overlap. We
demonstrate that the proposed data augmentation approach is simple yet
effective to mitigate the degradation problem with only 70k synthetic examples.
Our data is publicly available at
https://github.com/KazutoshiShinoda/Synonym-Replacement.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Don't be Contradicted with Anything! CI-ToD: Towards Benchmarking Consistency for Task-oriented Dialogue System. (arXiv:2109.11292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11292">
<div class="article-summary-box-inner">
<span><p>Consistency Identification has obtained remarkable success on open-domain
dialogue, which can be used for preventing inconsistent response generation.
However, in contrast to the rapid development in open-domain dialogue, few
efforts have been made to the task-oriented dialogue direction. In this paper,
we argue that consistency problem is more urgent in task-oriented domain. To
facilitate the research, we introduce CI-ToD, a novel dataset for Consistency
Identification in Task-oriented Dialog system. In addition, we not only
annotate the single label to enable the model to judge whether the system
response is contradictory, but also provide more fine-grained labels (i.e.,
Dialogue History Inconsistency, User Query Inconsistency and Knowledge Base
Inconsistency) to encourage model to know what inconsistent sources lead to it.
Empirical results show that state-of-the-art methods only achieve 51.3%, which
is far behind the human performance of 93.2%, indicating that there is ample
room for improving consistency identification ability. Finally, we conduct
exhaustive experiments and qualitative analysis to comprehend key challenges
and provide guidance for future directions. All datasets and models are
publicly available at \url{https://github.com/yizhen20133868/CI-ToD}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Knowledge Distillation for Pre-trained Language Models. (arXiv:2109.11295v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11295">
<div class="article-summary-box-inner">
<span><p>Knowledge distillation~(KD) has been proved effective for compressing
large-scale pre-trained language models. However, existing methods conduct KD
statically, e.g., the student model aligns its output distribution to that of a
selected teacher model on the pre-defined training dataset. In this paper, we
explore whether a dynamic knowledge distillation that empowers the student to
adjust the learning procedure according to its competency, regarding the
student performance and learning efficiency. We explore the dynamical
adjustments on three aspects: teacher model adoption, data selection, and KD
objective adaptation. Experimental results show that (1) proper selection of
teacher model can boost the performance of student model; (2) conducting KD
with 10% informative instances achieves comparable performance while greatly
accelerates the training; (3) the student performance can be boosted by
adjusting the supervision contribution of different alignment objective. We
find dynamic knowledge distillation is promising and provide discussions on
potential future directions towards more efficient KD methods. Our code is
available at https://github.com/lancopku/DynamicKD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Breaking BERT: Understanding its Vulnerabilities for Named Entity Recognition through Adversarial Attack. (arXiv:2109.11308v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11308">
<div class="article-summary-box-inner">
<span><p>Both generic and domain-specific BERT models are widely used for natural
language processing (NLP) tasks. In this paper we investigate the vulnerability
of BERT models to variation in input data for Named Entity Recognition (NER)
through adversarial attack. Experimental results show that the original as well
as the domain-specific BERT models are highly vulnerable to entity replacement:
They can be fooled in 89.2 to 99.4% of the cases to mislabel previously correct
entities. BERT models are also vulnerable to variation in the entity context
with 20.2 to 45.0% of entities predicted completely wrong and another 29.3 to
53.3% of entities predicted wrong partially. Often a single change is
sufficient to fool the model. BERT models seem most vulnerable to changes in
the local context of entities. Of the two domain-specific BERT models, the
vulnerability of BioBERT is comparable to the original BERT model whereas
SciBERT is even more vulnerable. Our results chart the vulnerabilities of BERT
models for NER and emphasize the importance of further research into uncovering
and reducing these weaknesses.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ParaShoot: A Hebrew Question Answering Dataset. (arXiv:2109.11314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11314">
<div class="article-summary-box-inner">
<span><p>NLP research in Hebrew has largely focused on morphology and syntax, where
rich annotated datasets in the spirit of Universal Dependencies are available.
Semantic datasets, however, are in short supply, hindering crucial advances in
the development of NLP technology in Hebrew. In this work, we present
ParaShoot, the first question answering dataset in modern Hebrew. The dataset
follows the format and crowdsourcing methodology of SQuAD, and contains
approximately 3000 annotated examples, similar to other question-answering
datasets in low-resource languages. We provide the first baseline results using
recently-released BERT-style models for Hebrew, showing that there is
significant room for improvement on this task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Active Learning for Argument Strength Estimation. (arXiv:2109.11319v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11319">
<div class="article-summary-box-inner">
<span><p>High-quality arguments are an essential part of decision-making.
Automatically predicting the quality of an argument is a complex task that
recently got much attention in argument mining. However, the annotation effort
for this task is exceptionally high. Therefore, we test uncertainty-based
active learning (AL) methods on two popular argument-strength data sets to
estimate whether sample-efficient learning can be enabled. Our extensive
empirical evaluation shows that uncertainty-based acquisition functions can not
surpass the accuracy reached with the random acquisition on these data sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transferring Knowledge from Vision to Language: How to Achieve it and how to Measure it?. (arXiv:2109.11321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11321">
<div class="article-summary-box-inner">
<span><p>Large language models are known to suffer from the hallucination problem in
that they are prone to output statements that are false or inconsistent,
indicating a lack of knowledge. A proposed solution to this is to provide the
model with additional data modalities that complements the knowledge obtained
through text. We investigate the use of visual data to complement the knowledge
of large language models by proposing a method for evaluating visual knowledge
transfer to text for uni- or multimodal language models. The method is based on
two steps, 1) a novel task querying for knowledge of memory colors, i.e.
typical colors of well-known objects, and 2) filtering of model training data
to clearly separate knowledge contributions. Additionally, we introduce a model
architecture that involves a visual imagination step and evaluate it with our
proposed method. We find that our method can successfully be used to measure
visual knowledge transfer capabilities in models and that our novel model
architecture shows promising results for leveraging multimodal knowledge in a
unimodal setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Current State of Finnish NLP. (arXiv:2109.11326v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11326">
<div class="article-summary-box-inner">
<span><p>There are a lot of tools and resources available for processing Finnish. In
this paper, we survey recent papers focusing on Finnish NLP related to many
different subcategories of NLP such as parsing, generation, semantics and
speech. NLP research is conducted in many different research groups in Finland,
and it is frequently the case that NLP tools and models resulting from academic
research are made available for others to use on platforms such as Github.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Integrating Pattern- and Fact-based Fake News Detection via Model Preference Learning. (arXiv:2109.11333v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11333">
<div class="article-summary-box-inner">
<span><p>To defend against fake news, researchers have developed various methods based
on texts. These methods can be grouped as 1) pattern-based methods, which focus
on shared patterns among fake news posts rather than the claim itself; and 2)
fact-based methods, which retrieve from external sources to verify the claim's
veracity without considering patterns. The two groups of methods, which have
different preferences of textual clues, actually play complementary roles in
detecting fake news. However, few works consider their integration. In this
paper, we study the problem of integrating pattern- and fact-based models into
one framework via modeling their preference differences, i.e., making the
pattern- and fact-based models focus on respective preferred parts in a post
and mitigate interference from non-preferred parts as possible. To this end, we
build a Preference-aware Fake News Detection Framework (Pref-FEND), which
learns the respective preferences of pattern- and fact-based models for joint
detection. We first design a heterogeneous dynamic graph convolutional network
to generate the respective preference maps, and then use these maps to guide
the joint learning of pattern- and fact-based models for final prediction.
Experiments on two real-world datasets show that Pref-FEND effectively captures
model preferences and improves the performance of models based on patterns,
facts, or both.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Second Pandemic? Analysis of Fake News About COVID-19 Vaccines in Qatar. (arXiv:2109.11372v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11372">
<div class="article-summary-box-inner">
<span><p>While COVID-19 vaccines are finally becoming widely available, a second
pandemic that revolves around the circulation of anti-vaxxer fake news may
hinder efforts to recover from the first one. With this in mind, we performed
an extensive analysis of Arabic and English tweets about COVID-19 vaccines,
with focus on messages originating from Qatar. We found that Arabic tweets
contain a lot of false information and rumors, while English tweets are mostly
factual. However, English tweets are much more propagandistic than Arabic ones.
In terms of propaganda techniques, about half of the Arabic tweets express
doubt, and 1/5 use loaded language, while English tweets are abundant in loaded
language, exaggeration, fear, name-calling, doubt, and flag-waving. Finally, in
terms of framing, Arabic tweets adopt a health and safety perspective, while in
English economic concerns dominate.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WRENCH: A Comprehensive Benchmark for Weak Supervision. (arXiv:2109.11377v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11377">
<div class="article-summary-box-inner">
<span><p>Recent \emph{Weak Supervision (WS)} approaches have had widespread success in
easing the bottleneck of labeling training data for machine learning by
synthesizing labels from multiple potentially noisy supervision sources.
However, proper measurement and analysis of these approaches remain a
challenge. First, datasets used in existing works are often private and/or
custom, limiting standardization. Second, WS datasets with the same name and
base data often vary in terms of the labels and weak supervision sources used,
a significant "hidden" source of evaluation variance. Finally, WS studies often
diverge in terms of the evaluation protocol and ablations used. To address
these problems, we introduce a benchmark platform, \benchmark, for a thorough
and standardized evaluation of WS approaches. It consists of 22 varied
real-world datasets for classification and sequence tagging; a range of real,
synthetic, and procedurally-generated weak supervision sources; and a modular,
extensible framework for WS evaluation, including implementations for popular
WS methods. We use \benchmark to conduct extensive comparisons over more than
100 method variants to demonstrate its efficacy as a benchmark platform. The
code is available at \url{https://github.com/JieyuZ2/wrench}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cluster-based Mention Typing for Named Entity Disambiguation. (arXiv:2109.11389v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11389">
<div class="article-summary-box-inner">
<span><p>An entity mention in text such as "Washington" may correspond to many
different named entities such as the city "Washington D.C." or the newspaper
"Washington Post." The goal of named entity disambiguation is to identify the
mentioned named entity correctly among all possible candidates. If the type
(e.g. location or person) of a mentioned entity can be correctly predicted from
the context, it may increase the chance of selecting the right candidate by
assigning low probability to the unlikely ones. This paper proposes
cluster-based mention typing for named entity disambiguation. The aim of
mention typing is to predict the type of a given mention based on its context.
Generally, manually curated type taxonomies such as Wikipedia categories are
used. We introduce cluster-based mention typing, where named entities are
clustered based on their contextual similarities and the cluster ids are
assigned as types. The hyperlinked mentions and their context in Wikipedia are
used in order to obtain these cluster-based types. Then, mention typing models
are trained on these mentions, which have been labeled with their cluster-based
types through distant supervision. At the named entity disambiguation phase,
first the cluster-based types of a given mention are predicted and then, these
types are used as features in a ranking model to select the best entity among
the candidates. We represent entities at multiple contextual levels and obtain
different clusterings (and thus typing models) based on each level. As each
clustering breaks the entity space differently, mention typing based on each
clustering discriminates the mention differently. When predictions from all
typing models are used together, our system achieves better or comparable
results based on randomization tests with respect to the state-of-the-art
levels on four defacto test sets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Named Entity Recognition and Classification on Historical Documents: A Survey. (arXiv:2109.11406v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11406">
<div class="article-summary-box-inner">
<span><p>After decades of massive digitisation, an unprecedented amount of historical
documents is available in digital format, along with their machine-readable
texts. While this represents a major step forward with respect to preservation
and accessibility, it also opens up new opportunities in terms of content
mining and the next fundamental challenge is to develop appropriate
technologies to efficiently search, retrieve and explore information from this
'big data of the past'. Among semantic indexing opportunities, the recognition
and classification of named entities are in great demand among humanities
scholars. Yet, named entity recognition (NER) systems are heavily challenged
with diverse, historical and noisy inputs. In this survey, we present the array
of challenges posed by historical documents to NER, inventory existing
resources, describe the main approaches deployed so far, and identify key
priorities for future developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Algorithm for Generating Gap-Fill Multiple Choice Questions of an Expert System. (arXiv:2109.11421v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11421">
<div class="article-summary-box-inner">
<span><p>This research is aimed to propose an artificial intelligence algorithm
comprising an ontology-based design, text mining, and natural language
processing for automatically generating gap-fill multiple choice questions
(MCQs). The simulation of this research demonstrated an application of the
algorithm in generating gap-fill MCQs about software testing. The simulation
results revealed that by using 103 online documents as inputs, the algorithm
could automatically produce more than 16 thousand valid gap-fill MCQs covering
a variety of topics in the software testing domain. Finally, in the discussion
section of this paper we suggest how the proposed algorithm should be applied
to produce gap-fill MCQs being collected in a question pool used by a knowledge
expert system.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automated Fact-Checking: A Survey. (arXiv:2109.11427v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11427">
<div class="article-summary-box-inner">
<span><p>As online false information continues to grow, automated fact-checking has
gained an increasing amount of attention in recent years. Researchers in the
field of Natural Language Processing (NLP) have contributed to the task by
building fact-checking datasets, devising automated fact-checking pipelines and
proposing NLP methods to further research in the development of different
components. This paper reviews relevant research on automated fact-checking
covering both the claim detection and claim validation components.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Corpus and Models for Lemmatisation and POS-tagging of Old French. (arXiv:2109.11442v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11442">
<div class="article-summary-box-inner">
<span><p>Old French is a typical example of an under-resourced historic languages,
that furtherly displays animportant amount of linguistic variation. In this
paper, we present the current results of a long going project (2015-...) and
describe how we broached the difficult question of providing lemmatisation
andPOS models for Old French with the help of neural taggers and the
progressive constitution of dedicated corpora.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Putting Words in BERT's Mouth: Navigating Contextualized Vector Spaces with Pseudowords. (arXiv:2109.11491v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11491">
<div class="article-summary-box-inner">
<span><p>We present a method for exploring regions around individual points in a
contextualized vector space (particularly, BERT space), as a way to investigate
how these regions correspond to word senses. By inducing a contextualized
"pseudoword" as a stand-in for a static embedding in the input layer, and then
performing masked prediction of a word in the sentence, we are able to
investigate the geometry of the BERT-space in a controlled manner around
individual instances. Using our method on a set of carefully constructed
sentences targeting ambiguous English words, we find substantial regularity in
the contextualized space, with regions that correspond to distinct word senses;
but between these regions there are occasionally "sense voids" -- regions that
do not correspond to any intelligible sense.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finding a Balanced Degree of Automation for Summary Evaluation. (arXiv:2109.11503v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11503">
<div class="article-summary-box-inner">
<span><p>Human evaluation for summarization tasks is reliable but brings in issues of
reproducibility and high costs. Automatic metrics are cheap and reproducible
but sometimes poorly correlated with human judgment. In this work, we propose
flexible semiautomatic to automatic summary evaluation metrics, following the
Pyramid human evaluation method. Semi-automatic Lite2Pyramid retains the
reusable human-labeled Summary Content Units (SCUs) for reference(s) but
replaces the manual work of judging SCUs' presence in system summaries with a
natural language inference (NLI) model. Fully automatic Lite3Pyramid further
substitutes SCUs with automatically extracted Semantic Triplet Units (STUs) via
a semantic role labeling (SRL) model. Finally, we propose in-between metrics,
Lite2.xPyramid, where we use a simple regressor to predict how well the STUs
can simulate SCUs and retain SCUs that are more difficult to simulate, which
provides a smooth transition and balance between automation and manual
evaluation. Comparing to 15 existing metrics, we evaluate human-metric
correlations on 3 existing meta-evaluation datasets and our newly-collected
PyrXSum (with 100/10 XSum examples/systems). It shows that Lite2Pyramid
consistently has the best summary-level correlations; Lite3Pyramid works better
than or comparable to other automatic metrics; Lite2.xPyramid trades off small
correlation drops for larger manual effort reduction, which can reduce costs
for future data collection. Our code and data are publicly available at:
https://github.com/ZhangShiyue/Lite2-3Pyramid
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MARMOT: A Deep Learning Framework for Constructing Multimodal Representations for Vision-and-Language Tasks. (arXiv:2109.11526v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.11526">
<div class="article-summary-box-inner">
<span><p>Political activity on social media presents a data-rich window into political
behavior, but the vast amount of data means that almost all content analyses of
social media require a data labeling step. However, most automated machine
classification methods ignore the multimodality of posted content, focusing
either on text or images. State-of-the-art vision-and-language models are
unusable for most political science research: they require all observations to
have both image and text and require computationally expensive pretraining.
This paper proposes a novel vision-and-language framework called multimodal
representations using modality translation (MARMOT). MARMOT presents two
methodological contributions: it can construct representations for observations
missing image or text, and it replaces the computationally expensive
pretraining with modality translation. MARMOT outperforms an ensemble text-only
classifier in 19 of 20 categories in multilabel classifications of tweets
reporting election incidents during the 2016 U.S. general election. Moreover,
MARMOT shows significant improvements over the results of benchmark multimodal
models on the Hateful Memes dataset, improving the best result set by
VisualBERT in terms of accuracy from 0.6473 to 0.6760 and area under the
receiver operating characteristic curve (AUC) from 0.7141 to 0.7530.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Variant Advertisement Text Generation with Association Knowledge. (arXiv:2004.06438v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06438">
<div class="article-summary-box-inner">
<span><p>Online advertising is an important revenue source for many IT companies. In
the search advertising scenario, advertisement text that meets the need of the
search query would be more attractive to the user. However, the manual creation
of query-variant advertisement texts for massive items is expensive.
Traditional text generation methods tend to focus on the general searching
needs with high frequency while ignoring the diverse personalized searching
needs with low frequency. In this paper, we propose the query-variant
advertisement text generation task that aims to generate candidate
advertisement texts for different web search queries with various needs based
on queries and item keywords. To solve the problem of ignoring low-frequency
needs, we propose a dynamic association mechanism to expand the receptive field
based on external knowledge, which can obtain associated words to be added to
the input. These associated words can serve as bridges to transfer the ability
of the model from the familiar high-frequency words to the unfamiliar
low-frequency words. With association, the model can make use of various
personalized needs in queries and generate query-variant advertisement texts.
Both automatic and human evaluations show that our model can generate more
attractive advertisement text than baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Summary-Source Proposition-level Alignment: Task, Datasets and Supervised Baseline. (arXiv:2009.00590v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.00590">
<div class="article-summary-box-inner">
<span><p>Aligning sentences in a reference summary with their counterparts in source
documents was shown as a useful auxiliary summarization task, notably for
generating training data for salience detection. Despite its assessed utility,
the alignment step was mostly approached with heuristic unsupervised methods,
typically ROUGE-based, and was never independently optimized or evaluated. In
this paper, we propose establishing summary-source alignment as an explicit
task, while introducing two major novelties: (1) applying it at the more
accurate proposition span level, and (2) approaching it as a supervised
classification task. To that end, we created a novel training dataset for
proposition-level alignment, derived automatically from available summarization
evaluation data. In addition, we crowdsourced dev and test datasets, enabling
model development and proper evaluation. Utilizing these data, we present a
supervised proposition alignment baseline model, showing improved
alignment-quality over the unsupervised approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">N-LTP: An Open-source Neural Language Technology Platform for Chinese. (arXiv:2009.11616v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.11616">
<div class="article-summary-box-inner">
<span><p>We introduce \texttt{N-LTP}, an open-source neural language technology
platform supporting six fundamental Chinese NLP tasks: {lexical analysis}
(Chinese word segmentation, part-of-speech tagging, and named entity
recognition), {syntactic parsing} (dependency parsing), and {semantic parsing}
(semantic dependency parsing and semantic role labeling). Unlike the existing
state-of-the-art toolkits, such as \texttt{Stanza}, that adopt an independent
model for each task, \texttt{N-LTP} adopts the multi-task framework by using a
shared pre-trained model, which has the advantage of capturing the shared
knowledge across relevant Chinese tasks. In addition, a knowledge distillation
method \cite{DBLP:journals/corr/abs-1907-04829} where the single-task model
teaches the multi-task model is further introduced to encourage the multi-task
model to surpass its single-task teacher. Finally, we provide a collection of
easy-to-use APIs and a visualization tool to make users to use and view the
processing results more easily and directly. To the best of our knowledge, this
is the first toolkit to support six Chinese NLP fundamental tasks. Source code,
documentation, and pre-trained models are available at
\url{https://github.com/HIT-SCIR/ltp}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Simultaneous Translation by Incorporating Pseudo-References with Fewer Reorderings. (arXiv:2010.11247v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11247">
<div class="article-summary-box-inner">
<span><p>Simultaneous translation is vastly different from full-sentence translation,
in the sense that it starts translation before the source sentence ends, with
only a few words delay. However, due to the lack of large-scale, high-quality
simultaneous translation datasets, most such systems are still trained on
conventional full-sentence bitexts. This is far from ideal for the simultaneous
scenario due to the abundance of unnecessary long-distance reorderings in those
bitexts. We propose a novel method that rewrites the target side of existing
full-sentence corpora into simultaneous-style translation. Experiments on
Zh-&gt;En and Ja-&gt;En simultaneous translation show substantial improvements (up to
+2.7 BLEU) with the addition of these generated pseudo-references.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Validating Label Consistency in NER Data Annotation. (arXiv:2101.08698v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.08698">
<div class="article-summary-box-inner">
<span><p>Data annotation plays a crucial role in ensuring your named entity
recognition (NER) projects are trained with the right information to learn
from. Producing the most accurate labels is a challenge due to the complexity
involved with annotation. Label inconsistency between multiple subsets of data
annotation (e.g., training set and test set, or multiple training subsets) is
an indicator of label mistakes. In this work, we present an empirical method to
explore the relationship between label (in-)consistency and NER model
performance. It can be used to validate the label consistency (or catches the
inconsistency) in multiple sets of NER data annotation. In experiments, our
method identified the label inconsistency of test data in SCIERC and CoNLL03
datasets (with 26.7% and 5.4% label mistakes). It validated the consistency in
the corrected version of both datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking. (arXiv:2104.04466v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.04466">
<div class="article-summary-box-inner">
<span><p>Dialogue State Tracking is central to multi-domain task-oriented dialogue
systems, responsible for extracting information from user utterances. We
present a novel hybrid architecture that augments GPT-2 with representations
derived from Graph Attention Networks in such a way to allow causal, sequential
prediction of slot values. The model architecture captures inter-slot
relationships and dependencies across domains that otherwise can be lost in
sequential prediction. We report improvements in state tracking performance in
MultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified
sparse training scenario in which DST models are trained only on session-level
annotations but evaluated at the turn level. We further report detailed
analyses to demonstrate the effectiveness of graph models in DST by showing
that the proposed graph modules capture inter-slot dependencies and improve the
predictions of values that are common to multiple domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effect of Visual Extensions on Natural Language Understanding in Vision-and-Language Models. (arXiv:2104.08066v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08066">
<div class="article-summary-box-inner">
<span><p>A method for creating a vision-and-language (V&amp;L) model is to extend a
language model through structural modifications and V&amp;L pre-training. Such an
extension aims to make a V&amp;L model inherit the capability of natural language
understanding (NLU) from the original language model. To see how well this is
achieved, we propose to evaluate V&amp;L models using an NLU benchmark (GLUE). We
compare five V&amp;L models, including single-stream and dual-stream models,
trained with the same pre-training. Dual-stream models, with their higher
modality independence achieved by approximately doubling the number of
parameters, are expected to preserve the NLU capability better. Our main
finding is that the dual-stream scores are not much different than the
single-stream scores, contrary to expectation. Further analysis shows that
pre-training causes the performance drop in NLU tasks with few exceptions.
These results suggest that adopting a single-stream structure and devising the
pre-training could be an effective method for improving the maintenance of
language knowledge in V&amp;L extensions.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers: "The End of History" for NLP?. (arXiv:2105.00813v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.00813">
<div class="article-summary-box-inner">
<span><p>Recent advances in neural architectures, such as the Transformer, coupled
with the emergence of large-scale pre-trained models such as BERT, have
revolutionized the field of Natural Language Processing (NLP), pushing the
state of the art for a number of NLP tasks. A rich family of variations of
these models has been proposed, such as RoBERTa, ALBERT, and XLNet, but
fundamentally, they all remain limited in their ability to model certain kinds
of information, and they cannot cope with certain information sources, which
was easy for pre-existing models. Thus, here we aim to shed light on some
important theoretical limitations of pre-trained BERT-style models that are
inherent in the general Transformer architecture. First, we demonstrate in
practice on two general types of tasks -- segmentation and segment labeling --
and on four datasets that these limitations are indeed harmful and that
addressing them, even in some very simple and naive ways, can yield sizable
improvements over vanilla RoBERTa and XLNet models. Then, we offer a more
general discussion on desiderata for future additions to the Transformer
architecture that would increase its expressiveness, which we hope could help
in the design of the next generation of deep NLP architectures.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Neural Diarization for Unlimited Numbers of Speakers Using Global and Local Attractors. (arXiv:2107.01545v2 [eess.AS] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01545">
<div class="article-summary-box-inner">
<span><p>Attractor-based end-to-end diarization is achieving comparable accuracy to
the carefully tuned conventional clustering-based methods on challenging
datasets. However, the main drawback is that it cannot deal with the case where
the number of speakers is larger than the one observed during training. This is
because its speaker counting relies on supervised learning. In this work, we
introduce an unsupervised clustering process embedded in the attractor-based
end-to-end diarization. We first split a sequence of frame-wise embeddings into
short subsequences and then perform attractor-based diarization for each
subsequence. Given subsequence-wise diarization results, inter-subsequence
speaker correspondence is obtained by unsupervised clustering of the vectors
computed from the attractors from all the subsequences. This makes it possible
to produce diarization results of a large number of speakers for the whole
recording even if the number of output speakers for each subsequence is
limited. Experimental results showed that our method could produce accurate
diarization results of an unseen number of speakers. Our method achieved 11.84
%, 28.33 %, and 19.49 % on the CALLHOME, DIHARD II, and DIHARD III datasets,
respectively, each of which is better than the conventional end-to-end
diarization methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Token-Level Supervised Contrastive Learning for Punctuation Restoration. (arXiv:2107.09099v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.09099">
<div class="article-summary-box-inner">
<span><p>Punctuation is critical in understanding natural language text. Currently,
most automatic speech recognition (ASR) systems do not generate punctuation,
which affects the performance of downstream tasks, such as intent detection and
slot filling. This gives rise to the need for punctuation restoration. Recent
work in punctuation restoration heavily utilizes pre-trained language models
without considering data imbalance when predicting punctuation classes. In this
work, we address this problem by proposing a token-level supervised contrastive
learning method that aims at maximizing the distance of representation of
different punctuation marks in the embedding space. The result shows that
training with token-level supervised contrastive learning obtains up to 3.2%
absolute F1 improvement on the test set.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Finetuning Pretrained Transformers into Variational Autoencoders. (arXiv:2108.02446v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02446">
<div class="article-summary-box-inner">
<span><p>Text variational autoencoders (VAEs) are notorious for posterior collapse, a
phenomenon where the model's decoder learns to ignore signals from the encoder.
Because posterior collapse is known to be exacerbated by expressive decoders,
Transformers have seen limited adoption as components of text VAEs. Existing
studies that incorporate Transformers into text VAEs (Li et al., 2020; Fang et
al., 2021) mitigate posterior collapse using massive pretraining, a technique
unavailable to most of the research community without extensive computing
resources. We present a simple two-phase training scheme to convert a
sequence-to-sequence Transformer into a VAE with just finetuning. The resulting
language model is competitive with massively pretrained Transformer-based VAEs
in some internal metrics while falling short on others. To facilitate training
we comprehensively explore the impact of common posterior collapse alleviation
techniques in the literature. We release our code for reproducability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Adapted End-to-End Coreference Resolution System for Anaphoric Identities in Dialogues. (arXiv:2109.00185v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00185">
<div class="article-summary-box-inner">
<span><p>We present an effective system adapted from the end-to-end neural coreference
resolution model, targeting on the task of anaphora resolution in dialogues.
Three aspects are specifically addressed in our approach, including the support
of singletons, encoding speakers and turns throughout dialogue interactions,
and knowledge transfer utilizing existing resources. Despite the simplicity of
our adaptation strategies, they are shown to bring significant impact to the
final performance, with up to 27 F1 improvement over the baseline. Our final
system ranks the 1st place on the leaderboard of the anaphora resolution track
in the CRAC 2021 shared task, and achieves the best evaluation results on all
four datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Boosting Cross-Lingual Transfer via Self-Learning with Uncertainty Estimation. (arXiv:2109.00194v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00194">
<div class="article-summary-box-inner">
<span><p>Recent multilingual pre-trained language models have achieved remarkable
zero-shot performance, where the model is only finetuned on one source language
and directly evaluated on target languages. In this work, we propose a
self-learning framework that further utilizes unlabeled data of target
languages, combined with uncertainty estimation in the process to select
high-quality silver labels. Three different uncertainties are adapted and
analyzed specifically for the cross lingual transfer: Language
Heteroscedastic/Homoscedastic Uncertainty (LEU/LOU), Evidential Uncertainty
(EVI). We evaluate our framework with uncertainties on two cross-lingual tasks
including Named Entity Recognition (NER) and Natural Language Inference (NLI)
covering 40 languages in total, which outperforms the baselines significantly
by 10 F1 on average for NER and 2.5 accuracy score for NLI.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Model Bias in NLP -- Application to Hate Speech Classification. (arXiv:2109.09725v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09725">
<div class="article-summary-box-inner">
<span><p>This document sums up our results forthe NLP lecture at ETH in the spring
semester 2021. In this work, a BERT based neural network model (Devlin et
al.,2018) is applied to the JIGSAW dataset (Jigsaw/Conversation AI, 2019) in
order to create a model identifying hateful and toxic comments (strictly
seperated from offensive language) in online social platforms (English
language), inthis case Twitter. Three other neural network architectures and a
GPT-2 (Radfordet al., 2019) model are also applied on the provided data set in
order to compare these different models. The trained BERT model is then applied
on two different data sets to evaluate its generalisation power, namely on
another Twitter data set (Tom Davidson, 2017) (Davidsonet al., 2017) and the
data set HASOC 2019 (Thomas Mandl, 2019) (Mandl et al.,2019) which includes
Twitter and also Facebook comments; we focus on the English HASOC 2019 data. In
addition, it can be shown that by fine-tuning the trained BERT model on these
two datasets by applying different transfer learning scenarios via retraining
partial or all layers the predictive scores improve compared to simply applying
the model pre-trained on the JIGSAW data set. Withour results, we get
precisions from 64% to around 90% while still achieving acceptable recall
values of at least lower 60s%, proving that BERT is suitable for real usecases
in social platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning. (arXiv:2109.10510v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10510">
<div class="article-summary-box-inner">
<span><p>Despite the success of neural dialogue systems in achieving high performance
on the leader-board, they cannot meet users' requirements in practice, due to
their poor reasoning skills. The underlying reason is that most neural dialogue
models only capture the syntactic and semantic information, but fail to model
the logical consistency between the dialogue history and the generated
response. Recently, a new multi-turn dialogue reasoning task has been proposed,
to facilitate dialogue reasoning research. However, this task is challenging,
because there are only slight differences between the illogical response and
the dialogue history. How to effectively solve this challenge is still worth
exploring. This paper proposes a Fine-grained Comparison Model (FCM) to tackle
this problem. Inspired by human's behavior in reading comprehension, a
comparison mechanism is proposed to focus on the fine-grained differences in
the representation of each response candidate. Specifically, each candidate
representation is compared with the whole history to obtain a history
consistency representation. Furthermore, the consistency signals between each
candidate and the speaker's own history are considered to drive a model to
prefer a candidate that is logically consistent with the speaker's history
logic. Finally, the above consistency representations are employed to output a
ranking list of the candidate responses for multi-turn dialogue reasoning.
Experimental results on two public dialogue datasets show that our method
obtains higher ranking scores than the baseline models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Small-Bench NLP: Benchmark for small single GPU trained models in Natural Language Processing. (arXiv:2109.10847v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10847">
<div class="article-summary-box-inner">
<span><p>Recent progress in the Natural Language Processing domain has given us
several State-of-the-Art (SOTA) pretrained models which can be finetuned for
specific tasks. These large models with billions of parameters trained on
numerous GPUs/TPUs over weeks are leading in the benchmark leaderboards. In
this paper, we discuss the need for a benchmark for cost and time effective
smaller models trained on a single GPU. This will enable researchers with
resource constraints experiment with novel and innovative ideas on
tokenization, pretraining tasks, architecture, fine tuning methods etc. We set
up Small-Bench NLP, a benchmark for small efficient neural language models
trained on a single GPU. Small-Bench NLP benchmark comprises of eight NLP tasks
on the publicly available GLUE datasets and a leaderboard to track the progress
of the community. Our ELECTRA-DeBERTa (15M parameters) small model architecture
achieves an average score of 81.53 which is comparable to that of BERT-Base's
82.20 (110M parameters). Our models, code and leaderboard are available at
https://github.com/smallbenchnlp
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-24 04:27:42.785737794 UTC">2021-09-24 04:27:42 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>