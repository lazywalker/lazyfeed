<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-22T04:27:29.457142492Z">09-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">actix_web</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=6413ef06-e6be-4998-aaab-5f9268b2250b">
<div class="article-summary-box-inner">
<span><p>、</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-21 Rustacean 中秋节快乐</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=f83ce9ba-7c21-4673-87b2-a97a9c639764">
<div class="article-summary-box-inner">
<span><p>所有 Rustaceans，中秋节快乐。</p>
<h3>组合 Axum, Hyper, Tonic 和 Tower 一起，开发一个混合的 web/gRPC 应用：第四部分</h3>
<p>本系列已经更新到第四部分了，也是终结篇。欢迎跟进。</p>
<p>https://www.fpcomplete.com/blog/axum-hyper-tonic-tower-part4/</p>
<h3>【播客】使用 Tarpaulin 进行 Rust 工程测试率覆盖</h3>
<p>Allen Wyma 与软件工程师 Daniel McKenna，也是 Tarpaulin 覆盖测试工具的作者的访谈节目。欢迎收听。</p>
<p>https://rustacean-station.org/episode/037-daniel-mckenna/</p>
<h3>Trunk - 一个 Rust 的 WASM web 应用打包器</h3>
<p>Trunk 会打包 WASM，JS 代码片断，静态资源（images, css, scss 等）。它的配置使用 HTML 文件。</p>
<p>Trunk 支持所有基于 wasm-bindgen 的框架，包括但不仅限于 Yew 和 Seed。</p>
<p>官网：https://trunkrs.dev/</p>
<p>代码仓库：https://github.com/thedodd/trunk</p>
<h3>Perseus - 另一个前端集成 Web UI 框架</h3>
<p>perseus 采用 No-VDOM 技术实现页面渲染。实现纯 Rust 前端 Web UI 开发。</p>
<ul>
<li>支持服务端静态页面生成</li>
<li>支持服务端动态渲染</li>
<li>支持增量生成</li>
<li>各种定制渲染策略</li>
<li>命令行工具</li>
<li>基于 <a href="https://projectfluent.org/" rel="noopener noreferrer">Fluent</a> 的 i18n 支持</li>
</ul>
<p>它基于强大的 <a href="https://github.com/sycamore-rs/sycamore" rel="noopener noreferrer">sycamore</a> 实现。实际上，Perseus 与 Yew, Seed 等算竞争对手，但是所采用的技术思路实际是不一样的。</p>
<p>https://github.com/arctic-hen7/perseus</p>
<p>--</p>
<p>From 日报小组 Mike Tang</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li>Rustcc论坛: 支持rss</li>
<li>微信公众号：Rust语言中文社区</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">在rust的async-std中怎么获取当前时间</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=603210f7-0575-44f9-b925-579e3e13a9ba">
<div class="article-summary-box-inner">
<span><p>请问在async_std包裹的区域内怎么获取当前时间， 我使用了chrono获取时间，但似乎因为chrono没有实现futures，所以在代码中有问题</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【武汉 or 远程】来个接地气的招聘</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=76aac56f-ea37-4695-9eb3-7937fe0bd389">
<div class="article-summary-box-inner">
<span><h2>你们做什么</h2>
<p><a href="https://rustdesk.com/" rel="noopener noreferrer">RustDesk</a>是一款远程桌面软件，目前桌面客户端开源（<a href="https://github.com/rustdesk/rustdesk" rel="noopener noreferrer">项目地址</a>），起源于一个 Rust 练手项目，主要使用 Rust 开发，移动端 UI 采用 Flutter，定位于更开放、更安全、更注重隐私保护，不断完善用户体验。</p>
<h2>你们团队怎么样</h2>
<p>最近才拿到两笔投资，一笔国内，一笔国外，目前团队里只有原作者一人，没有硅谷、华尔街亦或者常青藤背景，也没有经历过 996，只是一名华科的普通老毕业生，选择武汉是为了能够更方便照顾老父老母，摸一摸母校的老梧桐。深知这是一个竞争相当激烈的市场，前路布满荆棘，所以更加期待你的加入，大家一起努力，做好产品，接受市场的考验。</p>
<h2>你们的技术栈是什么？</h2>
<p>Rust 、Flutter 、React/Javascript</p>
<h2>我能从这份工作中得到什么？</h2>
<p>你将是团队的第一批员工，见证一个产品的完整成长过程，团队文化也将由你们来定义。如果你喜欢 Rust，并且不断学习，不甘愿做螺丝钉，追求成就感，欣赏积极主动的工作态度，请考虑加入 RustDesk，我们一起探索国内新的 IT 职业生态。</p>
<h2>岗位</h2>
<h3>全栈开发工程师 [15K-35K + 期权（如果你有兴趣）]</h3>
<p>也许你不喜欢全栈这个词汇，但是 RustDesk 的确在目前阶段还是一款重客户端，轻服务端的跨平台产品。根据你的经验或者喜好，你可以选择你的侧重点。</p>
<h4>岗位要求：</h4>
<ul>
<li>写过 Rust</li>
<li>了解基础数据结构和算法</li>
<li>喜欢学习新东西，主动思考，提问前先 Google</li>
<li>能够接受他人意见，不要对自己的代码迷之自信，也不要轻易吐槽他人的代码</li>
<li>加分项：不错的 GitHub 项目、能够写出漂亮的 UI 、视频编解码开发经验、网络通信安全开发经验、后端高并发开发经验、网络协议栈开发经验</li>
</ul>
<h2>面试方式</h2>
<p>你不需要准备 LeetCode，也无需通读算法导论，但是请熟悉 GNU STL 里的基础数据结构和算法。我希望你花点时间了解 RustDesk，然后自行选择 GitHub 上的 3 个 issues，我们一起在面试中讨论分享。</p>
<h2>投递简历</h2>
<p>Email：info [at] rustdesk.com</p>
<p>请投递 PDF 版本完整简历（教育+工作经历），包括籍贯</p>
<p>我会在第一时间回复你，如果你在两天内没有收到回复，请包涵，你依然很优秀，只是我眼拙没能找到彼此的契合点。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">请问如何给标准库String增加个方法呢？</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=1f6297e3-ddb4-41c6-9a35-7974d8a19063">
<div class="article-summary-box-inner">
<span><p>比如JS可以这样实现</p>
<pre><code>String.prototype.countEx = function (sub) {
	let i = 0
	for (var v of this) {
		if (sub.exist(v)) i++
	}
	return i
}


"123145".countEx("1")

</code></pre>
<p>请问我用Rust应该如何实现，学了几天Rust。资料实在太少啦
是不是trait可以实现？ 还请各位大老帮帮我~</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust Developer(ST) for SIL.Finance (for Soh.cool, Solana Branch)</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=b367e266-92ef-400c-81b0-18bc0346560b">
<div class="article-summary-box-inner">
<span><h2>Location</h2>
<ul>
<li>San Mateo, California, On-site.</li>
<li>Hangzhou, China, On-site.</li>
<li>Globe, Remote.</li>
</ul>
<h3>Rust Engineer</h3>
<ul>
<li>
<p>Responsibilities</p>
<ul>
<li>Build smart contracts in Rust, understand Solana BPF</li>
<li>Build smart contracts in Solidity as well</li>
<li>Design, scope, and estimate tasks based on requirements</li>
<li>Collaborate with team to plan projects at the task level</li>
<li>Collaborate with cross-functional partners on all aspects of product development</li>
<li>Identify and advocate for team-wide areas of improvement and best practices</li>
<li>Envision and develop features to help grow SIL</li>
</ul>
</li>
<li>
<p>Qualifications</p>
<ul>
<li>Bachelor's or Master's degree in CS or equivalent experience</li>
<li>Experience with Rust</li>
<li>Experience with Solana blockchain</li>
<li>Experience with Layer 2 scaling</li>
</ul>
</li>
<li>
<p>Good to have</p>
<ul>
<li>Experience with Solidity, and understands gas optimization</li>
<li>Experience with Truffle/Hardhat</li>
<li>Experience with migrations and deploy code to EVM compatible networks</li>
</ul>
</li>
</ul>
<h3>Solidity Engineer</h3>
<ul>
<li>
<p>Responsibilities</p>
<ul>
<li>Build smart contracts in Solidity for the EVM compatible blockchains</li>
<li>Design, scope, and estimate tasks based on requirements</li>
<li>Collaborate with team to plan projects at the task level</li>
<li>Collaborate with cross-functional partners on all aspects of product development</li>
<li>Identify and advocate for team-wide areas of improvement and best practices</li>
<li>Envision and develop features to help grow SIL</li>
</ul>
</li>
<li>
<p>Qualifications</p>
<ul>
<li>Bachelor's or Master's degree in CS or equivalent experience</li>
<li>Experience with Solidity, and understands gas optimization</li>
<li>Experience with Truffle/Hardhat</li>
<li>Experience with migrations and deploy code to EVM compatible networks</li>
</ul>
</li>
<li>
<p>Good to have</p>
<ul>
<li>Experience with Layer 2 scaling</li>
<li>Experience with Rust</li>
<li>Experience with Solana blockchain</li>
</ul>
</li>
</ul>
<h3>Blockchain QA Engineer</h3>
<ul>
<li>
<p>Responsibilities</p>
<ul>
<li>Setup end to end testing tools on dApp (fronend, blockchain)</li>
<li>Write appropriate end to end tests in order to protect the product from regressions bugs</li>
<li>Collaborate with team to plan projects at the task level</li>
<li>Identify and advocate for team-wide areas of improvement and best practices</li>
</ul>
</li>
<li>
<p>Qualifications</p>
<ul>
<li>Experience with automated tests</li>
<li>Experience with dApp products</li>
</ul>
</li>
<li>
<p>Good to have</p>
<ul>
<li>Experience with EVM compatible blockchains</li>
<li>Experience with Solidity</li>
<li>Experience with JavaScript/TypeScript</li>
</ul>
</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">招远程Rust/Solana工程师</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=bec1bd60-b6dc-4128-9683-a986ec950e9e">
<div class="article-summary-box-inner">
<span><p>申请链接： https://cryptocurrencyjobs.co/engineering/vovo-finance-solana-engineer/<br>
薪资：4万 - 8 万/每月<br>
团队在新加坡和美国</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">异步下的生命周期问题</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=a61c0207-65b5-4f5a-a123-34b553fe13fb">
<div class="article-summary-box-inner">
<span><pre><code>
mod test {
    use std::{pin::Pin, task::Poll};

    use futures_io::{AsyncBufRead, AsyncRead};

    //不能通过编译
    struct Body(Vec&lt;u8&gt;);

    impl AsyncRead for Body {
        fn poll_read(
            self: Pin&lt;&amp;mut Self&gt;,
            cx: &amp;mut std::task::Context&lt;'_&gt;,
            buf: &amp;mut [u8],
        ) -&gt; Poll&lt;futures_io::Result&lt;usize&gt;&gt; {
            todo!()
        }
    }

    impl AsyncBufRead for Body {
        fn poll_fill_buf(
            self: Pin&lt;&amp;mut Self&gt;,
            cx: &amp;mut std::task::Context&lt;'_&gt;,
        ) -&gt; Poll&lt;futures_io::Result&lt;&amp;[u8]&gt;&gt; {
            Poll::Ready(Ok(&amp;self.0))
        }

        fn consume(self: Pin&lt;&amp;mut Self&gt;, amt: usize) {
            todo!()
        }
    }
}

</code></pre>
<p>不能通过编译</p>
<pre><code>
error[E0515]: cannot return value referencing function parameter `self`
  --&gt; src/main.rs:24:13
   |
24 |             Poll::Ready(Ok(&amp;self.0))
   |             ^^^^^^^^^^^^^^^^----^^^^
   |             |               |
   |             |               `self` is borrowed here
   |             returns a value referencing data owned by the current function

For more information about this error, try `rustc --explain E0515`.

</code></pre>
<p>https://play.rust-lang.org/#:~:text=Permalink%20to%20the%20playground</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">enum和struct下的生命周期问题</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=a45f8532-4e48-4495-91b9-df56b6ce33de">
<div class="article-summary-box-inner">
<span><pre><code>
mod test {
//不能通过编译
    enum Body {
        Bytes(Vec&lt;u8&gt;),
    }

    impl std::io::Read for Body {
        fn read(&amp;mut self, buf: &amp;mut [u8]) -&gt; futures_io::Result&lt;usize&gt; {
            todo!()
        }
    }

    impl std::io::BufRead for Body {
        fn fill_buf(&amp;mut self) -&gt; futures_io::Result&lt;&amp;[u8]&gt; {
            match self{
                Body::Bytes(bytes) =&gt; Ok(bytes.as_ref()),
            }
        }

        fn consume(&amp;mut self, amt: usize) {
            todo!()
        }
    }
}

mod test2 {
//能通过编译
    struct Body (Vec&lt;u8&gt;);
    

    impl std::io::Read for Body {
        fn read(&amp;mut self, buf: &amp;mut [u8]) -&gt; futures_io::Result&lt;usize&gt; {
            todo!()
        }
    }

    impl std::io::BufRead for Body {
        fn fill_buf(&amp;mut self) -&gt; futures_io::Result&lt;&amp;[u8]&gt; {
            Ok(self.0.as_ref())
            
        }

        fn consume(&amp;mut self, amt: usize) {
            todo!()
        }
    }
}

fn main(){
    
}

</code></pre>
<p>为什么使用enum就无法通过编译，使用struct就能编译？</p>
<pre><code>
error[E0515]: cannot return value referencing local variable `bytes`
  --&gt; src/main.rs:16:39
   |
16 |                 Body::Bytes(bytes) =&gt; Ok(bytes.as_ref()),
   |                                       ^^^-----^^^^^^^^^^
   |                                       |  |
   |                                       |  `bytes` is borrowed here
   |                                       returns a value referencing data owned by the current function

</code></pre>
<p>更新：改成<code>Body::Bytes(ref bytes)</code>就过编译了，不知道为什么。</p>
<p>playground:</p>
<p>https://play.rust-lang.org/#:~:text=Permalink%20to%20the%20playground</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-20 Rust CI/CD: github action 使用</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=f08915d7-1ffc-405c-b627-91d5473c46ec">
<div class="article-summary-box-inner">
<span><h1>Rust CI/CD: github action 使用</h1>
<p>和任何其他语言一样, 在我们掌握语法之外, 我们往往还有 CI/CD 的需求:</p>
<ul>
<li>需要哪些组件来组成CI管道，以确保我的代码是健康的？</li>
<li>如何部署？</li>
<li>我需要编写自定义工具还是有社区资源可用？</li>
</ul>
<p>作者会用三篇文章来讲解 Rust在 github 中如何使用 action 来完成 CI/CD.</p>
<p><a href="https://www.homeops.dev/continuous-integration-with-github-actions-and-rust/" rel="noopener noreferrer">原文链接</a></p>
<h1>使用 Axum, Hyper, Tonic, and Tower 打造 web/gRPC 应用</h1>
<p>这是使用 Axum, Hyper, Tonic, and Tower 来打造 web/gRPC 应用系列的第四部分. 本次主要讲解如何组合 Axum 和 Tonic.</p>
<p><a href="https://www.fpcomplete.com/blog/axum-hyper-tonic-tower-part4/" rel="noopener noreferrer">原文链接</a></p>
<h1>compact_str: 一种内存高效的不可变 string 类型</h1>
<p>CompactStr 是一种内存效率更高的不可变字符串类型，它可以在堆栈上存储较小的字符串，并透明地在堆上存储更长的字符串。它们大多可以用作String的替换，在解析、反序列化或任何其他可能有较小字符串的应用程序中特别有用。</p>
<p><a href="https://github.com/ParkMyCar/compact_str" rel="noopener noreferrer">github 地址</a></p>
<h1>Caches: rust版本的 LRU</h1>
<p>这是一个 Rust 版本的 LRU 实现. golang 的实现: https://github.com/hashicorp/golang-lru</p>
<p><a href="https://github.com/al8n/caches-rs" rel="noopener noreferrer">github 地址 </a></p>
<p>--</p>
<p>From 日报小组 BobQin，FBI小白</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b">
<div class="article-summary-box-inner">
<span><p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>
<ol>
<li>认识面向基础架构语言</li>
<li>理解 Rust 所有权</li>
<li>通过实战理解 Rust 宏</li>
<li>通过 Datafuse 理解全链路跟踪</li>
<li>Rust 异步编程入门 Future Part 1</li>
<li>Rust 异步编程入门 Future Part 2</li>
</ol>
<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。
本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>
<hr>
<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>
<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>
<p>分享讲师： 董泽润</p>
<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现
讲师介绍：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<hr>
<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>
<p>分享时间： 周日晚上2021-09-12 20:00-21:00</p>
<p>分享讲师： 苏林</p>
<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>
<ol>
<li>利用 Tokio 实现一个 Mini Http server</li>
<li>基于 Http server提供内容动态的 API 网关</li>
<li>利用 Redis 实现对 API 网关加速</li>
<li>学习 Rust RPC 调用，实现微服务调用</li>
</ol>
<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<p>Rust 异步编程入门 Future Part 1 | Vol. 5
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<p>Rust 异步编程入门 Future Part 2 | Vol. 6
https://www.bilibili.com/video/bv1oy4y1G7jC</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust 学习随笔</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8">
<div class="article-summary-box-inner">
<span><h1>切换镜像源</h1>
<p>crm =&gt; https://github.com/wtklbm/crm</p>
<p>常用命令就是 <code>crm best</code></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">pretree 补全文档发布了,再次谢谢大神的指点终于入门了。</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827">
<div class="article-summary-box-inner">
<span><h1>Pretree</h1>
<p>pretree is a package for storing and querying routing rules with prefix tree .</p>
<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>
<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>
<p>Inspired by <a href="https://github.com/obity/pretree" rel="noopener noreferrer">obity/pretree</a> (golang)</p>
<h1>Doc</h1>
<p>See this document at <a href="https://docs.rs/pretree" rel="noopener noreferrer">API documentation</a></p>
<h1>Install</h1>
<p>Add the following line to your Cargo.toml file:</p>
<pre><code>pretree = "1.0.0"
</code></pre>
<h1>Example</h1>
<pre><code>use pretree::Pretree;
let mut p = Pretree::new();
p.store("GET","account/{id}/info/:name");
p.store("GET","account/:id/login");
p.store("GET","account/{id}");
p.store("GET","bacteria/count_number_by_month");
let (ok,rule,vars) = p.query("GET","account/929239");
println!("ok:{} rule:{} vars:{:#?}",ok,rule,vars);

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>
<p><strong>课程时间:</strong> 2021年9月5日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识, Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>
<h3>课程大纲</h3>
<p>1、回顾 Rust 异步编程模型.</p>
<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>
<p>3、Tokio 介绍.</p>
<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>
<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/
Rust 异步编程入门 Future Part 1 回放地址：
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课：《 Rust 异步编程入门 Future 》|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>
<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是 Rust 异步编程的核心基础。</p>
<h3>课程大纲</h3>
<p>1、为什么需要异步.</p>
<p>2、理解异步编程模型.</p>
<p>3、Future 编程模型讲解.</p>
<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>
<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>异步引擎 C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D 游戏引擎</h3>
<ul>
<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>
<li><strong>延迟着色</strong></li>
<li><strong>内置保存/加载</strong></li>
<li><strong>独立场景编辑器</strong></li>
<li><strong>高级物理模型</strong></li>
<li><strong>分层模型资源</strong></li>
<li><strong>几何实例化</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>
<p><strong>课程时间：</strong> 2021年8月22日 20:30-21:30</p>
<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>
<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>
<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>
<h3>课程大纲</h3>
<ol>
<li>
<p>什么是分布式追踪系统OpenTracing及应用场景</p>
</li>
<li>
<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>
</li>
<li>
<p>为什么需要tokio-rs/tracing库</p>
</li>
<li>
<p>演示Datafuse项目中tokio-rs/tracing的使用</p>
</li>
</ol>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">论坛github账户无法登录解决笔记</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>有反映这两天github账户无法登录了。</p>
<p>报这个错：</p>
<pre><code>get github user info err
</code></pre>
<p>查了几个地方：</p>
<ol>
<li>代码是否运行正常：Ok</li>
<li>https代理是否正常：Ok</li>
<li>检查了github返回日志，发现是：</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>进入这个地址一看：<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>
<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>
<p>特此记录。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 的 Future 与 Javascript 的 Promise 功能对照参考</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>
<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>
<blockquote>
<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* 什么都不做 */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被 （1）跨线程传递 （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：《通过实战理解 Rust 宏》| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>
<p><strong>课程时间：</strong> 2021年8月15日 20:30-21:30</p>
<p><strong>课程介绍：</strong></p>
<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>这就是通过宏实现配置的统一行为，代码参考：
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>
<h3>课程大纲</h3>
<ul>
<li>什么是 Rust 宏</li>
<li>什么是宏运行原理</li>
<li>如何创建 Rust 宏过程</li>
<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>
</ul>
<p><strong>讲师介绍</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-22T01:30:00Z">09-22</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">DisCoDisCo at the DISRPT2021 Shared Task: A System for Discourse Segmentation, Classification, and Connective Detection. (arXiv:2109.09777v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09777">
<div class="article-summary-box-inner">
<span><p>This paper describes our submission to the DISRPT2021 Shared Task on
Discourse Unit Segmentation, Connective Detection, and Relation Classification.
Our system, called DisCoDisCo, is a Transformer-based neural classifier which
enhances contextualized word embeddings (CWEs) with hand-crafted features,
relying on tokenwise sequence tagging for discourse segmentation and connective
detection, and a feature-rich, encoder-less sentence pair classifier for
relation classification. Our results for the first two tasks outperform SOTA
scores from the previous 2019 shared task, and results on relation
classification suggest strong performance on the new 2021 benchmark. Ablation
tests show that including features beyond CWEs are helpful for both tasks, and
a partial evaluation of multiple pre-trained Transformer-based language models
indicates that models pre-trained on the Next Sentence Prediction (NSP) task
are optimal for relation classification.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERT Has Uncommon Sense: Similarity Ranking for Word Sense BERTology. (arXiv:2109.09780v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09780">
<div class="article-summary-box-inner">
<span><p>An important question concerning contextualized word embedding (CWE) models
like BERT is how well they can represent different word senses, especially
those in the long tail of uncommon senses. Rather than build a WSD system as in
previous work, we investigate contextualized embedding neighborhoods directly,
formulating a query-by-example nearest neighbor retrieval task and examining
ranking performance for words and senses in different frequency bands. In an
evaluation on two English sense-annotated corpora, we find that several popular
CWE models all outperform a random baseline even for proportionally rare
senses, without explicit sense supervision. However, performance varies
considerably even among models with similar architectures and pretraining
regimes, with especially large differences for rare word senses, revealing that
CWE models are not all created equal when it comes to approximating word senses
in their native representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Inspecting the Factuality of Hallucinated Entities in Abstractive Summarization. (arXiv:2109.09784v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09784">
<div class="article-summary-box-inner">
<span><p>State-of-the-art abstractive summarization systems often generate
\emph{hallucinations}; i.e., content that is not directly inferable from the
source text. Despite being assumed incorrect, many of the hallucinated contents
are consistent with world knowledge (factual hallucinations). Including these
factual hallucinations into a summary can be beneficial in providing additional
background information. In this work, we propose a novel detection approach
that separates factual from non-factual hallucinations of entities. Our method
is based on an entity's prior and posterior probabilities according to
pre-trained and finetuned masked language models, respectively. Empirical
results suggest that our method vastly outperforms three strong baselines in
both accuracy and F1 scores and has a strong correlation with human judgments
on factuality classification tasks. Furthermore, our approach can provide
insight into whether a particular hallucination is caused by the summarizer's
pre-training or fine-tuning step.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dependency Induction Through the Lens of Visual Perception. (arXiv:2109.09790v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09790">
<div class="article-summary-box-inner">
<span><p>Most previous work on grammar induction focuses on learning phrasal or
dependency structure purely from text. However, because the signal provided by
text alone is limited, recently introduced visually grounded syntax models make
use of multimodal information leading to improved performance in constituency
grammar induction. However, as compared to dependency grammars, constituency
grammars do not provide a straightforward way to incorporate visual information
without enforcing language-specific heuristics. In this paper, we propose an
unsupervised grammar induction model that leverages word concreteness and a
structural vision-based heuristic to jointly learn constituency-structure and
dependency-structure grammars. Our experiments find that concreteness is a
strong indicator for learning dependency grammars, improving the direct
attachment score (DAS) by over 50\% as compared to state-of-the-art models
trained on pure text. Next, we propose an extension of our model that leverages
both word concreteness and visual semantic role labels in constituency and
dependency parsing. Our experiments show that the proposed extension
outperforms the current state-of-the-art visually grounded models in
constituency parsing even with a smaller grammar size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transforming Fake News: Robust Generalisable News Classification Using Transformers. (arXiv:2109.09796v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09796">
<div class="article-summary-box-inner">
<span><p>As online news has become increasingly popular and fake news increasingly
prevalent, the ability to audit the veracity of online news content has become
more important than ever. Such a task represents a binary classification
challenge, for which transformers have achieved state-of-the-art results. Using
the publicly available ISOT and Combined Corpus datasets, this study explores
transformers' abilities to identify fake news, with particular attention given
to investigating generalisation to unseen datasets with varying styles, topics
and class distributions. Moreover, we explore the idea that opinion-based news
articles cannot be classified as real or fake due to their subjective nature
and often sensationalised language, and propose a novel two-step classification
pipeline to remove such articles from both model training and the final
deployed inference system. Experiments over the ISOT and Combined Corpus
datasets show that transformers achieve an increase in F1 scores of up to 4.9%
for out of distribution generalisation compared to baseline approaches, with a
further increase of 10.1% following the implementation of our two-step
classification pipeline. To the best of our knowledge, this study is the first
to investigate generalisation of transformers in this context.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Span Representation for Domain-adapted Coreference Resolution. (arXiv:2109.09811v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09811">
<div class="article-summary-box-inner">
<span><p>Recent work has shown fine-tuning neural coreference models can produce
strong performance when adapting to different domains. However, at the same
time, this can require a large amount of annotated target examples. In this
work, we focus on supervised domain adaptation for clinical notes, proposing
the use of concept knowledge to more efficiently adapt coreference models to a
new domain. We develop methods to improve the span representations via (1) a
retrofitting loss to incentivize span representations to satisfy a
knowledge-based distance function and (2) a scaffolding loss to guide the
recovery of knowledge from the span representation. By integrating these
losses, our model is able to improve our baseline precision and F-1 score. In
particular, we show that incorporating knowledge with end-to-end coreference
models results in better performance on the most challenging, domain-specific
spans.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Data Augmentation Methods for Anaphoric Zero Pronouns. (arXiv:2109.09825v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09825">
<div class="article-summary-box-inner">
<span><p>In pro-drop language like Arabic, Chinese, Italian, Japanese, Spanish, and
many others, unrealized (null) arguments in certain syntactic positions can
refer to a previously introduced entity, and are thus called anaphoric zero
pronouns. The existing resources for studying anaphoric zero pronoun
interpretation are however still limited. In this paper, we use five data
augmentation methods to generate and detect anaphoric zero pronouns
automatically. We use the augmented data as additional training materials for
two anaphoric zero pronoun systems for Arabic. Our experimental results show
that data augmentation improves the performance of the two systems, surpassing
the state-of-the-art results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">StreamSide: A Fully-Customizable Open-Source Toolkit for Efficient Annotation of Meaning Representations. (arXiv:2109.09853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09853">
<div class="article-summary-box-inner">
<span><p>This demonstration paper presents StreamSide, an open-source toolkit for
annotating multiple kinds of meaning representations. StreamSide supports
frame-based annotation schemes e.g., Abstract Meaning Representation (AMR) and
frameless annotation schemes e.g., Widely Interpretable Semantic Representation
(WISeR). Moreover, it supports both sentence-level and document-level
annotation by allowing annotators to create multi-rooted graphs for input text.
It can open and automatically convert between several types of input formats
including plain text, Penman notation, and its own JSON format enabling richer
annotation. It features reference frames for AMR predicate argument structures,
and also concept-to-text alignment. StreamSide is released under the Apache 2.0
license, and is completely open-source so that it can be customized to annotate
enriched meaning representations in different languages (e.g., Uniform Meaning
Representations). All StreamSide resources are publicly distributed through our
open source project at: https://github.com/emorynlp/StreamSide.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Intensionalizing Abstract Meaning Representations: Non-Veridicality and Scope. (arXiv:2109.09858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09858">
<div class="article-summary-box-inner">
<span><p>Abstract Meaning Representation (AMR) is a graphical meaning representation
language designed to represent propositional information about argument
structure. However, at present it is unable to satisfyingly represent
non-veridical intensional contexts, often licensing inappropriate inferences.
In this paper, we show how to resolve the problem of non-veridicality without
appealing to layered graphs through a mapping from AMRs into Simply-Typed
Lambda Calculus (STLC). At least for some cases, this requires the introduction
of a new role :content which functions as an intensional operator. The
translation proposed is inspired by the formal linguistics literature on the
event semantics of attitude reports. Next, we address the interaction of
quantifier scope and intensional operators in so-called de re/de dicto
ambiguities. We adopt a scope node from the literature and provide an explicit
multidimensional semantics utilizing Cooper storage which allows us to derive
the de re and de dicto scope readings as well as intermediate scope readings
which prove difficult for accounts without a scope node.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Language Identification with a Reciprocal Rank Classifier. (arXiv:2109.09862v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09862">
<div class="article-summary-box-inner">
<span><p>Language identification is a critical component of language processing
pipelines (Jauhiainen et al.,2019) and is not a solved problem in real-world
settings. We present a lightweight and effective language identifier that is
robust to changes of domain and to the absence of copious training data.
</p>
<p>The key idea for classification is that the reciprocal of the rank in a
frequency table makes an effective additive feature score, hence the term
Reciprocal Rank Classifier (RRC). The key finding for language classification
is that ranked lists of words and frequencies of characters form a sufficient
and robust representation of the regularities of key languages and their
orthographies.
</p>
<p>We test this on two 22-language data sets and demonstrate zero-effort domain
adaptation from a Wikipedia training set to a Twitter test set. When trained on
Wikipedia but applied to Twitter the macro-averaged F1-score of a
conventionally trained SVM classifier drops from 90.9% to 77.7%. By contrast,
the macro F1-score of RRC drops only from 93.1% to 90.6%. These classifiers are
compared with those from fastText and langid. The RRC performs better than
these established systems in most experiments, especially on short Wikipedia
texts and Twitter.
</p>
<p>The RRC classifier can be improved for particular domains and conversational
situations by adding words to the ranked lists. Using new terms learned from
such conversations, we demonstrate a further 7.9% increase in accuracy of
sample message classification, and 1.7% increase for conversation
classification. Surprisingly, this made results on Twitter data slightly worse.
</p>
<p>The RRC classifier is available as an open source Python package
(https://github.com/LivePersonInc/lplangid).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Representation Learning for Short Text Clustering. (arXiv:2109.09894v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09894">
<div class="article-summary-box-inner">
<span><p>Effective representation learning is critical for short text clustering due
to the sparse, high-dimensional and noise attributes of short text corpus.
Existing pre-trained models (e.g., Word2vec and BERT) have greatly improved the
expressiveness for short text representations with more condensed,
low-dimensional and continuous features compared to the traditional
Bag-of-Words (BoW) model. However, these models are trained for general
purposes and thus are suboptimal for the short text clustering task. In this
paper, we propose two methods to exploit the unsupervised autoencoder (AE)
framework to further tune the short text representations based on these
pre-trained text models for optimal clustering performance. In our first method
Structural Text Network Graph Autoencoder (STN-GAE), we exploit the structural
text information among the corpus by constructing a text network, and then
adopt graph convolutional network as encoder to fuse the structural features
with the pre-trained text features for text representation learning. In our
second method Soft Cluster Assignment Autoencoder (SCA-AE), we adopt an extra
soft cluster assignment constraint on the latent space of autoencoder to
encourage the learned text representations to be more clustering-friendly. We
tested two methods on seven popular short text datasets, and the experimental
results show that when only using the pre-trained model for short text
clustering, BERT performs better than BoW and Word2vec. However, as long as we
further tune the pre-trained representations, the proposed method like SCA-AE
can greatly increase the clustering performance, and the accuracy improvement
compared to use BERT alone could reach as much as 14\%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalization in Text-based Games via Hierarchical Reinforcement Learning. (arXiv:2109.09968v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09968">
<div class="article-summary-box-inner">
<span><p>Deep reinforcement learning provides a promising approach for text-based
games in studying natural language communication between humans and artificial
agents. However, the generalization still remains a big challenge as the agents
depend critically on the complexity and variety of training tasks. In this
paper, we address this problem by introducing a hierarchical framework built
upon the knowledge graph-based RL agent. In the high level, a meta-policy is
executed to decompose the whole game into a set of subtasks specified by
textual goals, and select one of them based on the KG. Then a sub-policy in the
low level is executed to conduct goal-conditioned reinforcement learning. We
carry out experiments on games with various difficulty levels and show that the
proposed method enjoys favorable generalizability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Kernel-Smoothed Machine Translation with Retrieved Examples. (arXiv:2109.09991v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09991">
<div class="article-summary-box-inner">
<span><p>How to effectively adapt neural machine translation (NMT) models according to
emerging cases without retraining? Despite the great success of neural machine
translation, updating the deployed models online remains a challenge. Existing
non-parametric approaches that retrieve similar examples from a database to
guide the translation process are promising but are prone to overfit the
retrieved examples. However, non-parametric methods are prone to overfit the
retrieved examples. In this work, we propose to learn Kernel-Smoothed
Translation with Example Retrieval (KSTER), an effective approach to adapt
neural machine translation models online. Experiments on domain adaptation and
multi-domain machine translation datasets show that even without expensive
retraining, KSTER is able to achieve improvement of 1.1 to 1.5 BLEU scores over
the best existing online adaptation methods. The code and trained models are
released at https://github.com/jiangqn/KSTER.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Negation-Instance Based Evaluation of End-to-End Negation Resolution. (arXiv:2109.10013v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10013">
<div class="article-summary-box-inner">
<span><p>In this paper, we revisit the task of negation resolution, which includes the
subtasks of cue detection (e.g. "not", "never") and scope resolution. In the
context of previous shared tasks, a variety of evaluation metrics have been
proposed. Subsequent works usually use different subsets of these, including
variations and custom implementations, rendering meaningful comparisons between
systems difficult. Examining the problem both from a linguistic perspective and
from a downstream viewpoint, we here argue for a negation-instance based
approach to evaluating negation resolution. Our proposed metrics correspond to
expectations over per-instance scores and hence are intuitively interpretable.
To render research comparable and to foster future work, we provide results for
a set of current state-of-the-art systems for negation resolution on three
English corpora, and make our implementation of the evaluation scripts publicly
available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not All Comments are Equal: Insights into Comment Moderation from a Topic-Aware Model. (arXiv:2109.10033v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10033">
<div class="article-summary-box-inner">
<span><p>Moderation of reader comments is a significant problem for online news
platforms. Here, we experiment with models for automatic moderation, using a
dataset of comments from a popular Croatian newspaper. Our analysis shows that
while comments that violate the moderation rules mostly share common linguistic
and thematic features, their content varies across the different sections of
the newspaper. We therefore make our models topic-aware, incorporating semantic
features from a topic model into the classification decision. Our results show
that topic information improves the performance of the model, increases its
confidence in correct outputs, and helps us understand the model's outputs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Something Old, Something New: Grammar-based CCG Parsing with Transformer Models. (arXiv:2109.10044v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10044">
<div class="article-summary-box-inner">
<span><p>This report describes the parsing problem for Combinatory Categorial Grammar
(CCG), showing how a combination of Transformer-based neural models and a
symbolic CCG grammar can lead to substantial gains over existing approaches.
The report also documents a 20-year research program, showing how NLP methods
have evolved over this time. The staggering accuracy improvements provided by
neural models for CCG parsing can be seen as a reflection of the improvements
seen in NLP more generally. The report provides a minimal introduction to CCG
and CCG parsing, with many pointers to the relevant literature. It then
describes the CCG supertagging problem, and some recent work from Tian et al.
(2020) which applies Transformer-based models to supertagging with great
effect. I use this existing model to develop a CCG multitagger, which can serve
as a front-end to an existing CCG parser. Simply using this new multitagger
provides substantial gains in parsing accuracy. I then show how a
Transformer-based model from the parsing literature can be combined with the
grammar-based CCG parser, setting a new state-of-the-art for the CCGbank
parsing task of almost 93% F-score for labelled dependencies, with complete
sentence accuracies of over 50%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?. (arXiv:2109.10052v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10052">
<div class="article-summary-box-inner">
<span><p>In this paper, we investigate what types of stereotypical information are
captured by pretrained language models. We present the first dataset comprising
stereotypical attributes of a range of social groups and propose a method to
elicit stereotypes encoded by pretrained language models in an unsupervised
fashion. Moreover, we link the emergent stereotypes to their manifestation as
basic emotions as a means to study their emotional effects in a more
generalized manner. To demonstrate how our methods can be used to analyze
emotion and stereotype shifts due to linguistic experience, we use fine-tuning
on news sources as a case study. Our experiments expose how attitudes towards
different social groups vary across models and how quickly emotions and
stereotypes can shift at the fine-tuning stage.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NADE: A Benchmark for Robust Adverse Drug Events Extraction in Face of Negations. (arXiv:2109.10080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10080">
<div class="article-summary-box-inner">
<span><p>Adverse Drug Event (ADE) extraction mod-els can rapidly examine large
collections of so-cial media texts, detecting mentions of drug-related adverse
reactions and trigger medicalinvestigations. However, despite the recent
ad-vances in NLP, it is currently unknown if suchmodels are robust in face
ofnegation, which ispervasive across language varieties.In this paper we
evaluate three state-of-the-artsystems, showing their fragility against
nega-tion, and then we introduce two possible strate-gies to increase the
robustness of these mod-els: a pipeline approach, relying on a
specificcomponent for negation detection; an augmen-tation of an ADE extraction
dataset to artifi-cially create negated samples and further trainthe models.We
show that both strategies bring significantincreases in performance, lowering
the num-ber of spurious entities predicted by the mod-els. Our dataset and code
will be publicly re-leased to encourage research on the topic.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SPLADE v2: Sparse Lexical and Expansion Model for Information Retrieval. (arXiv:2109.10086v1 [cs.IR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10086">
<div class="article-summary-box-inner">
<span><p>In neural Information Retrieval (IR), ongoing research is directed towards
improving the first retriever in ranking pipelines. Learning dense embeddings
to conduct retrieval using efficient approximate nearest neighbors methods has
proven to work well. Meanwhile, there has been a growing interest in learning
\emph{sparse} representations for documents and queries, that could inherit
from the desirable properties of bag-of-words models such as the exact matching
of terms and the efficiency of inverted indexes. Introduced recently, the
SPLADE model provides highly sparse representations and competitive results
with respect to state-of-the-art dense and sparse approaches. In this paper, we
build on SPLADE and propose several significant improvements in terms of
effectiveness and/or efficiency. More specifically, we modify the pooling
mechanism, benchmark a model solely based on document expansion, and introduce
models trained with distillation. We also report results on the BEIR benchmark.
Overall, SPLADE is considerably improved with more than $9$\% gains on NDCG@10
on TREC DL 2019, leading to state-of-the-art results on the BEIR benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">InvBERT: Text Reconstruction from Contextualized Embeddings used for Derived Text Formats of Literary Works. (arXiv:2109.10104v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10104">
<div class="article-summary-box-inner">
<span><p>Digital Humanities and Computational Literary Studies apply text mining
methods to investigate literature. Such automated approaches enable
quantitative studies on large corpora which would not be feasible by manual
inspection alone. However, due to copyright restrictions, the availability of
relevant digitized literary works is limited. Derived Text Formats (DTFs) have
been proposed as a solution. Here, textual materials are transformed in such a
way that copyright-critical features are removed, but that the use of certain
analytical methods remains possible. Contextualized word embeddings produced by
transformer-encoders (like BERT) are promising candidates for DTFs because they
allow for state-of-the-art performance on various analytical tasks and, at
first sight, do not disclose the original text. However, in this paper we
demonstrate that under certain conditions the reconstruction of the original
copyrighted text becomes feasible and its publication in the form of
contextualized word representations is not safe. Our attempts to invert BERT
suggest, that publishing parts of the encoder together with the contextualized
embeddings is critical, since it allows to generate data to train a decoder
with a reconstruction accuracy sufficient to violate copyright laws.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Difficulty of Segmenting Words with Attention. (arXiv:2109.10107v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10107">
<div class="article-summary-box-inner">
<span><p>Word segmentation, the problem of finding word boundaries in speech, is of
interest for a range of tasks. Previous papers have suggested that for
sequence-to-sequence models trained on tasks such as speech translation or
speech recognition, attention can be used to locate and segment the words. We
show, however, that even on monolingual data this approach is brittle. In our
experiments with different input types, data sizes, and segmentation
algorithms, only models trained to predict phones from words succeed in the
task. Models trained to predict words from either phones or speech (i.e., the
opposite direction needed to generalize to new data), yield much worse results,
suggesting that attention-based segmentation is only useful in limited
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Comprehensive Review on Summarizing Financial News Using Deep Learning. (arXiv:2109.10118v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10118">
<div class="article-summary-box-inner">
<span><p>Investors make investment decisions depending on several factors such as
fundamental analysis, technical analysis, and quantitative analysis. Another
factor on which investors can make investment decisions is through sentiment
analysis of news headlines, the sole purpose of this study. Natural Language
Processing techniques are typically used to deal with such a large amount of
data and get valuable information out of it. NLP algorithms convert raw text
into numerical representations that machines can easily understand and
interpret. This conversion can be done using various embedding techniques. In
this research, embedding techniques used are BoW, TF-IDF, Word2Vec, BERT,
GloVe, and FastText, and then fed to deep learning models such as RNN and LSTM.
This work aims to evaluate these model's performance to choose the robust model
in identifying the significant factors influencing the prediction. During this
research, it was expected that Deep Leaming would be applied to get the desired
results or achieve better accuracy than the state-of-the-art. The models are
compared to check their outputs to know which one has performed better.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConvFiT: Conversational Fine-Tuning of Pretrained Language Models. (arXiv:2109.10126v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10126">
<div class="article-summary-box-inner">
<span><p>Transformer-based language models (LMs) pretrained on large text collections
are proven to store a wealth of semantic knowledge. However, 1) they are not
effective as sentence encoders when used off-the-shelf, and 2) thus typically
lag behind conversationally pretrained (e.g., via response selection) encoders
on conversational tasks such as intent detection (ID). In this work, we propose
ConvFiT, a simple and efficient two-stage procedure which turns any pretrained
LM into a universal conversational encoder (after Stage 1 ConvFiT-ing) and
task-specialised sentence encoder (after Stage 2). We demonstrate that 1)
full-blown conversational pretraining is not required, and that LMs can be
quickly transformed into effective conversational encoders with much smaller
amounts of unannotated data; 2) pretrained LMs can be fine-tuned into
task-specialised sentence encoders, optimised for the fine-grained semantics of
a particular task. Consequently, such specialised sentence encoders allow for
treating ID as a simple semantic similarity task based on interpretable nearest
neighbours retrieval. We validate the robustness and versatility of the ConvFiT
framework with such similarity-based inference on the standard ID evaluation
sets: ConvFiT-ed LMs achieve state-of-the-art ID performance across the board,
with particular gains in the most challenging, few-shot setups.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Are Transformers a Modern Version of ELIZA? Observations on French Object Verb Agreement. (arXiv:2109.10133v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10133">
<div class="article-summary-box-inner">
<span><p>Many recent works have demonstrated that unsupervised sentence
representations of neural networks encode syntactic information by observing
that neural language models are able to predict the agreement between a verb
and its subject. We take a critical look at this line of research by showing
that it is possible to achieve high accuracy on this agreement task with simple
surface heuristics, indicating a possible flaw in our assessment of neural
networks' syntactic ability. Our fine-grained analyses of results on the
long-range French object-verb agreement show that contrary to LSTMs,
Transformers are able to capture a non-trivial amount of grammatical structure.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge Distillation with Noisy Labels for Natural Language Understanding. (arXiv:2109.10147v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10147">
<div class="article-summary-box-inner">
<span><p>Knowledge Distillation (KD) is extensively used to compress and deploy large
pre-trained language models on edge devices for real-world applications.
However, one neglected area of research is the impact of noisy (corrupted)
labels on KD. We present, to the best of our knowledge, the first study on KD
with noisy labels in Natural Language Understanding (NLU). We document the
scope of the problem and present two methods to mitigate the impact of label
noise. Experiments on the GLUE benchmark show that our methods are effective
even under high noise levels. Nevertheless, our results indicate that more
research is necessary to cope with label noise under the KD.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">RAIL-KD: RAndom Intermediate Layer Mapping for Knowledge Distillation. (arXiv:2109.10164v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10164">
<div class="article-summary-box-inner">
<span><p>Intermediate layer knowledge distillation (KD) can improve the standard KD
technique (which only targets the output of teacher and student models)
especially over large pre-trained language models. However, intermediate layer
distillation suffers from excessive computational burdens and engineering
efforts required for setting up a proper layer mapping. To address these
problems, we propose a RAndom Intermediate Layer Knowledge Distillation
(RAIL-KD) approach in which, intermediate layers from the teacher model are
selected randomly to be distilled into the intermediate layers of the student
model. This randomized selection enforce that: all teacher layers are taken
into account in the training process, while reducing the computational cost of
intermediate layer distillation. Also, we show that it act as a regularizer for
improving the generalizability of the student model. We perform extensive
experiments on GLUE tasks as well as on out-of-domain test sets. We show that
our proposed RAIL-KD approach outperforms other state-of-the-art intermediate
layer KD methods considerably in both performance and training-time.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Familiar Does That Sound? Cross-Lingual Representational Similarity Analysis of Acoustic Word Embeddings. (arXiv:2109.10179v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10179">
<div class="article-summary-box-inner">
<span><p>How do neural networks "perceive" speech sounds from unknown languages? Does
the typological similarity between the model's training language (L1) and an
unknown language (L2) have an impact on the model representations of L2 speech
signals? To answer these questions, we present a novel experimental design
based on representational similarity analysis (RSA) to analyze acoustic word
embeddings (AWEs) -- vector representations of variable-duration spoken-word
segments. First, we train monolingual AWE models on seven Indo-European
languages with various degrees of typological similarity. We then employ RSA to
quantify the cross-lingual similarity by simulating native and non-native
spoken-word processing using AWEs. Our experiments show that typological
similarity indeed affects the representational similarity of the models in our
study. We further discuss the implications of our work on modeling speech
processing and language similarity with neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TranslateLocally: Blazing-fast translation running on the local CPU. (arXiv:2109.10194v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10194">
<div class="article-summary-box-inner">
<span><p>Every day, millions of people sacrifice their privacy and browsing habits in
exchange for online machine translation. Companies and governments with
confidentiality requirements often ban online translation or pay a premium to
disable logging. To bring control back to the end user and demonstrate speed,
we developed translateLocally. Running locally on a desktop or laptop CPU,
translateLocally delivers cloud-like translation speed and quality even on 10
year old hardware. The open-source software is based on Marian and runs on
Linux, Windows, and macOS.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">One Source, Two Targets: Challenges and Rewards of Dual Decoding. (arXiv:2109.10197v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10197">
<div class="article-summary-box-inner">
<span><p>Machine translation is generally understood as generating one target text
from an input source document. In this paper, we consider a stronger
requirement: to jointly generate two texts so that each output side effectively
depends on the other. As we discuss, such a device serves several practical
purposes, from multi-target machine translation to the generation of controlled
variations of the target text. We present an analysis of possible
implementations of dual decoding, and experiment with four applications.
Viewing the problem from multiple angles allows us to better highlight the
challenges of dual decoding and to also thoroughly analyze the benefits of
generating matched, rather than independent, translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Blindness to Modality Helps Entailment Graph Mining. (arXiv:2109.10227v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10227">
<div class="article-summary-box-inner">
<span><p>Understanding linguistic modality is widely seen as important for downstream
tasks such as Question Answering and Knowledge Graph Population. Entailment
Graph learning might also be expected to benefit from attention to modality. We
build Entailment Graphs using a news corpus filtered with a modality parser,
and show that stripping modal modifiers from predicates in fact increases
performance. This suggests that for some tasks, the pragmatics of modal
modification of predicates allows them to contribute as evidence of entailment.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">BERTweetFR : Domain Adaptation of Pre-Trained Language Models for French Tweets. (arXiv:2109.10234v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10234">
<div class="article-summary-box-inner">
<span><p>We introduce BERTweetFR, the first large-scale pre-trained language model for
French tweets. Our model is initialized using the general-domain French
language model CamemBERT which follows the base architecture of RoBERTa.
Experiments show that BERTweetFR outperforms all previous general-domain French
language models on two downstream Twitter NLP tasks of offensiveness
identification and named entity recognition. The dataset used in the
offensiveness detection task is first created and annotated by our team,
filling in the gap of such analytic datasets in French. We make our model
publicly available in the transformers library with the aim of promoting future
research in analytic tasks for French tweets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Does Vision-and-Language Pretraining Improve Lexical Grounding?. (arXiv:2109.10246v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10246">
<div class="article-summary-box-inner">
<span><p>Linguistic representations derived from text alone have been criticized for
their lack of grounding, i.e., connecting words to their meanings in the
physical world. Vision-and-Language (VL) models, trained jointly on text and
image or video data, have been offered as a response to such criticisms.
However, while VL pretraining has shown success on multimodal tasks such as
visual question answering, it is not yet known how the internal linguistic
representations themselves compare to their text-only counterparts. This paper
compares the semantic representations learned via VL vs. text-only pretraining
for two recent VL models using a suite of analyses (clustering, probing, and
performance on a commonsense question answering task) in a language-only
setting. We find that the multimodal models fail to significantly outperform
the text-only variants, suggesting that future work is required if multimodal
pretraining is to be pursued as a means of improving NLP in general.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Audiomer: A Convolutional Transformer for Keyword Spotting. (arXiv:2109.10252v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10252">
<div class="article-summary-box-inner">
<span><p>Transformers have seen an unprecedented rise in Natural Language Processing
and Computer Vision tasks. However, in audio tasks, they are either infeasible
to train due to extremely large sequence length of audio waveforms or reach
competitive performance after feature extraction through Fourier-based methods,
incurring a loss-floor. In this work, we introduce an architecture, Audiomer,
where we combine 1D Residual Networks with Performer Attention to achieve
state-of-the-art performance in Keyword Spotting with raw audio waveforms,
out-performing all previous methods while also being computationally cheaper,
much more parameter and data-efficient. Audiomer allows for deployment in
compute-constrained devices and training on smaller datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-Task Learning with Sentiment, Emotion, and Target Detection to Recognize Hate Speech and Offensive Language. (arXiv:2109.10255v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10255">
<div class="article-summary-box-inner">
<span><p>The recognition of hate speech and offensive language (HOF) is commonly
formulated as a classification task to decide if a text contains HOF. We
investigate whether HOF detection can profit by taking into account the
relationships between HOF and similar concepts: (a) HOF is related to sentiment
analysis because hate speech is typically a negative statement and expresses a
negative opinion; (b) it is related to emotion analysis, as expressed hate
points to the author experiencing (or pretending to experience) anger while the
addressees experience (or are intended to experience) fear. (c) Finally, one
constituting element of HOF is the mention of a targeted person or group. On
this basis, we hypothesize that HOF detection shows improvements when being
modeled jointly with these concepts, in a multi-task learning setup. We base
our experiments on existing data sets for each of these concepts (sentiment,
emotion, target of HOF) and evaluate our models as a participant (as team
IMS-SINAI) in the HASOC FIRE 2021 English Subtask 1A. Based on model-selection
experiments in which we consider multiple available resources and submissions
to the shared task, we find that the combination of the CrowdFlower emotion
corpus, the SemEval 2016 Sentiment Corpus, and the OffensEval 2019 target
detection data leads to an F1 =.79 in a multi-head multi-task learning model
based on BERT, in comparison to .7895 of plain BERT. On the HASOC 2019 test
data, this result is more substantial with an increase by 2pp in F1 and a
considerable increase in recall. Across both data sets (2019, 2021), the recall
is particularly increased for the class of HOF (6pp for the 2019 data and 3pp
for the 2021 data), showing that MTL with emotion, sentiment, and target
identification is an appropriate approach for early warning systems that might
be deployed in social media platforms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Trade-offs of Domain Adaptation for Neural Language Models. (arXiv:2109.10274v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10274">
<div class="article-summary-box-inner">
<span><p>In this paper, we connect language model adaptation with concepts of machine
learning theory. We consider a training setup with a large out-of-domain set
and a small in-domain set. As a first contribution, we derive how the benefit
of training a model on either set depends on the size of the sets and the
distance between their underlying distribution. As a second contribution, we
present how the most popular data selection techniques -- importance sampling,
intelligent data selection and influence functions -- can be presented in a
common framework which highlights their similarity and also their subtle
differences.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models. (arXiv:2109.10282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10282">
<div class="article-summary-box-inner">
<span><p>Text recognition is a long-standing research problem for document
digitalization. Existing approaches for text recognition are usually built
based on CNN for image understanding and RNN for char-level text generation. In
addition, another language model is usually needed to improve the overall
accuracy as a post-processing step. In this paper, we propose an end-to-end
text recognition approach with pre-trained image Transformer and text
Transformer models, namely TrOCR, which leverages the Transformer architecture
for both image understanding and wordpiece-level text generation. The TrOCR
model is simple but effective, and can be pre-trained with large-scale
synthetic data and fine-tuned with human-labeled datasets. Experiments show
that the TrOCR model outperforms the current state-of-the-art models on both
printed and handwritten text recognition tasks. The code and models will be
publicly available at https://aka.ms/TrOCR.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">From English to Signal Temporal Logic. (arXiv:2109.10294v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10294">
<div class="article-summary-box-inner">
<span><p>Formal methods provide very powerful tools and techniques for the design and
analysis of complex systems. Their practical application remains however
limited, due to the widely accepted belief that formal methods require
extensive expertise and a steep learning curve. Writing correct formal
specifications in form of logical formulas is still considered to be a
difficult and error prone task.
</p>
<p>In this paper we propose DeepSTL, a tool and technique for the translation of
informal requirements, given as free English sentences, into Signal Temporal
Logic (STL), a formal specification language for cyber-physical systems, used
both by academia and advanced research labs in industry. A major challenge to
devise such a translator is the lack of publicly available informal
requirements and formal specifications. We propose a two-step workflow to
address this challenge. We first design a grammar-based generation technique of
synthetic data, where each output is a random STL formula and its associated
set of possible English translations. In the second step, we use a
state-of-the-art transformer-based neural translation technique, to train an
accurate attentional translator of English to STL. The experimental results
show high translation quality for patterns of English requirements that have
been well trained, making this workflow promising to be extended for processing
more complex translation tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Document-Level Translation Enables Zero-Shot Transfer From Sentences to Documents. (arXiv:2109.10341v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10341">
<div class="article-summary-box-inner">
<span><p>Document-level neural machine translation (DocNMT) delivers coherent
translations by incorporating cross-sentence context. However, for most
language pairs there's a shortage of parallel documents, although parallel
sentences are readily available. In this paper, we study whether and how
contextual modeling in DocNMT is transferable from sentences to documents in a
zero-shot fashion (i.e. no parallel documents for student languages) through
multilingual modeling. Using simple concatenation-based DocNMT, we explore the
effect of 3 factors on multilingual transfer: the number of document-supervised
teacher languages, the data schedule for parallel documents at training, and
the data condition of parallel documents (genuine vs. backtranslated). Our
experiments on Europarl-7 and IWSLT-10 datasets show the feasibility of
multilingual transfer for DocNMT, particularly on document-specific metrics. We
observe that more teacher languages and adequate data schedule both contribute
to better transfer quality. Surprisingly, the transfer is less sensitive to the
data condition and multilingual DocNMT achieves comparable performance with
both back-translated and genuine document pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Relation-Guided Pre-Training for Open-Domain Question Answering. (arXiv:2109.10346v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.10346">
<div class="article-summary-box-inner">
<span><p>Answering complex open-domain questions requires understanding the latent
relations between involving entities. However, we found that the existing QA
datasets are extremely imbalanced in some types of relations, which hurts the
generalization performance over questions with long-tail relations. To remedy
this problem, in this paper, we propose a Relation-Guided Pre-Training
(RGPT-QA) framework. We first generate a relational QA dataset covering a wide
range of relations from both the Wikidata triplets and Wikipedia hyperlinks. We
then pre-train a QA model to infer the latent relations from the question, and
then conduct extractive QA to get the target answer entity. We demonstrate that
by pretraining with propoed RGPT-QA techique, the popular open-domain QA model,
Dense Passage Retriever (DPR), achieves 2.2%, 2.4%, and 6.3% absolute
improvement in Exact Match accuracy on Natural Questions, TriviaQA, and
WebQuestions. Particularly, we show that RGPT-QA improves significantly on
questions with long-tail relations
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Informed Sampling for Diversity in Concept-to-Text NLG. (arXiv:2004.14364v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.14364">
<div class="article-summary-box-inner">
<span><p>Deep-learning models for language generation tasks tend to produce repetitive
output. Various methods have been proposed to encourage lexical diversity
during decoding, but this often comes at a cost to the perceived fluency and
adequacy of the output. In this work, we propose to ameliorate this cost by
using an Imitation Learning approach to explore the level of diversity that a
language generation model can reliably produce. Specifically, we augment the
decoding process with a meta-classifier trained to distinguish which words at
any given timestep will lead to high-quality output. We focus our experiments
on concept-to-text generation where models are sensitive to the inclusion of
irrelevant words due to the strict relation between input and output. Our
analysis shows that previous methods for diversity underperform in this
setting, while human evaluation suggests that our proposed method achieves a
high level of diversity with minimal effect to the output's fluency and
adequacy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Energy-Based Reranking: Improving Neural Machine Translation Using Energy-Based Models. (arXiv:2009.13267v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.13267">
<div class="article-summary-box-inner">
<span><p>The discrepancy between maximum likelihood estimation (MLE) and task measures
such as BLEU score has been studied before for autoregressive neural machine
translation (NMT) and resulted in alternative training algorithms (Ranzato et
al., 2016; Norouzi et al., 2016; Shen et al., 2016; Wu et al., 2018). However,
MLE training remains the de facto approach for autoregressive NMT because of
its computational efficiency and stability. Despite this mismatch between the
training objective and task measure, we notice that the samples drawn from an
MLE-based trained NMT support the desired distribution -- there are samples
with much higher BLEU score comparing to the beam decoding output. To benefit
from this observation, we train an energy-based model to mimic the behavior of
the task measure (i.e., the energy-based model assigns lower energy to samples
with higher BLEU score), which is resulted in a re-ranking algorithm based on
the samples drawn from NMT: energy-based re-ranking (EBR). We use both marginal
energy models (over target sentence) and joint energy models (over both source
and target sentences). Our EBR with the joint energy model consistently
improves the performance of the Transformer-based NMT: +4 BLEU points on
IWSLT'14 German-English, +3.0 BELU points on Sinhala-English, +1.2 BLEU on
WMT'16 English-German tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Not all parameters are born equal: Attention is mostly what you need. (arXiv:2010.11859v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11859">
<div class="article-summary-box-inner">
<span><p>Transformers are widely used in state-of-the-art machine translation, but the
key to their success is still unknown. To gain insight into this, we consider
three groups of parameters: embeddings, attention, and feed forward neural
network (FFN) layers. We examine the relative importance of each by performing
an ablation study where we initialise them at random and freeze them, so that
their weights do not change over the course of the training. Through this, we
show that the attention and FFN are equally important and fulfil the same
functionality in a model. We show that the decision about whether a component
is frozen or allowed to train is at least as important for the final model
performance as its number of parameters. At the same time, the number of
parameters alone is not indicative of a component's importance. Finally, while
the embedding layer is the least essential for machine translation tasks, it is
the most important component for language modelling tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Highs and Lows of Simple Lexical Domain Adaptation Approaches for Neural Machine Translation. (arXiv:2101.00421v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00421">
<div class="article-summary-box-inner">
<span><p>Machine translation systems are vulnerable to domain mismatch, especially in
a low-resource scenario. Out-of-domain translations are often of poor quality
and prone to hallucinations, due to exposure bias and the decoder acting as a
language model. We adopt two approaches to alleviate this problem: lexical
shortlisting restricted by IBM statistical alignments, and hypothesis
re-ranking based on similarity. The methods are computationally cheap, widely
known, but not extensively experimented on domain adaptation. We demonstrate
success on low-resource out-of-domain test sets, however, the methods are
ineffective when there is sufficient data or too great domain mismatch. This is
due to both the IBM model losing its advantage over the implicitly learned
neural alignment, and issues with subword segmentation of out-of-domain words.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Open-Retrieval Conversational Machine Reading. (arXiv:2102.08633v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.08633">
<div class="article-summary-box-inner">
<span><p>In conversational machine reading, systems need to interpret natural language
rules, answer high-level questions such as "May I qualify for VA health care
benefits?", and ask follow-up clarification questions whose answer is necessary
to answer the original question. However, existing works assume the rule text
is provided for each user question, which neglects the essential retrieval step
in real scenarios. In this work, we propose and investigate an open-retrieval
setting of conversational machine reading. In the open-retrieval setting, the
relevant rule texts are unknown so that a system needs to retrieve
question-relevant evidence from a collection of rule texts, and answer users'
high-level questions according to multiple retrieved rule texts in a
conversational manner. We propose MUDERN, a Multi-passage Discourse-aware
Entailment Reasoning Network which extracts conditions in the rule texts
through discourse segmentation, conducts multi-passage entailment reasoning to
answer user questions directly, or asks clarification follow-up questions to
inquiry more information. On our created OR-ShARC dataset, MUDERN achieves the
state-of-the-art performance, outperforming existing single-passage
conversational machine reading models as well as a new multi-passage
conversational machine reading baseline by a large margin. In addition, we
conduct in-depth analyses to provide new insights into this new setting and our
model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-autoregressive Mandarin-English Code-switching Speech Recognition. (arXiv:2104.02258v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.02258">
<div class="article-summary-box-inner">
<span><p>Mandarin-English code-switching (CS) is frequently used among East and
Southeast Asian people. However, the intra-sentence language switching of the
two very different languages makes recognizing CS speech challenging.
Meanwhile, the recent successful non-autoregressive (NAR) ASR models remove the
need for left-to-right beam decoding in autoregressive (AR) models and achieved
outstanding performance and fast inference speed, but it has not been applied
to Mandarin-English CS speech recognition. This paper takes advantage of the
Mask-CTC NAR ASR framework to tackle the CS speech recognition issue. We
further propose to change the Mandarin output target of the encoder to Pinyin
for faster encoder training and introduce the Pinyin-to-Mandarin decoder to
learn contextualized information. Moreover, we use word embedding label
smoothing to regularize the decoder with contextualized information and
projection matrix regularization to bridge that gap between the encoder and
decoder. We evaluate these methods on the SEAME corpus and achieved exciting
results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders. (arXiv:2104.03630v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.03630">
<div class="article-summary-box-inner">
<span><p>Powerful sentence encoders trained for multiple languages are on the rise.
These systems are capable of embedding a wide range of linguistic properties
into vector representations. While explicit probing tasks can be used to verify
the presence of specific linguistic properties, it is unclear whether the
vector representations can be manipulated to indirectly steer such properties.
For efficient learning, we investigate the use of a geometric mapping in
embedding space to transform linguistic properties, without any tuning of the
pre-trained sentence encoder or decoder. We validate our approach on three
linguistic properties using a pre-trained multilingual autoencoder and analyze
the results in both monolingual and cross-lingual settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Condenser: a Pre-training Architecture for Dense Retrieval. (arXiv:2104.08253v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08253">
<div class="article-summary-box-inner">
<span><p>Pre-trained Transformer language models (LM) have become go-to text
representation encoders. Prior research fine-tunes deep LMs to encode text
sequences such as sentences and passages into single dense vector
representations for efficient text comparison and retrieval. However, dense
encoders require a lot of data and sophisticated techniques to effectively
train and suffer in low data situations. This paper finds a key reason is that
standard LMs' internal attention structure is not ready-to-use for dense
encoders, which needs to aggregate text information into the dense
representation. We propose to pre-train towards dense encoder with a novel
Transformer architecture, Condenser, where LM prediction CONditions on DENSE
Representation. Our experiments show Condenser improves over standard LM by
large margins on various text retrieval and similarity tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Identifying Offensive Expressions of Opinion in Context. (arXiv:2104.12227v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12227">
<div class="article-summary-box-inner">
<span><p>Classic information extraction techniques consist in building questions and
answers about the facts. Indeed, it is still a challenge to subjective
information extraction systems to identify opinions and feelings in context. In
sentiment-based NLP tasks, there are few resources to information extraction,
above all offensive or hateful opinions in context. To fill this important gap,
this short paper provides a new cross-lingual and contextual offensive lexicon,
which consists of explicit and implicit offensive and swearing expressions of
opinion, which were annotated in two different classes: context dependent and
context-independent offensive. In addition, we provide markers to identify hate
speech. Annotation approach was evaluated at the expression-level and achieves
high human inter-annotator agreement. The provided offensive lexicon is
available in Portuguese and English languages.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contextual Lexicon-Based Approach for Hate Speech and Offensive Language Detection. (arXiv:2104.12265v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.12265">
<div class="article-summary-box-inner">
<span><p>This paper provides a new approach for offensive language and hate speech
detection on social media. Our approach incorporates an offensive lexicon
composed of implicit and explicit offensive and swearing expressions annotated
with binary classes: context-dependent and context-independent offensive. Due
to the severity of the hate speech and offensive comments in Brazil, and the
lack of research in Portuguese, Brazilian Portuguese is the language used to
validate the proposed method. Nevertheless, our proposal may be applied to any
other language or domain. Based on the obtained results, the proposed approach
showed high-performance overcoming the current baselines for European and
Brazilian Portuguese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DiaKG: an Annotated Diabetes Dataset for Medical Knowledge Graph Construction. (arXiv:2105.15033v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2105.15033">
<div class="article-summary-box-inner">
<span><p>Knowledge Graph has been proven effective in modeling structured information
and conceptual knowledge, especially in the medical domain. However, the lack
of high-quality annotated corpora remains a crucial problem for advancing the
research and applications on this task. In order to accelerate the research for
domain-specific knowledge graphs in the medical domain, we introduce DiaKG, a
high-quality Chinese dataset for Diabetes knowledge graph, which contains
22,050 entities and 6,890 relations in total. We implement recent typical
methods for Named Entity Recognition and Relation Extraction as a benchmark to
evaluate the proposed dataset thoroughly. Empirical results show that the DiaKG
is challenging for most existing methods and further analysis is conducted to
discuss future research direction for improvements. We hope the release of this
dataset can assist the construction of diabetes knowledge graphs and facilitate
AI-based applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Bilateral Personalized Dialogue Generation with Contrastive Learning. (arXiv:2106.07857v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07857">
<div class="article-summary-box-inner">
<span><p>Generating personalized responses is one of the major challenges in natural
human-robot interaction. Current researches in this field mainly focus on
generating responses consistent with the robot's pre-assigned persona, while
ignoring the user's persona. Such responses may be inappropriate or even
offensive, which may lead to the bad user experience. Therefore, we propose a
Bilateral Personalized Dialogue Generation (BPDG) method for dyadic
conversation, which integrates user and robot personas into dialogue generation
via designing a dynamic persona-aware fusion method. To bridge the gap between
the learning objective function and evaluation metrics, the Conditional Mutual
Information Maximum (CMIM) criterion is adopted with contrastive learning to
select the proper response from the generated candidates. Moreover, a bilateral
persona accuracy metric is designed to measure the degree of bilateral
personalization. Experimental results demonstrate that, compared with several
state-of-the-art methods, the final results of the proposed method are more
personalized and consistent with bilateral personas in terms of both automatic
and manual evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scientific Language Models for Biomedical Knowledge Base Completion: An Empirical Study. (arXiv:2106.09700v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.09700">
<div class="article-summary-box-inner">
<span><p>Biomedical knowledge graphs (KGs) hold rich information on entities such as
diseases, drugs, and genes. Predicting missing links in these graphs can boost
many important applications, such as drug design and repurposing. Recent work
has shown that general-domain language models (LMs) can serve as "soft" KGs,
and that they can be fine-tuned for the task of KG completion. In this work, we
study scientific LMs for KG completion, exploring whether we can tap into their
latent knowledge to enhance biomedical link prediction. We evaluate several
domain-specific LMs, fine-tuning them on datasets centered on drugs and
diseases that we represent as KGs and enrich with textual entity descriptions.
We integrate the LM-based models with KG embedding models, using a router
method that learns to assign each input example to either type of model and
provides a substantial boost in performance. Finally, we demonstrate the
advantage of LM models in the inductive setting with novel scientific entities.
Our datasets and code are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">You should evaluate your language model on marginal likelihood over tokenisations. (arXiv:2109.02550v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02550">
<div class="article-summary-box-inner">
<span><p>Neural language models typically tokenise input text into sub-word units to
achieve an open vocabulary. The standard approach is to use a single canonical
tokenisation at both train and test time. We suggest that this approach is
unsatisfactory and may bottleneck our evaluation of language model performance.
Using only the one-best tokenisation ignores tokeniser uncertainty over
alternative tokenisations, which may hurt model out-of-domain performance.
</p>
<p>In this paper, we argue that instead, language models should be evaluated on
their marginal likelihood over tokenisations. We compare different estimators
for the marginal likelihood based on sampling, and show that it is feasible to
estimate the marginal likelihood with a manageable number of samples. We then
evaluate pretrained English and German language models on both the
one-best-tokenisation and marginal perplexities, and show that the marginal
perplexity can be significantly better than the one best, especially on
out-of-domain data. We link this difference in perplexity to the tokeniser
uncertainty as measured by tokeniser entropy. We discuss some implications of
our results for language model training and evaluation, particularly with
regard to tokenisation robustness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Perspective-taking and Pragmatics for Generating Empathetic Responses Focused on Emotion Causes. (arXiv:2109.08828v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.08828">
<div class="article-summary-box-inner">
<span><p>Empathy is a complex cognitive ability based on the reasoning of others'
affective states. In order to better understand others and express stronger
empathy in dialogues, we argue that two issues must be tackled at the same
time: (i) identifying which word is the cause for the other's emotion from his
or her utterance and (ii) reflecting those specific words in the response
generation. However, previous approaches for recognizing emotion cause words in
text require sub-utterance level annotations, which can be demanding. Taking
inspiration from social cognition, we leverage a generative estimator to infer
emotion cause words from utterances with no word-level label. Also, we
introduce a novel method based on pragmatics to make dialogue models focus on
targeted words in the input during generation. Our method is applicable to any
dialogue models with no additional training on the fly. We show our approach
improves multiple best-performing dialogue agents on generating more focused
empathetic responses in terms of both automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What BERT Based Language Models Learn in Spoken Transcripts: An Empirical Study. (arXiv:2109.09105v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09105">
<div class="article-summary-box-inner">
<span><p>Language Models (LMs) have been ubiquitously leveraged in various tasks
including spoken language understanding (SLU). Spoken language requires careful
understanding of speaker interactions, dialog states and speech induced
multimodal behaviors to generate a meaningful representation of the
conversation. In this work, we propose to dissect SLU into three representative
properties:conversational (disfluency, pause, overtalk), channel (speaker-type,
turn-tasks) and ASR (insertion, deletion,substitution). We probe BERT based
language models (BERT, RoBERTa) trained on spoken transcripts to investigate
its ability to understand multifarious properties in absence of any speech
cues. Empirical results indicate that LM is surprisingly good at capturing
conversational properties such as pause prediction and overtalk detection from
lexical tokens. On the downsides, the LM scores low on turn-tasks and ASR
errors predictions. Additionally, pre-training the LM on spoken transcripts
restrain its linguistic understanding. Finally, we establish the efficacy and
transferability of the mentioned properties on two benchmark datasets:
Switchboard Dialog Act and Disfluency datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Investigating Crowdsourcing Protocols for Evaluating the Factual Consistency of Summaries. (arXiv:2109.09195v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.09195">
<div class="article-summary-box-inner">
<span><p>Current pre-trained models applied to summarization are prone to factual
inconsistencies which either misrepresent the source text or introduce
extraneous information. Thus, comparing the factual consistency of summaries is
necessary as we develop improved models. However, the optimal human evaluation
setup for factual consistency has not been standardized. To address this issue,
we crowdsourced evaluations for factual consistency using the rating-based
Likert scale and ranking-based Best-Worst Scaling protocols, on 100 articles
from each of the CNN-Daily Mail and XSum datasets over four state-of-the-art
models, to determine the most reliable evaluation framework. We find that
ranking-based protocols offer a more reliable measure of summary quality across
datasets, while the reliability of Likert ratings depends on the target dataset
and the evaluation design. Our crowdsourcing templates and summary evaluations
will be publicly available to facilitate future research on factual consistency
in summarization.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-22 04:27:29.489734709 UTC">2021-09-22 04:27:29 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>