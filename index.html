<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-01T01:56:19.980087076Z">09-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€å…¨èŒè¿œç¨‹ã€‘15K-30K/ç¡…è°·æ— ç AIå›¢é˜Ÿæ‹›/åç«¯å¼€å‘å·¥ç¨‹å¸ˆ/Java or Scala</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=6c2bed71-6adf-45ac-bd60-904427c5c51c">
<div class="article-summary-box-inner">
<span><p>å…¬å¸ç®€ä»‹
æˆ‘ä»¬æ˜¯ä¸€ä¸ªåˆšåˆšæˆç«‹åœ¨ä¸Šæµ·çš„åˆ›ä¸šå…¬å¸ã€‚åˆ›å§‹äººåœ¨ç¡…è°·ï¼Œå…¨å‘˜è¿œç¨‹ï¼Œè‡´åŠ›äºæ‰“é€ ä¸‹ä¸€ä»£æ— ç AIæ•°æ®äº§å“ï¼ŒæŠ€æœ¯æ°›å›´æµ“åšã€‚ç›®å‰å’Œå›½å†…ä¸€å®¶çŸ¥åCDPå¹³å°å‚å•†åˆç†æ¨å‡ºäº†ç¬¬ä¸€ç‰ˆæ— ä»£ç æœºå™¨å­¦ä¹ é¢„æµ‹å¹³å°ï¼Œå¸®åŠ©ä¼ä¸šå®ç°è¿è¥çš„æ™ºèƒ½åŒ–ã€‚</p>
<p>å·¥ä½œèŒè´£
1ã€è´Ÿè´£æœºå™¨å­¦ä¹ å¹³å°çš„åç«¯æŠ€æœ¯æ¶æ„;</p>
<p>2ã€è´Ÿè´£æœºå™¨å­¦ä¹ å¹³å°çš„åç«¯ä»£ç å®ç°åŠå•å…ƒæµ‹è¯•ï¼›</p>
<p>3ã€è´Ÿè´£ç¼–å†™æŠ€æœ¯è®¾è®¡æ–‡æ¡£ã€APIæ–‡æ¡£ã€‚</p>
<p>ä»»èŒè¦æ±‚
1ã€è®¡ç®—æœºç›¸å…³ä¸“ä¸šï¼Œæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼›</p>
<p>2ã€ ç†Ÿç»ƒä½¿ç”¨Java æˆ– Scala å¼€å‘ï¼Œ5å¹´ä»¥ä¸Šçš„å¼€å‘ç»éªŒï¼›</p>
<p>3ã€ç†Ÿç»ƒä½¿ç”¨SprintBootï¼Œç†Ÿæ‚‰ç›¸å…³çš„ç”Ÿæ€å’Œä½¿ç”¨æ–¹æ³•ï¼›</p>
<p>4ã€æœ‰å¤§æ•°æ®å¤„ç†ç»éªŒä¼˜å…ˆã€‚</p>
<p>å…³äºæ²Ÿé€š
1ã€ä½¿ç”¨é£ä¹¦ä½œä¸ºæ²Ÿé€šå’Œæ–‡æ¡£å·¥å…·ï¼›</p>
<p>2ã€æ¯å¤©ä¸Šåˆ 9 ç‚¹ï¼ˆå†¬ä»¤æ—¶ï¼Œå¤ä»¤æ—¶æ˜¯ ä¸Šåˆ 8:30 ï¼‰ä¼šæœ‰ç®€å•çš„åŒæ­¥ï¼›</p>
<p>3ã€æ¯å¤©å†™æ—¥æŠ¥ï¼Œè¯´æ˜ä»»åŠ¡çš„è¿›åº¦ï¼Œä»¥åŠå‘ç°å“ªäº›é—®é¢˜å’Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼›</p>
<p>4ã€æ¯ä¸ª Sprint ä¼šèŠ±æ—¶é—´æ‹†è§£ Story å’Œåˆ†é…ä»»åŠ¡ï¼Œéœ€è¦å„è‡ªåˆ†æå‡ºå„ä¸ªéœ€æ±‚ç‚¹å’Œå…³é”®ç‚¹ï¼Œå‘ç°é£é™©å’Œä¸ç¡®å®šçš„åœ°æ–¹åŠæ—©ç¡®è®¤ã€‚</p>
<p>è–ªèµ„å¾…é‡
15K-30Kï¼Œå…¨èŒï¼ˆä¸æ¥å—å…¼èŒï¼‰ï¼Œæä¾›äº”é™©ä¸€é‡‘ã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mike Tang å¼ æ±‰ä¸œ è€æ²¹æ¡å„ä½å¤§ä½¬è¯·è¿›ï¼Œç´ æ•°å¤šçº¿ç¨‹é—®é¢˜</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=b10b7a68-e2bf-42b6-a583-ef99478e50d3">
<div class="article-summary-box-inner">
<span><p>å„ä½å¤§ä½¬ï¼š</p>
<p>æ˜¨å¤©è…¾è®¯è§†é¢‘èŠçš„æ¯”è¾ƒå¼€å¿ƒï¼Œç•™äº†ä¸€ä¸ªå°¾å·´ã€‚æˆ‘ç°åœ¨æŠŠæˆ‘çš„å›°æƒ‘æ”¾å‡ºæ¥ï¼Œå¤§å®¶å°½æƒ…æ‹ç –ã€‚</p>
<p>è¿™ä¸ªæ˜¯æˆ‘å†™çš„æ–‡ç« ï¼Œæ–‡ç« åˆ†äº†ä¸¤ç«  ï¼š</p>
<p><a href="https://github.com/sunnyrust/rustBible/blob/master/books/6.2.md" rel="noopener noreferrer">6.2 å¤šçº¿ç¨‹â€”â€”channel</a></p>
<p><a href="https://github.com/sunnyrust/rustBible/blob/master/books/6.3.md" rel="noopener noreferrer">6.3 å¤šçº¿ç¨‹â€”â€”future</a></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rustæ—¥æŠ¥ã€‘2021-08-31 Rust edition</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d6a7f8d7-4d99-4488-a933-1d70cefcd2cd">
<div class="article-summary-box-inner">
<span><h2><code>delicate</code> åˆ†å¸ƒå¼è°ƒåº¦ç³»ç»Ÿ</h2>
<p>å‘¨äºŒçš„å” å—‘å®¤ä¸»é¢˜
ä¸»æŒäººï¼šæ§Ÿæ©™ç‚®ç‚®
ç®€ä»‹ï¼šRustç¤¾åŒºä¹‹å‰æ²¡æœ‰æ´»è·ƒçš„åˆ†å¸ƒå¼è°ƒåº¦ç³»ç»Ÿé¡¹ç›®ï¼Œä¸ºäº†å¡«è¡¥è¿™ä¸ªç©ºç™½æˆ‘å¼€å§‹è°ƒç ”å®ç°é¡¹ç›®ï¼Œç›®å‰å·²ç»å¿«è¦å‘å¸ƒV1.1äº†ã€‚</p>
<p>åœ¨é¡¹ç›®è®¾è®¡ä¸åº•å±‚åº“çš„å®ç°ä»smolå¥—ä»¶ä¸­è·å¾—äº†å¾ˆå¤šçµæ„Ÿï¼Œä¹Ÿä¼šç®€å•è·Ÿå¤§å®¶ä»‹ç»ä¸‹ smol &amp; tokio ä¸€äº›å„è‡ªçš„è®¾è®¡å“²å­¦ï¼Œasync-process async-io async-task ä¸€äº›æ¼‚äº®çš„ä»£ç ç‰‡æ®µã€‚</p>
<ul>
<li>æ–‡æ¡£åœ°å€ï¼šhttps://delicate-rs.github.io/</li>
<li>æºç ï¼šhttps://github.com/BinChengZhao/delicate</li>
</ul>
<h2>datafusion-5.0.0 å‘å¸ƒ</h2>
<p>datafusion æ˜¯åŸºäº Apache Arrow åˆ—æ ¼å¼ã€ä½¿ç”¨ Rust å®ç°çš„å¯æ‰©å±•æŸ¥è¯¢æ‰§è¡Œæ¡†æ¶ï¼Œæ”¯æŒSQL å’Œ DataFrame APIï¼Œä¹Ÿå¯ä»¥é€šè¿‡ ballista crate(ä¹Ÿå‘å¸ƒäº† v0.5.0) æ”¯æŒåˆ†å¸ƒå¼æŸ¥è¯¢</p>
<ul>
<li>å‘å¸ƒé¡µï¼šhttps://arrow.apache.org/blog/2021/08/18/datafusion-5.0.0/</li>
<li>ä»“åº“ï¼š https://github.com/apache/arrow-datafusion</li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€åŸåˆ›ã€‘Rust tokio å¦‚ä½•ä»¥å¼‚æ­¥éé˜»å¡æ–¹å¼è¿è¡Œå¤§é‡ä»»åŠ¡</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=ba4f86c6-667d-4acb-89a1-e2fb0617f524">
<div class="article-summary-box-inner">
<span><p>tokio å®˜æ–¹ç»™äº†ä¸€ä¸ªå®Œæ•´çš„<a href="https://tokio.rs/tokio/topics/bridging#spawning-things-on-a-runtime" rel="noopener noreferrer">ä¾‹å­</a>ï¼šæ‰‹åŠ¨æ„å»º runtime ï¼Œåˆ©ç”¨ block_on æ¥è¿è¡Œå¤šä¸ªä»»åŠ¡ã€‚
tokio çš„ä»»åŠ¡æ˜¯ç”± <code>tokio::spawn</code> ä¹‹ç±»çš„å‡½æ•°äº§ç”Ÿçš„ <code>JoinHandle</code> ç±»å‹ï¼Œè€Œä¸”æ˜¯ä¸ª <code>Future</code> ã€‚</p>
<p>è€Œä¸‹é¢åˆ©ç”¨ <code>#[tokio::main]</code> å’Œ await ç¼–å†™äº†ç­‰ä»·çš„ç‰ˆæœ¬ï¼ˆä¸ºäº†ç›´è§‚å¯¹æ¯”ä»»åŠ¡å®Œæˆçš„å®é™…é¡ºåºå’Œæ€»è€—æ—¶ï¼Œæˆ‘å¯¹ sleep çš„æ—¶é—´åšäº†ä¸€äº›ç®€åŒ–ï¼‰ï¼š</p>
<pre><code>use std::time::Instant;
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() -&gt; std::io::Result&lt;()&gt; {
    let now = Instant::now();

    let mut handles = Vec::with_capacity(10);
    for i in 0..10 {
        handles.push(tokio::spawn(my_bg_task(i)));
    }

    // Do something time-consuming while the background tasks execute.
    std::thread::sleep(Duration::from_millis(120));
    println!("Finished time-consuming task.");

    // Wait for all of them to complete.
    for handle in handles {
        handle.await?;
    }

    println!("æ€»è€—æ—¶ï¼š{} ms", now.elapsed().as_millis());
    Ok(())
}

async fn my_bg_task(i: u64) {
    let millis = 100;
    println!("Task {} sleeping for {} ms.", i, millis);
    sleep(Duration::from_millis(millis)).await;
    println!("Task {} stopping.", i);
}
</code></pre>
<p>è¾“å‡ºç»“æœï¼š</p>
<pre><code>Task 0 sleeping for 100 ms.
Task 1 sleeping for 100 ms.
Task 2 sleeping for 100 ms.
Task 3 sleeping for 100 ms.
Task 4 sleeping for 100 ms.
Task 5 sleeping for 100 ms.
Task 6 sleeping for 100 ms.
Task 7 sleeping for 100 ms.
Task 8 sleeping for 100 ms.
Task 9 sleeping for 100 ms.
Task 9 stopping.
Task 0 stopping.
Task 1 stopping.
Task 2 stopping.
Task 3 stopping.
Task 4 stopping.
Task 5 stopping.
Task 6 stopping.
Task 7 stopping.
Task 8 stopping.
Finished time-consuming task.
æ€»è€—æ—¶ï¼š120 ms
</code></pre>
<p>å¦‚æœæŠŠä¸»çº¿ç¨‹çš„çš„ sleep æ—¶é—´æ”¹æˆ 100 msï¼š<code>std::thread::sleep(Duration::from_millis(100));</code>
åˆ™äº§ç”Ÿä¸‹é¢çš„ç»“æœï¼š</p>
<pre><code>Task 0 sleeping for 100 ms.
Task 1 sleeping for 100 ms.
Task 2 sleeping for 100 ms.
Task 3 sleeping for 100 ms.
Task 4 sleeping for 100 ms.
Task 5 sleeping for 100 ms.
Task 6 sleeping for 100 ms.
Task 7 sleeping for 100 ms.
Task 8 sleeping for 100 ms.
Task 9 sleeping for 100 ms.
Finished time-consuming task.
Task 3 stopping.
Task 0 stopping.
Task 1 stopping.
Task 2 stopping.
Task 9 stopping.
Task 4 stopping.
Task 5 stopping.
Task 6 stopping.
Task 7 stopping.
Task 8 stopping.
æ€»è€—æ—¶ï¼š103 ms
</code></pre>
<p>å¯ä»¥çœ‹åˆ°ï¼Œ<code>my_bg_task</code> å®é™…æ˜¯å¼‚æ­¥éé˜»å¡æ‰§è¡Œçš„ ğŸ‘ ï¼š</p>
<ul>
<li>å¼‚æ­¥ï¼šå› ä¸ºæ¯ä¸ªä»»åŠ¡ä¸å¿…ç­‰å¾…å…¶ç»“æœå°±å¯ä»¥å¼€å§‹ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼Œå³ï¼›</li>
</ul>
<pre><code>// å¼‚æ­¥
Task 0 sleeping for 100 ms.
Task 1 sleeping for 100 ms.
...

// åŒæ­¥
Task 0 sleeping for 100 ms.
Task 0 stopping.
Task 1 sleeping for 100 ms.
Task 1 stopping.
...
</code></pre>
<ul>
<li>éé˜»å¡ï¼šæ¯ä¸ªä»»åŠ¡ä¹‹é—´å¯ä»¥å¿«é€Ÿåˆ‡æ¢ï¼Œä¸å¿…ç­‰å¾…å…¶ä»–ä»»åŠ¡å®Œæˆæ‰åˆ‡æ¢ï¼Œè¿™ä¸ªä¾‹å­è¡¨ç°åœ¨ï¼š
<ul>
<li>ä»»åŠ¡ 0-9 ä»¥ä¹±åºæ–¹å¼ stop</li>
<li><code>Finished time-consuming task.</code> ä¸ <code>Task x stopping.</code> çš„æ‰“å°é¡ºåºåªä¸ä»»åŠ¡å„è‡ªçš„è¿è¡Œ (sleep) æ—¶é—´æœ‰å…³ï¼Œä¸æºä»£ç çš„å£°æ˜æ‰§è¡Œé¡ºåºæ— å…³ã€‚åªæœ‰ä»»åŠ¡ä¹‹é—´å¿«é€Ÿåˆ‡æ¢æ‰èƒ½åšåˆ°è¿™ä¸€ç‚¹ã€‚å›é¡¾å®˜ç½‘çš„ä¾‹å­ï¼š10 ä¸ªä»»åŠ¡çš„ sleep æ—¶é—´çº¿æ€§é€’å‡ ï¼ˆ<code>let millis = 1000 - 50 * i;</code>ï¼‰ï¼Œä» 6 ä¸ªä»»åŠ¡å¼€å§‹å°äºä¸»çº¿ç¨‹ sleep ä»»åŠ¡çš„æ—¶é—´ï¼ˆ750 msï¼‰ï¼Œè€Œç­‰å¾… 10 ä¸ªä»»åŠ¡æ‰§è¡Œçš„è¯­å¥ <code>for handle in handles { ... }</code> æ˜¾ç„¶ä½äº <code>std::thread::sleep</code> ä¹‹åï¼Œæ‰€ä»¥ä»»åŠ¡ä¹‹é—´éé˜»å¡æ‰§è¡Œçš„è¯ï¼Œæ‰“å°ç»“æœä¸º sleep æ—¶é—´è¶ŠçŸ­çš„ä»»åŠ¡å…ˆå®Œæˆï¼Œæ—¶é—´è¶Šé•¿çš„ä»»åŠ¡åå®Œæˆï¼Œæ€»è€—æ—¶ä¸ºä»»åŠ¡ä¸­çš„æœ€é•¿è€—æ—¶ï¼š</li>
</ul>
</li>
</ul>
<pre><code>Task 0 sleeping for 1000 ms.
Task 1 sleeping for 950 ms.
Task 2 sleeping for 900 ms.
Task 3 sleeping for 850 ms.
Task 4 sleeping for 800 ms.
Task 5 sleeping for 750 ms.
Task 6 sleeping for 700 ms.
Task 7 sleeping for 650 ms.
Task 8 sleeping for 600 ms.
Task 9 sleeping for 550 ms.
Task 9 stopping.
Task 8 stopping.
Task 7 stopping.
Task 6 stopping.
Finished time-consuming task.
Task 5 stopping.
Task 4 stopping.
Task 3 stopping.
Task 2 stopping.
Task 1 stopping.
Task 0 stopping.
æ€»è€—æ—¶ï¼š1001 ms // éå¸¸å®Œç¾
</code></pre>
<p>ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¯¹äº async block/fn ä½ è‡³å°‘æœ‰ä»¥ä¸‹ä¸€äº›åšæ³•ï¼š</p>
<ol>
<li>å¯¹ async block/fn è°ƒç”¨ <code>.await</code> æ¥ç­‰å¾…ç»“æœï¼›</li>
<li>å¯¹å¯åˆ—ä¸¾çš„å°‘æ•° Future è°ƒç”¨ <code>join!</code> æˆ–è€… <code>select!</code> æ¥åŒæ—¶ç­‰å¾…å¤šä¸ªç»“æœ æˆ–è€… ç­‰å¾…å¤šä¸ªåˆ†æ”¯çš„ç¬¬ä¸€ä¸ªç»“æœï¼›</li>
<li>å¯¹å¤§é‡ Future è°ƒç”¨ <a href="https://docs.rs/futures/0.3.17/futures/?search=join" rel="noopener noreferrer">join</a> æˆ–è€… <a href="https://docs.rs/futures/0.3.17/futures/?search=select" rel="noopener noreferrer">select</a> ä¸€ç±»æ”¯æŒä¼ å…¥ Vec / iter å‚æ•°ç±»å‹çš„å‡½æ•°ï¼Œæ¯”å¦‚è¿™ä¸ªä¾‹å­ä¸­çš„ <code>for handle in handles { ... }</code> éƒ¨åˆ†å°±å¯ä»¥æ”¹å†™æˆ <code>futures::future::join_all(handles).await;</code> ï¼›</li>
<li>æŠŠ async block/fn å˜æˆä»»åŠ¡ï¼Œç„¶åè°ƒç”¨ <code>Runtime::block_on</code> ï¼ˆç­‰ä»·åœ°ï¼Œå¯¹ä»»åŠ¡ awaitï¼‰æ¥æ‰§è¡Œè®¸å¤šä»»åŠ¡ã€‚</li>
</ol>
<p>å®¹æ˜“çŠ¯çš„é”™è¯¯æ˜¯ï¼Œå¸Œæœ›å¼‚æ­¥éé˜»å¡æ—¶ï¼Œå¯¹æ‰€æœ‰ async block/fn è¿›è¡Œäº† awaitï¼Œè€Œæ²¡æœ‰è¿›è¡Œä»»åŠ¡åŒ–å¤„ç†ï¼ˆå³ æŠŠ Future é€šè¿‡ spwan å‡½æ•°è½¬åŒ–æˆä»»åŠ¡ï¼‰ï¼š</p>
<pre><code>use std::time::Instant;
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() {
    let now = Instant::now();

    let mut handles = Vec::with_capacity(10);
    for i in 0..10 {
        handles.push(my_bg_task(i)); // æ²¡æœ‰æŠŠ Future å˜æˆä»»åŠ¡
    }

    std::thread::sleep(Duration::from_millis(120));
    println!("Finished time-consuming task.");

    for handle in handles {
        handle.await; // è€Œä¸”æ¯ä¸ª handle å¿…é¡»æ‰§è¡Œå®Œæ‰èƒ½æ‰§è¡Œä¸‹ä¸€ä¸ª handle
    }
    println!("æ€»è€—æ—¶ï¼š{} ms", now.elapsed().as_millis());
}

async fn my_bg_task(i: u64) {
    let millis = 100;
    println!("Task {} sleeping for {} ms.", i, millis);
    sleep(Duration::from_millis(millis)).await;
    println!("Task {} stopping.", i);
}
</code></pre>
<p>è¿è¡Œç»“æœï¼šåŒæ­¥é˜»å¡</p>
<pre><code>Finished time-consuming task.
Task 0 sleeping for 100 ms.
Task 0 stopping.
Task 1 sleeping for 100 ms.
Task 1 stopping.
Task 2 sleeping for 100 ms.
Task 2 stopping.
Task 3 sleeping for 100 ms.
Task 3 stopping.
Task 4 sleeping for 100 ms.
Task 4 stopping.
Task 5 sleeping for 100 ms.
Task 5 stopping.
Task 6 sleeping for 100 ms.
Task 6 stopping.
Task 7 sleeping for 100 ms.
Task 7 stopping.
Task 8 sleeping for 100 ms.
Task 8 stopping.
Task 9 sleeping for 100 ms.
Task 9 stopping.
æ€»è€—æ—¶ï¼š1130 ms
</code></pre>
<hr>
<p>æˆ–è€…åƒè¿™æ ·ï¼š</p>
<pre><code>use std::time::Instant;
use tokio::time::{sleep, Duration};

#[tokio::main]
async fn main() {
    let now = Instant::now();

    let mut handles = Vec::with_capacity(10);
    for i in 0..10 {
        handles.push(my_bg_task(i)); // æ²¡æœ‰æŠŠ Future å˜æˆä»»åŠ¡
    }

    std::thread::sleep(Duration::from_millis(120));
    println!("Finished time-consuming task.");

    futures::future::join_all(handles).await; // ä½†æ˜¯ join_all ä¼šç­‰å¾…æ‰€æœ‰ Future å¹¶å‘æ‰§è¡Œå®Œ
    println!("æ€»è€—æ—¶ï¼š{} ms", now.elapsed().as_millis());
}

async fn my_bg_task(i: u64) {
    let millis = 100;
    println!("Task {} sleeping for {} ms.", i, millis);
    sleep(Duration::from_millis(millis)).await;
    println!("Task {} stopping.", i);
}
</code></pre>
<p>è¿è¡Œç»“æœï¼šå¼‚æ­¥é˜»å¡</p>
<pre><code>Finished time-consuming task.
Task 0 sleeping for 100 ms.
Task 1 sleeping for 100 ms.
Task 2 sleeping for 100 ms.
Task 3 sleeping for 100 ms.
Task 4 sleeping for 100 ms.
Task 5 sleeping for 100 ms.
Task 6 sleeping for 100 ms.
Task 7 sleeping for 100 ms.
Task 8 sleeping for 100 ms.
Task 9 sleeping for 100 ms.
Task 0 stopping.
Task 1 stopping.
Task 2 stopping.
Task 3 stopping.
Task 4 stopping.
Task 5 stopping.
Task 6 stopping.
Task 7 stopping.
Task 8 stopping.
Task 9 stopping.
æ€»è€—æ—¶ï¼š221 ms
</code></pre>
<p>â€‹</p>
<p><em>P.S. å…³äºä»£ç ä¸­ <code>std::thread::sleep</code> å’Œ <code>tokio::time::sleep</code> çš„åŒºåˆ«ï¼Œå‚è€ƒè¿™ç¯‡æ–‡ç«  <a href="https://ryhl.io/blog/async-what-is-blocking/" rel="noopener noreferrer">Async: What is blocking? (by Alice Ryhl)</a> ã€‚</em></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">[é—®é¢˜å·²è§£å†³ï¼Œéå¸¸é€‚åˆæ–°æ‰‹çœ‹]æ–°æ‰‹æ±‚åŠ©æŒ‡é’ˆçš„ä½¿ç”¨é—®é¢˜ï¼Œå†™äº†ä¸ªå‰ç¼€æ ‘ç®—æ³•ä¸€ç›´ä¸èƒ½ç¼–è¯‘æœ‰è°å¸®å¿™çœ‹ä¸€ä¸‹ã€‚è¯·é«˜æ‰‹èµæ•™ã€‚</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=56e27ff0-860c-41c8-81d4-4756fffd5abc">
<div class="article-summary-box-inner">
<span><pre><code>fn insert(mut self, url_rule: &amp;str) {
        let mut current = self.clone(); // ä½œä¸ºæ¸¸æ ‡æŒ‡é’ˆä½¿ç”¨ï¼Œå¥½åƒæ²¡æœ‰è¾¾åˆ°æ¸¸æ ‡çš„æ•ˆæœ
        let list = parse_path(url_rule);
        for word in &amp;list {
            let mut is_exist = false;
            for n in current.child() {
                if n.name == word.to_string() {
                    is_exist = true;
                    current = n.clone();
                    break;
                }
            }

            if is_exist {
                continue;
            }
            let mut node = Tree::new(word);
            if is_variable(word) {
                node.is_variable = true
            };
            current.append_child(&amp;node);
            current = node.clone()
        }

        current.rule = url_rule.to_string();
        current.is_end = true;
    }

</code></pre>
<p>ä¸Šé¢çš„currentæ¸¸æ ‡æˆ‘åº”è¯¥ç”¨ä»€ä¹ˆç±»å‹æŒ‡é’ˆï¼Œå› ä¸ºæ˜¯è‡ªå®šä¹‰ç±»å‹Treeï¼Œç»‘å®šåªèƒ½ç”¨Cloneã€‚æ€ä¹ˆç”¨å¼•ç”¨æŒ‡é’ˆä¿®æ”¹å­èŠ‚ç‚¹çš„æ•°æ®
golangçš„å®ç°åœ¨https://github.com/obity/pretree/blob/main/pretree.goè¿™ä¸ªæ˜¯æ²¡æœ‰é—®é¢˜çš„ã€‚rustå®åœ¨æ˜¯ä¸ä¼šå†™ã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rustè¿›ç¨‹å…³é—­æ—¶çš„å›è°ƒé’©å­</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=77a67a30-75d9-49f5-bf60-7a650477f552">
<div class="article-summary-box-inner">
<span><p>è¯·æ•™ä¸€ä¸‹è¿›ç¨‹å…³é—­æ—¶çš„å›è°ƒå‡½æ•°æ€ä¹ˆå®ç°ï¼Ÿ</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">å¦‚ä½•æŠŠrustç¼–è¯‘çš„å¯æ‰§è¡Œæ–‡ä»¶ä¸­çš„ä¼—å¤šæ–‡ä»¶åå»é™¤</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=fdd79d8b-5b4f-471a-84ea-47f832d1f4c1">
<div class="article-summary-box-inner">
<span><p>ä¸‹é¢æ˜¯ä»å¯æ‰§è¡Œæ–‡ä»¶ä¸­æˆªå–çš„ä¸€æ®µã€‚é‡Œé¢åŒ…å«å¤§é‡çš„æ–‡ä»¶åå’Œå­—ç¬¦ä¸²ã€‚è¿™äº›æ–‡ä»¶åå ç”¨å¾ˆå¤šç©ºé—´ã€‚è¯·é—®å¦‚ä½•ä¸è®©è¿™ä¸ªæ–‡ä»¶åç¼–è¯‘åˆ°å¯æ‰§è¡Œæ–‡ä»¶ä¸­ï¼Ÿ</p>
<pre><code>library/core/src/fmt/mod.rs^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^D^D^D^D^D^@^@^@^@^@^@^@^@^@^@^@attempted to index str up to maximum usizelibrary/core/src/str/pattern.rs^@library/core/src/str/lossy.rsassertion failed: broken.is_empty()EmptyParseIntErrorInvalidDigitPosOverflowNegOverflowUtf8Errorvalid_up_toerror_len.debug_str_offsets.debug_str.debug_rnglists.debug_ranges.debug_line_str.debug_line.debug_info.debug_addr.debug_abbrevassertion failed: src.len() == dst.len()assertion failed: edge.height == self.height - 1assertion failed: idx &lt; CAPACITY/rustc/a15f484b918a4533ad633ea903ccce82910af342/library/alloc/src/collections/btree/node.rs/rustc/a15f484b918a4533ad633ea903ccce82910af342/library/alloc/src/collections/btree/map/entry.rs/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/gimli-0.23.0/src/read/abbrev.rsJimI am hello world
^@^@^@^@^@^@^@^@^@^@^@^@^@Invalid archive member headerInvalid archive terminatorInvalid archive member sizeArchive member size is too large^@^@^@Invalid archive extended name offsetInvalid archive extended name length::@*&amp;&lt;&gt;(,/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/rustc-demangle-0.1.18/src/legacy.rs?[]::{closure#}, _-false...!f64f32usizeu64u32u16u8isizei64i32i16i8()str/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/rustc-demangle-0.1.18/src/v0.rs0x' const ; &gt;  +  = Cunsafe " fn(punycode{.llvm.^@^@^@^@^@^@^@^@^@^@/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/rustc-demangle-0.1.18/src/lib.rsAccessErroruse of std::thread::current() is not possible after the thread's local data has been destroyedalready mutably borrowedcalled `Option::unwrap()` on a `None` valuelibrary/std/src/sys_common/thread_info.rsthread name may not contain interior null bytesfailed to generate unique thread ID: bitspace exhausted^@^@^@Â»Â±Â°&lt;^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@Â§Â«Âª2^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@called `Result::unwrap()` on an `Err` valuelibrary/std/src/sys/unix/mutex.rsOsmessageCustomerrorUnexpectedEofConnectionRefusedConnectionResetConnectionAbortedNotConnectedAddrInUseBrokenPipeAlreadyExistsWouldBlockInvalidInputInvalidDataWriteZeroInterruptedOtherstrerror_r failurelibrary/std/src/sys/unix/os.rslibrary/std/src/ffi/c_str.rslibrary/std/src/thread/mod.rscannot access a Thread Local Storage value during or after destruction^@^@^@^@^@^@rwlock maximum reader count exceededrwlock read lock would result in deadlockfatal runtime error:
thread panicked while panicking. aborting.
RUST_BACKTRACE&lt;unnamed&gt;formatter errorfailed to write whole bufferlibrary/std/src/io/mod.rsnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace
note: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rustæ—¥æŠ¥ã€‘2021-08-30 å¦‚ä½•æ¥çœ‹å¾… unwrap</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=59dad850-933e-49dd-9ab8-5370d5c77857">
<div class="article-summary-box-inner">
<span><h1>å¦‚ä½•æ¥çœ‹å¾… unwrap</h1>
<p><code>unwrap</code> æ–¹æ³•å¯èƒ½ä¼šè®©æ–°æ‰‹æ„Ÿåˆ°å›°æƒ‘ã€‚ä¸€äº›å»ºè®®:</p>
<ul>
<li>å¯ä»¥ä½¿ç”¨ Expect (&amp;str) è€Œä¸æ˜¯ unwrap() ä¸º panic æä¾›ä¸Šä¸‹æ–‡ã€‚</li>
<li>ä½¿ç”¨ unwrap å’Œ expect ç±»ä¼¼äºæ–­è¨€ã€‚å¦‚æœä»–ä»¬ panicï¼Œé‚£åªæœ‰åœ¨ä¸å¯æŒ½å›çš„æƒ…å†µä¸‹æ‰ä¼šå‘ç”Ÿã€‚</li>
<li>é¿å…åœ¨åº“ä»£ç ä¸­ä½¿ç”¨ã€‚</li>
</ul>
<p><a href="https://owengage.com/writing/2021-08-30-how-to-think-of-unwrap/" rel="noopener noreferrer">åŸæ–‡é“¾æ¥</a></p>
<h1>singleton-cell: ä¸€ä¸ªæ›´å¼ºå¤§çš„ ghost cell æ‰©å±•</h1>
<p>è¿™ä¸ªåº“æä¾›äº†ä¸€ä¸ªå®‰å…¨çš„ã€é›¶å¼€é”€çš„æ¥å£ï¼Œç”¨äºé€šè¿‡è®¿é—®å¦ä¸€ä¸ªå•ä¾‹ä»¤ç‰Œæ¥ä¿æŠ¤å¯¹å…±äº«æ•°æ®çš„è®¿é—®ã€‚å®ƒæ˜¯ GhostCellçš„æ‰©å±•ï¼Œé™¤äº†å“ç‰Œä»¤ç‰Œå¤–ï¼Œå®ƒè¿˜å…è®¸æ›´å¤šæ™®é€šçš„å•ä¾‹ï¼Œä½¿æ•°æ®æˆä¸ºâ€œé™æ€çš„â€</p>
<p>è¿™ä¸ªåº“æœ¬èº«ä¹Ÿæä¾›äº†ä¸¤ä¸ªå•ä¾‹å®ç°:</p>
<ul>
<li>é€šè¿‡with_tokenå°†é™å®šèŒƒå›´çš„æ ‡è®°ä»¤ç‰Œä½œä¸º GhostCell</li>
<li>é€šè¿‡new_singletonç®€å•åœ°åˆ›å»ºä¸€æ¬¡å•ä¾‹ç»“æ„</li>
</ul>
<p><a href="https://crates.io/crates/singleton-cell" rel="noopener noreferrer">crate åœ°å€</a></p>
<h1>Learning Rust: Interfacing with C</h1>
<p>é€šè¿‡æœ¬æ–‡å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Rust è°ƒç”¨ C æ–¹æ³•ä»¥åŠå¦‚ä½•åœ¨ C ä¸­è°ƒç”¨ Rust æ–¹æ³•.</p>
<p><a href="https://piware.de/post/2021-08-27-rust-and-c/" rel="noopener noreferrer">åŸæ–‡é“¾æ¥</a></p>
<h1>RefineDB: Rustç¼–å†™çš„å¼ºç±»å‹æ–‡æ¡£æ•°æ®åº“</h1>
<p>è¿è¡Œåœ¨ä»»ä½•äº‹åŠ¡æ€§ é”®å€¼å­˜å‚¨ä¸Šçš„ å¼ºç±»å‹ æ–‡æ¡£æ•°æ®åº“ã€‚</p>
<p>ç›®å‰æ”¯æŒçš„ backends æœ‰:</p>
<ul>
<li>FoundationDB</li>
<li>å•æœºéƒ¨ç½²çš„ SQLiteã€‚</li>
<li>ä¸€ä¸ªç®€å•çš„å†…å­˜é”®å€¼å­˜å‚¨ã€‚</li>
</ul>
<p><a href="https://github.com/losfair/RefineDB" rel="noopener noreferrer">github åœ°å€</a></p>
<p>--</p>
<p>From æ—¥æŠ¥å°ç»„ BobQinï¼ŒFBIå°ç™½</p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustccè®ºå›: æ”¯æŒrss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">è¿œç¨‹åŠå…¬ï¼Œä¸é™åœ°åŸŸï¼Œç¼´çº³ç¤¾ä¿å…¬ç§¯é‡‘ï¼Œå‘¨æœ«åŒä¼‘ï¼Œå‘Šåˆ« 996ï¼Œæ‹’ç» 007ï¼ŒNervina Labs æ¬¢è¿ä½ ï¼</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=a90b0635-b332-4e23-9a44-eb9282f519ef">
<div class="article-summary-box-inner">
<span><p>rustå¼€å‘å·¥ç¨‹å¸ˆ
å²—ä½èŒè´£ï¼š1ã€è´Ÿè´£æ™ºèƒ½åˆçº¦çš„å¼€å‘åŠè®¾è®¡ï¼›2ã€è´Ÿè´£åŒºå—é“¾ä¸šåŠ¡ç³»ç»Ÿåˆ†æä¸è®¾è®¡å·¥ä½œï¼›3ã€è´Ÿè´£æ™ºèƒ½åˆçº¦ä»£ç æµ‹è¯•ã€è¿è¡Œå’Œç»´æŠ¤ã€‚ä»»èŒè¦æ±‚ï¼š1ã€è®¡ç®—æœºç›¸å…³ä¸“ä¸šæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼Œ3å¹´ä»¥ä¸Šå·¥ä½œç»éªŒï¼›2ã€ç†Ÿç»ƒæŒæ¡ C/C++ã€Rust ç­‰ç³»ç»Ÿå¼€å‘è¯­è¨€è‡³å°‘ä¸€ç§ï¼Œè‡³å°‘æœ‰è¿‡ä¸¤å¹´ç›¸å…³å¼€å‘ç»éªŒï¼›3ã€å¯¹æ•°æ®ç»“æ„å’Œç®—æ³•ï¼Œå¯¹å¯†ç å­¦ï¼Œå®‰å…¨åè®®å’ŒåŠ å¯†ç®—æ³•æœ‰ç ”ç©¶è€…ä¼˜å…ˆï¼›4ã€ä¼˜ç§€çš„è‹±è¯­æ–‡æ¡£æ’°å†™ä¸é˜…è¯»èƒ½åŠ›è€…ä¼˜å…ˆï¼›5ã€äº†è§£åŒºå—é“¾ï¼Œæœ‰åˆçº¦å¼€å‘ç»éªŒæ›´ä½³ã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">æ„å»ºå®‰å…¨æ˜“ç”¨çš„é“¾è¡¨</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=273831e7-932d-476f-9d31-323151afb123">
<div class="article-summary-box-inner">
<span><p>å†™äº†ä¸€ä¸ªé“¾è¡¨çš„Crateï¼Œæ„¿æ™¯æ˜¯æ„å»ºå®‰å…¨ä¸”æ˜“ç”¨çš„é“¾è¡¨ã€‚</p>
<p>æ¬¢è¿å¤§å®¶æ¥æ‰¾èŒ¬ï¼ˆBugï¼‰æˆ–æéœ€æ±‚ :)</p>
<p>Crate IOé“¾æ¥ï¼š<a href="https://crates.io/crates/cyclic_list" rel="noopener noreferrer">https://crates.io/crates/cyclic_list</a>;</p>
<p>GitHubé“¾æ¥ï¼š<a href="https://github.com/whjpji/cyclic_list" rel="noopener noreferrer">https://github.com/whjpji/cyclic_list</a></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</h3>
<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´8æœˆ29æ—¥ 20:00-21:00</p>
<p><strong>è¯¾ç¨‹ä»‹ç»:</strong> è®²åˆ° Rust ä½¿ç”¨ Future å¼‚æ­¥ç¼–ç¨‹ï¼Œå°±ä¸å¾—ä¸è¯´ futures å’Œ tokio è¿™ä¸¤ä¸ª crateï¼Œå…¶å®æ ‡å‡†åº“ä¸­çš„ futureï¼Œä»¥åŠ async/await å°±æ˜¯ä» futures åº“ä¸­æ•´åˆè¿›æ ‡å‡†åº“çš„, Tokio æ‹¥æœ‰æå¿«çš„æ€§èƒ½ï¼Œæ˜¯å¤§éƒ¨åˆ†ç³»ç»Ÿå¼‚æ­¥å¤„ç†çš„é€‰æ‹©ï¼Œå…¶æ„å»ºäº future ä¹‹ä¸Šã€‚Future æ˜¯ Rust å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒåŸºç¡€ã€‚</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<p>1ã€ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥.</p>
<p>2ã€ç†è§£å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>
<p>3ã€Future ç¼–ç¨‹æ¨¡å‹è®²è§£.</p>
<p>4ã€å¸¦é¢†å¤§å®¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ future , å†æ¬¡å¸®å¿™å¤§å®¶ç†è§£</p>
<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>
<ol>
<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>
<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>
<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>
<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>
</ol>
<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>
<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ã€Rustæ—¥æŠ¥ã€‘2021-08-19 -- Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</h3>
<p>å·²ç»åœ¨ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªcrate ä¸Šæµ‹è¯•äº†ç‰ˆæœ¬è¿ç§»,å¹¶ä¸”å°†æµ‹è¯•æ‰€æœ‰å…¬å…±çš„ crateã€‚</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>å¼‚æ­¥å¼•æ“ C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D æ¸¸æˆå¼•æ“</h3>
<ul>
<li><strong>PCï¼ˆWindowsã€Linuxã€macOSï¼‰å’Œ Web (WebAssembly)</strong> æ”¯æŒã€‚</li>
<li><strong>å»¶è¿Ÿç€è‰²</strong></li>
<li><strong>å†…ç½®ä¿å­˜/åŠ è½½</strong></li>
<li><strong>ç‹¬ç«‹åœºæ™¯ç¼–è¾‘å™¨</strong></li>
<li><strong>é«˜çº§ç‰©ç†æ¨¡å‹</strong></li>
<li><strong>åˆ†å±‚æ¨¡å‹èµ„æº</strong></li>
<li><strong>å‡ ä½•å®ä¾‹åŒ–</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From æ—¥æŠ¥å°ç»„ å†°å±±ä¸Šçš„ mook &amp;&amp; æŒºè‚¥</p>
<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustccè®ºå›: æ”¯æŒrss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">å…¬å¼€è¯¾: é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Šé€šè¿‡Datafuseç†è§£å…¨é“¾è·¯è·Ÿè¸ªã€‹| Vol. 4</strong></p>
<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong> 2021å¹´8æœˆ22æ—¥ 20:30-21:30</p>
<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong> æ•°æ®åº“ç³»ç»Ÿä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚ï¼Œåºå¤§çš„ç³»ç»Ÿã€‚ç‰¹åˆ«æ˜¯åœ¨è°ƒè¯•å’Œè§‚å¯ŸSQLæ‰§è¡Œï¼Œå¤šçº¿ç¨‹ä»»åŠ¡åˆ‡æ¢ï¼Œå› ä¸ºæ²¡æœ‰å†…å­˜è°ƒç”¨æˆ–å †æ ˆè·Ÿè¸ªï¼Œè¿™ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¿½è¸ªçš„ç”±æ¥ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°å¤šè¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ªä¸ºæè¿°å’Œåˆ†æè·¨è¿›ç¨‹äº‹åŠ¡æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚Google Dapper(Dapper: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿé“¾è·¯è¿½è¸ªåŸºç¡€è®¾æ–½)è®ºæ–‡(å„tracerçš„åŸºç¡€)ä¸­æè¿°äº†åˆ†å¸ƒå¼è¿½è¸ªçš„ä¸€äº›ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­ç¨³æ€é—®é¢˜ã€åˆ†å¸ƒå¼åˆ†æã€èµ„æºå±æ€§å’Œå¾®æœåŠ¡çš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ã€‚</p>
<p>æœ¬æ¬¡å…¬å¼€è¯¾é€š Google çš„ OpenTraceing ä»‹ç»ï¼Œç»“åˆRustçš„ tokio-rs/tracing ä½¿ç”¨ï¼Œæœ€ç»ˆç»“åˆ Datafuse é¡¹ç›®ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹å¤§å‹åº”ç”¨çš„å…¨é“¾è·¯è·Ÿè¸ªåˆ†æè¿‡ç¨‹ã€‚</p>
<p>å…³äºDatafuse : https://github.com/datafuselabs/datafuse</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<ol>
<li>
<p>ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»ŸOpenTracingåŠåº”ç”¨åœºæ™¯</p>
</li>
<li>
<p>ä»‹ç» tokio-rs/tracing åŠåœ¨ç¨‹åºå¼€å‘ä¸­çš„ä½œç”¨</p>
</li>
<li>
<p>ä¸ºä»€ä¹ˆéœ€è¦tokio-rs/tracingåº“</p>
</li>
<li>
<p>æ¼”ç¤ºDatafuseé¡¹ç›®ä¸­tokio-rs/tracingçš„ä½¿ç”¨</p>
</li>
</ol>
<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>
<ol>
<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>
<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>
<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>
<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>
</ol>
<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>
<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">è®ºå›githubè´¦æˆ·æ— æ³•ç™»å½•è§£å†³ç¬”è®°</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>æœ‰åæ˜ è¿™ä¸¤å¤©githubè´¦æˆ·æ— æ³•ç™»å½•äº†ã€‚</p>
<p>æŠ¥è¿™ä¸ªé”™ï¼š</p>
<pre><code>get github user info err
</code></pre>
<p>æŸ¥äº†å‡ ä¸ªåœ°æ–¹ï¼š</p>
<ol>
<li>ä»£ç æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼šOk</li>
<li>httpsä»£ç†æ˜¯å¦æ­£å¸¸ï¼šOk</li>
<li>æ£€æŸ¥äº†githubè¿”å›æ—¥å¿—ï¼Œå‘ç°æ˜¯ï¼š</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>è¿›å…¥è¿™ä¸ªåœ°å€ä¸€çœ‹ï¼š<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>åŸæ¥2020å¹´2æœˆå°±å·²ç»è¯´äº†ï¼Œè¦æ”¹è¦æ”¹ã€‚ä¸è¿‡æˆ‘ç¡®å®æ²¡ç•™æ„åˆ°è¿™ä¸ªä¿¡æ¯ã€‚ï¼šï¼ˆ</p>
<p>æ„æ€å°±æ˜¯è¯´access_tokenä¸è¦æ”¾åœ¨queryå‚æ•°ä¸­ï¼Œè€Œæ˜¯è¦æ”¾åœ¨headeré‡Œé¢ã€‚ç…§å®ƒè¯´çš„ï¼Œæ”¹äº†åå°±å¥½äº†ã€‚</p>
<p>ç‰¹æ­¤è®°å½•ã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust çš„ Future ä¸ Javascript çš„ Promise åŠŸèƒ½å¯¹ç…§å‚è€ƒ</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>çš„<code>Future</code>ä¸<code>Javascript</code>çš„<code>Promise</code>åŠŸèƒ½å¯¹ç…§å‚è€ƒ</h1>
<p>å­¦ä¹ æ–°é²œæŠ€æœ¯æ—¶ï¼Œæˆ‘æ€»æ˜¯ä¼šä¹ æƒ¯æ€§å‘æ›¾ç»ç†Ÿæ‚‰çš„å†…å®¹ä¸Šé ï¼Œç”šè‡³å¥—ç”¨ç°æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¿™æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œå¯¹ç…§<code>Javascript - Promise/A+ API</code>æ¥è®°å¿†ä¸€éƒ¨åˆ†<code>Rust Future</code>å¸¸ç”¨<code>API</code>ã€‚</p>
<blockquote>
<p>æ³¨æ„ï¼šæ‰€æœ‰çš„<code>Rust - Future</code>æ“ä½œéƒ½æ˜¯ä»¥<code>.await</code>ç»“å°¾çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œä¸åŒäº<code>Javascript - Promise/A+</code>ï¼Œ<code>Rust - Future</code>æ˜¯æƒ°æ€§çš„ã€‚åªæœ‰è¢«<code>.await</code>æŒ‡ä»¤æ¿€æ´»åï¼Œåœ¨<code>Rust - Future</code>å†…å°è£…çš„æ“ä½œæ‰ä¼šè¢«çœŸæ­£åœ°æ‰§è¡Œã€‚</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">æè¿°</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* ä»€ä¹ˆéƒ½ä¸åš */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll ä¸èƒ½è¢«ç”¨æ¥æ„é€ åŒ…å«äº†å¼‚æ­¥æ“ä½œçš„ Future å®ä¾‹ï¼Œå› ä¸ºã€å›è°ƒé—­åŒ…ã€‘å†…çš„ã€å¯ä¿®æ”¹å¼•ç”¨ã€‘&amp;mut Context&lt;'_&gt; ä¸èƒ½è¢« ï¼ˆ1ï¼‰è·¨çº¿ç¨‹ä¼ é€’ ï¼ˆ2ï¼‰ä¼ é€’å‡ºé—­åŒ…ä½œç”¨åŸŸ2. task::spawn_blocking() ã€å›è°ƒé—­åŒ…ã€‘è¾“å…¥å‚æ•°å†…çš„ thread::sleep() ä¸æ˜¯é˜»å¡è¿è¡Œ task::spawn_blocking() çš„ä¸»çº¿ç¨‹ï¼Œè€Œæ˜¯é˜»å¡ä»ã€é˜»å¡ä»»åŠ¡çº¿ç¨‹æ± ã€‘ä¸­åˆ†é…æ¥è¿è¡Œé˜»å¡ä»»åŠ¡çš„ã€å·¥ä½œçº¿ç¨‹ã€‘ã€‚</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. æœ‰ä¸€ä¸ª promise/future å¤±è´¥å°±æ•´ä½“æ€§åœ°å¤±è´¥ã€‚2. try_join æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future çš„æˆåŠŸä¸å¤±è´¥ç»“æœéƒ½æ”¶é›†2. è¿”å›ç»“æœï¼š(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. ä»…åªæ”¶é›†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ promise/future2. try_race æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. æ”¶é›†ç¬¬ä¸€ä¸ªç»“æŸçš„ promise/futureï¼Œæ— è®ºå®ƒæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯å¤±è´¥æ”¶åœºã€‚2. è¿”å›ç»“æœï¼šT</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rustå…¬å¼€è¯¾ï¼šã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼š</strong>ã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹</p>
<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong> 2021å¹´8æœˆ15æ—¥ 20:30-21:30</p>
<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong></p>
<p>å¦‚æœæƒ³ç”¨ Rust å¼€å‘å¤§å‹ç›®ï¼Œæˆ–è€…å­¦ä¹ å¤§å‹é¡¹ç›®ä»£ç ï¼Œç‰¹åˆ«æ˜¯æ¡†æ¶çº§åˆ«çš„é¡¹ç›®ï¼Œé‚£ä¹ˆ Rust çš„å®æœºåˆ¶è‚¯å®šæ˜¯ä¸€ä¸ªå¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚ ä¾‹å¦‚ datafuse ä¸­çš„ä¸€äº›é…ç½®ç®¡ç†ï¼š
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>è¿™å°±æ˜¯é€šè¿‡å®å®ç°é…ç½®çš„ç»Ÿä¸€è¡Œä¸ºï¼Œä»£ç å‚è€ƒï¼š
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust è¯­è¨€å¼ºå¤§çš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¯ä»¥åˆ›å»ºå’Œåˆ©ç”¨å®ï¼Œä¸è¿‡åˆ›å»ºå®çœ‹èµ·æ¥æŒºå¤æ‚ï¼Œå¸¸å¸¸ä»¤åˆšæ¥è§¦ Rust çš„å¼€å‘è€…ç”Ÿç•æƒ§ã€‚ åœ¨æœ¬æ¬¡å…¬å¼€è¯¾ä¸­å¸®åŠ©ä½ ç†è§£ Rust Macro çš„åŸºæœ¬åŸç†ï¼Œå­¦ä¹ å¦‚ä½•åˆ›è‡ªå·²çš„ Rust å®ï¼Œä»¥åŠæŸ¥çœ‹æºç å­¦ä¹ å®çš„å®ç°ã€‚</p>
<h3>è¯¾ç¨‹å¤§çº²</h3>
<ul>
<li>ä»€ä¹ˆæ˜¯ Rust å®</li>
<li>ä»€ä¹ˆæ˜¯å®è¿è¡ŒåŸç†</li>
<li>å¦‚ä½•åˆ›å»º Rust å®è¿‡ç¨‹</li>
<li>é˜…è¯» datafuse é¡¹ç›®æºç ï¼Œ å­¦ä¹ é¡¹ç›®ä¸­å®çš„å®ç°</li>
</ul>
<p><strong>è®²å¸ˆä»‹ç»</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>
<p>Ruståœ¨çº¿ç¼–è¾‘å™¨: https://play.rust-lang.org/</p>
<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹: https://kaisery.github.io/trpl-zh-cn/</p>
<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rustå…¬å¼€è¯¾ï¼šç†è§£Rustçš„æ‰€æœ‰æƒ| Vol 2</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c107b830-9fe1-43dd-94a3-9efcd5544205">
<div class="article-summary-box-inner">
<span><p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼šã€Šç†è§£Rustæ‰€æœ‰æƒã€‹</strong></p>
<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š2021å¹´8æœˆ8æ—¥ 20:30-21:30</strong></p>
<p><strong>å˜‰å®¾è®²å¸ˆï¼š è‹æ—</strong></p>
<p><strong>å˜‰å®¾ä»‹ç»ï¼š</strong></p>
<p>Rustä¸­æ–‡ç¤¾åŒºæˆå‘˜ï¼Œå¤šç‚¹DmallæŠ€æœ¯Leaderï¼Œå‰æŠ˜800äº’è”ç½‘ç ”å‘å›¢é˜Ÿè´Ÿè´£äººã€10ä½™å¹´ä¸€çº¿ç ”å‘ç»éªŒã€‚å…·æœ‰å¤šå¹´çš„è½¯ä»¶å¼€å‘ç»éªŒ, ç†Ÿç»ƒRubyã€Javaã€Rustç­‰å¼€å‘è¯­è¨€, åŒæ—¶ä¹Ÿå‚ä¸è¿‡Rustä¸­æ–‡ç¤¾åŒºæ—¥æŠ¥ç»´æŠ¤å·¥ä½œã€‚</p>
<p><strong>è¯¾ç¨‹ä»‹ç»</strong></p>
<p>æœ¬æ¬¡è¯¾ç¨‹é€šè¿‡10ä¸ªå·¦å³çš„å°ä¾‹å­ï¼Œå¸¦å¤§å®¶ç†è§£ä¸€ä¸‹Rustçš„æ‰€æœ‰æƒï¼ŒRustå¼•ç”¨å’Œå€Ÿç”¨ï¼ŒRustå˜é‡å…‹éš†å’Œå¤åˆ¶çš„ç†å¿µã€‚</p>
<p><strong>å‚åŠ è¯¾ç¨‹</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/Rust-pbc-1.jpg" alt></p>
<p><strong>è¯¾ç¨‹è§„åˆ’</strong></p>
<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloudé¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">æ•°æ®è¡¨ Timestamp æ—¥æœŸ Serialize</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2ff8a69e-59bb-4502-87c0-c3416ffae8a0">
<div class="article-summary-box-inner">
<span><p>ä¸»è¦å‚è€ƒï¼š<a href="https://github.com/rustcc/forustm" rel="noopener noreferrer">Rustccç½‘ç«™æºç åº“</a></p>
<p>åœ¨å¤„ç†æ•°æ®è¡¨ä¸­æ—¥æœŸç›¸å…³æ•°æ®æ—¶ï¼ŒSeralizeåºåˆ—åŒ–ç›¸å…³æ“ä½œä¼šæŠ¥é”™ï¼Œæç¤º DateTime å­—æ®µä¸è¯†åˆ«ï¼Œ
æŸ¥äº† rustcc æºç æ‰å‘ç°ä¾èµ–ä¸­éœ€è¦å¼€å¯ç›¸åº”çš„featureã€‚ç‰¹æ­¤è®°å½•ã€‚</p>
<h2>1.ä¾èµ–çš„åº“ï¼š</h2>
<pre><code>[dependencies]
# æ—¥æœŸæ—¶é—´å¤„ç† éœ€è¦å¼€å¯ serde ç‰¹å¾ æ”¯æŒåºåˆ—åŒ–
chrono = { version = "0.4.19", features = ["serde"] }

# æ•°æ®åº“ORM
diesel = { version = "1.4.4", features = ["postgres", "chrono", "uuid", "r2d2"] }
dotenv = "0.15.0"
serde = { version = "1.0.127", features = ["derive"] }
serde_json = "1.0.66"
uuid = { version = "0.8.2", features = ["serde", "v4"] }
</code></pre>
<h2>2.åˆ›å»ºæ•°æ®è¡¨</h2>
<pre><code>CREATE TABLE characters (
    id SERIAL PRIMARY KEY,
    name VARCHAR(128) UNIQUE NOT NULL,
    age INTEGER NOT NULL DEFAULT 0,
    friends VARCHAR NOT NULL DEFAULT '',
    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
)
</code></pre>
<h2>3.æ•°æ®è¡¨å¯¹åº”çš„ model</h2>
<pre><code>use chrono::{NaiveDateTime};
use serde::{Deserialize, Serialize};

#[derive(Queryable, Serialize, Deserialize, Debug)]
pub struct Characters {
    pub id: i32,
    pub name: String,
    pub age: i32,
    pub friends: String,
    // è¿™é‡Œçš„ NaiveDateTime æ—¥æœŸæ ¼å¼åºåˆ—åŒ–éœ€è¦å¼€å¯ç›¸å…³ features
    pub created_at: NaiveDateTime,
}
</code></pre>
<h2>4.è·å–æ•°æ®</h2>
<pre><code>use db::schema::characters;
use db::{get_connection};
use db::models::{Characters, NewCharacter};
use db::schema::characters::dsl::*;
use diesel::QueryDsl;
use diesel::prelude::*;

fn main() {
    let conn = get_connection();

    // æŸ¥è¯¢å¹´é¾„å¤§äº30çš„10æ¡æ•°æ®
    let arr: Vec&lt;Characters&gt; = characters.filter(characters::age.gt(30))
        .limit(10)
        .load::&lt;Characters&gt;(&amp;conn)
        .expect("Loading Error");

    let date_arr = arr.iter()
        .map(|item| {
	    // æ•°æ®æ ¼å¼åŒ–
            let t = item.created_at.format("%Y-%m-%d %H:%M:%S").to_string();
            println!("{} {}", item.name, t);
            t
        })
        .collect::&lt;Vec&lt;String&gt;&gt;();
}
</code></pre>
<p>è¾“å‡ºç»“æœç±»ä¼¼ï¼š</p>
<pre><code>Box 2021-08-05 09:39:34
Bobe 2021-08-05 09:39:34
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cargo workspace config</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=c3dcce30-1fc0-4819-8992-142365c7e21c">
<div class="article-summary-box-inner">
<span><p><a href="https://kaisery.github.io/trpl-zh-cn/ch14-03-cargo-workspaces.html" rel="noopener noreferrer">Workspace æ–‡æ¡£é“¾æ¥</a></p>
<h2>ç›®å½•ç»“æ„</h2>
<pre><code>workspace-test/
    Cargo.toml
    db/
        src/
            bin/
                init.rs
        Cargo.tml
</code></pre>
<h2>workspace</h2>
<p>workspace-test/Cargo.toml</p>
<pre><code>[workspace]
members = ["db"]
default-member = "db"
</code></pre>
<h2>å­é¡¹ç›®</h2>
<p>workspace-test/db/Cargo.toml</p>
<pre><code>[package]
name = "db"
version = "0.1.0"
edition = "2018"

[dependencies]

# å¯é€‰çš„å¯æ‰§è¡Œæ–‡ä»¶é…ç½®
# [[bin]]
# name = "init"
# path = "src/bin/init.rs"
</code></pre>
<h2>æ“ä½œ</h2>
<pre><code># è¿è¡Œ init
cargo run --bin init
# -p æŒ‡å®šé¡¹ç›®
cargo run -p db --bin init
</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=120035c3-944d-4a79-9b3a-8390697a6e13">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰</h1>
<p>ä¸åŒäº<code>javascript</code>çš„<code>new Promise((resolve, reject) =&gt; {...})</code>æ„é€ å³è¿è¡Œï¼Œ<code>Rust</code>ä¸­çš„<code>Future</code>æ˜¯Â·æƒ°æ€§Â·çŠ¶æ€æœºã€‚è¿™ä½“ç°ä¸ºï¼š</p>
<ol>
<li>ã€è°ƒç”¨å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€æ‰§è¡Œå¼‚æ­¥å—ã€‘ä»…åªæ„é€ ä¸€ä¸ª<code>Future trait object</code>ã€‚</li>
<li>å› ä¸º<code>Future</code>æ˜¯æƒ°æ€§çŠ¶æ€æœºï¼Œæ‰€ä»¥å®ƒä¸ä¼šè‡ªåŠ¨æ‰§è¡Œã€å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€å¼‚æ­¥å—ã€‘å†…çš„ä»»ä½•ä¸€è¡Œä»£ç  --- æ­¤ç‚¹ä¸<code>javascript</code>çš„Â·æ´»æ€§Â·çŠ¶æ€æœºå®Œå…¨ä¸åŒã€‚ç›¸åï¼Œéœ€è¦äººå·¥æ¿€æ´»è§¦å‘ã€‚</li>
<li>äººå·¥å¯åŠ¨<code>Future</code>è¿è¡Œï¼Œåˆåˆ†ä¸ºä¸¤ä¸ªåœºæ™¯çš„ä¸¤ç§æƒ…å†µï¼š
<ol>
<li>
<p>å·²ç»åœ¨<code>async fn</code>å†…ï¼Œ<code>Future.await</code>æ¿€æ´»ã€‚ä½†ï¼ŒåŒæ—¶<strong>é˜»å¡</strong>å½“å‰å¼‚æ­¥ç¨‹åºæ‰§è¡Œæµã€‚</p>
</li>
<li>
<p>åœ¨<code>async fn</code>å¤–ï¼Œéœ€è¦å€ŸåŠ©ç”±ã€è¿è¡Œæ—¶ã€‘æä¾›çš„ã€æ‰§è¡Œå™¨ã€‘ã€‚å°±<code>async-std</code>åº“è€Œè¨€ï¼Œæœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š</p>
<ol>
<li><code>task::block_on(Future)</code> æ‰§è¡Œ<code>Future</code>ä¸”é˜»å¡å½“å‰çº¿ç¨‹ç›´åˆ°<code>Future</code>è¢«å®Œæˆã€‚</li>
<li><code>task::spawn(Future)</code>ä»…æ‰§è¡Œ<code>Future</code>å’Œä¸é˜»å¡å½“å‰çº¿ç¨‹ã€‚</li>
</ol>
<p>æ— è®ºé€‰æ‹©ä¸Šé¢å“ªç§æ–¹å¼ï¼Œè‹¥åœ¨<code>Future</code>æ‰§è¡ŒæœŸé—´å‡ºç°äº†<code>panic</code>ï¼Œå…¶éƒ½ä¼šç»ˆæ­¢ï¼ˆ<code>abort</code>ï¼‰æ­£åœ¨å…±äº«åŒä¸€ä¸ªæ‰§è¡Œçº¿ç¨‹ï¼ˆ<code>thread</code>ï¼‰çš„æ‰€æœ‰<code>task</code>ï¼ˆÂ·æ— æ ˆÂ·åç¨‹ï¼‰çš„è¿è¡Œã€‚</p>
</li>
</ol>
</li>
</ol>
<p>é¢˜å¤–è¯ï¼Œ</p>
<ol>
<li>ç»¿è‰²çº¿ç¨‹æ˜¯Â·æœ‰æ ˆÂ·åç¨‹ï¼›å¼‚æ­¥å‡½æ•°ä¸å¼‚æ­¥å—æ˜¯Â·æ— æ ˆÂ·åç¨‹ã€‚</li>
<li>åœ¨<code>async-std</code>åº“çš„è¯æ±‡è¡¨å†…ï¼Œåç¨‹è¢«ç§°ä½œ<code>task</code>è€Œä¸æ˜¯æƒ¯ä¾‹çš„<code>coroutine</code>ã€‚</li>
<li><code>task::spawn(Future)</code>ä¹Ÿèƒ½è¢«ä½¿ç”¨äº<code>async fn</code>æˆ–<code>async {...}</code>å†…ã€‚å®ƒè¢«ç”¨æ¥ä»£æ›¿<code>.await</code>æŒ‡ä»¤ï¼Œä»¥<strong>éé˜»å¡</strong><code>async fn</code>æˆ–<code>async {...}</code>çš„æ–¹å¼ï¼Œæ¿€æ´»ä¸æ‰§è¡Œä¸€ä¸ª<code>Future</code>å®ä¾‹ã€‚</li>
</ol>
<h2>ä¾‹ç¨‹</h2>
<pre><code>async fn accept_loop(addr: impl ToSocketAddrs) -&gt; Result&lt;()&gt; {
    // 1. TcpListener::bind(addr) è¿”å› Future
    // 2. .await äº Future å–å¾— Result&lt;T, E&gt;
    // 3. Result&lt;T, E&gt;? å†æ‹¿å¾— Ok&lt;T&gt; ä¸­çš„ T
    let listener = TcpListener::bind(addr).await?; // å¼‚æ­¥å‡½æ•°å†…çš„äººå·¥å¯åŠ¨ Future
    let mut incoming = listener.incoming();
    // å› ä¸ºæ²¡æœ‰ä»è¯­è¨€å±‚é¢æ”¯æŒ async for loopï¼Œæ‰€ä»¥ while loop + Iterator&lt;Item = T&gt; æ¥æ¨¡æ‹Ÿä¹‹ã€‚
    while let Some(stream) = incoming.next().await {
        // TODO
    }
    Ok(())
}
fn main() {
    let fut = accept_loop("127.0.0.1:8080");
    task::block_on(fut); // å¼‚æ­¥å‡½æ•°å¤–çš„äººå·¥å¯åŠ¨ Future
}
</code></pre>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-01T01:30:00Z">09-01</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Want To Reduce Labeling Cost? GPT-3 Can Help. (arXiv:2108.13487v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13487">
<div class="article-summary-box-inner">
<span><p>Data annotation is a time-consuming and labor-intensive process for many NLP
tasks. Although there exist various methods to produce pseudo data labels, they
are often task-specific and require a decent amount of labeled data to start
with. Recently, the immense language model GPT-3 with 175 billion parameters
has achieved tremendous improvement across many few-shot learning tasks. In
this paper, we explore ways to leverage GPT-3 as a low-cost data labeler to
train other models. We find that, to make the downstream model achieve the same
performance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use
labels from GPT-3 than using labels from humans. Furthermore, we propose a
novel framework of combining pseudo labels from GPT-3 with human labels, which
leads to even better performance with limited labeling budget. These results
present a cost-effective data labeling methodology that is generalizable to
many practical applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Semi-Supervised Exaggeration Detection of Health Science Press Releases. (arXiv:2108.13493v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13493">
<div class="article-summary-box-inner">
<span><p>Public trust in science depends on honest and factual communication of
scientific papers. However, recent studies have demonstrated a tendency of news
media to misrepresent scientific papers by exaggerating their findings. Given
this, we present a formalization of and study into the problem of exaggeration
detection in science communication. While there are an abundance of scientific
papers and popular media articles written about them, very rarely do the
articles include a direct link to the original paper, making data collection
challenging. We address this by curating a set of labeled press
release/abstract pairs from existing expert annotated studies on exaggeration
in press releases of scientific papers suitable for benchmarking the
performance of machine learning models on the task. Using limited data from
this and previous studies on exaggeration detection in science, we introduce
MT-PET, a multi-task version of Pattern Exploiting Training (PET), which
leverages knowledge from complementary cloze-style QA tasks to improve few-shot
learning. We demonstrate that MT-PET outperforms PET and supervised learning
both when data is limited, as well as when there is an abundance of data for
the main task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ConVIScope: Visual Analytics for Exploring Patient Conversations. (arXiv:2108.13514v1 [cs.HC])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13514">
<div class="article-summary-box-inner">
<span><p>The proliferation of text messaging for mobile health is generating a large
amount of patient-doctor conversations that can be extremely valuable to health
care professionals. We present ConVIScope, a visual text analytic system that
tightly integrates interactive visualization with natural language processing
in analyzing patient-doctor conversations. ConVIScope was developed in
collaboration with healthcare professionals following a user-centered iterative
design. Case studies with six domain experts suggest the potential utility of
ConVIScope and reveal lessons for further developments.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution. (arXiv:2108.13530v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13530">
<div class="article-summary-box-inner">
<span><p>We consider the task of document-level entity linking (EL), where it is
important to make consistent decisions for entity mentions over the full
document jointly. We aim to leverage explicit "connections" among mentions
within the document itself: we propose to join the EL task with that of
coreference resolution (coref). This is complementary to related works that
exploit either (i) implicit document information (e.g., latent relations among
entity mentions, or general language models) or (ii) connections between the
candidate links (e.g, as inferred from the external knowledge base).
Specifically, we cluster mentions that are linked via coreference, and enforce
a single EL for all of the clustered mentions together. The latter constraint
has the added benefit of increased coverage by joining EL candidate lists for
the thus clustered mentions. We formulate the coref+EL problem as a structured
prediction task over directed trees and use a globally normalized model to
solve it. Experimental results on two datasets show a boost of up to +5%
F1-score on both coref and EL tasks, compared to their standalone counterparts.
For a subset of hard cases, with individual mentions lacking the correct EL in
their candidate entity list, we obtain a +50% increase in accuracy.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Linguistic Characterization of Divisive Topics Online: Case Studies on Contentiousness in Abortion, Climate Change, and Gun Control. (arXiv:2108.13556v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13556">
<div class="article-summary-box-inner">
<span><p>As public discourse continues to move and grow online, conversations about
divisive topics on social media platforms have also increased. These divisive
topics prompt both contentious and non-contentious conversations. Although what
distinguishes these conversations, often framed as what makes these
conversations contentious, is known in broad strokes, much less is known about
the linguistic signature of these conversations. Prior work has shown that
contentious content and structure can be a predictor for this task, however,
most of them have been focused on conversation in general, very specific
events, or complex structural analysis. Additionally, many models used in prior
work have lacked interpret-ability, a key factor in online moderation. Our work
fills these gaps by focusing on conversations from highly divisive topics
(abortion, climate change, and gun control), operationalizing a set of novel
linguistic and conversational characteristics and user factors, and
incorporating them to build interpretable models. We demonstrate that such
characteristics can largely improve the performance of prediction on this task,
and also enable nuanced interpretability. Our case studies on these three
contentious topics suggest that certain generic linguistic characteristics are
highly correlated with contentiousness in conversations while others
demonstrate significant contextual influences on specific divisive topics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP. (arXiv:2108.13587v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13587">
<div class="article-summary-box-inner">
<span><p>Transformers are the dominant architecture in NLP, but their training and
fine-tuning is still very challenging. In this paper, we present the design and
implementation of a visual analytic framework for assisting researchers in such
process, by providing them with valuable insights about the model's intrinsic
properties and behaviours. Our framework offers an intuitive overview that
allows the user to explore different facets of the model (e.g., hidden states,
attention) through interactive visualization, and allows a suite of built-in
algorithms that compute the importance of model components and different parts
of the input sequence. Case studies and feedback from a user focus group
indicate that the framework is useful, and suggest several improvements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How Does Adversarial Fine-Tuning Benefit BERT?. (arXiv:2108.13602v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13602">
<div class="article-summary-box-inner">
<span><p>Adversarial training (AT) is one of the most reliable methods for defending
against adversarial attacks in machine learning. Variants of this method have
been used as regularization mechanisms to achieve SOTA results on NLP
benchmarks, and they have been found to be useful for transfer learning and
continual learning. We search for the reasons for the effectiveness of AT by
contrasting vanilla and adversarially fine-tuned BERT models. We identify
partial preservation of BERT's syntactic abilities during fine-tuning as the
key to the success of AT. We observe that adversarially fine-tuned models
remain more faithful to BERT's language modeling behavior and are more
sensitive to the word order. As concrete examples of syntactic abilities, an
adversarially fine-tuned model could have an advantage of up to 38% on anaphora
agreement and up to 11% on dependency parsing. Our analysis demonstrates that
vanilla fine-tuning oversimplifies the sentence representation by focusing
heavily on one or a few label-indicative words. AT, however, moderates the
effect of these influential words and encourages representational diversity.
This allows for a more hierarchical representation of a sentence and leads to
the mitigation of BERT's loss of syntactic abilities.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-Lingual Text Classification of Transliterated Hindi and Malayalam. (arXiv:2108.13620v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13620">
<div class="article-summary-box-inner">
<span><p>Transliteration is very common on social media, but transliterated text is
not adequately handled by modern neural models for various NLP tasks. In this
work, we combine data augmentation approaches with a Teacher-Student training
scheme to address this issue in a cross-lingual transfer setting for
fine-tuning state-of-the-art pre-trained multilingual language models such as
mBERT and XLM-R. We evaluate our method on transliterated Hindi and Malayalam,
also introducing new datasets for benchmarking on real-world scenarios: one on
sentiment classification in transliterated Malayalam, and another on crisis
tweet classification in transliterated Hindi and Malayalam (related to the 2013
North India and 2018 Kerala floods). Our method yielded an average improvement
of +5.6% on mBERT and +4.7% on XLM-R in F1 scores over their strong baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dynamic Sliding Window for Meeting Summarization. (arXiv:2108.13629v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13629">
<div class="article-summary-box-inner">
<span><p>Recently abstractive spoken language summarization raises emerging research
interest, and neural sequence-to-sequence approaches have brought significant
performance improvement. However, summarizing long meeting transcripts remains
challenging. Due to the large length of source contents and targeted summaries,
neural models are prone to be distracted on the context, and produce summaries
with degraded quality. Moreover, pre-trained language models with input length
limitations cannot be readily applied to long sequences. In this work, we first
analyze the linguistic characteristics of meeting transcripts on a
representative corpus, and find that the sentences comprising the summary
correlate with the meeting agenda. Based on this observation, we propose a
dynamic sliding window strategy for meeting summarization. Experimental results
show that performance benefit from the proposed method, and outputs obtain
higher factual consistency than the base model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory. (arXiv:2108.13630v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13630">
<div class="article-summary-box-inner">
<span><p>Lip reading, aiming to recognize spoken sentences according to the given
video of lip movements without relying on the audio stream, has attracted great
interest due to its application in many scenarios. Although prior works that
explore lip reading have obtained salient achievements, they are all trained in
a non-simultaneous manner where the predictions are generated requiring access
to the full video. To breakthrough this constraint, we study the task of
simultaneous lip reading and devise SimulLR, a simultaneous lip Reading
transducer with attention-guided adaptive memory from three aspects: (1) To
address the challenge of monotonic alignments while considering the syntactic
structure of the generated sentences under simultaneous setting, we build a
transducer-based model and design several effective training strategies
including CTC pre-training, model warm-up and curriculum learning to promote
the training of the lip reading transducer. (2) To learn better spatio-temporal
representations for simultaneous encoder, we construct a truncated 3D
convolution and time-restricted self-attention layer to perform the
frame-to-frame interaction within a video segment containing fixed number of
frames. (3) The history information is always limited due to the storage in
real-time scenarios, especially for massive video data. Therefore, we devise a
novel attention-guided adaptive memory to organize semantic information of
history segments and enhance the visual representations with acceptable
computation-aware latency. The experiments show that the SimulLR achieves the
translation speedup 9.10$\times$ compared with the state-of-the-art
non-simultaneous methods, and also obtains competitive results, which indicates
the effectiveness of our proposed methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Explaining Classes through Word Attribution. (arXiv:2108.13653v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13653">
<div class="article-summary-box-inner">
<span><p>In recent years, several methods have been proposed for explaining individual
predictions of deep learning models, yet there has been little study of how to
aggregate these predictions to explain how such models view classes as a whole
in text classification tasks. In this work, we propose a method for explaining
classes using deep learning models and the Integrated Gradients feature
attribution technique by aggregating explanations of individual examples in
text classification to general descriptions of the classes. We demonstrate the
approach on Web register (genre) classification using the XML-R model and the
Corpus of Online Registers of English (CORE), finding that the method
identifies plausible and discriminative keywords characterizing all but the
smallest class.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Discretized Integrated Gradients for Explaining Language Models. (arXiv:2108.13654v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13654">
<div class="article-summary-box-inner">
<span><p>As a prominent attribution-based explanation algorithm, Integrated Gradients
(IG) is widely adopted due to its desirable explanation axioms and the ease of
gradient computation. It measures feature importance by averaging the model's
output gradient interpolated along a straight-line path in the input data
space. However, such straight-line interpolated points are not representative
of text data due to the inherent discreteness of the word embedding space. This
questions the faithfulness of the gradients computed at the interpolated points
and consequently, the quality of the generated explanations. Here we propose
Discretized Integrated Gradients (DIG), which allows effective attribution
along non-linear interpolation paths. We develop two interpolation strategies
for the discrete word embedding space that generates interpolation points that
lie close to actual words in the embedding space, yielding more faithful
gradient computation. We demonstrate the effectiveness of DIG over IG through
experimental and human evaluations on multiple sentiment classification
datasets. We provide the source code of DIG to encourage reproducible research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MELM: Data Augmentation with Masked Entity Language Modeling for Cross-lingual NER. (arXiv:2108.13655v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13655">
<div class="article-summary-box-inner">
<span><p>Data augmentation for cross-lingual NER requires fine-grained control over
token labels of the augmented text. Existing augmentation approach based on
masked language modeling may replace a labeled entity with words of a different
class, which makes the augmented sentence incompatible with the original label
sequence, and thus hurts the performance.We propose a data augmentation
framework with Masked-Entity Language Modeling (MELM) which effectively ensures
the replacing entities fit the original labels. Specifically, MELM linearizes
NER labels into sentence context, and thus the fine-tuned MELM is able to
predict masked tokens by explicitly conditioning on their labels. Our MELM is
agnostic to the source of data to be augmented. Specifically, when MELM is
applied to augment training data of the source language, it achieves up to 3.5%
F1 score improvement for cross-lingual NER. When unlabeled target data is
available and MELM can be further applied to augment pseudo-labeled target
data, the performance gain reaches 5.7%. Moreover, MELM consistently
outperforms multiple baseline methods for data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13658">
<div class="article-summary-box-inner">
<span><p>The understanding of time expressions includes two sub-tasks: recognition and
normalization. In recent years, significant progress has been made in the
recognition of time expressions while research on normalization has lagged
behind. Existing SOTA normalization methods highly rely on rules or grammars
designed by experts, which limits their performance on emerging corpora, such
as social media texts. In this paper, we model time expression normalization as
a sequence of operations to construct the normalized temporal value, and we
present a novel method called ARTime, which can automatically generate
normalization rules from training data without expert interventions.
Specifically, ARTime automatically captures possible operation sequences from
annotated data and generates normalization rules on time expressions with
common surface forms. The experimental results show that ARTime can
significantly surpass SOTA methods on the Tweets benchmark, and achieves
competitive results with existing expert-engineered rule methods on the
TempEval-3 benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Gray Cycles of Maximum Length Related to k-Character Substitutions. (arXiv:2108.13659v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13659">
<div class="article-summary-box-inner">
<span><p>Given a word binary relation $\tau$ we define a $\tau$-Gray cycle over a
finite language $X$ to be a permutation $\left(w_{[i]}\right)_{0\le i\le
|X|-1}$ of $X$ such that each word $w_i$ is an image of the previous word
$w_{i-1}$ by $\tau$. In that framework, we introduce the complexity measure
$\lambda(n)$, equal to the largest cardinality of a language $X$ having words
of length at most $n$, and such that a $\tau$-Gray cycle over $X$ exists. The
present paper is concerned with the relation $\tau=\sigma_k$, the so-called
$k$-character substitution, where $(u,v)$ belongs to $\sigma_k$ if, and only
if, the Hamming distance of $u$ and $v$ is $k$. We compute the bound
$\lambda(n)$ for all cases of the alphabet cardinality and the argument $n$.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13679">
<div class="article-summary-box-inner">
<span><p>In this paper, we propose to formulate the task-oriented dialogue system as
the purely natural language generation task, so as to fully leverage the
large-scale pre-trained models like GPT-2 and simplify complicated
delexicalization prepossessing. However, directly applying this method heavily
suffers from the dialogue entity inconsistency caused by the removal of
delexicalized tokens, as well as the catastrophic forgetting problem of the
pre-trained model during fine-tuning, leading to unsatisfactory performance. To
alleviate these problems, we design a novel GPT-Adapter-CopyNet network, which
incorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve
better performance on transfer learning and dialogue entity generation.
Experimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ
dataset demonstrate that our proposed approach significantly outperforms
baseline models with a remarkable performance on automatic and human
evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization. (arXiv:2108.13684v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13684">
<div class="article-summary-box-inner">
<span><p>Despite recent progress in abstractive summarization, systems still suffer
from faithfulness errors. While prior work has proposed models that improve
faithfulness, it is unclear whether the improvement comes from an increased
level of extractiveness of the model outputs as one naive way to improve
faithfulness is to make summarization models more extractive. In this work, we
present a framework for evaluating the effective faithfulness of summarization
systems, by generating a faithfulnessabstractiveness trade-off curve that
serves as a control at different operating points on the abstractiveness
spectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as
well as a recently proposed method for improving faithfulness, are both worse
than the control at the same level of abstractiveness. Finally, we learn a
selector to identify the most faithful and abstractive summary for a given
document, and show that this system can attain higher faithfulness scores in
human evaluations while being more abstractive than the baseline system on two
datasets. Moreover, we show that our system is able to achieve a better
faithfulness-abstractiveness trade-off than the control at the same level of
abstractiveness.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge-Grounded Dialogue with Reward-Driven Knowledge Selection. (arXiv:2108.13686v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13686">
<div class="article-summary-box-inner">
<span><p>Knowledge-grounded dialogue is a task of generating a fluent and informative
response based on both conversation context and a collection of external
knowledge, in which knowledge selection plays an important role and attracts
more and more research interest. However, most existing models either select
only one knowledge or use all knowledge for responses generation. The former
may lose valuable information in discarded knowledge, while the latter may
bring a lot of noise. At the same time, many approaches need to train the
knowledge selector with knowledge labels that indicate ground-truth knowledge,
but these labels are difficult to obtain and require a large number of manual
annotations. Motivated by these issues, we propose Knoformer, a dialogue
response generation model based on reinforcement learning, which can
automatically select one or more related knowledge from the knowledge pool and
does not need knowledge labels during training. Knoformer is evaluated on two
knowledge-guided conversation datasets, and achieves state-of-the-art
performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TNNT: The Named Entity Recognition Toolkit. (arXiv:2108.13700v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13700">
<div class="article-summary-box-inner">
<span><p>Extraction of categorised named entities from text is a complex task given
the availability of a variety of Named Entity Recognition (NER) models and the
unstructured information encoded in different source document formats.
Processing the documents to extract text, identifying suitable NER models for a
task, and obtaining statistical information is important in data analysis to
make informed decisions. This paper presents TNNT, a toolkit that automates the
extraction of categorised named entities from unstructured information encoded
in source documents, using diverse state-of-the-art Natural Language Processing
(NLP) tools and NER models. TNNT integrates 21 different NER models as part of
a Knowledge Graph Construction Pipeline (KGCP) that takes a document set as
input and processes it based on the defined settings, applying the selected
blocks of NER models to output the results. The toolkit generates all results
with an integrated summary of the extracted entities, enabling enhanced data
analysis to support the KGCP, and also, to aid further NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Plan-then-Generate: Controlled Data-to-Text Generation via Planning. (arXiv:2108.13740v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13740">
<div class="article-summary-box-inner">
<span><p>Recent developments in neural networks have led to the advance in
data-to-text generation. However, the lack of ability of neural models to
control the structure of generated output can be limiting in certain real-world
applications. In this study, we propose a novel Plan-then-Generate (PlanGen)
framework to improve the controllability of neural data-to-text models.
Extensive experiments and analyses are conducted on two benchmark datasets,
ToTTo and WebNLG. The results show that our model is able to control both the
intra-sentence and inter-sentence structure of the generated output.
Furthermore, empirical comparisons against previous state-of-the-art methods
show that our model improves the generation quality as well as the output
diversity as judged by human and automatic evaluations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13741">
<div class="article-summary-box-inner">
<span><p>Recent researches have demonstrated that BERT shows potential in a wide range
of natural language processing tasks. It is adopted as an encoder for many
state-of-the-art automatic summarizing systems, which achieve excellent
performance. However, so far, there is not much work done for Vietnamese. In
this paper, we showcase how BERT can be implemented for extractive text
summarization in Vietnamese. We introduce a novel comparison between different
multilingual and monolingual BERT models. The experiment results indicate that
monolingual models produce promising results compared to other multilingual
models and previous text summarizing models for Vietnamese.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Search Engine for Discovery of Biomedical Challenges and Directions. (arXiv:2108.13751v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13751">
<div class="article-summary-box-inner">
<span><p>The ability to keep track of scientific challenges, advances and emerging
directions is a fundamental part of research. However, researchers face a flood
of papers that hinders discovery of important knowledge. In biomedicine, this
directly impacts human lives. To address this problem, we present a novel task
of extraction and search of scientific challenges and directions, to facilitate
rapid knowledge discovery. We construct and release an expert-annotated corpus
of texts sampled from full-length papers, labeled with novel semantic
categories that generalize across many types of challenges and directions. We
focus on a large corpus of interdisciplinary work relating to the COVID-19
pandemic, ranging from biomedicine to areas such as AI and economics. We apply
a model trained on our data to identify challenges and directions across the
corpus and build a dedicated search engine for this information. In studies
with researchers, including those working directly on COVID-19, we outperform a
popular scientific search engine in assisting knowledge discovery. Finally, we
show that models trained on our resource generalize to the wider biomedical
domain, highlighting its broad utility. We make our data, model and search
engine publicly available. https://challenges.apps.allenai.org
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience. (arXiv:2108.13759v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13759">
<div class="article-summary-box-inner">
<span><p>Pretrained transformer-based models such as BERT have demonstrated
state-of-the-art predictive performance when adapted into a range of natural
language processing tasks. An open problem is how to improve the faithfulness
of explanations (rationales) for the predictions of these models. In this
paper, we hypothesize that salient information extracted a priori from the
training data can complement the task-specific information learned by the model
during fine-tuning on a downstream task. In this way, we aim to help BERT not
to forget assigning importance to informative input tokens when making
predictions by proposing SaLoss; an auxiliary loss function for guiding the
multi-head attention mechanism during training to be close to salient
information extracted a priori using TextRank. Experiments for explanation
faithfulness across five datasets, show that models trained with SaLoss
consistently provide more faithful explanations across four different feature
attribution methods compared to vanilla BERT. Using the rationales extracted
from vanilla BERT and SaLoss models to train inherently faithful classifiers,
we further show that the latter result in higher predictive performance in
downstream tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The five Is: Key principles for interpretable and safe conversational AI. (arXiv:2108.13766v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13766">
<div class="article-summary-box-inner">
<span><p>In this position paper, we present five key principles, namely
interpretability, inherent capability to explain, independent data, interactive
learning, and inquisitiveness, for the development of conversational AI that,
unlike the currently popular black box approaches, is transparent and
accountable. At present, there is a growing concern with the use of black box
statistical language models: While displaying impressive average performance,
such systems are also prone to occasional spectacular failures, for which there
is no clear remedy. In an effort to initiate a discussion on possible
alternatives, we outline and exemplify how our five principles enable the
development of conversational AI systems that are transparent and thus safer
for use. We also present some of the challenges inherent in the implementation
of those principles.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Network psychometrics and cognitive network science open new ways for detecting, understanding and tackling the complexity of math anxiety: A review. (arXiv:2108.13800v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13800">
<div class="article-summary-box-inner">
<span><p>Math anxiety is a clinical pathology impairing cognitive processing in
math-related contexts. Originally thought to affect only inexperienced,
low-achieving students, recent investigations show how math anxiety is vastly
diffused even among high-performing learners. This review of data-informed
studies outlines math anxiety as a complex system that: (i) cripples
well-being, self-confidence and information processing on both conscious and
subconscious levels, (ii) can be transmitted by social interactions, like a
pathogen, and worsened by distorted perceptions, (iii) affects roughly 20% of
students in 63 out of 64 worldwide educational systems but correlates weakly
with academic performance, and (iv) poses a concrete threat to students'
well-being, computational literacy and career prospects in science. These
patterns underline the crucial need to go beyond performance for estimating
math anxiety. Recent advances with network psychometrics and cognitive network
science provide ideal frameworks for detecting, interpreting and intervening
upon such clinical condition. Merging education research, psychology and data
science, the approaches reviewed here reconstruct psychological constructs as
complex systems, represented either as multivariate correlation models (e.g.
graph exploratory analysis) or as cognitive networks of semantic/emotional
associations (e.g. free association networks or forma mentis networks). Not
only can these interconnected networks detect otherwise hidden levels of math
anxiety but - more crucially - they can unveil the specific layout of
interacting factors, e.g. key sources and targets, behind math anxiety in a
given cohort. As discussed here, these network approaches open concrete ways
for unveiling students' perceptions, emotions and mental well-being, and can
enable future powerful data-informed interventions untangling math anxiety.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TREND: Trigger-Enhanced Relation-Extraction Network for Dialogues. (arXiv:2108.13811v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13811">
<div class="article-summary-box-inner">
<span><p>The goal of dialogue relation extraction (DRE) is to identify the relation
between two entities in a given dialogue. During conversations, speakers may
expose their relations to certain entities by some clues, such evidences called
"triggers". However, none of the existing work on DRE tried to detect triggers
and leverage the information for enhancing the performance. This paper proposes
TREND, a multi-tasking BERT-based model which learns to identify triggers for
improving relation extraction. The experimental results show that the proposed
method achieves the state-of-the-art on the benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Open-Domain Question Answering. (arXiv:2108.13817v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13817">
<div class="article-summary-box-inner">
<span><p>Open-domain Question Answering (ODQA) has achieved significant results in
terms of supervised learning manner. However, data annotation cannot also be
irresistible for its huge demand in an open domain. Though unsupervised QA or
unsupervised Machine Reading Comprehension (MRC) has been tried more or less,
unsupervised ODQA has not been touched according to our best knowledge. This
paper thus pioneers the work of unsupervised ODQA by formally introducing the
task and proposing a series of key data construction methods. Our exploration
in this work inspiringly shows unsupervised ODQA can reach up to 86%
performance of supervised ones.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Domain Adaptation for Question Answering using Limited Text Corpora. (arXiv:2108.13854v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13854">
<div class="article-summary-box-inner">
<span><p>Question generation has recently shown impressive results in customizing
question answering (QA) systems to new domains. These approaches circumvent the
need for manually annotated training data from the new domain and, instead,
generate synthetic question-answer pairs that are used for training. However,
existing methods for question generation rely on large amounts of synthetically
generated datasets and costly computational resources, which render these
techniques widely inaccessible when the text corpora is of limited size. This
is problematic as many niche domains rely on small text corpora, which
naturally restricts the amount of synthetic data that can be generated. In this
paper, we propose a novel framework for domain adaptation called contrastive
domain adaptation for QA (CAQA). Specifically, CAQA combines techniques from
question generation and domain-invariant learning to answer out-of-domain
questions in settings with limited text corpora. Here, we train a QA system on
both source data and generated data from the target domain with a contrastive
adaptation loss that is incorporated in the training objective. By combining
techniques from question generation and domain-invariant learning, our model
achieved considerable improvements compared to state-of-the-art baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs. (arXiv:2108.13873v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13873">
<div class="article-summary-box-inner">
<span><p>Machine-learning-as-a-service (MLaaS) has attracted millions of users to
their outperforming sophisticated models. Although published as black-box APIs,
the valuable models behind these services are still vulnerable to imitation
attacks. Recently, a series of works have demonstrated that attackers manage to
steal or extract the victim models. Nonetheless, none of the previous stolen
models can outperform the original black-box APIs. In this work, we take the
first step of showing that attackers could potentially surpass victims via
unsupervised domain adaptation and multi-victim ensemble. Extensive experiments
on benchmark datasets and real-world APIs validate that the imitators can
succeed in outperforming the original black-box models. We consider this as a
milestone in the research of imitation attack, especially on NLP APIs, as the
superior performance could influence the defense or even publishing strategy of
API providers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13875">
<div class="article-summary-box-inner">
<span><p>Scenario-based question answering (SQA) requires retrieving and reading
paragraphs from a large corpus to answer a question which is contextualized by
a long scenario description. Since a scenario contains both keyphrases for
retrieval and much noise, retrieval for SQA is extremely difficult. Moreover,
it can hardly be supervised due to the lack of relevance labels of paragraphs
for SQA. To meet the challenge, in this paper we propose a joint
retriever-reader model called JEEVES where the retriever is implicitly
supervised only using QA labels via a novel word weighting mechanism. JEEVES
significantly outperforms a variety of strong baselines on multiple-choice
questions in three SQA datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning. (arXiv:2108.13888v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13888">
<div class="article-summary-box-inner">
<span><p>\textbf{P}re-\textbf{T}rained \textbf{M}odel\textbf{s} have been widely
applied and recently proved vulnerable under backdoor attacks: the released
pre-trained weights can be maliciously poisoned with certain triggers. When the
triggers are activated, even the fine-tuned model will predict pre-defined
labels, causing a security threat. These backdoors generated by the poisoning
methods can be erased by changing hyper-parameters during fine-tuning or
detected by finding the triggers. In this paper, we propose a stronger
weight-poisoning attack method that introduces a layerwise weight poisoning
strategy to plant deeper backdoors; we also introduce a combinatorial trigger
that cannot be easily detected. The experiments on text classification tasks
show that previous defense methods cannot resist our weight-poisoning method,
which indicates that our method can be widely applied and may provide hints for
future model robustness studies.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Like Article, Like Audience: Enforcing Multimodal Correlations for Disinformation Detection. (arXiv:2108.13892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13892">
<div class="article-summary-box-inner">
<span><p>User-generated content (e.g., tweets and profile descriptions) and shared
content between users (e.g., news articles) reflect a user's online identity.
This paper investigates whether correlations between user-generated and
user-shared content can be leveraged for detecting disinformation in online
news articles. We develop a multimodal learning algorithm for disinformation
detection. The latent representations of news articles and user-generated
content allow that during training the model is guided by the profile of users
who prefer content similar to the news article that is evaluated, and this
effect is reinforced if that content is shared among different users. By only
leveraging user information during model optimization, the model does not rely
on user profiling when predicting an article's veracity. The algorithm is
successfully applied to three widely used neural classifiers, and results are
obtained on different datasets. Visualization techniques show that the proposed
model learns feature representations of unseen news articles that better
discriminate between fake and real news texts.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13897">
<div class="article-summary-box-inner">
<span><p>The MS MARCO ranking dataset has been widely used for training deep learning
models for IR tasks, achieving considerable effectiveness on diverse zero-shot
scenarios. However, this type of resource is scarce in other languages than
English. In this work we present mMARCO, a multilingual version of the MS MARCO
passage ranking dataset comprising 8 languages that was created using machine
translation. We evaluated mMARCO by fine-tuning mono and multilingual
re-ranking models on it. Experimental results demonstrate that multilingual
models fine-tuned on our translated dataset achieve superior effectiveness than
models fine-tuned on the original English version alone. Also, our distilled
multilingual re-ranker is competitive with non-distilled models while having
5.4 times fewer parameters. The translated datasets as well as fine-tuned
models are available at https://github.com/unicamp-dl/mMARCO.git.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The emojification of sentiment on social media: Collection and analysis of a longitudinal Twitter sentiment dataset. (arXiv:2108.13898v1 [cs.SI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13898">
<div class="article-summary-box-inner">
<span><p>Social media, as a means for computer-mediated communication, has been
extensively used to study the sentiment expressed by users around events or
topics. There is however a gap in the longitudinal study of how sentiment
evolved in social media over the years. To fill this gap, we develop TM-Senti,
a new large-scale, distantly supervised Twitter sentiment dataset with over 184
million tweets and covering a time period of over seven years. We describe and
assess our methodology to put together a large-scale, emoticon- and emoji-based
labelled sentiment analysis dataset, along with an analysis of the resulting
dataset. Our analysis highlights interesting temporal changes, among others in
the increasing use of emojis over emoticons. We publicly release the dataset
for further research in tasks including sentiment analysis and text
classification of tweets. The dataset can be fully rehydrated including tweet
metadata and without missing tweets thanks to the archive of tweets publicly
available on the Internet Archive, which the dataset is based on.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Robust Retrieval Augmented Generation for Zero-shot Slot Filling. (arXiv:2108.13934v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13934">
<div class="article-summary-box-inner">
<span><p>Automatically inducing high quality knowledge graphs from a given collection
of documents still remains a challenging problem in AI. One way to make headway
for this problem is through advancements in a related task known as slot
filling. In this task, given an entity query in form of [Entity, Slot, ?], a
system is asked to fill the slot by generating or extracting the missing value
exploiting evidence extracted from relevant passage(s) in the given document
collection. The recent works in the field try to solve this task in an
end-to-end fashion using retrieval-based language models. In this paper, we
present a novel approach to zero-shot slot filling that extends dense passage
retrieval with hard negatives and robust training procedures for retrieval
augmented generation models. Our model reports large improvements on both T-REx
and zsRE slot filling datasets, improving both passage retrieval and slot value
generation, and ranking at the top-1 position in the KILT leaderboard.
Moreover, we demonstrate the robustness of our system showing its domain
adaptation capability on a new variant of the TACRED dataset for slot filling,
through a combination of zero/few-shot learning. We release the source code and
pre-trained models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools. (arXiv:2108.13961v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13961">
<div class="article-summary-box-inner">
<span><p>In the language domain, as in other domains, neural explainability takes an
ever more important role, with feature attribution methods on the forefront.
Many such methods require considerable computational resources and expert
knowledge about implementation details and parameter choices. To facilitate
research, we present Thermostat which consists of a large collection of model
explanations and accompanying analysis tools. Thermostat allows easy access to
over 200k explanations for the decisions of prominent state-of-the-art models
spanning across different NLP tasks, generated with multiple explainers. The
dataset took over 10k GPU hours (&gt; one year) to compile; compute time that the
community now saves. The accompanying software tools allow to analyse
explanations instance-wise but also accumulatively on corpus level. Users can
investigate and compare models, datasets and explainers without the need to
orchestrate implementation details. Thermostat is fully open source,
democratizes explainability research in the language domain, circumvents
redundant computations and increases comparability and replicability.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13990">
<div class="article-summary-box-inner">
<span><p>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,
but how to properly use them for dialogue state tracking has not been
systematically investigated. In this paper, we study this problem from the
perspectives of pre-training objectives as well as the formats of context
representations. We demonstrate that the choice of pre-training objective makes
a significant difference to the state tracking quality. In particular, we find
that masked span prediction is more effective than auto-regressive language
modeling. We also explore using Pegasus, a span prediction-based pre-training
objective for text summarization, for the state tracking model. We found that
pre-training for the seemingly distant summarization task works surprisingly
well for dialogue state tracking. In addition, we found that while recurrent
state context representation works also reasonably well, the model may have a
hard time recovering from earlier mistakes. We conducted experiments on the
MultiWOZ 2.1-2.4 data sets with consistent observations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Generative Approach for Mitigating Structural Biases in Natural Language Inference. (arXiv:2108.14006v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.14006">
<div class="article-summary-box-inner">
<span><p>Many natural language inference (NLI) datasets contain biases that allow
models to perform well by only using a biased subset of the input, without
considering the remainder features. For instance, models are able to make a
classification decision by only using the hypothesis, without learning the true
relationship between it and the premise. These structural biases lead
discriminative models to learn unintended superficial features and to
generalize poorly out of the training distribution. In this work, we
reformulate the NLI task as a generative task, where a model is conditioned on
the biased subset of the input and the label and generates the remaining subset
of the input. We show that by imposing a uniform prior, we obtain a provably
unbiased model. Through synthetic experiments, we find that this approach is
highly robust to large amounts of bias. We then demonstrate empirically on two
types of natural bias that this approach leads to fully unbiased models in
practice. However, we find that generative models are difficult to train and
they generally perform worse than discriminative baselines. We highlight the
difficulty of the generative modeling task in the context of NLI as a cause for
this worse performance. Finally, by fine-tuning the generative model with a
discriminative objective, we reduce the performance gap between the generative
model and the discriminative baseline, while allowing for a small amount of
bias.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HUMBO: Bridging Response Generation and Facial Expression Synthesis. (arXiv:1905.11240v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1905.11240">
<div class="article-summary-box-inner">
<span><p>Spoken dialogue systems that assist users to solve complex tasks such as
movie ticket booking have become an emerging research topic in artificial
intelligence and natural language processing areas. With a well-designed
dialogue system as an intelligent personal assistant, people can accomplish
certain tasks more easily via natural language interactions. Today there are
several virtual intelligent assistants in the market; however, most systems
only focus on textual or vocal interaction. In this paper, we present HUMBO, a
system aiming at generating dialogue responses and simultaneously synthesize
corresponding visual expressions on faces for better multimodal interaction.
HUMBO can (1) let users determine the appearances of virtual assistants by a
single image, and (2) generate coherent emotional utterances and facial
expressions on the user-provided image. This is not only a brand new research
direction but more importantly, an ultimate step toward more human-like virtual
assistants.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer. (arXiv:2004.06698v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06698">
<div class="article-summary-box-inner">
<span><p>Visual dialog is a task of answering a sequence of questions grounded in an
image using the previous dialog history as context. In this paper, we study how
to address two fundamental challenges for this task: (1) reasoning over
underlying semantic structures among dialog rounds and (2) identifying several
appropriate answers to the given question. To address these challenges, we
propose a Sparse Graph Learning (SGL) method to formulate visual dialog as a
graph structure learning task. SGL infers inherently sparse dialog structures
by incorporating binary and score edges and leveraging a new structural loss
function. Next, we introduce a Knowledge Transfer (KT) method that extracts the
answer predictions from the teacher model and uses them as pseudo labels. We
propose KT to remedy the shortcomings of single ground-truth labels, which
severely limit the ability of a model to obtain multiple reasonable answers. As
a result, our proposed model significantly improves reasoning capability
compared to baseline methods and outperforms the state-of-the-art approaches on
the VisDial v1.0 dataset. The source code is available at
https://github.com/gicheonkang/SGLKT-VisDial.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Neural CRF Model for Sentence Alignment in Text Simplification. (arXiv:2005.02324v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2005.02324">
<div class="article-summary-box-inner">
<span><p>The success of a text simplification system heavily depends on the quality
and quantity of complex-simple sentence pairs in the training corpus, which are
extracted by aligning sentences between parallel articles. To evaluate and
improve sentence alignment quality, we create two manually annotated
sentence-aligned datasets from two commonly used text simplification corpora,
Newsela and Wikipedia. We propose a novel neural CRF alignment model which not
only leverages the sequential nature of sentences in parallel documents but
also utilizes a neural sentence pair model to capture semantic similarity.
Experiments demonstrate that our proposed approach outperforms all the previous
work on monolingual sentence alignment task by more than 5 points in F1. We
apply our CRF aligner to construct two new text simplification datasets,
Newsela-Auto and Wiki-Auto, which are much larger and of better quality
compared to the existing datasets. A Transformer-based seq2seq model trained on
our datasets establishes a new state-of-the-art for text simplification in both
automatic and human evaluation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Dialogue Response Selection with Hierarchical Curriculum Learning. (arXiv:2012.14756v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2012.14756">
<div class="article-summary-box-inner">
<span><p>We study the learning of a matching model for dialogue response selection.
Motivated by the recent finding that models trained with random negative
samples are not ideal in real-world scenarios, we propose a hierarchical
curriculum learning framework that trains the matching model in an
"easy-to-difficult" scheme. Our learning framework consists of two
complementary curricula: (1) corpus-level curriculum (CC); and (2)
instance-level curriculum (IC). In CC, the model gradually increases its
ability in finding the matching clues between the dialogue context and a
response candidate. As for IC, it progressively strengthens the model's ability
in identifying the mismatching information between the dialogue context and a
response candidate. Empirical studies on three benchmark datasets with three
state-of-the-art matching models demonstrate that the proposed learning
framework significantly improves the model performance across various
evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Progressive Transformer-Based Generation of Radiology Reports. (arXiv:2102.09777v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.09777">
<div class="article-summary-box-inner">
<span><p>Inspired by Curriculum Learning, we propose a consecutive (i.e.,
image-to-text-to-text) generation framework where we divide the problem of
radiology report generation into two steps. Contrary to generating the full
radiology report from the image at once, the model generates global concepts
from the image in the first step and then reforms them into finer and coherent
texts using a transformer architecture. We follow the transformer-based
sequence-to-sequence paradigm at each step. We improve upon the
state-of-the-art on two benchmark datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06979">
<div class="article-summary-box-inner">
<span><p>Learning sentence embeddings often requires a large amount of labeled data.
However, for most tasks and domains, labeled data is seldom available and
creating it is expensive. In this work, we present a new state-of-the-art
unsupervised method based on pre-trained Transformers and Sequential Denoising
Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.
It can achieve up to 93.1% of the performance of in-domain supervised
approaches. Further, we show that TSDAE is a strong domain adaptation and
pre-training method for sentence embeddings, significantly outperforming other
approaches like Masked Language Model.
</p>
<p>A crucial shortcoming of previous studies is the narrow evaluation: Most work
mainly evaluates on the single task of Semantic Textual Similarity (STS), which
does not require any domain knowledge. It is unclear if these proposed methods
generalize to other domains and tasks. We fill this gap and evaluate TSDAE and
other recent approaches on four different datasets from heterogeneous domains.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08315">
<div class="article-summary-box-inner">
<span><p>Large language models have shown promising results in zero-shot settings
(Brown et al.,2020; Radford et al., 2019). For example, they can perform
multiple choice tasks simply by conditioning on a question and selecting the
answer with the highest probability.
</p>
<p>However, ranking by string probability can be problematic due to surface form
competition-wherein different surface forms compete for probability mass, even
if they represent the same underlying concept, e.g. "computer" and "PC." Since
probability mass is finite, this lowers the probability of the correct answer,
due to competition from other strings that are valid answers (but not one of
the multiple choice options).
</p>
<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative
scoring function that directly compensates for surface form competition by
simply reweighing each option according to a term that is proportional to its a
priori likelihood within the context of the specific zero-shot task. It
achieves consistent gains in zero-shot performance over both calibrated (Zhao
et al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models
over a variety of multiple choice datasets.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08821">
<div class="article-summary-box-inner">
<span><p>This paper presents SimCSE, a simple contrastive learning framework that
greatly advances the state-of-the-art sentence embeddings. We first describe an
unsupervised approach, which takes an input sentence and predicts itself in a
contrastive objective, with only standard dropout used as noise. This simple
method works surprisingly well, performing on par with previous supervised
counterparts. We hypothesize that dropout acts as minimal data augmentation and
removing it leads to a representation collapse. Then, we incorporate annotated
pairs from natural language inference datasets into our contrastive learning
framework, by using "entailment" pairs as positives and "contradiction" pairs
as hard negatives. We evaluate SimCSE on standard semantic textual similarity
(STS) tasks, and our unsupervised and supervised models using BERT-base achieve
an average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2 and
2.2 points improvement compared to previous best results. We also show -- both
theoretically and empirically -- that contrastive learning objective
regularizes pre-trained embeddings' anisotropic space to be more uniform, and
it better aligns positive pairs when supervised signals are available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction. (arXiv:2106.04847v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.04847">
<div class="article-summary-box-inner">
<span><p>Keyphrase Prediction (KP) task aims at predicting several keyphrases that can
summarize the main idea of the given document. Mainstream KP methods can be
categorized into purely generative approaches and integrated models with
extraction and generation. However, these methods either ignore the diversity
among keyphrases or only weakly capture the relation across tasks implicitly.
In this paper, we propose UniKeyphrase, a novel end-to-end learning framework
that jointly learns to extract and generate keyphrases. In UniKeyphrase,
stacked relation layer and bag-of-words constraint are proposed to fully
exploit the latent semantic relation between extraction and generation in the
view of model structure and training process, respectively. Experiments on KP
benchmarks demonstrate that our joint approach outperforms mainstream methods
by a large margin.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.03158">
<div class="article-summary-box-inner">
<span><p>Data augmentation, the artificial creation of training data for machine
learning by transformations, is a widely studied research field across machine
learning disciplines. While it is useful for increasing the generalization
capabilities of a model, it can also address many other challenges and
problems, from overcoming a limited amount of training data over regularizing
the objective to limiting the amount data used to protect privacy. Based on a
precise description of the goals and applications of data augmentation (C1) and
a taxonomy for existing works (C2), this survey is concerned with data
augmentation methods for textual classification and aims to achieve a concise
and comprehensive overview for researchers and practitioners (C3). Derived from
the taxonomy, we divided more than 100 methods into 12 different groupings and
provide state-of-the-art references expounding which methods are highly
promising (C4). Finally, research perspectives that may constitute a building
block for future work are given (C5).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.12079">
<div class="article-summary-box-inner">
<span><p>Dialogue systems are widely used in AI to support timely and interactive
communication with users. We propose a general-purpose dialogue system
architecture that leverages computational argumentation to perform reasoning
and provide consistent and explainable answers. We illustrate the system using
a COVID-19 vaccine information case study.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Goal-Oriented Script Construction. (arXiv:2107.13189v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.13189">
<div class="article-summary-box-inner">
<span><p>The knowledge of scripts, common chains of events in stereotypical scenarios,
is a valuable asset for task-oriented natural language understanding systems.
We propose the Goal-Oriented Script Construction task, where a model produces a
sequence of steps to accomplish a given goal. We pilot our task on the first
multilingual script learning dataset supporting 18 languages collected from
wikiHow, a website containing half a million how-to articles. For baselines, we
consider both a generation-based approach using a language model and a
retrieval-based approach by first retrieving the relevant steps from a large
candidate pool and then ordering them. We show that our task is practical,
feasible but challenging for state-of-the-art Transformer models, and that our
methods can be readily deployed for various other datasets and domains with
decent zero-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.02854">
<div class="article-summary-box-inner">
<span><p>Languages differ in terms of the absence or presence of gender features, the
number of gender classes and whether and where gender features are explicitly
marked. These cross-linguistic differences can lead to ambiguities that are
difficult to resolve, especially for sentence-level MT systems. The
identification of ambiguity and its subsequent resolution is a challenging task
for which currently there aren't any specific resources or challenge sets
available. In this paper, we introduce gENder-IT, an English--Italian challenge
set focusing on the resolution of natural gender phenomena by providing
word-level gender tags on the English source side and multiple gender
alternative translations, where needed, on the Italian target side.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (arXiv:2108.05921v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.05921">
<div class="article-summary-box-inner">
<span><p>Detecting online hate is a complex task, and low-performing models have
harmful consequences when used for sensitive applications such as content
moderation. Emoji-based hate is a key emerging challenge for automated
detection. We present HatemojiCheck, a test suite of 3,930 short-form
statements that allows us to evaluate performance on hateful language expressed
with emoji. Using the test suite, we expose weaknesses in existing hate
detection models. To address these weaknesses, we create the HatemojiTrain
dataset using a human-and-model-in-the-loop approach. Models trained on these
5,912 adversarial examples perform substantially better at detecting
emoji-based hate, while retaining strong performance on text-only hate. Both
HatemojiCheck and HatemojiTrain are made publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">CushLEPOR: Customised hLEPOR Metric Using LABSE Distilled Knowledge Model to Improve Agreement with Human Judgements. (arXiv:2108.09484v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09484">
<div class="article-summary-box-inner">
<span><p>Human evaluation has always been expensive while researchers struggle to
trust the automatic metrics. To address this, we propose to customise
traditional metrics by taking advantages of the pre-trained language models
(PLMs) and the limited available human labelled scores. We first re-introduce
the hLEPOR metric factors, followed by the Python portable version we developed
which achieved the automatic tuning of the weighting parameters in hLEPOR
metric. Then we present the customised hLEPOR (cushLEPOR) which uses LABSE
distilled knowledge model to improve the metric agreement with human judgements
by automatically optimised factor weights regarding the exact MT language pairs
that cushLEPOR is deployed to. We also optimise cushLEPOR towards human
evaluation data based on MQM and pSQM framework on English-German and
Chinese-English language pairs. The experimental investigations show cushLEPOR
boosts hLEPOR performances towards better agreements to PLMs like LABSE with
much lower cost, and better agreements to human evaluations including MQM and
pSQM scores, and yields much better performances than BLEU (data available at
\url{https://github.com/poethan/cushLEPOR}).
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sarcasm Detection in Twitter -- Performance Impact while using Data Augmentation: Word Embeddings. (arXiv:2108.09924v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.09924">
<div class="article-summary-box-inner">
<span><p>Sarcasm is the use of words usually used to either mock or annoy someone, or
for humorous purposes. Sarcasm is largely used in social networks and
microblogging websites, where people mock or censure in a way that makes it
difficult even for humans to tell if what is said is what is meant. Failure to
identify sarcastic utterances in Natural Language Processing applications such
as sentiment analysis and opinion mining will confuse classification algorithms
and generate false results. Several studies on sarcasm detection have utilized
different learning algorithms. However, most of these learning models have
always focused on the contents of expression only, leaving the contextual
information in isolation. As a result, they failed to capture the contextual
information in the sarcastic expression. Moreover, some datasets used in
several studies have an unbalanced dataset which impacting the model result. In
this paper, we propose a contextual model for sarcasm identification in twitter
using RoBERTa, and augmenting the dataset by applying Global Vector
representation (GloVe) for the construction of word embedding and context
learning to generate more data and balancing the dataset. The effectiveness of
this technique is tested with various datasets and data augmentation settings.
In particular, we achieve performance gain by 3.2% in the iSarcasm dataset when
using data augmentation to increase 20% of data labeled as sarcastic, resulting
F-score of 40.4% compared to 37.2% without data augmentation.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions. (arXiv:2108.12189v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12189">
<div class="article-summary-box-inner">
<span><p>This paper presents Macquarie University's participation to the BioASQ
Synergy Task, and BioASQ9b Phase B. In each of these tasks, our participation
focused on the use of query-focused extractive summarisation to obtain the
ideal answers to medical questions. The Synergy Task is an end-to-end question
answering task on COVID-19 where systems are required to return relevant
documents, snippets, and answers to a given question. Given the absence of
training data, we used a query-focused summarisation system that was trained
with the BioASQ8b training data set and we experimented with methods to
retrieve the documents and snippets. Considering the poor quality of the
documents and snippets retrieved by our system, we observed reasonably good
quality in the answers returned. For phase B of the BioASQ9b task, the relevant
documents and snippets were already included in the test data. Our system split
the snippets into candidate sentences and used BERT variants under a sentence
classification setup. The system used the question and candidate sentence as
input and was trained to predict the likelihood of the candidate sentence being
part of the ideal answer. The runs obtained either the best or second best
ROUGE-F1 results of all participants to all batches of BioASQ9b. This shows
that using BERT in a classification setup is a very strong baseline for the
identification of ideal answers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Few-Shot Table-to-Text Generation with Prototype Memory. (arXiv:2108.12516v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12516">
<div class="article-summary-box-inner">
<span><p>Neural table-to-text generation models have achieved remarkable progress on
an array of tasks. However, due to the data-hungry nature of neural models,
their performances strongly rely on large-scale training examples, limiting
their applicability in real-world applications. To address this, we propose a
new framework: Prototype-to-Generate (P2G), for table-to-text generation under
the few-shot scenario. The proposed framework utilizes the retrieved
prototypes, which are jointly selected by an IR system and a novel prototype
selector to help the model bridging the structural gap between tables and
texts. Experimental results on three benchmark datasets with three
state-of-the-art models demonstrate that the proposed framework significantly
improves the model performance across various evaluation metrics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation. (arXiv:2108.12582v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12582">
<div class="article-summary-box-inner">
<span><p>Despite the remarkable performance of large-scale generative models in
open-domain conversation, they are known to be less practical for building
real-time conversation systems due to high latency. On the other hand,
retrieval models could return responses with much lower latency but show
inferior performance to the large-scale generative models since the
conversation quality is bounded by the pre-defined response set. To take
advantage of both approaches, we propose a new training method called G2R
(Generative-to-Retrieval distillation) that preserves the efficiency of a
retrieval model while leveraging the conversational ability of a large-scale
generative model by infusing the knowledge of the generative model into the
retrieval model. G2R consists of two distinct techniques of distillation: the
data-level G2R augments the dialogue dataset with additional responses
generated by the large-scale generative model, and the model-level G2R
transfers the response quality score assessed by the generative model to the
score of the retrieval model by the knowledge distillation loss. Through
extensive experiments including human evaluation, we demonstrate that our
retrieval-based conversation system trained with G2R shows a substantially
improved performance compared to the baseline retrieval model while showing
significantly lower inference latency than the large-scale generative models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Scheduled Sampling Based on Decoding Steps for Neural Machine Translation. (arXiv:2108.12963v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12963">
<div class="article-summary-box-inner">
<span><p>Scheduled sampling is widely used to mitigate the exposure bias problem for
neural machine translation. Its core motivation is to simulate the inference
scene during training by replacing ground-truth tokens with predicted tokens,
thus bridging the gap between training and inference. However, vanilla
scheduled sampling is merely based on training steps and equally treats all
decoding steps. Namely, it simulates an inference scene with uniform error
rates, which disobeys the real inference scene, where larger decoding steps
usually have higher error rates due to error accumulations. To alleviate the
above discrepancy, we propose scheduled sampling methods based on decoding
steps, increasing the selection chance of predicted tokens with the growth of
decoding steps. Consequently, we can more realistically simulate the inference
scene during training, thus better bridging the gap between training and
inference. Moreover, we investigate scheduled sampling based on both training
steps and decoding steps for further improvements. Experimentally, our
approaches significantly outperform the Transformer baseline and vanilla
scheduled sampling on three large-scale WMT tasks. Additionally, our approaches
also generalize well to the text summarization task on two popular benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.13161">
<div class="article-summary-box-inner">
<span><p>Large-scale pre-trained language models have contributed significantly to
natural language processing by demonstrating remarkable abilities as few-shot
learners. However, their effectiveness depends mainly on scaling the model
parameters and prompt design, hindering their implementation in most real-world
applications. This study proposes a novel pluggable, extensible, and efficient
approach named DifferentiAble pRompT (DART), which can convert small language
models into better few-shot learners without any prompt engineering. The main
principle behind this approach involves reformulating potential natural
language processing tasks into the task of a pre-trained language model and
differentially optimizing the prompt template as well as the target label with
backpropagation. Furthermore, the proposed approach can be: (i) Plugged to any
pre-trained language models; (ii) Extended to widespread classification tasks.
A comprehensive evaluation of standard NLP tasks demonstrates that the proposed
approach achieves a better few-shot performance.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v1 [cs.CL] CROSS LISTED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12971">
<div class="article-summary-box-inner">
<span><p>A smart contract is a program executed on a blockchain, based on which many
cryptocurrencies are implemented, and is being used for automating
transactions. Due to the large amount of money that smart contracts deal with,
there is a surging demand for a method that can statically and formally verify
them.
</p>
<p>This article describes our type-based static verification tool HELMHOLTZ for
Michelson, which is a statically typed stack-based language for writing smart
contracts that are executed on the blockchain platform Tezos. HELMHOLTZ is
designed on top of our extension of Michelson's type system with refinement
types. HELMHOLTZ takes a Michelson program annotated with a user-defined
specification written in the form of a refinement type as input; it then
typechecks the program against the specification based on the refinement type
system, discharging the generated verification conditions with the SMT solver
Z3. We briefly introduce our refinement type system for the core calculus
Mini-Michelson of Michelson, which incorporates the characteristic features
such as compound datatypes (e.g., lists and pairs), higher-order functions, and
invocation of another contract. \HELMHOLTZ{} successfully verifies several
practical Michelson programs, including one that transfers money to an account
and that checks a digital signature.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-01 01:56:20.021897409 UTC">2021-09-01 01:56:20 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.1</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>