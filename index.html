<!DOCTYPE html>
<html lang="en">
<head>
<title>ArxivDaily</title>
<meta charset="utf-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
<meta name="robots" content="noindex, nofollow"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
<link href="index.css" rel="stylesheet"/>
</head>
<body>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-10T01:54:26.787753978Z">09-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">Rust.cc</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">【标准库】✨ Rust v1.55 中文版更新了</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=5845ae5a-002c-4b14-971e-02c1d876084d">
<div class="article-summary-box-inner">
<span><p>中文版 Rust 标准库翻译更新了，这个版本的翻译可以说是迄今为止最好的中文版翻译。</p>
<p>可以到这里下载：<a href="https://github.com/wtklbm/rust-library-i18n" rel="noopener noreferrer">https://github.com/wtklbm/rust-library-i18n</a>。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust&Rocket 0.5&Redis的使用demo</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=cc9af0e5-fcff-4e16-b2a5-64a25e8b96a1">
<div class="article-summary-box-inner">
<span></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust 日报】2021-9-9</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=0e671427-facc-4848-998d-a8c4156e076b">
<div class="article-summary-box-inner">
<span><h3>Rust 1.56 beta1 (2021版)现已发布！!</h3>
<p>你现在可以用rustup安装2021测试版了。</p>
<p>使用<code>rustup default beta</code>切换到最新的测试版，然后你可以将你的toml文件迁移到 edition="2021" 或者用<code>cargo new</code>启动一个使用21版的新项目。</p>
<p>关于现有项目的迁移过程的一些信息:</p>
<p>https://doc.rust-lang.org/cargo/commands/cargo-fix.html</p>
<h3>rcore tutorial book 更新了</h3>
<p>陈渝老师的rcore系列操作系统课程更新了:</p>
<blockquote>
<p><code>rCore-Tutorial-Book</code> 在暑假期间又进行了一轮修改，算是从0.35版进化到0.50版了。请对学习用Rust写OS感兴趣的朋友看看。如果有问题、建议，发现了bug，请直接在每节下方的交互窗口留言。如果想一起来参与写作，请直接联系 陈渝或吴一凡。谢谢！ 本书定位是以尽量简单的编程和尽量少的OS/CPU知识来逐步设计实现一个一个的小OS，让学生知道操作系统的概念的实际体现和操作系统的全貌。经过我们讨论，虽然这本书是基于单处理器讲解的，但觉得还是要加入OS的同步互斥支持，与传统方式不同，这一章主要讲解操作系统如何支持用户态线程的同步互斥操作。所以，目前还缺的是关于同步互斥的一章，各种图，相关OS历史的介绍，相关知识点的进一步补充。争取本月完成。</p>
</blockquote>
<p><a href="https://rcore-os.github.io/rCore-Tutorial-Book-v3/" rel="noopener noreferrer">Read More</a>: https://rcore-os.github.io/rCore-Tutorial-Book-v3/</p>
<h3>Rust for Linux 研讨会 9.13 ～ 9.15</h3>
<p>9.13 分享：</p>
<ol>
<li>High Velocity Kernel Modules in Rust with Bento —— Samantha Miller</li>
</ol>
<p>Bento 是 Safe Rust 实现的 linux 内核 文件系统。 通过实现安全的 API 并使用安全的内核函数包装器，文件系统是用安全的 Rust 编写的。 这些安全接口尽可能接近现有的用户空间（主要是标准库）接口，因此只需将 Bento 包含更改为用户空间 Rust 库和/或 bento_utils 包含，就可以将文件系统重新编译为 FUSE 文件系统。更多信息：https://github.com/smiller123/bento</p>
<p>这次分享主要讨论 Bento ，用于加速 Linux 内核开发的框架。目前已经为文件系统模块实现了Bento框架，并利用它实现了一个性能类似于ext4的文件系统，可以在不卸载的情况下进行升级，而且只需要15ms的停机时间。我们目前正在努力扩展Bento，以支持自定义的TCP/IP堆栈。</p>
<ol start="2">
<li>How can we formally verify Rust for Linux? —— Alastair Reid</li>
</ol>
<p>在Linux中使用Rust的目的是创建更稳固、更安全的代码：通过利用Rust的语言特性和设计一个安全的API供驱动程序使用，避免内存安全问题和并发问题。该分享研究了我们如何/是否可以利用自动形式验证工具走得更远。</p>
<p>9.14 分享：</p>
<ol>
<li>Rust key concepts for the Linux kernel (I) —— Miguel Ojeda</li>
</ol>
<p>介绍 在 Linux 内核领域应用 Rust 的一些关键概念，第一部分</p>
<ol start="2">
<li>The Thread wrapper for Rust in Linux kernel — Boqun Feng （冯博群）</li>
</ol>
<p>内核线程是内核中最重要的组件之一，它也是实现内核中其他核心子系统的必要环节。本专题将分享如何在Linux内核中实现Rust类线程的封装器的学习过程，以及目前的状况和未来的工作。</p>
<ol start="3">
<li>Implementing the Iterator trait for seq_file —— Adam Bratschi-Kaye</li>
</ol>
<p>内核中的seq_file接口允许通过实现一个迭代可以打印的值的接口来轻松创建虚拟文件。这似乎应该直接转化为Rust的Iterator特性，其中Item实现了Display，但当然，魔鬼在细节中。该分享将展示如何为Rust代码提供seq_file的接口。</p>
<p>9.15 分享</p>
<ol>
<li>Rust key concepts for the Linux kernel (II) —— Wedson Almeida Filho</li>
</ol>
<p>介绍在 Linux 内核领域应用 Rust 的一些关键概念，第二部分</p>
<ol start="2">
<li>Writing an embedded SPI-based Linux driver in Rust —— Arthur Cohen, Esteban Blanc, Martin Schmidt</li>
</ol>
<p>虽然Linux主要不是一个以嵌入式为重点的操作系统，但它仍然被用于诸如Raspberry Pi这样的平台。在这些平台上，内核模块提供了一种有用的方式，可以在内核层面与各种设备进行交互，这些设备通常使用低级协议进行通信，如SPI或I2C。</p>
<p>在这种工作负载中使用Rust有很多优势，虽然这些协议的内核API已经被尝试和测试了很长时间，但目前还没有Rust的抽象。</p>
<p>在该分享中，将谈论在ARM64平台上为Linux带来一个安全的Rust的SPI协议的抽象，以及如何使用它来实现一个简单的设备驱动程序。该分享将与C语言的原始实现进行比较，后者提供了同样多的功能。最后，将深入探讨所使用的技术和他们使用Rust-for-Linux的经验。</p>
<p>以上三天的研讨会，应该是线下的，因为并没有提供线上参与链接。</p>
<p>对此话题感兴趣的可以关注：https://github.com/Rust-for-Linux/linux，</p>
<p>也可以登记参加在线讨论： https://rust-for-linux.zulipchat.com</p>
<p>研讨会官网： https://kangrejos.com/</p>
<h3>在Chrome中更安全地使用C++</h3>
<p>谷歌发布了一篇名为<code>在Chrome中更安全地使用C++</code>，它有一些关于这些安全问题如何困扰Chrome和Android代码库的安全报告的链接，这部分链接内提到了Rust。</p>
<p><a href="https://docs.google.com/document/d/e/2PACX-1vRZr-HJcYmf2Y76DhewaiJOhRNpjGHCxliAQTBhFxzv1QTae9o8mhBmDl32CRIuaWZLt5kVeH9e9jXv/pub" rel="noopener noreferrer">Read More</a>: https://docs.google.com/document/d/e/2PACX-1vRZr-HJcYmf2Y76DhewaiJOhRNpjGHCxliAQTBhFxzv1QTae9o8mhBmDl32CRIuaWZLt5kVeH9e9jXv/pub</p>
<h3><code>Tokio Cosole</code> 开发日记#1</h3>
<blockquote>
<p>自从几个月前开始制作 <code>Tokio Console</code> 原型以来，我们一直在努力工作，把它变成一个很棒的异步任务调试器。我们想围绕它的进展情况提供一些集中的更新。</p>
</blockquote>
<p><a href="https://tokio.rs/blog/2021-09-console-dev-diary-1" rel="noopener noreferrer">Read More</a>: https://tokio.rs/blog/2021-09-console-dev-diary-1</p>
<h3>zerocopy 0.6.0刚刚发布，带来了很多新的功能!</h3>
<p>最值得激动的新功能当然是：</p>
<blockquote>
<p><code>simd</code>和<code>simd-nightly</code>特性使得支持稳定和不稳定的SIMD</p>
</blockquote>
<p><a href="https://docs.rs/zerocopy/0.6.0/zerocopy/" rel="noopener noreferrer">Read More</a>: https://docs.rs/zerocopy/0.6.0/zerocopy/</p>
<h3><code>rpg-cli</code>发布1.0版本！</h3>
<p>rpg-cli是一个用Rust编写的极简主义<a href="https://en.wikipedia.org/wiki/Role-playing_video_game" rel="noopener noreferrer">computer RPG</a>。它的命令行界面可以作为<code>CD</code>的替代品，当你改变目录时，你会随机遇到敌人。</p>
<p><img src="https://raw.githubusercontent.com/facundoolano/rpg-cli/main/rpg-cli.png" alt="demo"></p>
<p><a href="https://github.com/facundoolano/rpg-cli/" rel="noopener noreferrer">Github</a>: https://github.com/facundoolano/rpg-cli/</p>
<h3>本周周报</h3>
<p>第 407 期。</p>
<p><a href="https://this-week-in-rust.org/blog/2021/09/08/this-week-in-rust-407/" rel="noopener noreferrer">Read More</a>: https://this-week-in-rust.org/blog/2021/09/08/this-week-in-rust-407/</p>
<p>From 日报小组 Cupnfish</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rust.cc 论坛: 支持 rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust 语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-08 -- 本月 Rust OSDev（2021 年 8 月）</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=e6eaa344-4be9-4ead-9864-9f1c5dc568ad">
<div class="article-summary-box-inner">
<span><h3>本月 Rust OSDev（2021 年 8 月）</h3>
<h4>项目更新</h4>
<ul>
<li>
<p>acpi - 包含用于解析 ACPI 表的 crate - 现代计算机的固件用于将有关硬件的信息传递给操作系统的数据结构。</p>
<p>已将处理器 UID 和本地 APIC ID 的表示更改 <code>acpi::platform::Processor</code>为<code>u32</code>，以支持 X2APIC 用于支持更多处理器的更广泛的 ID。</p>
</li>
<li>
<p>uefi-rs - 该uefi箱提供了安全，高性能包装UEFI。</p>
</li>
<li>
<p>x86_64 - 提供了用于各种抽象x86_64系统，包括包装为CPU指令，访问处理器的专用寄存器，和抽象类型体系结构的特定结构，如页表和描述符表。</p>
</li>
<li>
<p>bootloader - 实现了64位ELF可执行文件便于装载基于rust定制引导程序。</p>
</li>
<li>
<p>multboot2 - 提供一个抽象类型multiboot2引导程序的引导信息。</p>
</li>
<li>
<p>pic_8259 - 提供了用于8259个8259A可编程中断控制器（PICS）的抽象。</p>
</li>
</ul>
<p>ReadMore:<a href="https://rust-osdev.com/this-month/2021-08/" rel="noopener noreferrer">https://rust-osdev.com/this-month/2021-08/</a></p>
<h3>为什么 Rust 需要 Pin 和 unpin</h3>
<ul>
<li>什么是Futures</li>
<li>什么是自引用类型</li>
<li>为什么他们不安全</li>
<li>Pin/Unpin 如何使它们安全</li>
<li>使用 Pin/Unpin 编写复杂的futures</li>
</ul>
<p>ReadMore:<a href="https://blog.cloudflare.com/pin-and-unpin-in-rust/" rel="noopener noreferrer">https://blog.cloudflare.com/pin-and-unpin-in-rust/</a></p>
<h3>htmlq</h3>
<p>像 jq，但用于 HTML。使用 CSS 选择器从 HTML 文件中提取部分内容。</p>
<p>例子:</p>
<pre><code>$ curl --silent https://www.rust-lang.org/ | htmlq '#get-help'
&lt;div class="four columns mt3 mt0-l" id="get-help"&gt;
        &lt;h4&gt;Get help!&lt;/h4&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href="https://doc.rust-lang.org"&gt;Documentation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href="https://users.rust-lang.org"&gt;Ask a Question on the Users Forum&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href="http://ping.rust-lang.org"&gt;Check Website Status&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
        &lt;div class="languages"&gt;
            &lt;label class="hidden" for="language-footer"&gt;Language&lt;/label&gt;
            &lt;select id="language-footer"&gt;
                &lt;option title="English (US)" value="en-US"&gt;English (en-US)&lt;/option&gt;
&lt;option title="French" value="fr"&gt;Français (fr)&lt;/option&gt;
&lt;option title="German" value="de"&gt;Deutsch (de)&lt;/option&gt;

            &lt;/select&gt;
        &lt;/div&gt;
      &lt;/div&gt;
</code></pre>
<p>ReadMore:<a href="https://github.com/mgdm/htmlq" rel="noopener noreferrer">https://github.com/mgdm/htmlq</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">刚学,对生命周期的疑问</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=3582ba85-7c0f-451f-aea8-9da5d6b69905">
<div class="article-summary-box-inner">
<span><p>中文文档中的示例 10-24
fn main() {
let string1 = String::from("long string is long");
let result;
{
let string2 = String::from("xyz");
result = longest(string1.as_str(), string2.as_str());
}
println!("The longest string is {}", result);
}
fn longest&lt;'a&gt;(x: &amp;'a str, y: &amp;'a str) -&gt; &amp;'a str {
if x.len() &gt; y.len() {
x
} else {
y
}
}
会出现<code>string2</code> does not live long enough。
可是将
let string1 ="long string is long";
{
let string2 = "xyz";
//如上
}
改成这样却能正确输出结果</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">由于微软更新了webview2版本，tauri的程序不能打包release版本了</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=6d630089-a902-4eca-a98c-86c6e72d70f6">
<div class="article-summary-box-inner">
<span><p>rust GUI太难了</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust 调用c返回的动态数组</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=a048c6cc-cbdd-4965-80d7-5aa0097678a7">
<div class="article-summary-box-inner">
<span><p>C 使用malloc生成一个动态长度的数组，具体类型可知。C如何传递给rust 才能使rust知道该数组的长度和地址？</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">多地招聘 ｜安全性极高的Rust，想来学吗？</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=a48abd3f-43dc-4456-bade-0f94ccfa72b4">
<div class="article-summary-box-inner">
<span><p>非凸科技（https://ft.tech）正成为国内金融市场智能投资交易平台的引领者，在原有基础上全面升级到互联网新一代技术架构，采用Rust构建智能算法交易平台，逐步迭代，持续为券商、量化私募等众多大型金融机构提供优质的算法服务。</p>
<p>Rust已经连续五年被开发人员评为“最为喜爱”的编程语言，因为它可以避免某些类型的内存安全错误，能从根本上改善软件漏洞的现状。</p>
<p>Why Rust？安全、并发、实用，新时代的系统级编程语言。</p>
<p>高性能: Rust 速度惊人且内存利用率极高。由于没有运行时和垃圾回收，它能够胜任对性能要求特别高的服务，可以在嵌入式设备上运行，还能轻松和其他语言集成。</p>
<p>可靠性：Rust 丰富的类型系统和所有权模型保证了内存安全和线程安全，让您在编译期就能够消除各种各样的错误。</p>
<p>生产力：Rust 拥有出色的文档、友好的编译器和清晰的错误提示信息， 还集成了一流的工具——包管理器和构建工具，智能地自动补全和类型检验的多编辑器支持， 以及自动格式化代码等。</p>
<p>让Rust声名远播的优点还包括：提供C和C++的速度和控制能力，同时还提供了其他语言（如Go和Python）的安全性和安全性保证。MSRC将近70％的漏洞归类为内存安全问题，因此消除此类漏洞至关重要。</p>
<p>现阶段，非凸科技正在寻找行业内优秀的Rust开发工程师，薪资福利超级优厚。关键是团队有很好的Rust开发氛围，Rust大神手把手辅导，助你从Rust新人不断升级。欢迎加入我们！</p>
<p>公司福利：
1.提供租房补贴；
2.日常不间断网红零食、饮料、茶水供给；
3.不定期组织各部门技术交流学习研讨会，分享心得，互相成长；
4.团建活动丰富多彩，放松心情缓解疲劳。</p>
<p>工作地点：北京、上海、成都、深圳、美国加州</p>
<p>岗位：Rust开发工程师</p>
<p>岗位职责：
1.设计并开发基于RUST的高性能，低时延算法交易系统；
2.设计并开发数据处理平台，监控运维平台；
3.设计并开发面向客户的高可用交易工具等；
4.设计并开发策略相关的回测平台。</p>
<p>岗位要求：
1.本科及以上学历（985优先）。编程基础扎实，具有良好的计算机理论基础；
2.熟练掌握Linux操作，性能分析，具备Rust/C++/Java/Go丰富开发经验，熟悉常用的设计模式，有分布式相关经验加分；
3.有研发高性能，低时延系统经验加分；
4.对技术充满热情，思考深入。自我驱动，能快速学习新鲜事物。</p>
<p>岗位薪酬：Base 30K-60K+，有期权激励</p>
<p>投递邮箱：recruit@non-convex.com
联系人加微信：SweeneyTodd333333</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-09-07</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=fd95c63b-08d3-468a-9743-82feb56df3f2">
<div class="article-summary-box-inner">
<span><h2>【大家的项目】 <code>Poem-openapi</code> v0.2.1 发布</h2>
<p><a href="https://www.reddit.com/r/rust/comments/pjhoiu/poemopenapi_021_released/" rel="noopener noreferrer">Poem-openapi 0.2.1 released!</a></p>
<p><strong>Poem-openapi</strong>:</p>
<blockquote>
<p><code>Poem-openapi</code> 使你能够方便快捷的构建符合 <code>OpenAPIv3</code>标准的应用程序接口; 通过使用过程宏来生成大量样板代码，你将有更多时间和精力来专注于实现更重要的业务逻辑。</p>
</blockquote>
<p><strong>功能特性</strong>:</p>
<ul>
<li>完全支持 <code>async/await</code>;</li>
<li>类型安全;</li>
<li>对<code>Rustfmt</code>用户友好 （过程宏）;</li>
<li>最小开销;</li>
</ul>
<p><strong>示例</strong>:</p>
<p>代码：</p>
<pre><code>use poem_openapi::{payload::PlainText, OpenAPI, API};

struct Api;

#[API]
impl Api {
    #[oai(path = "/", method = "get")]
    async fn index(&amp;self, #[oai(name = "name", in = "query")] name: Option&lt;String&gt;) -&gt; PlainText {
        match name {
            Some(name) =&gt; format!("hello, {}!", name).into(),
            None =&gt; "hello!".into(),
        }
    }
}

#[tokio::main]
async fn main() {
    poem::Server::bind("127.0.0.1:3000")
        .await
        .unwrap()
        .run(OpenAPI::new(Api).title("hello World").ui_path("/ui"))
        .await
        .unwrap();
}
</code></pre>
<p>运行：</p>
<pre><code>[test@localhost poem-openapi]$ cargo run --example hello_world

[test@localhost poem-openapi]$ curl http://localhost:3000
hello!

[test@localhost poem-openapi]$ curl http://localhost:3000\?name\=sunli
hello, sunli!
</code></pre>
<p><strong>更多信息</strong>：</p>
<ul>
<li>项目地址：<a href="https://github.com/poem-web/poem-openapi" rel="noopener noreferrer">https://github.com/poem-web/poem-openapi</a></li>
<li>开源声明：<a href="https://rustcc.cn/article?id=334551fe-a8a9-4561-aa63-11987b10e81f" rel="noopener noreferrer">Poem-openapi开源了!</a></li>
<li>作者信息：<a href="https://github.com/sunli829" rel="noopener noreferrer">油条哥主页</a></li>
</ul>
<hr>
<h2>Relm4 v0.1 发布</h2>
<p><a href="https://aaronerhardt.github.io/blog/posts/announcing_relm4/" rel="noopener noreferrer">Announcing Relm4 v0.1</a></p>
<p>在第一个测试版发布大约一个月后，经过无数个小时的工作，作者高兴地宣布Relm4的第一个稳定版本正式发布！</p>
<p><strong>关于Relm4</strong>：</p>
<p><code>Relm4</code>是一个受<code>Elm</code>启发并基于<code>gtk4-rs</code>的惯用GUI库。它是一个从头开始构建的<code>relm</code>的新版本，并且兼容<code>gtk4</code>和<code>libadwaita</code>。<code>Relm4</code>的主要目标是生产效率、灵活性、简单性和可维护性。</p>
<p><strong>功能特性</strong></p>
<ul>
<li>支持<code>libadwaita</code>;</li>
<li>配套书籍<a href="https://aaronerhardt.github.io/relm4-book/book/" rel="noopener noreferrer"><em>GUI development with Relm4</em></a> 已完结;</li>
<li>新增支持非阻塞IO的消息句柄;</li>
<li>更多的可复用组件;</li>
<li>许多其他的改进和修复;</li>
</ul>
<p>完整的ChangeLog可以参见：
<a href="https://github.com/AaronErhardt/relm4/blob/main/CHANGES.md" rel="noopener noreferrer">https://github.com/AaronErhardt/relm4/blob/main/CHANGES.md</a></p>
<p><strong>更多信息</strong>：</p>
<ul>
<li>项目地址：<a href="https://github.com/AaronErhardt/relm4" rel="noopener noreferrer">https://github.com/AaronErhardt/relm4</a></li>
<li>项目文档：<a href="https://aaronerhardt.github.io/docs/relm4/relm4/" rel="noopener noreferrer">https://aaronerhardt.github.io/docs/relm4/relm4/</a></li>
<li>参考书籍：<a href="https://aaronerhardt.github.io/relm4-book/book/" rel="noopener noreferrer"><em>GUI development with Relm4</em></a></li>
</ul>
<hr>
<h2>Ockam示例: 构建一个可以安全访问远程私有网络的通道</h2>
<p><a href="https://github.com/ockam-network/ockam/tree/develop/documentation/use-cases/secure-remote-access-tunnels#readme" rel="noopener noreferrer">Build a secure access tunnel to a service in a remote private network</a></p>
<p>Ockam是一个支持端到端加密、双向认证、网络安全的Rust和Elixir语言通信库。</p>
<p>在本篇博文中，作者详细的介绍了如何使用<code>Ockam</code>在Rust中通过约<code>20行</code>代码来构建一个可以安全访问远程私有网络中设备的通道。</p>
<hr>
<h2>Skiff: 一门用Rust编写的逐渐类型化的函数式编程语言</h2>
<p><a href="https://www.reddit.com/r/ProgrammingLanguages/comments/pjcewi/introducing_skiff_a_gradually_typed_functional/" rel="noopener noreferrer">Introducing Skiff, a gradually typed functional language written in Rust</a></p>
<p>Skiff，是一门用Rust编写的逐渐类型化的函数式编程语言。所谓逐渐类型化是指作者计划下一步通过添加类型化关键字来区分完全类型函数和部分类型函数。</p>
<p>Skiff受<code>Elm</code>/<code>Pyret</code>/<code>Python</code>语言启发，并受<code>Rust</code>/<code>Javascript</code>/<code>Typescript</code>/<code>Haskell</code>/<code>OCaml</code>/<code>Lua</code>等语言影响，当前语言功能还在持续完善中，作者提供了一个由<code>wasm!</code>驱动的<a href="https://skiff.paulbiberstein.me/" rel="noopener noreferrer">网页编辑器</a>可供读者学习使用，更多信息请访问项目主页的<a href="https://github.com/P-bibs/skiff/" rel="noopener noreferrer">Readme</a>。</p>
<p><strong>更多信息</strong>：</p>
<ul>
<li>项目地址：<a href="https://github.com/P-bibs/skiff/" rel="noopener noreferrer">https://github.com/P-bibs/skiff/</a></li>
<li>网页编辑器：<a href="https://skiff.paulbiberstein.me/" rel="noopener noreferrer">https://skiff.paulbiberstein.me/</a></li>
</ul>
<hr>
<p>From 日报小组 odd-cat</p>
<p>社区学习交流平台订阅：</p>
<p><a href="https://rustcc.cn/" rel="noopener noreferrer">Rust.cc 论坛: 支持 rss</a></p>
<p><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust 语言中文社区</a></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">40k-80k招聘全职远程区块链开发工程师</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=5c42bdfd-0e97-4fc4-a494-4c9d14a5679f">
<div class="article-summary-box-inner">
<span><p>公司简介
我们是一家总部位于硅谷的区块链初创企业,我们跟绝大部分区块链公司不同的是，我们有顶级的产品来支撑我们的业务和发展远景。重要的事情说三遍，有产品，有产品，有产品，而且是业界领先的产品！</p>
<p>目前我们在全球150多个国家拥有20,000+的用户，30,000+节点。</p>
<p>目前公司已经盈利，现金贮备丰厚，正在对接业界最等级的风险投资机构，处于起飞的前夕。</p>
<p>基本要求
1、扎实的计算机科学基础知识；</p>
<p>2、动手能力强，有死磕精神；</p>
<p>3、有丰富的 Rust 开发经验；</p>
<p>4、曾经独立完成或者主导完成过具有挑战性的项目；</p>
<p>5、对工作有强大的责任心。</p>
<p>加分项
1、区块链相关数据结构与算法；</p>
<p>2、Substrate或其他区块链节点开发经验；</p>
<p>3、跨链、Layer 2 开发经验。</p>
<p>薪资福利
1、月薪40k-80k；</p>
<p>2、入职满一年，表现合格者可以获得公司的股票或期权；</p>
<p>3、优秀者提供移民美国、加拿大的机会。</p>
<p>工作方式
1、远程办公</p>
<p>2、工作时间：无固定时间，工作完成后自由安排</p>
<p>3、很少有跨时区的会议，除特殊、紧急工作任务对接外</p>
<p>4、工作会议主要为中文交流</p>
<p>工作语言
1、英文为主，中文为辅</p>
<p>2、英文要求读写能力合格及优秀</p>
<p>面试流程
1、收到简历后，将安排1到3轮电话语音面试；</p>
<p>2、电话语音面试结束后，将进入试用期（全额工资）；</p>
<p>3、试用期结束后，正式开始工作。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b">
<div class="article-summary-box-inner">
<span><p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>
<ol>
<li>认识面向基础架构语言</li>
<li>理解 Rust 所有权</li>
<li>通过实战理解 Rust 宏</li>
<li>通过 Datafuse 理解全链路跟踪</li>
<li>Rust 异步编程入门 Future Part 1</li>
<li>Rust 异步编程入门 Future Part 2</li>
</ol>
<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。
本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>
<hr>
<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>
<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>
<p>分享讲师： 董泽润</p>
<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现
讲师介绍：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<hr>
<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>
<p>分享时间： 周日晚上2021-09-12 20:00-21:00</p>
<p>分享讲师： 苏林</p>
<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>
<ol>
<li>利用 Tokio 实现一个 Mini Http server</li>
<li>基于 Http server提供内容动态的 API 网关</li>
<li>利用 Redis 实现对 API 网关加速</li>
<li>学习 Rust RPC 调用，实现微服务调用</li>
</ol>
<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<p>Rust 异步编程入门 Future Part 1 | Vol. 5
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<p>Rust 异步编程入门 Future Part 2 | Vol. 6
https://www.bilibili.com/video/bv1oy4y1G7jC</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">rust 学习随笔</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8">
<div class="article-summary-box-inner">
<span><h1>切换镜像源</h1>
<p>crm =&gt; https://github.com/wtklbm/crm</p>
<p>常用命令就是 <code>crm best</code></p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">pretree 补全文档发布了,再次谢谢大神的指点终于入门了。</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827">
<div class="article-summary-box-inner">
<span><h1>Pretree</h1>
<p>pretree is a package for storing and querying routing rules with prefix tree .</p>
<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>
<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>
<p>Inspired by <a href="https://github.com/obity/pretree" rel="noopener noreferrer">obity/pretree</a> (golang)</p>
<h1>Doc</h1>
<p>See this document at <a href="https://docs.rs/pretree" rel="noopener noreferrer">API documentation</a></p>
<h1>Install</h1>
<p>Add the following line to your Cargo.toml file:</p>
<pre><code>pretree = "1.0.0"
</code></pre>
<h1>Example</h1>
<pre><code>use pretree::Pretree;
let mut p = Pretree::new();
p.store("GET","account/{id}/info/:name");
p.store("GET","account/:id/login");
p.store("GET","account/{id}");
p.store("GET","bacteria/count_number_by_month");
let (ok,rule,vars) = p.query("GET","account/929239");
println!("ok:{} rule:{} vars:{:#?}",ok,rule,vars);

</code></pre>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>
<p><strong>课程时间:</strong> 2021年9月5日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识, Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>
<h3>课程大纲</h3>
<p>1、回顾 Rust 异步编程模型.</p>
<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>
<p>3、Tokio 介绍.</p>
<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>
<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/
Rust 异步编程入门 Future Part 1 回放地址：
https://www.bilibili.com/video/BV1mf4y1N7MJ/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课：《 Rust 异步编程入门 Future 》|Vol. 5</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70">
<div class="article-summary-box-inner">
<span><h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>
<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>
<p><strong>课程介绍:</strong> 讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是 Rust 异步编程的核心基础。</p>
<h3>课程大纲</h3>
<p>1、为什么需要异步.</p>
<p>2、理解异步编程模型.</p>
<p>3、Future 编程模型讲解.</p>
<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<p>通过 Datafuse 理解全链路跟踪 | Vol. 4 https://www.bilibili.com/video/BV1YA411c7ia/</p>
<h3>课程中推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c">
<div class="article-summary-box-inner">
<span><h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>
<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>
<p>ReadMore:<a href="https://twitter.com/m_ou_se/status/1427666611977297924" rel="noopener noreferrer">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>
<h3>异步引擎 C++20, Rust &amp; Zig</h3>
<p>ReadMore:<a href="https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/" rel="noopener noreferrer">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>
<h3>RG3D -- Rust 3D 游戏引擎</h3>
<ul>
<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>
<li><strong>延迟着色</strong></li>
<li><strong>内置保存/加载</strong></li>
<li><strong>独立场景编辑器</strong></li>
<li><strong>高级物理模型</strong></li>
<li><strong>分层模型资源</strong></li>
<li><strong>几何实例化</strong></li>
</ul>
<p>ReadMore:<a href="https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/" rel="noopener noreferrer">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>
<p>ReadMore:<a href="https://github.com/rg3dengine/rg3d" rel="noopener noreferrer">https://github.com/rg3dengine/rg3d</a></p>
<hr>
<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>
<p>社区学习交流平台订阅：</p>
<ul>
<li><a href="https://rustcc.cn/" rel="noopener noreferrer">Rustcc论坛: 支持rss</a></li>
<li><a href="https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62" rel="noopener noreferrer">微信公众号：Rust语言中文社区</a></li>
</ul>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8">
<div class="article-summary-box-inner">
<span><p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>
<p><strong>课程时间：</strong> 2021年8月22日 20:30-21:30</p>
<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>
<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>
<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>
<h3>课程大纲</h3>
<ol>
<li>
<p>什么是分布式追踪系统OpenTracing及应用场景</p>
</li>
<li>
<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>
</li>
<li>
<p>为什么需要tokio-rs/tracing库</p>
</li>
<li>
<p>演示Datafuse项目中tokio-rs/tracing的使用</p>
</li>
</ol>
<h3><strong>讲师介绍</strong></h3>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>获取 T-Shirt 的方法：</h3>
<ol>
<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>
<li>进行 Rust，大数据，数据库方面的公开课分享</li>
<li>社区里分享 datafuse 相关文章</li>
<li>datafuse.rs 上面文档翻译工作</li>
</ol>
<h3>往期课程回放</h3>
<p>认识面向基础架构语言 Rust | Vol. 1 https://www.bilibili.com/video/BV1mg411778g</p>
<p>理解 Rust 的所有权 | Vol. 2 https://www.bilibili.com/video/BV1264y1i7U9</p>
<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
<p>Rust宏的练习项目： https://github.com/dtolnay/proc-macro-workshop</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">论坛github账户无法登录解决笔记</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190">
<div class="article-summary-box-inner">
<span><p>有反映这两天github账户无法登录了。</p>
<p>报这个错：</p>
<pre><code>get github user info err
</code></pre>
<p>查了几个地方：</p>
<ol>
<li>代码是否运行正常：Ok</li>
<li>https代理是否正常：Ok</li>
<li>检查了github返回日志，发现是：</li>
</ol>
<pre><code>get_github_user_info: response body: "{\"message\":\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\",\"documentation_url\":\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\"}"
get_github_user_info: Got: Err(Custom("read json login error"))
</code></pre>
<p>进入这个地址一看：<a href="https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/" rel="noopener noreferrer">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>
<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>
<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>
<p>特此记录。</p>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust 的 Future 与 Javascript 的 Promise 功能对照参考</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095">
<div class="article-summary-box-inner">
<span><h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>
<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>
<blockquote>
<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>
</blockquote>
<table>
<thead>
<tr>
<th>javascript</th>
<th align="center">rust</th>
<th align="center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>Promise.resolve(...)</td>
<td align="center">use ::async_std::future;future::ready(Ok(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.reject(...)</td>
<td align="center">use ::async_std::future;future::ready(Err(...))</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>Promise.catch(err =&gt; err)</td>
<td align="center">use ::async_std::future;future::ready(...)</td>
<td align="center">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>
</tr>
<tr>
<td>new Promise(() =&gt; {/* 什么都不做 */})</td>
<td align="center">use ::async_std::future;future::pending()</td>
<td align="center"></td>
</tr>
<tr>
<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; { if (Math.random() &gt; .5) { resolve(1); } else { reject(new Error('1')); }}, 500))</td>
<td align="center">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| { thread::sleep(Duration::from_millis(500)); let mut rng = rand::thread_rng(); if rng.gen() &gt; 0.5f64 { Ok(1) } else { Err('1') }}).await;</td>
<td align="center">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被 （1）跨线程传递 （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>
</tr>
<tr>
<td>Promise.all([promise1, promise2, promise3])</td>
<td align="center">future1.try_join(future2).try_join(future3).await</td>
<td align="center">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>
</tr>
<tr>
<td>Promise.all([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.join(future2).join(future3).await</td>
<td align="center">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>
</tr>
<tr>
<td>Promise.race([promise1, promise2, promise3])</td>
<td align="center">future1.try_race(future2).try_race(future3).await</td>
<td align="center">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>
</tr>
<tr>
<td>Promise.race([ promise1.catch(err =&gt; err), promise2.catch(err =&gt; err) promise3.catch(err =&gt; err)])</td>
<td align="center">future1.race(future2).race(future3).await</td>
<td align="center">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>
</tr>
</tbody>
</table>
</span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Rust公开课：《通过实战理解 Rust 宏》| Vol. 3</summary>
<a class="article-summary-link article-summary-box-outer" href="https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21">
<div class="article-summary-box-inner">
<span><p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>
<p><strong>课程时间：</strong> 2021年8月15日 20:30-21:30</p>
<p><strong>课程介绍：</strong></p>
<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg" alt></p>
<p>这就是通过宏实现配置的统一行为，代码参考：
https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>
<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>
<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>
<h3>课程大纲</h3>
<ul>
<li>什么是 Rust 宏</li>
<li>什么是宏运行原理</li>
<li>如何创建 Rust 宏过程</li>
<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>
</ul>
<p><strong>讲师介绍</strong>
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png" alt></p>
<p><img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png" alt></p>
<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。
<img src="https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png" alt></p>
<h3>课程中苏林老师推荐入门资料：</h3>
<p>Rust在线编辑器: https://play.rust-lang.org/</p>
<p>《Rust语言程序设计》: https://kaisery.github.io/trpl-zh-cn/</p>
<p>打怪通关学习方式Rustlings: https://github.com/rust-lang/rustlings</p>
<p>Rust优秀项目Datafuse： https://github.com/datafuselabs/datafuse</p>
</span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<section class="daily-content">
<h2 class="daily-heading">
<time datetime="2021-09-10T01:30:00Z">09-10</time>
</h2>
<ul class="sources card">
<li class="source">
<section>
<h3 class="source-name">cs.CL updates on arXiv.org</h3>
<section class="articles-per-source">
<article>
<details class="article-expander">
<summary class="article-expander__title">Powering Comparative Classification with Sentiment Analysis via Domain Adaptive Knowledge Transfer. (arXiv:2109.03819v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03819">
<div class="article-summary-box-inner">
<span><p>We study Comparative Preference Classification (CPC) which aims at predicting
whether a preference comparison exists between two entities in a given sentence
and, if so, which entity is preferred over the other. High-quality CPC models
can significantly benefit applications such as comparative question answering
and review-based recommendations. Among the existing approaches, non-deep
learning methods suffer from inferior performances. The state-of-the-art graph
neural network-based ED-GAT (Ma et al., 2020) only considers syntactic
information while ignoring the critical semantic relations and the sentiments
to the compared entities. We proposed sentiment Analysis Enhanced COmparative
Network (SAECON) which improves CPC ac-curacy with a sentiment analyzer that
learns sentiments to individual entities via domain adaptive knowledge
transfer. Experiments on the CompSent-19 (Panchenko et al., 2019) dataset
present a significant improvement on the F1 scores over the best existing CPC
approaches.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v1 [cs.CR])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03848">
<div class="article-summary-box-inner">
<span><p>Cyber intelligence is widely and abundantly available in numerous open online
sources with reports on vulnerabilities and incidents. This constant stream of
noisy information requires new tools and techniques if it is to be used for the
benefit of analysts and investigators in various organizations. In this paper
we present and implement a novel knowledge graph and knowledge mining framework
for extracting relevant information from free-form text about incidents in the
cyber domain. Our framework includes a machine learning based pipeline as well
as crawling methods for generating graphs of entities, attackers and the
related information with our non-technical cyber ontology. We test our
framework on publicly available cyber incident datasets to evaluate the
accuracy of our knowledge mining methods as well as the usefulness of the
framework in the use of cyber analysts. Our results show analyzing the
knowledge graph constructed using the novel framework, an analyst can infer
additional information from the current cyber landscape in terms of risk to
various entities and the propagation of risk between industries and countries.
Expanding the framework to accommodate more technical and operational level
information can increase the accuracy and explainability of trends and risk in
the knowledge graph.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Bayesian Framework for Information-Theoretic Probing. (arXiv:2109.03853v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03853">
<div class="article-summary-box-inner">
<span><p>Pimentel et al. (2020) recently analysed probing from an
information-theoretic perspective. They argue that probing should be seen as
approximating a mutual information. This led to the rather unintuitive
conclusion that representations encode exactly the same information about a
target task as the original sentences. The mutual information, however, assumes
the true probability distribution of a pair of random variables is known,
leading to unintuitive results in settings where it is not. This paper proposes
a new framework to measure what we term Bayesian mutual information, which
analyses information from the perspective of Bayesian agents -- allowing for
more intuitive findings in scenarios with finite data. For instance, under
Bayesian MI we have that data can add information, processing can help, and
information can hurt, which makes it more intuitive for machine learning
applications. Finally, we apply our framework to probing where we believe
Bayesian mutual information naturally operationalises ease of extraction by
explicitly limiting the available background knowledge to solve a task.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation. (arXiv:2109.03858v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03858">
<div class="article-summary-box-inner">
<span><p>Recent works have found evidence of gender bias in models of machine
translation and coreference resolution using mostly synthetic diagnostic
datasets. While these quantify bias in a controlled experiment, they often do
so on a small scale and consist mostly of artificial, out-of-distribution
sentences. In this work, we find grammatical patterns indicating stereotypical
and non-stereotypical gender-role assignments (e.g., female nurses versus male
dancers) in corpora from three domains, resulting in a first large-scale gender
bias dataset of 108K diverse real-world English sentences. We manually verify
the quality of our corpus and use it to evaluate gender bias in various
coreference resolution and machine translation models. We find that all tested
models tend to over-rely on gender stereotypes when presented with natural
inputs, which may be especially harmful when deployed in commercial systems.
Finally, we show that our dataset lends itself to finetuning a coreference
resolution model, finding it mitigates bias on a held out set. Our dataset and
models are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will
spur future research into gender bias evaluation mitigation techniques in
realistic settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Sparsity and Sentence Structure in Encoder-Decoder Attention of Summarization Systems. (arXiv:2109.03888v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03888">
<div class="article-summary-box-inner">
<span><p>Transformer models have achieved state-of-the-art results in a wide range of
NLP tasks including summarization. Training and inference using large
transformer models can be computationally expensive. Previous work has focused
on one important bottleneck, the quadratic self-attention mechanism in the
encoder. Modified encoder architectures such as LED or LoBART use local
attention patterns to address this problem for summarization. In contrast, this
work focuses on the transformer's encoder-decoder attention mechanism. The cost
of this attention becomes more significant in inference or training approaches
that require model-generated histories. First, we examine the complexity of the
encoder-decoder attention. We demonstrate empirically that there is a sparse
sentence structure in document summarization that can be exploited by
constraining the attention mechanism to a subset of input sentences, whilst
maintaining system performance. Second, we propose a modified architecture that
selects the subset of sentences to constrain the encoder-decoder attention.
Experiments are carried out on abstractive summarization tasks, including
CNN/DailyMail, XSum, Spotify Podcast, and arXiv.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Retrieve, Caption, Generate: Visual Grounding for Enhancing Commonsense in Text Generation Models. (arXiv:2109.03892v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03892">
<div class="article-summary-box-inner">
<span><p>We investigate the use of multimodal information contained in images as an
effective method for enhancing the commonsense of Transformer models for text
generation. We perform experiments using BART and T5 on concept-to-text
generation, specifically the task of generative commonsense reasoning, or
CommonGen. We call our approach VisCTG: Visually Grounded Concept-to-Text
Generation. VisCTG involves captioning images representing appropriate everyday
scenarios, and using these captions to enrich and steer the generation process.
Comprehensive evaluation and analysis demonstrate that VisCTG noticeably
improves model performance while successfully addressing several issues of the
baseline generations, including poor commonsense, fluency, and specificity.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ELIT: Emory Language and Information Toolkit. (arXiv:2109.03903v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03903">
<div class="article-summary-box-inner">
<span><p>We introduce ELIT, the Emory Language and Information Toolkit, which is a
comprehensive NLP framework providing transformer-based end-to-end models for
core tasks with a special focus on memory efficiency while maintaining
state-of-the-art accuracy and speed. Compared to existing toolkits, ELIT
features an efficient Multi-Task Learning (MTL) model with many downstream
tasks that include lemmatization, part-of-speech tagging, named entity
recognition, dependency parsing, constituency parsing, semantic role labeling,
and AMR parsing. The backbone of ELIT's MTL framework is a pre-trained
transformer encoder that is shared across tasks to speed up their inference.
ELIT provides pre-trained models developed on a remix of eight datasets. To
scale up its service, ELIT also integrates a RESTful Client/Server combination.
On the server side, ELIT extends its functionality to cover other tasks such as
tokenization and coreference resolution, providing an end user with agile
research experience. All resources including the source codes, documentation,
and pre-trained models are publicly available at
https://github.com/emorynlp/elit.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Recipe For Arbitrary Text Style Transfer with Large Language Models. (arXiv:2109.03910v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03910">
<div class="article-summary-box-inner">
<span><p>In this paper, we leverage large language models (LMs) to perform zero-shot
text style transfer. We present a prompting method that we call augmented
zero-shot learning, which frames style transfer as a sentence rewriting task
and requires only a natural language instruction, without model fine-tuning or
exemplars in the target style. Augmented zero-shot learning is simple and
demonstrates promising results not just on standard style transfer tasks such
as sentiment, but also on arbitrary transformations such as "make this
melodramatic" or "insert a metaphor."
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Ensemble Fine-tuned mBERT for Translation Quality Estimation. (arXiv:2109.03914v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03914">
<div class="article-summary-box-inner">
<span><p>Quality Estimation (QE) is an important component of the machine translation
workflow as it assesses the quality of the translated output without consulting
reference translations. In this paper, we discuss our submission to the WMT
2021 QE Shared Task. We participate in Task 2 sentence-level sub-task that
challenge participants to predict the HTER score for sentence-level
post-editing effort. Our proposed system is an ensemble of multilingual BERT
(mBERT)-based regression models, which are generated by fine-tuning on
different input settings. It demonstrates comparable performance with respect
to the Pearson's correlation and beats the baseline system in MAE/ RMSE for
several language pairs. In addition, we adapt our system for the zero-shot
setting by exploiting target language-relevant language pairs and
pseudo-reference translations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Transformers in the loop: Polarity in neural models of language. (arXiv:2109.03926v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03926">
<div class="article-summary-box-inner">
<span><p>Representation of linguistic phenomena in computational language models is
typically assessed against the predictions of existing linguistic theories of
these phenomena. Using the notion of polarity as a case study, we show that
this is not always the most adequate set-up. We probe polarity via so-called
'negative polarity items' (in particular, English 'any') in two pre-trained
Transformer-based models (BERT and GPT-2). We show that -- at least for
polarity -- metrics derived from language models are more consistent with data
from psycholinguistic experiments than linguistic theory predictions.
Establishing this allows us to more adequately evaluate the performance of
language models and also to use language models to discover new insights into
natural language grammar beyond existing linguistic theories. Overall, our
results encourage a closer tie between experiments with human subjects and with
language models. We propose methods to enable this closer tie, with language
models as part of experimental pipeline, and show this pipeline at work.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">What's Hidden in a One-layer Randomly Weighted Transformer?. (arXiv:2109.03939v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03939">
<div class="article-summary-box-inner">
<span><p>We demonstrate that, hidden within one-layer randomly weighted neural
networks, there exist subnetworks that can achieve impressive performance,
without ever modifying the weight initializations, on machine translation
tasks. To find subnetworks for one-layer randomly weighted neural networks, we
apply different binary masks to the same weight matrix to generate different
layers. Hidden within a one-layer randomly weighted Transformer, we find that
subnetworks that can achieve 29.45/17.29 BLEU on IWSLT14/WMT14. Using a fixed
pre-trained embedding layer, the previously found subnetworks are smaller than,
but can match 98%/92% (34.14/25.24 BLEU) of the performance of, a trained
Transformer small/base on IWSLT14/WMT14. Furthermore, we demonstrate the
effectiveness of larger and deeper transformers in this setting, as well as the
impact of different initialization methods. We released the source code at
https://github.com/sIncerass/one_layer_lottery_ticket.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Unsupervised Pre-training with Structured Knowledge for Improving Natural Language Inference. (arXiv:2109.03941v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03941">
<div class="article-summary-box-inner">
<span><p>While recent research on natural language inference has considerably
benefited from large annotated datasets, the amount of inference-related
knowledge (including commonsense) provided in the annotated data is still
rather limited. There have been two lines of approaches that can be used to
further address the limitation: (1) unsupervised pretraining can leverage
knowledge in much larger unstructured text data; (2) structured (often
human-curated) knowledge has started to be considered in neural-network-based
models for NLI. An immediate question is whether these two approaches
complement each other, or how to develop models that can bring together their
advantages. In this paper, we propose models that leverage structured knowledge
in different components of pre-trained models. Our results show that the
proposed models perform better than previous BERT-based state-of-the-art
models. Although our models are proposed for NLI, they can be easily extended
to other sentence or sentence-pair classification problems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Formal Description of Sorani Kurdish Morphology. (arXiv:2109.03942v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03942">
<div class="article-summary-box-inner">
<span><p>Sorani Kurdish, also known as Central Kurdish, has a complex morphology,
particularly due to the patterns in which morphemes appear. Although several
aspects of Kurdish morphology have been studied, such as pronominal endoclitics
and Izafa constructions, Sorani Kurdish morphology has received trivial
attention in computational linguistics. Moreover, some morphemes, such as the
emphasis endoclitic =\^i\c{s}, and derivational morphemes have not been
previously studied. To tackle the complex morphology of Sorani, we provide a
thorough description of Sorani Kurdish morphological and morphophonological
constructions in a formal way such that they can be used as finite-state
transducers for morphological analysis and synthesis.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multilingual Speech Recognition for Low-Resource Indian Languages using Multi-Task conformer. (arXiv:2109.03969v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03969">
<div class="article-summary-box-inner">
<span><p>Transformers have recently become very popular for sequence-to-sequence
applications such as machine translation and speech recognition. In this work,
we propose a multi-task learning-based transformer model for low-resource
multilingual speech recognition for Indian languages. Our proposed model
consists of a conformer [1] encoder and two parallel transformer decoders. We
use a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme
decoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme
recognition task as an auxiliary task for our multi-task learning framework. We
jointly optimize the network for both phoneme and grapheme recognition tasks
using Joint CTC-Attention [2] training. We use a conditional decoding scheme to
inject the language information into the model before predicting the grapheme
sequence. Our experiments show that our proposed approach can obtain
significant improvement over previous approaches [4]. We also show that our
conformer-based dual-decoder approach outperforms both the transformer-based
dual-decoder approach and single decoder approach. Finally, We compare
monolingual ASR models with our proposed multilingual ASR approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Competence-based Curriculum Learning for Multilingual Machine Translation. (arXiv:2109.04002v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04002">
<div class="article-summary-box-inner">
<span><p>Currently, multilingual machine translation is receiving more and more
attention since it brings better performance for low resource languages (LRLs)
and saves more space. However, existing multilingual machine translation models
face a severe challenge: imbalance. As a result, the translation performance of
different languages in multilingual translation models are quite different. We
argue that this imbalance problem stems from the different learning
competencies of different languages. Therefore, we focus on balancing the
learning competencies of different languages and propose Competence-based
Curriculum Learning for Multilingual Machine Translation, named CCL-M.
Specifically, we firstly define two competencies to help schedule the high
resource languages (HRLs) and the low resource languages: 1) Self-evaluated
Competence, evaluating how well the language itself has been learned; and 2)
HRLs-evaluated Competence, evaluating whether an LRL is ready to be learned
according to HRLs' Self-evaluated Competence. Based on the above competencies,
we utilize the proposed CCL-M algorithm to gradually add new languages into the
training set in a curriculum learning manner. Furthermore, we propose a novel
competenceaware dynamic balancing sampling strategy for better selecting
training samples in multilingual training. Experimental results show that our
approach has achieved a steady and significant performance gain compared to the
previous state-of-the-art approach on the TED talks dataset.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graph Based Network with Contextualized Representations of Turns in Dialogue. (arXiv:2109.04008v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04008">
<div class="article-summary-box-inner">
<span><p>Dialogue-based relation extraction (RE) aims to extract relation(s) between
two arguments that appear in a dialogue. Because dialogues have the
characteristics of high personal pronoun occurrences and low information
density, and since most relational facts in dialogues are not supported by any
single sentence, dialogue-based relation extraction requires a comprehensive
understanding of dialogue. In this paper, we propose the TUrn COntext awaRE
Graph Convolutional Network (TUCORE-GCN) modeled by paying attention to the way
people understand dialogues. In addition, we propose a novel approach which
treats the task of emotion recognition in conversations (ERC) as a
dialogue-based RE. Experiments on a dialogue-based RE dataset and three ERC
datasets demonstrate that our model is very effective in various dialogue-based
natural language understanding tasks. In these experiments, TUCORE-GCN
outperforms the state-of-the-art models on most of the benchmark datasets. Our
code is available at https://github.com/BlackNoodle/TUCORE-GCN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Weakly-Supervised Visual-Retriever-Reader for Knowledge-based Question Answering. (arXiv:2109.04014v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04014">
<div class="article-summary-box-inner">
<span><p>Knowledge-based visual question answering (VQA) requires answering questions
with external knowledge in addition to the content of images. One dataset that
is mostly used in evaluating knowledge-based VQA is OK-VQA, but it lacks a gold
standard knowledge corpus for retrieval. Existing work leverage different
knowledge bases (e.g., ConceptNet and Wikipedia) to obtain external knowledge.
Because of varying knowledge bases, it is hard to fairly compare models'
performance. To address this issue, we collect a natural language knowledge
base that can be used for any VQA system. Moreover, we propose a Visual
Retriever-Reader pipeline to approach knowledge-based VQA. The visual retriever
aims to retrieve relevant knowledge, and the visual reader seeks to predict
answers based on given knowledge. We introduce various ways to retrieve
knowledge using text and images and two reader styles: classification and
extraction. Both the retriever and reader are trained with weak supervision.
Our experimental results show that a good retriever can significantly improve
the reader's performance on the OK-VQA challenge. The code and corpus are
provided in https://github.com/luomancs/retriever\_reader\_for\_okvqa.git
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Graphine: A Dataset for Graph-aware Terminology Definition Generation. (arXiv:2109.04018v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04018">
<div class="article-summary-box-inner">
<span><p>Precisely defining the terminology is the first step in scientific
communication. Developing neural text generation models for definition
generation can circumvent the labor-intensity curation, further accelerating
scientific discovery. Unfortunately, the lack of large-scale terminology
definition dataset hinders the process toward definition generation. In this
paper, we present a large-scale terminology definition dataset Graphine
covering 2,010,648 terminology definition pairs, spanning 227 biomedical
subdisciplines. Terminologies in each subdiscipline further form a directed
acyclic graph, opening up new avenues for developing graph-aware text
generation models. We then proposed a novel graph-aware definition generation
model Graphex that integrates transformer with graph neural network. Our model
outperforms existing text generation models by exploiting the graph structure
of terminologies. We further demonstrated how Graphine can be used to evaluate
pretrained language models, compare graph representation learning methods and
predict sentence granularity. We envision Graphine to be a unique resource for
definition generation and many other NLP tasks in biomedicine.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Distributionally Robust Multilingual Machine Translation. (arXiv:2109.04020v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04020">
<div class="article-summary-box-inner">
<span><p>Multilingual neural machine translation (MNMT) learns to translate multiple
language pairs with a single model, potentially improving both the accuracy and
the memory-efficiency of deployed models. However, the heavy data imbalance
between languages hinders the model from performing uniformly across language
pairs. In this paper, we propose a new learning objective for MNMT based on
distributionally robust optimization, which minimizes the worst-case expected
loss over the set of language pairs. We further show how to practically
optimize this objective for large translation corpora using an iterated best
response scheme, which is both effective and incurs negligible additional
computational cost compared to standard empirical risk minimization. We perform
extensive experiments on three sets of languages from two datasets and show
that our method consistently outperforms strong baseline methods in terms of
average and per-language performance under both many-to-one and one-to-many
translation settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Table-based Fact Verification with Salience-aware Learning. (arXiv:2109.04053v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04053">
<div class="article-summary-box-inner">
<span><p>Tables provide valuable knowledge that can be used to verify textual
statements. While a number of works have considered table-based fact
verification, direct alignments of tabular data with tokens in textual
statements are rarely available. Moreover, training a generalized fact
verification model requires abundant labeled training data. In this paper, we
propose a novel system to address these problems. Inspired by counterfactual
causality, our system identifies token-level salience in the statement with
probing-based salience estimation. Salience estimation allows enhanced learning
of fact verification from two perspectives. From one perspective, our system
conducts masked salient token prediction to enhance the model for alignment and
reasoning between the table and the statement. From the other perspective, our
system applies salience-aware data augmentation to generate a more diverse set
of training instances by replacing non-salient terms. Experimental results on
TabFact show the effective improvement by the proposed salience-aware learning
techniques, leading to the new SOTA performance on the benchmark. Our code is
publicly available at https://github.com/luka-group/Salience-aware-Learning .
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension. (arXiv:2109.04066v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04066">
<div class="article-summary-box-inner">
<span><p>Multi-party multi-turn dialogue comprehension brings unprecedented challenges
on handling the complicated scenarios from multiple speakers and criss-crossed
discourse relationship among speaker-aware utterances. Most existing methods
deal with dialogue contexts as plain texts and pay insufficient attention to
the crucial speaker-aware clues. In this work, we propose an enhanced
speaker-aware model with masking attention and heterogeneous graph networks to
comprehensively capture discourse clues from both sides of speaker property and
speaker-aware relationships. With such comprehensive speaker-aware modeling,
experimental results show that our speaker-aware model helps achieves
state-of-the-art performance on the benchmark dataset Molweni. Case analysis
shows that our model enhances the connections between utterances and their own
speakers and captures the speaker-aware discourse relations, which are critical
for dialogue modeling.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining. (arXiv:2109.04080v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04080">
<div class="article-summary-box-inner">
<span><p>With the rapid increase in the volume of dialogue data from daily life, there
is a growing demand for dialogue summarization. Unfortunately, training a large
summarization model is generally infeasible due to the inadequacy of dialogue
data with annotated summaries. Most existing works for low-resource dialogue
summarization directly pretrain models in other domains, e.g., the news domain,
but they generally neglect the huge difference between dialogues and
conventional articles. To bridge the gap between out-of-domain pretraining and
in-domain fine-tuning, in this work, we propose a multi-source pretraining
paradigm to better leverage the external summary data. Specifically, we exploit
large-scale in-domain non-summary data to separately pretrain the dialogue
encoder and the summary decoder. The combined encoder-decoder model is then
pretrained on the out-of-domain summary data using adversarial critics, aiming
to facilitate domain-agnostic summarization. The experimental results on two
public datasets show that with only limited training data, our approach
achieves competitive performance and generalizes well in different dialogue
scenarios.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Thinking Clearly, Talking Fast: Concept-Guided Non-Autoregressive Generation for Open-Domain Dialogue Systems. (arXiv:2109.04084v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04084">
<div class="article-summary-box-inner">
<span><p>Human dialogue contains evolving concepts, and speakers naturally associate
multiple concepts to compose a response. However, current dialogue models with
the seq2seq framework lack the ability to effectively manage concept
transitions and can hardly introduce multiple concepts to responses in a
sequential decoding manner. To facilitate a controllable and coherent dialogue,
in this work, we devise a concept-guided non-autoregressive model (CG-nAR) for
open-domain dialogue generation. The proposed model comprises a multi-concept
planning module that learns to identify multiple associated concepts from a
concept graph and a customized Insertion Transformer that performs
concept-guided non-autoregressive generation to complete a response. The
experimental results on two public datasets show that CG-nAR can produce
diverse and coherent responses, outperforming state-of-the-art baselines in
both automatic and human evaluations with substantially faster inference speed.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Debiasing Methods in Natural Language Understanding Make Bias More Accessible. (arXiv:2109.04095v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04095">
<div class="article-summary-box-inner">
<span><p>Model robustness to bias is often determined by the generalization on
carefully designed out-of-distribution datasets. Recent debiasing methods in
natural language understanding (NLU) improve performance on such datasets by
pressuring models into making unbiased predictions. An underlying assumption
behind such methods is that this also leads to the discovery of more robust
features in the model's inner representations. We propose a general
probing-based framework that allows for post-hoc interpretation of biases in
language models, and use an information-theoretic approach to measure the
extractability of certain biases from the model's representations. We
experiment with several NLU datasets and known biases, and show that,
counter-intuitively, the more a language model is pushed towards a debiased
regime, the more bias is actually encoded in its inner representations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Three-Stage Learning Framework for Low-Resource Knowledge-Grounded Dialogue Generation. (arXiv:2109.04096v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04096">
<div class="article-summary-box-inner">
<span><p>Neural conversation models have shown great potentials towards generating
fluent and informative responses by introducing external background knowledge.
Nevertheless, it is laborious to construct such knowledge-grounded dialogues,
and existing models usually perform poorly when transfer to new domains with
limited training samples. Therefore, building a knowledge-grounded dialogue
system under the low-resource setting is a still crucial issue. In this paper,
we propose a novel three-stage learning framework based on weakly supervised
learning which benefits from large scale ungrounded dialogues and unstructured
knowledge base. To better cooperate with this framework, we devise a variant of
Transformer with decoupled decoder which facilitates the disentangled learning
of response generation and knowledge incorporation. Evaluation results on two
benchmarks indicate that our approach can outperform other state-of-the-art
methods with less training data, and even in zero-resource scenario, our
approach still performs well.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ARMAN: Pre-training with Semantically Selecting and Reordering of Sentences for Persian Abstractive Summarization. (arXiv:2109.04098v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04098">
<div class="article-summary-box-inner">
<span><p>Abstractive text summarization is one of the areas influenced by the
emergence of pre-trained language models. Current pre-training works in
abstractive summarization give more points to the summaries with more words in
common with the main text and pay less attention to the semantic similarity
between generated sentences and the original document. We propose ARMAN, a
Transformer-based encoder-decoder model pre-trained with three novel objectives
to address this issue. In ARMAN, salient sentences from a document are selected
according to a modified semantic score to be masked and form a pseudo summary.
To summarize more accurately and similar to human writing patterns, we applied
modified sentence reordering. We evaluated our proposed models on six
downstream Persian summarization tasks. Experimental results show that our
proposed model achieves state-of-the-art performance on all six summarization
tasks measured by ROUGE and BERTScore. Our models also outperform prior works
in textual entailment, question paraphrasing, and multiple choice question
answering. Finally, we established a human evaluation and show that using the
semantic score significantly improves summarization results.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TimeTraveler: Reinforcement Learning for Temporal Knowledge Graph Forecasting. (arXiv:2109.04101v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04101">
<div class="article-summary-box-inner">
<span><p>Temporal knowledge graph (TKG) reasoning is a crucial task that has gained
increasing research interest in recent years. Most existing methods focus on
reasoning at past timestamps to complete the missing facts, and there are only
a few works of reasoning on known TKGs to forecast future facts. Compared with
the completion task, the forecasting task is more difficult that faces two main
challenges: (1) how to effectively model the time information to handle future
timestamps? (2) how to make inductive inference to handle previously unseen
entities that emerge over time? To address these challenges, we propose the
first reinforcement learning method for forecasting. Specifically, the agent
travels on historical knowledge graph snapshots to search for the answer. Our
method defines a relative time encoding function to capture the timespan
information, and we design a novel time-shaped reward based on Dirichlet
distribution to guide the model learning. Furthermore, we propose a novel
representation method for unseen entities to improve the inductive inference
ability of the model. We evaluate our method for this link prediction task at
future timestamps. Extensive experiments on four benchmark datasets demonstrate
substantial performance improvement meanwhile with higher explainability, less
calculation, and fewer parameters when compared with existing state-of-the-art
methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MapRE: An Effective Semantic Mapping Approach for Low-resource Relation Extraction. (arXiv:2109.04108v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04108">
<div class="article-summary-box-inner">
<span><p>Neural relation extraction models have shown promising results in recent
years; however, the model performance drops dramatically given only a few
training samples. Recent works try leveraging the advance in few-shot learning
to solve the low resource problem, where they train label-agnostic models to
directly compare the semantic similarities among context sentences in the
embedding space. However, the label-aware information, i.e., the relation label
that contains the semantic knowledge of the relation itself, is often neglected
for prediction. In this work, we propose a framework considering both
label-agnostic and label-aware semantic mapping information for low resource
relation extraction. We show that incorporating the above two types of mapping
information in both pretraining and fine-tuning can significantly improve the
model performance on low-resource relation extraction tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04114">
<div class="article-summary-box-inner">
<span><p>We apply imitation learning (IL) to tackle the NMT exposure bias problem with
error-correcting oracles, and evaluate an SMT lattice-based oracle which,
despite its excellent performance in an unconstrained oracle translation task,
turned out to be too pruned and idiosyncratic to serve as the oracle for IL.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Word-Level Coreference Resolution. (arXiv:2109.04127v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04127">
<div class="article-summary-box-inner">
<span><p>Recent coreference resolution models rely heavily on span representations to
find coreference links between word spans. As the number of spans is $O(n^2)$
in the length of text and the number of potential links is $O(n^4)$, various
pruning techniques are necessary to make this approach computationally
feasible. We propose instead to consider coreference links between individual
words rather than word spans and then reconstruct the word spans. This reduces
the complexity of the coreference model to $O(n^2)$ and allows it to consider
all potential mentions without pruning any of them out. We also demonstrate
that, with these changes, SpanBERT for coreference resolution will be
significantly outperformed by RoBERTa. While being highly efficient, our model
performs competitively with recent coreference resolution systems on the
OntoNotes benchmark.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fusing task-oriented and open-domain dialogues in conversational agents. (arXiv:2109.04137v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04137">
<div class="article-summary-box-inner">
<span><p>The goal of building intelligent dialogue systems has largely been
\textit{separately} pursued under two paradigms: task-oriented dialogue (TOD)
systems, which perform goal-oriented functions, and open-domain dialogue (ODD)
systems, which focus on non-goal-oriented chitchat. The two dialogue modes can
potentially be intertwined together seamlessly in the same conversation, as
easily done by a friendly human assistant. Such ability is desirable in
conversational agents, as the integration makes them more accessible and
useful. Our paper addresses this problem of fusing TODs and ODDs in multi-turn
dialogues. Based on the popular TOD dataset MultiWOZ, we build a new dataset
FusedChat, by rewriting the existing TOD turns and adding new ODD turns. This
procedure constructs conversation sessions containing exchanges from both
dialogue modes. It features inter-mode contextual dependency, i.e., the
dialogue turns from the two modes depend on each other. Rich dependency
patterns including co-reference and ellipsis are features. The new dataset,
with 60k new human-written ODD turns and 5k re-written TOD turns, offers a
benchmark to test a dialogue model's ability to perform inter-mode
conversations. This is a more challenging task since the model has to determine
the appropriate dialogue mode and generate the response based on the inter-mode
context. But such models would better mimic human-level conversation
capabilities. We evaluate baseline models on this task, including
\textit{classification-based} two-stage models and \textit{two-in-one} fused
models. We publicly release FusedChat and the baselines to propel future work
on inter-mode dialogue systems https://github.com/tomyoung903/FusedChat.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Avoiding Inference Heuristics in Few-shot Prompt-based Finetuning. (arXiv:2109.04144v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04144">
<div class="article-summary-box-inner">
<span><p>Recent prompt-based approaches allow pretrained language models to achieve
strong performances on few-shot finetuning by reformulating downstream tasks as
a language modeling problem. In this work, we demonstrate that, despite its
advantages on low data regimes, finetuned prompt-based models for sentence pair
classification tasks still suffer from a common pitfall of adopting inference
heuristics based on lexical overlap, e.g., models incorrectly assuming a
sentence pair is of the same meaning because they consist of the same set of
words. Interestingly, we find that this particular inference heuristic is
significantly less present in the zero-shot evaluation of the prompt-based
model, indicating how finetuning can be destructive to useful knowledge learned
during the pretraining. We then show that adding a regularization that
preserves pretraining weights is effective in mitigating this destructive
tendency of few-shot finetuning. Our evaluation on three datasets demonstrates
promising improvements on the three corresponding challenge datasets used to
diagnose the inference heuristics.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Lexico-semantic and affective modelling of Spanish poetry: A semi-supervised learning approach. (arXiv:2109.04152v1 [cs.AI])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04152">
<div class="article-summary-box-inner">
<span><p>Text classification tasks have improved substantially during the last years
by the usage of transformers. However, the majority of researches focus on
prose texts, with poetry receiving less attention, specially for Spanish
language. In this paper, we propose a semi-supervised learning approach for
inferring 21 psychological categories evoked by a corpus of 4572 sonnets, along
with 10 affective and lexico-semantic multiclass ones. The subset of poems used
for training an evaluation includes 270 sonnets. With our approach, we achieve
an AUC beyond 0.7 for 76% of the psychological categories, and an AUC over 0.65
for 60% on the multiclass ones. The sonnets are modelled using transformers,
through sentence embeddings, along with lexico-semantic and affective features,
obtained by using external lexicons. Consequently, we see that this approach
provides an AUC increase of up to 0.12, as opposed to using transformers alone.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Efficient Nearest Neighbor Language Models. (arXiv:2109.04212v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04212">
<div class="article-summary-box-inner">
<span><p>Non-parametric neural language models (NLMs) learn predictive distributions
of text utilizing an external datastore, which allows them to learn through
explicitly memorizing the training datapoints. While effective, these models
often require retrieval from a large datastore at test time, significantly
increasing the inference overhead and thus limiting the deployment of
non-parametric NLMs in practical applications. In this paper, we take the
recently proposed $k$-nearest neighbors language model (Khandelwal et al.,
2019) as an example, exploring methods to improve its efficiency along various
dimensions. Experiments on the standard WikiText-103 benchmark and
domain-adaptation datasets show that our methods are able to achieve up to a 6x
speed-up in inference speed while retaining comparable performance. The
empirical analysis we present may provide guidelines for future research
seeking to develop or deploy more efficient non-parametric NLMs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KELM: Knowledge Enhanced Pre-Trained Language Representations with Message Passing on Hierarchical Relational Graphs. (arXiv:2109.04223v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04223">
<div class="article-summary-box-inner">
<span><p>Incorporating factual knowledge into pre-trained language models (PLM) such
as BERT is an emerging trend in recent NLP studies. However, most of the
existing methods combine the external knowledge integration module with a
modified pre-training loss and re-implement the pre-training process on the
large-scale corpus. Re-pretraining these models is usually resource-consuming,
and difficult to adapt to another domain with a different knowledge graph (KG).
Besides, those works either cannot embed knowledge context dynamically
according to textual context or struggle with the knowledge ambiguity issue. In
this paper, we propose a novel knowledge-aware language model framework based
on fine-tuning process, which equips PLM with a unified knowledge-enhanced text
graph that contains both text and multi-relational sub-graphs extracted from
KG. We design a hierarchical relational-graph-based message passing mechanism,
which can allow the representations of injected KG and text to mutually update
each other and can dynamically select ambiguous mentioned entities that share
the same text. Our empirical results show that our model can efficiently
incorporate world knowledge from KGs into existing language models such as
BERT, and achieve significant improvement on the machine reading comprehension
(MRC) task compared with other knowledge-enhanced models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MetaXT: Meta Cross-Task Transfer between Disparate Label Spaces. (arXiv:2109.04240v1 [cs.LG])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04240">
<div class="article-summary-box-inner">
<span><p>Albeit the universal representational power of pre-trained language models,
adapting them onto a specific NLP task still requires a considerably large
amount of labeled data. Effective task fine-tuning meets challenges when only a
few labeled examples are present for the task. In this paper, we aim to the
address of the problem of few shot task learning by exploiting and transferring
from a different task which admits a related but disparate label space.
Specifically, we devise a label transfer network (LTN) to transform the labels
from source task to the target task of interest for training. Both the LTN and
the model for task prediction are learned via a bi-level optimization
framework, which we term as MetaXT. MetaXT offers a principled solution to best
adapt a pre-trained language model to the target task by transferring knowledge
from the source task. Empirical evaluations on cross-task transfer settings for
four NLP tasks, from two different types of label space disparities,
demonstrate the effectiveness of MetaXT, especially when the labeled data in
the target task is limited.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cartography Active Learning. (arXiv:2109.04282v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04282">
<div class="article-summary-box-inner">
<span><p>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)
algorithm that exploits the behavior of the model on individual instances
during training as a proxy to find the most informative instances for labeling.
CAL is inspired by data maps, which were recently proposed to derive insights
into dataset quality (Swayamdipta et al., 2020). We compare our method on
popular text classification tasks to commonly used AL strategies, which instead
rely on post-training behavior. We demonstrate that CAL is competitive to other
common AL methods, showing that training dynamics derived from small seed data
can be successfully used for AL. We provide insights into our new AL method by
analyzing batch-level statistics utilizing the data maps. Our results further
show that CAL results in a more data-efficient learning strategy, achieving
comparable or better results with considerably less training data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Generalised Unsupervised Domain Adaptation of Neural Machine Translation with Cross-Lingual Data Selection. (arXiv:2109.04292v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04292">
<div class="article-summary-box-inner">
<span><p>This paper considers the unsupervised domain adaptation problem for neural
machine translation (NMT), where we assume the access to only monolingual text
in either the source or target language in the new domain. We propose a
cross-lingual data selection method to extract in-domain sentences in the
missing language side from a large generic monolingual corpus. Our proposed
method trains an adaptive layer on top of multilingual BERT by contrastive
learning to align the representation between the source and target language.
This then enables the transferability of the domain classifier between the
languages in a zero-shot manner. Once the in-domain data is detected by the
classifier, the NMT model is then adapted to the new domain by jointly learning
translation and domain discrimination tasks. We evaluate our cross-lingual data
selection method on NMT across five diverse domains in three language pairs, as
well as a real-world scenario of translation for COVID-19. The results show
that our proposed method outperforms other selection baselines up to +1.5 BLEU
score.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">MATE: Multi-view Attention for Table Transformer Efficiency. (arXiv:2109.04312v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04312">
<div class="article-summary-box-inner">
<span><p>This work presents a sparse-attention Transformer architecture for modeling
documents that contain large tables. Tables are ubiquitous on the web, and are
rich in information. However, more than 20% of relational tables on the web
have 20 or more rows (Cafarella et al., 2008), and these large tables present a
challenge for current Transformer models, which are typically limited to 512
tokens. Here we propose MATE, a novel Transformer architecture designed to
model the structure of web tables. MATE uses sparse attention in a way that
allows heads to efficiently attend to either rows or columns in a table. This
architecture scales linearly with respect to speed and memory, and can handle
documents containing more than 8000 tokens with current accelerators. MATE also
has a more appropriate inductive bias for tabular data, and sets a new
state-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,
2020b), a dataset that involves large documents containing tables, we improve
the best prior result by 19 points.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Variational Latent-State GPT for Semi-supervised Task-Oriented Dialog Systems. (arXiv:2109.04314v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04314">
<div class="article-summary-box-inner">
<span><p>Recently, two approaches, fine-tuning large pre-trained language models and
variational training, have attracted significant interests, separately, for
semi-supervised end-to-end task-oriented dialog (TOD) systems. In this paper,
we propose Variational Latent-State GPT model (VLS-GPT), which is the first to
combine the strengths of the two approaches. Among many options of models, we
propose the generative model and the inference model for variational learning
of the end-to-end TOD system, both as auto-regressive language models based on
GPT-2, which can be further trained over a mix of labeled and unlabeled dialog
data in a semi-supervised manner. We develop the strategy of
sampling-then-forward-computation, which successfully overcomes the memory
explosion issue of using GPT in variational learning and speeds up training.
Semi-supervised TOD experiments are conducted on two benchmark multi-domain
datasets of different languages - MultiWOZ2.1 and CrossWOZ. VLS-GPT is shown to
significantly outperform both supervised-only and semi-supervised baselines.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Translate & Fill: Improving Zero-Shot Multilingual Semantic Parsing with Synthetic Data. (arXiv:2109.04319v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04319">
<div class="article-summary-box-inner">
<span><p>While multilingual pretrained language models (LMs) fine-tuned on a single
language have shown substantial cross-lingual task transfer capabilities, there
is still a wide performance gap in semantic parsing tasks when target language
supervision is available. In this paper, we propose a novel Translate-and-Fill
(TaF) method to produce silver training data for a multilingual semantic
parser. This method simplifies the popular Translate-Align-Project (TAP)
pipeline and consists of a sequence-to-sequence filler model that constructs a
full parse conditioned on an utterance and a view of the same parse. Our filler
is trained on English data only but can accurately complete instances in other
languages (i.e., translations of the English training utterances), in a
zero-shot fashion. Experimental results on three multilingual semantic parsing
datasets show that data augmentation with TaF reaches accuracies competitive
with similar systems which rely on traditional alignment techniques.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Smoothed Contrastive Learning for Unsupervised Sentence Embedding. (arXiv:2109.04321v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04321">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has been gradually applied to learn high-quality
unsupervised sentence embedding. Among the previous un-supervised methods, the
latest state-of-the-art method, as far as we know, is unsupervised SimCSE
(unsup-SimCSE). Unsup-SimCSE uses the InfoNCE1loss function in the training
stage by pulling semantically similar sentences together and pushing apart
dis-similar ones.Theoretically, we expect to use larger batches in unsup-SimCSE
to get more adequate comparisons among samples and avoid overfitting. However,
increasing the batch size does not always lead to improvements, but instead
even lead to performance degradation when the batch size exceeds a threshold.
Through statistical observation, we find that this is probably due to the
introduction of low-confidence negative pairs after in-creasing the batch size.
To alleviate this problem, we introduce a simple smoothing strategy upon the
InfoNCE loss function, termedGaussian Smoothing InfoNCE
(GS-InfoNCE).Specifically, we add random Gaussian noise vectors as negative
samples, which act asa smoothing of the negative sample space.Though being
simple, the proposed smooth-ing strategy brings substantial improvements to
unsup-SimCSE. We evaluate GS-InfoNCEon the standard semantic text similarity
(STS)task. GS-InfoNCE outperforms the state-of-the-art unsup-SimCSE by an
average Spear-man correlation of 1.38%, 0.72%, 1.17% and0.28% on the base of
BERT-base, BERT-large,RoBERTa-base and RoBERTa-large, respectively.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning Opinion Summarizers by Selecting Informative Reviews. (arXiv:2109.04325v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04325">
<div class="article-summary-box-inner">
<span><p>Opinion summarization has been traditionally approached with unsupervised,
weakly-supervised and few-shot learning techniques. In this work, we collect a
large dataset of summaries paired with user reviews for over 31,000 products,
enabling supervised training. However, the number of reviews per product is
large (320 on average), making summarization - and especially training a
summarizer - impractical. Moreover, the content of many reviews is not
reflected in the human-written summaries, and, thus, the summarizer trained on
random review subsets hallucinates. In order to deal with both of these
challenges, we formulate the task as jointly learning to select informative
subsets of reviews and summarizing the opinions expressed in these subsets. The
choice of the review subset is treated as a latent variable, predicted by a
small and simple selector. The subset is then fed into a more powerful
summarizer. For joint training, we use amortized variational inference and
policy gradient methods. Our experiments demonstrate the importance of
selecting informative reviews resulting in improved quality of summaries and
reduced hallucinations.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">PPT: Pre-trained Prompt Tuning for Few-shot Learning. (arXiv:2109.04332v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04332">
<div class="article-summary-box-inner">
<span><p>Prompts for pre-trained language models (PLMs) have shown remarkable
performance by bridging the gap between pre-training tasks and various
downstream tasks. Among these methods, prompt tuning, which freezes PLMs and
only tunes soft prompts, provides an efficient and effective solution for
adapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to
be fully explored. In our pilot experiments, we find that prompt tuning
performs comparably with conventional full-model fine-tuning when downstream
data are sufficient, whereas it performs much worse under few-shot learning
settings, which may hinder the application of prompt tuning in practice. We
attribute this low performance to the manner of initializing soft prompts.
Therefore, in this work, we propose to pre-train prompts by adding soft prompts
into the pre-training stage to obtain a better initialization. We name this
Pre-trained Prompt Tuning framework "PPT". To ensure the generalization of PPT,
we formulate similar classification tasks into a unified task form and
pre-train soft prompts for this unified task. Extensive experiments show that
tuning pre-trained prompts for downstream tasks can reach or even outperform
full-model fine-tuning under both full-data and few-shot settings. Our approach
is effective and efficient for using large-scale PLMs in practice.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Uncertainty Measures in Neural Belief Tracking and the Effects on Dialogue Policy Performance. (arXiv:2109.04349v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04349">
<div class="article-summary-box-inner">
<span><p>The ability to identify and resolve uncertainty is crucial for the robustness
of a dialogue system. Indeed, this has been confirmed empirically on systems
that utilise Bayesian approaches to dialogue belief tracking. However, such
systems consider only confidence estimates and have difficulty scaling to more
complex settings. Neural dialogue systems, on the other hand, rarely take
uncertainties into account. They are therefore overconfident in their decisions
and less robust. Moreover, the performance of the tracking task is often
evaluated in isolation, without consideration of its effect on the downstream
policy optimisation. We propose the use of different uncertainty measures in
neural belief tracking. The effects of these measures on the downstream task of
policy optimisation are evaluated by adding selected measures of uncertainty to
the feature space of the policy and training policies through interaction with
a user simulator. Both human and simulated user results show that incorporating
these measures leads to improvements both of the performance and of the
robustness of the downstream dialogue policy. This highlights the importance of
developing neural dialogue belief trackers that take uncertainty into account.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multi-granularity Textual Adversarial Attack with Behavior Cloning. (arXiv:2109.04367v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04367">
<div class="article-summary-box-inner">
<span><p>Recently, the textual adversarial attack models become increasingly popular
due to their successful in estimating the robustness of NLP models. However,
existing works have obvious deficiencies. (1) They usually consider only a
single granularity of modification strategies (e.g. word-level or
sentence-level), which is insufficient to explore the holistic textual space
for generation; (2) They need to query victim models hundreds of times to make
a successful attack, which is highly inefficient in practice. To address such
problems, in this paper we propose MAYA, a Multi-grAnularitY Attack model to
effectively generate high-quality adversarial samples with fewer queries to
victim models. Furthermore, we propose a reinforcement-learning based method to
train a multi-granularity attack agent through behavior cloning with the expert
knowledge from our MAYA algorithm to further reduce the query times.
Additionally, we also adapt the agent to attack black-box models that only
output labels without confidence scores. We conduct comprehensive experiments
to evaluate our attack models by attacking BiLSTM, BERT and RoBERTa in two
different black-box attack settings and three benchmark datasets. Experimental
results show that our models achieve overall better attacking performance and
produce more fluent and grammatical adversarial samples compared to baseline
models. Besides, our adversarial attack agent significantly reduces the query
times in both attack settings. Our codes are released at
https://github.com/Yangyi-Chen/MAYA.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Tracking Turbulence Through Financial News During COVID-19. (arXiv:2109.04369v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04369">
<div class="article-summary-box-inner">
<span><p>Grave human toll notwithstanding, the COVID-19 pandemic created uniquely
unstable conditions in financial markets. In this work we uncover and discuss
relationships involving sentiment in financial publications during the 2020
pandemic-motivated U.S. financial crash. First, we introduce a set of expert
annotations of financial sentiment for articles from major American financial
news publishers. After an exploratory data analysis, we then describe a
CNN-based architecture to address the task of predicting financial sentiment in
this anomalous, tumultuous setting. Our best performing model achieves a
maximum weighted F1 score of 0.746, establishing a strong performance
benchmark. Using predictions from our top performing model, we close by
conducting a statistical correlation study with real stock market data, finding
interesting and strong relationships between financial news and the S\&amp;P 500
index, trading volume, market volatility, and different single-factor ETFs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ESimCSE: Enhanced Sample Building Method for Contrastive Learning of Unsupervised Sentence Embedding. (arXiv:2109.04380v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04380">
<div class="article-summary-box-inner">
<span><p>Contrastive learning has been attracting much attention for learning
unsupervised sentence embeddings. The current state-of-the-art unsupervised
method is the unsupervised SimCSE (unsup-SimCSE). Unsup-SimCSE takes dropout as
a minimal data augmentation method, and passes the same input sentence to a
pre-trained Transformer encoder (with dropout turned on) twice to obtain the
two corresponding embeddings to build a positive pair. As the length
information of a sentence will generally be encoded into the sentence
embeddings due to the usage of position embedding in Transformer, each positive
pair in unsup-SimCSE actually contains the same length information. And thus
unsup-SimCSE trained with these positive pairs is probably biased, which would
tend to consider that sentences of the same or similar length are more similar
in semantics. Through statistical observations, we find that unsup-SimCSE does
have such a problem. To alleviate it, we apply a simple repetition operation to
modify the input sentence, and then pass the input sentence and its modified
counterpart to the pre-trained Transformer encoder, respectively, to get the
positive pair. Additionally, we draw inspiration from the community of computer
vision and introduce a momentum contrast, enlarging the number of negative
pairs without additional calculations. The proposed two modifications are
applied on positive and negative pairs separately, and build a new sentence
embedding method, termed Enhanced Unsup-SimCSE (ESimCSE). We evaluate the
proposed ESimCSE on several benchmark datasets w.r.t the semantic text
similarity (STS) task. Experimental results show that ESimCSE outperforms the
state-of-the-art unsup-SimCSE by an average Spearman correlation of 2.02% on
BERT-base.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrasting Human- and Machine-Generated Word-Level Adversarial Examples for Text Classification. (arXiv:2109.04385v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04385">
<div class="article-summary-box-inner">
<span><p>Research shows that natural language processing models are generally
considered to be vulnerable to adversarial attacks; but recent work has drawn
attention to the issue of validating these adversarial inputs against certain
criteria (e.g., the preservation of semantics and grammaticality). Enforcing
constraints to uphold such criteria may render attacks unsuccessful, raising
the question of whether valid attacks are actually feasible. In this work, we
investigate this through the lens of human language ability. We report on
crowdsourcing studies in which we task humans with iteratively modifying words
in an input text, while receiving immediate model feedback, with the aim of
causing a sentiment classification model to misclassify the example. Our
findings suggest that humans are capable of generating a substantial amount of
adversarial examples using semantics-preserving word substitutions. We analyze
how human-generated adversarial examples compare to the recently proposed
TextFooler, Genetic, BAE and SememePSO attack algorithms on the dimensions
naturalness, preservation of sentiment, grammaticality and substitution rate.
Our findings suggest that human-generated adversarial examples are not more
able than the best algorithms to generate natural-reading, sentiment-preserving
examples, though they do so by being much more computationally efficient.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04400">
<div class="article-summary-box-inner">
<span><p>In cross-lingual text classification, it is required that task-specific
training data in high-resource source languages are available, where the task
is identical to that of a low-resource target language. However, collecting
such training data can be infeasible because of the labeling cost, task
characteristics, and privacy concerns. This paper proposes an alternative
solution that uses only task-independent word embeddings of high-resource
languages and bilingual dictionaries. First, we construct a dictionary-based
heterogeneous graph (DHG) from bilingual dictionaries. This opens the
possibility to use graph neural networks for cross-lingual transfer. The
remaining challenge is the heterogeneity of DHG because multiple languages are
considered. To address this challenge, we propose dictionary-based
heterogeneous graph neural network (DHGNet) that effectively handles the
heterogeneity of DHG by two-step aggregations, which are word-level and
language-level aggregations. Experimental results demonstrate that our method
outperforms pretrained models even though it does not access to large corpora.
Furthermore, it can perform well even though dictionaries contain many
incorrect translations. Its robustness allows the usage of a wider range of
dictionaries such as an automatically constructed dictionary and crowdsourced
dictionary, which are convenient for real-world applications.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">All Bark and No Bite: Rogue Dimensions in Transformer Language Models Obscure Representational Quality. (arXiv:2109.04404v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04404">
<div class="article-summary-box-inner">
<span><p>Similarity measures are a vital tool for understanding how language models
represent and process language. Standard representational similarity measures
such as cosine similarity and Euclidean distance have been successfully used in
static word embedding models to understand how words cluster in semantic space.
Recently, these measures have been applied to embeddings from contextualized
models such as BERT and GPT-2. In this work, we call into question the
informativity of such measures for contextualized language models. We find that
a small number of rogue dimensions, often just 1-3, dominate these measures.
Moreover, we find a striking mismatch between the dimensions that dominate
similarity measures and those which are important to the behavior of the model.
We show that simple postprocessing techniques such as standardization are able
to correct for rogue dimensions and reveal underlying representational quality.
We argue that accounting for rogue dimensions is essential for any
similarity-based analysis of contextual language models.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Learning from Uneven Training Data: Unlabeled, Single Label, and Multiple Labels. (arXiv:2109.04408v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04408">
<div class="article-summary-box-inner">
<span><p>Training NLP systems typically assumes access to annotated data that has a
single human label per example. Given imperfect labeling from annotators and
inherent ambiguity of language, we hypothesize that single label is not
sufficient to learn the spectrum of language interpretation. We explore new
label annotation distribution schemes, assigning multiple labels per example
for a small subset of training examples. Introducing such multi label examples
at the cost of annotating fewer examples brings clear gains on natural language
inference task and entity typing task, even when we simply first train with a
single label data and then fine tune with multi label examples. Extending a
MixUp data augmentation framework, we propose a learning algorithm that can
learn from uneven training examples (with zero, one, or multiple labels). This
algorithm efficiently combines signals from uneven training data and brings
additional gains in low annotation budget and cross domain settings. Together,
our method achieves consistent gains in both accuracy and label distribution
metrics in two tasks, suggesting training with uneven training data can be
beneficial for many NLP tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Non-autoregressive End-to-end Speech Translation with Parallel Autoregressive Rescoring. (arXiv:2109.04411v1 [eess.AS])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04411">
<div class="article-summary-box-inner">
<span><p>This article describes an efficient end-to-end speech translation (E2E-ST)
framework based on non-autoregressive (NAR) models. End-to-end speech
translation models have several advantages over traditional cascade systems
such as inference latency reduction. However, conventional AR decoding methods
are not fast enough because each token is generated incrementally. NAR models,
however, can accelerate the decoding speed by generating multiple tokens in
parallel on the basis of the token-wise conditional independence assumption. We
propose a unified NAR E2E-ST framework called Orthros, which has an NAR decoder
and an auxiliary shallow AR decoder on top of the shared encoder. The auxiliary
shallow AR decoder selects the best hypothesis by rescoring multiple candidates
generated from the NAR decoder in parallel (parallel AR rescoring). We adopt
conditional masked language model (CMLM) and a connectionist temporal
classification (CTC)-based model as NAR decoders for Orthros, referred to as
Orthros-CMLM and Orthros-CTC, respectively. We also propose two training
methods to enhance the CMLM decoder. Experimental evaluations on three
benchmark datasets with six language directions demonstrated that Orthros
achieved large improvements in translation quality with a very small overhead
compared with the baseline NAR model. Moreover, the Conformer encoder
architecture enabled large quality improvements, especially for CTC-based
models. Orthros-CTC with the Conformer encoder increased decoding speed by
3.63x on CPU with translation quality comparable to that of an AR model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">AStitchInLanguageModels: Dataset and Methods for the Exploration of Idiomaticity in Pre-Trained Language Models. (arXiv:2109.04413v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04413">
<div class="article-summary-box-inner">
<span><p>Despite their success in a variety of NLP tasks, pre-trained language models,
due to their heavy reliance on compositionality, fail in effectively capturing
the meanings of multiword expressions (MWEs), especially idioms. Therefore,
datasets and methods to improve the representation of MWEs are urgently needed.
Existing datasets are limited to providing the degree of idiomaticity of
expressions along with the literal and, where applicable, (a single)
non-literal interpretation of MWEs. This work presents a novel dataset of
naturally occurring sentences containing MWEs manually classified into a
fine-grained set of meanings, spanning both English and Portuguese. We use this
dataset in two tasks designed to test i) a language model's ability to detect
idiom usage, and ii) the effectiveness of a language model in generating
representations of sentences containing idioms. Our experiments demonstrate
that, on the task of detecting idiomatic usage, these models perform reasonably
well in the one-shot and few-shot scenarios, but that there is significant
scope for improvement in the zero-shot scenario. On the task of representing
idiomaticity, we find that pre-training is not always effective, while
fine-tuning could provide a sample efficient method of learning representations
of sentences containing MWEs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">TxT: Crossmodal End-to-End Learning with Transformers. (arXiv:2109.04422v1 [cs.CV])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04422">
<div class="article-summary-box-inner">
<span><p>Reasoning over multiple modalities, e.g. in Visual Question Answering (VQA),
requires an alignment of semantic concepts across domains. Despite the
widespread success of end-to-end learning, today's multimodal pipelines by and
large leverage pre-extracted, fixed features from object detectors, typically
Faster R-CNN, as representations of the visual world. The obvious downside is
that the visual representation is not specifically tuned to the multimodal task
at hand. At the same time, while transformer-based object detectors have gained
popularity, they have not been employed in today's multimodal pipelines. We
address both shortcomings with TxT, a transformer-based crossmodal pipeline
that enables fine-tuning both language and visual components on the downstream
task in a fully end-to-end manner. We overcome existing limitations of
transformer-based detectors for multimodal reasoning regarding the integration
of global context and their scalability. Our transformer-based multimodal model
achieves considerable gains from end-to-end learning for multimodal question
answering.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">HintedBT: Augmenting Back-Translation with Quality and Transliteration Hints. (arXiv:2109.04443v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04443">
<div class="article-summary-box-inner">
<span><p>Back-translation (BT) of target monolingual corpora is a widely used data
augmentation strategy for neural machine translation (NMT), especially for
low-resource language pairs. To improve effectiveness of the available BT data,
we introduce HintedBT -- a family of techniques which provides hints (through
tags) to the encoder and decoder. First, we propose a novel method of using
both high and low quality BT data by providing hints (as source tags on the
encoder) to the model about the quality of each source-target pair. We don't
filter out low quality data but instead show that these hints enable the model
to learn effectively from noisy data. Second, we address the problem of
predicting whether a source token needs to be translated or transliterated to
the target language, which is common in cross-script translation tasks (i.e.,
where source and target do not share the written script). For such cases, we
propose training the model with additional hints (as target tags on the
decoder) that provide information about the operation required on the source
(translation or both translation and transliteration). We conduct experiments
and detailed analyses on standard WMT benchmarks for three cross-script
low/medium-resource language pairs: {Hindi,Gujarati,Tamil}-to-English. Our
methods compare favorably with five strong and well established baselines. We
show that using these hints, both separately and together, significantly
improves translation quality and leads to state-of-the-art performance in all
three language pairs in corresponding bilingual settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers. (arXiv:2109.04448v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04448">
<div class="article-summary-box-inner">
<span><p>Pretrained vision-and-language BERTs aim to learn representations that
combine information from both modalities. We propose a diagnostic method based
on cross-modal input ablation to assess the extent to which these models
actually integrate cross-modal information. This method involves ablating
inputs from one modality, either entirely or selectively based on cross-modal
grounding alignments, and evaluating the model prediction performance on the
other modality. Model performance is measured by modality-specific tasks that
mirror the model pretraining objectives (e.g. masked language modelling for
text). Models that have learned to construct cross-modal representations using
both modalities are expected to perform worse when inputs are missing from a
modality. We find that recently proposed models have much greater relative
difficulty predicting text when visual information is ablated, compared to
predicting visual object categories when text is ablated, indicating that these
models are not symmetrically cross-modal.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Analysis of Language Change in Collaborative Instruction Following. (arXiv:2109.04452v1 [cs.CL])</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.04452">
<div class="article-summary-box-inner">
<span><p>We analyze language change over time in a collaborative, goal-oriented
instructional task, where utility-maximizing participants form conventions and
increase their expertise. Prior work studied such scenarios mostly in the
context of reference games, and consistently found that language complexity is
reduced along multiple dimensions, such as utterance length, as conventions are
formed. In contrast, we find that, given the ability to increase instruction
utility, instructors increase language complexity along these previously
studied dimensions to better collaborate with increasingly skilled instruction
followers.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization. (arXiv:1911.03437v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/1911.03437">
<div class="article-summary-box-inner">
<span><p>Transfer learning has fundamentally changed the landscape of natural language
processing (NLP) research. Many existing state-of-the-art models are first
pre-trained on a large text corpus and then fine-tuned on downstream tasks.
However, due to limited data resources from downstream tasks and the extremely
large capacity of pre-trained models, aggressive fine-tuning often causes the
adapted model to overfit the data of downstream tasks and forget the knowledge
of the pre-trained model. To address the above issue in a more principled
manner, we propose a new computational framework for robust and efficient
fine-tuning for pre-trained language models. Specifically, our proposed
framework contains two important ingredients: 1. Smoothness-inducing
regularization, which effectively manages the capacity of the model; 2. Bregman
proximal point optimization, which is a class of trust-region methods and can
prevent knowledge forgetting. Our experiments demonstrate that our proposed
method achieves the state-of-the-art performance on multiple NLP benchmarks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">On the Linguistic Capacity of Real-Time Counter Automata. (arXiv:2004.06866v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2004.06866">
<div class="article-summary-box-inner">
<span><p>Counter machines have achieved a newfound relevance to the field of natural
language processing (NLP): recent work suggests some strong-performing
recurrent neural networks utilize their memory as counters. Thus, one potential
way to understand the success of these networks is to revisit the theory of
counter computation. Therefore, we study the abilities of real-time counter
machines as formal grammars, focusing on formal properties that are relevant
for NLP models. We first show that several variants of the counter machine
converge to express the same class of formal languages. We also prove that
counter languages are closed under complement, union, intersection, and many
other common set operations. Next, we show that counter machines cannot
evaluate boolean expressions, even though they can weakly validate their
syntax. This has implications for the interpretability and evaluation of neural
network systems: successfully matching syntactic patterns does not guarantee
that counter memory accurately encodes compositional semantics. Finally, we
consider whether counter languages are semilinear. This work makes general
contributions to the theory of formal languages that are of potential interest
for understanding recurrent neural networks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Time-Aware Evidence Ranking for Fact-Checking. (arXiv:2009.06402v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.06402">
<div class="article-summary-box-inner">
<span><p>Truth can vary over time. Fact-checking decisions on claim veracity should
therefore take into account temporal information of both the claim and
supporting or refuting evidence. In this work, we investigate the hypothesis
that the timestamp of a Web page is crucial to how it should be ranked for a
given claim. We delineate four temporal ranking methods that constrain evidence
ranking differently and simulate hypothesis-specific evidence rankings given
the evidence timestamps as gold standard. Evidence ranking in three
fact-checking models is ultimately optimized using a learning-to-rank loss
function. Our study reveals that time-aware evidence ranking not only surpasses
relevance assumptions based purely on semantic similarity or position in a
search results list, but also improves veracity predictions of time-sensitive
claims in particular.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Mention Detector-Linker Interaction in Neural Coreference Resolution. (arXiv:2009.09363v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2009.09363">
<div class="article-summary-box-inner">
<span><p>Despite significant recent progress in coreference resolution, the quality of
current state-of-the-art systems still considerably trails behind human-level
performance. Using the CoNLL-2012 and PreCo datasets, we dissect the best
instantiation of the mainstream end-to-end coreference resolution model that
underlies most current best-performing coreference systems, and empirically
analyze the behavior of its two components: mention detector and mention
linker. While the detector traditionally focuses heavily on recall as a design
decision, we demonstrate the importance of precision, calling for their
balance. However, we point out the difficulty in building a precise detector
due to its inability to make important anaphoricity decisions. We also
highlight the enormous room for improving the linker and show that the rest of
its errors mainly involve pronoun resolution. We propose promising next steps
and hope our findings will help future research in coreference resolution.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Contrastive Cross-Modal Pre-Training: A General Strategy for Small Sample Medical Imaging. (arXiv:2010.03060v5 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.03060">
<div class="article-summary-box-inner">
<span><p>A key challenge in training neural networks for a given medical imaging task
is often the difficulty of obtaining a sufficient number of manually labeled
examples. In contrast, textual imaging reports, which are often readily
available in medical records, contain rich but unstructured interpretations
written by experts as part of standard clinical practice. We propose using
these textual reports as a form of weak supervision to improve the image
interpretation performance of a neural network without requiring additional
manually labeled examples. We use an image-text matching task to train a
feature extractor and then fine-tune it in a transfer learning setting for a
supervised task using a small labeled dataset. The end result is a neural
network that automatically interprets imagery without requiring textual reports
during inference. This approach can be applied to any task for which text-image
pairs are readily available. We evaluate our method on three classification
tasks and find consistent performance improvements, reducing the need for
labeled data by 67%-98%.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mathematical Word Problem Generation from Commonsense Knowledge Graph and Equations. (arXiv:2010.06196v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.06196">
<div class="article-summary-box-inner">
<span><p>There is an increasing interest in the use of mathematical word problem (MWP)
generation in educational assessment. Different from standard natural question
generation, MWP generation needs to maintain the underlying mathematical
operations between quantities and variables, while at the same time ensuring
the relevance between the output and the given topic. To address above problem,
we develop an end-to-end neural model to generate diverse MWPs in real-world
scenarios from commonsense knowledge graph and equations. The proposed model
(1) learns both representations from edge-enhanced Levi graphs of symbolic
equations and commonsense knowledge; (2) automatically fuses equation and
commonsense knowledge information via a self-planning module when generating
the MWPs. Experiments on an educational gold-standard set and a large-scale
generated MWP set show that our approach is superior on the MWP generation
task, and it outperforms the SOTA models in terms of both automatic evaluation
metrics, i.e., BLEU-4, ROUGE-L, Self-BLEU, and human evaluation metrics, i.e.,
equation relevance, topic relevance, and language coherence. To encourage
reproducible results, we make our code and MWP dataset public available at
\url{https://github.com/tal-ai/MaKE_EMNLP2021}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Compositional Generalization via Semantic Tagging. (arXiv:2010.11818v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.11818">
<div class="article-summary-box-inner">
<span><p>Although neural sequence-to-sequence models have been successfully applied to
semantic parsing, they fail at compositional generalization, i.e., they are
unable to systematically generalize to unseen compositions of seen components.
Motivated by traditional semantic parsing where compositionality is explicitly
accounted for by symbolic grammars, we propose a new decoding framework that
preserves the expressivity and generality of sequence-to-sequence models while
featuring lexicon-style alignments and disentangled information processing.
Specifically, we decompose decoding into two phases where an input utterance is
first tagged with semantic symbols representing the meaning of individual
words, and then a sequence-to-sequence model is used to predict the final
meaning representation conditioning on the utterance and the predicted tag
sequence. Experimental results on three semantic parsing datasets show that the
proposed approach consistently improves compositional generalization across
model architectures, domains, and semantic formalisms.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NUANCED: Natural Utterance Annotation for Nuanced Conversation with Estimated Distributions. (arXiv:2010.12758v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2010.12758">
<div class="article-summary-box-inner">
<span><p>Existing conversational systems are mostly agent-centric, which assumes the
user utterances would closely follow the system ontology (for NLU or dialogue
state tracking). However, in real-world scenarios, it is highly desirable that
the users can speak freely in their own way. It is extremely hard, if not
impossible, for the users to adapt to the unknown system ontology. In this
work, we attempt to build a user-centric dialogue system. As there is no clean
mapping for a user's free form utterance to an ontology, we first model the
user preferences as estimated distributions over the system ontology and map
the users' utterances to such distributions. Learning such a mapping poses new
challenges on reasoning over existing knowledge, ranging from factoid
knowledge, commonsense knowledge to the users' own situations. To this end, we
build a new dataset named NUANCED that focuses on such realistic settings for
conversational recommendation. Collected via dialogue simulation and
paraphrasing, NUANCED contains 5.1k dialogues, 26k turns of high-quality user
responses. We conduct experiments, showing both the usefulness and challenges
of our problem setting. We believe NUANCED can serve as a valuable resource to
push existing research from the agent-centric system to the user-centric
system. The code and data is publicly available at
\url{https://github.com/facebookresearch/nuanced}.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Understanding Few-Shot Commonsense Knowledge Models. (arXiv:2101.00297v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2101.00297">
<div class="article-summary-box-inner">
<span><p>Providing natural language processing systems with commonsense knowledge is a
critical challenge for achieving language understanding. Recently, commonsense
knowledge models have emerged as a suitable approach for hypothesizing
situation-relevant commonsense knowledge on-demand in natural language
applications. However, these systems are limited by the fixed set of relations
captured by schemas of the knowledge bases on which they're trained.
</p>
<p>To address this limitation, we investigate training commonsense knowledge
models in a few-shot setting with limited tuples per commonsense relation in
the graph. We perform five separate studies on different dimensions of few-shot
commonsense knowledge learning, providing a roadmap on best practices for
training these systems efficiently. Importantly, we find that human quality
ratings for knowledge produced from a few-shot trained system can achieve
performance within 6% of knowledge produced from fully supervised systems. This
few-shot performance enables coverage of a wide breadth of relations in future
commonsense systems.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Biomedical Question Answering: A Survey of Approaches and Challenges. (arXiv:2102.05281v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.05281">
<div class="article-summary-box-inner">
<span><p>Automatic Question Answering (QA) has been successfully applied in various
domains such as search engines and chatbots. Biomedical QA (BQA), as an
emerging QA task, enables innovative applications to effectively perceive,
access and understand complex biomedical knowledge. There have been tremendous
developments of BQA in the past two decades, which we classify into 5
distinctive approaches: classic, information retrieval, machine reading
comprehension, knowledge base and question entailment approaches. In this
survey, we introduce available datasets and representative methods of each BQA
approach in detail. Despite the developments, BQA systems are still immature
and rarely used in real-life settings. We identify and characterize several key
challenges in BQA that might lead to this issue, and discuss some potential
future directions to explore.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Position Information in Transformers: An Overview. (arXiv:2102.11090v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2102.11090">
<div class="article-summary-box-inner">
<span><p>Transformers are arguably the main workhorse in recent Natural Language
Processing research. By definition a Transformer is invariant with respect to
reordering of the input. However, language is inherently sequential and word
order is essential to the semantics and syntax of an utterance. In this
article, we provide an overview and theoretical comparison of existing methods
to incorporate position information into Transformer models. The objectives of
this survey are to (1) showcase that position information in Transformer is a
vibrant and extensive research area; (2) enable the reader to compare existing
methods by providing a unified notation and systematization of different
approaches along important model dimensions; (3) indicate what characteristics
of an application should be taken into account when selecting a position
encoding; (4) provide stimuli for future research.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Self-Diagnosis and Self-Debiasing: A Proposal for Reducing Corpus-Based Bias in NLP. (arXiv:2103.00453v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.00453">
<div class="article-summary-box-inner">
<span><p>When trained on large, unfiltered crawls from the internet, language models
pick up and reproduce all kinds of undesirable biases that can be found in the
data: they often generate racist, sexist, violent or otherwise toxic language.
As large models require millions of training examples to achieve good
performance, it is difficult to completely prevent them from being exposed to
such content. In this paper, we first demonstrate a surprising finding:
pretrained language models recognize, to a considerable degree, their
undesirable biases and the toxicity of the content they produce. We refer to
this capability as self-diagnosis. Based on this finding, we then propose a
decoding algorithm that, given only a textual description of the undesired
behavior, reduces the probability of a language model producing problematic
text. We refer to this approach as self-debiasing. Self-debiasing does not rely
on manually curated word lists, nor does it require any training data or
changes to the model's parameters. While we by no means eliminate the issue of
language models generating biased text, we believe our approach to be an
important step in this direction.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Optimal Embedding Calibration for Symbolic Music Similarity. (arXiv:2103.07656v2 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.07656">
<div class="article-summary-box-inner">
<span><p>In natural language processing (NLP), the semantic similarity task requires
large-scale, high-quality human-annotated labels for fine-tuning or evaluation.
By contrast, in cases of music similarity, such labels are expensive to collect
and largely dependent on the annotator's artistic preferences. Recent research
has demonstrated that embedding calibration technique can greatly increase
semantic similarity performance of the pre-trained language model without
fine-tuning. However, it is yet unknown which calibration method is the best
and how much performance improvement can be achieved. To address these issues,
we propose using composer information to construct labels for automatically
evaluating music similarity. Under this paradigm, we discover the optimal
combination of embedding calibration which achieves superior metrics than the
baseline methods.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Mitigating False-Negative Contexts in Multi-document Question Answering with Retrieval Marginalization. (arXiv:2103.12235v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2103.12235">
<div class="article-summary-box-inner">
<span><p>Question Answering (QA) tasks requiring information from multiple documents
often rely on a retrieval model to identify relevant information for reasoning.
The retrieval model is typically trained to maximize the likelihood of the
labeled supporting evidence. However, when retrieving from large text corpora
such as Wikipedia, the correct answer can often be obtained from multiple
evidence candidates. Moreover, not all such candidates are labeled as positive
during annotation, rendering the training signal weak and noisy. This problem
is exacerbated when the questions are unanswerable or when the answers are
Boolean, since the model cannot rely on lexical overlap to make a connection
between the answer and supporting evidence. We develop a new parameterization
of set-valued retrieval that handles unanswerable queries, and we show that
marginalizing over this set during training allows a model to mitigate false
negatives in supporting evidence annotations. We test our method on two
multi-document QA datasets, IIRC and HotpotQA. On IIRC, we show that joint
modeling with marginalization improves model performance by 5.5 F1 points and
achieves a new state-of-the-art performance of 50.5 F1. We also show that
retrieval marginalization results in 4.1 QA F1 improvement over a
non-marginalized baseline on HotpotQA in the fullwiki setting.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.06669">
<div class="article-summary-box-inner">
<span><p>Many implicit inferences exist in text depending on how it is structured that
can critically impact the text's interpretation and meaning. One such
structural aspect present in text with chronology is the order of its
presentation. For narratives or stories, this is known as the narrative order.
Reordering a narrative can impact the temporal, causal, event-based, and other
inferences readers draw from it, which in turn can have strong effects both on
its interpretation and interestingness. In this paper, we propose and
investigate the task of Narrative Reordering (NAREOR) which involves rewriting
a given story in a different narrative order while preserving its plot. We
present a dataset, NAREORC, with human rewritings of stories within ROCStories
in non-linear orders, and conduct a detailed analysis of it. Further, we
propose novel task-specific training methods with suitable evaluation metrics.
We perform experiments on NAREORC using state-of-the-art models such as BART
and T5 and conduct extensive automatic and human evaluations. We demonstrate
that although our models can perform decently, NAREOR is a challenging task
with potential for further exploration. We also investigate two applications of
NAREOR: generation of more interesting variations of stories and serving as
adversarial sets for temporal/event-related tasks, besides discussing other
prospective ones, such as for pedagogical setups related to language skills
like essay writing and applications to medicine involving clinical narratives.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">KnowPrompt: Knowledge-aware Prompt-tuning with Synergistic Optimization for Relation Extraction. (arXiv:2104.07650v5 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07650">
<div class="article-summary-box-inner">
<span><p>Recently, prompt-tuning has achieved promising results for certain few-shot
classification tasks. The core idea of prompt-tuning is to insert text pieces
(i.e., templates) into the input and transform a classification task into a
masked language modeling problem. However, for relation extraction, determining
an appropriate prompt template requires domain expertise, and it is cumbersome
and time-consuming to obtain a suitable label word. Furthermore, there exist
abundant semantic knowledge among the entities and relations that cannot be
ignored. To this end, we focus on incorporating knowledge into prompt-tuning
for relation extraction and propose a knowledge-aware prompt-tuning approach
with synergistic optimization (KnowPrompt). Specifically, we inject entity and
relation knowledge into prompt construction with learnable virtual template
words as well as answer words and synergistically optimize their representation
with knowledge constraints. Extensive experimental results on five datasets
with standard and low-resource settings demonstrate the effectiveness of our
approach.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How to Train BERT with an Academic Budget. (arXiv:2104.07705v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.07705">
<div class="article-summary-box-inner">
<span><p>While large language models a la BERT are used ubiquitously in NLP,
pretraining them is considered a luxury that only a few well-funded industry
labs can afford. How can one train such models with a more modest budget? We
present a recipe for pretraining a masked language model in 24 hours using a
single low-end deep learning server. We demonstrate that through a combination
of software optimizations, design choices, and hyperparameter tuning, it is
possible to produce models that are competitive with BERT-base on GLUE tasks at
a fraction of the original pretraining cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders. (arXiv:2104.08027v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08027">
<div class="article-summary-box-inner">
<span><p>Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent
years. However, previous work has indicated that off-the-shelf MLMs are not
effective as universal lexical or sentence encoders without further
task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks
using annotated task data. In this work, we demonstrate that it is possible to
turn MLMs into effective universal lexical and sentence encoders even without
any additional data and without any supervision. We propose an extremely
simple, fast and effective contrastive learning technique, termed Mirror-BERT,
which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30
seconds without any additional external knowledge. Mirror-BERT relies on fully
identical or slightly modified string pairs as positive (i.e., synonymous)
fine-tuning examples, and aims to maximise their similarity during identity
fine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in
both lexical-level and sentence-level tasks, across different domains and
different languages. Notably, in the standard sentence semantic similarity
(STS) tasks, our self-supervised Mirror-BERT model even matches the performance
of the task-tuned Sentence-BERT models from prior work. Finally, we delve
deeper into the inner workings of MLMs, and suggest some evidence on why this
simple approach can yield effective universal lexical and sentence encoders.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Editing Factual Knowledge in Language Models. (arXiv:2104.08164v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08164">
<div class="article-summary-box-inner">
<span><p>The factual knowledge acquired during pre-training and stored in the
parameters of Language Models (LMs) can be useful in downstream tasks (e.g.,
question answering or textual inference). However, some facts can be
incorrectly induced or become obsolete over time. We present KnowledgeEditor, a
method which can be used to edit this knowledge and, thus, fix 'bugs' or
unexpected predictions without the need for expensive re-training or
fine-tuning. Besides being computationally efficient, KnowledgeEditordoes not
require any modifications in LM pre-training (e.g., the use of meta-learning).
In our approach, we train a hyper-network with constrained optimization to
modify a fact without affecting the rest of the knowledge; the trained
hyper-network is then used to predict the weight update at test time. We show
KnowledgeEditor's efficacy with two popular architectures and
knowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and
ii) a sequence-to-sequence BART model for question answering. With our method,
changing a prediction on the specific wording of a query tends to result in a
consistent change in predictions also for its paraphrases. We show that this
can be further encouraged by exploiting (e.g., automatically-generated)
paraphrases during training. Interestingly, our hyper-network can be regarded
as a 'probe' revealing which components need to be changed to manipulate
factual knowledge; our analysis shows that the updates tend to be concentrated
on a small subset of components. Source code available at
https://github.com/nicola-decao/KnowledgeEditor
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">$Q^{2}$: Evaluating Factual Consistency in Knowledge-Grounded Dialogues via Question Generation and Question Answering. (arXiv:2104.08202v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08202">
<div class="article-summary-box-inner">
<span><p>Neural knowledge-grounded generative models for dialogue often produce
content that is factually inconsistent with the knowledge they rely on, making
them unreliable and limiting their applicability. Inspired by recent work on
evaluating factual consistency in abstractive summarization, we propose an
automatic evaluation metric for factual consistency in knowledge-grounded
dialogue using automatic question generation and question answering. Our
metric, denoted $Q^2$, compares answer spans using natural language inference
(NLI), instead of token-based matching as done in previous work. To foster
proper evaluation, we curate a novel dataset of dialogue system outputs for the
Wizard-of-Wikipedia dataset, manually annotated for factual consistency. We
perform a thorough meta-evaluation of $Q^2$ against other metrics using this
dataset and two others, where it consistently shows higher correlation with
human judgements.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">The Topic Confusion Task: A Novel Scenario for Authorship Attribution. (arXiv:2104.08530v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08530">
<div class="article-summary-box-inner">
<span><p>Authorship attribution is the problem of identifying the most plausible
author of an anonymous text from a set of candidate authors. Researchers have
investigated same-topic and cross-topic scenarios of authorship attribution,
which differ according to whether new, unseen topics are used in the testing
phase. However, neither scenario allows us to explain whether errors are caused
by a failure to capture authorship writing style or by a topic shift. Motivated
by this, we propose the \emph{topic confusion} task where we switch the
author-topic configuration between the training and testing sets. This setup
allows us to distinguish two types of errors: those caused by the topic shift
and those caused by the features' inability to capture the writing styles. We
show that stylometric features with part-of-speech tags are the least
susceptible to topic variations. We further show that combining them with other
features leads to significantly lower topic confusion and higher attribution
accuracy. Finally, we show that pretrained language models such as BERT and
RoBERTa perform poorly on this task and are surpassed by simple features such
as word-level $n$-grams.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Back-Training excels Self-Training at Unsupervised Domain Adaptation of Question Generation and Passage Retrieval. (arXiv:2104.08801v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08801">
<div class="article-summary-box-inner">
<span><p>In this work, we introduce back-training, an alternative to self-training for
unsupervised domain adaptation (UDA) from source to target domain. While
self-training generates synthetic training data where natural inputs are
aligned with noisy outputs, back-training results in natural outputs aligned
with noisy inputs. This significantly reduces the gap between the target domain
and synthetic data distribution, and reduces model overfitting to the source
domain. We run UDA experiments on question generation and passage retrieval
from the \textit{Natural Questions} domain to machine learning and biomedical
domains. We find that back-training vastly outperforms self-training by a mean
improvement of 7.8 BLEU-4 points on generation, and 17.6\% top-20 retrieval
accuracy across both domains. We further propose consistency filters to remove
low-quality synthetic data before training. We also release a new
domain-adaptation dataset- \textit{MLQuestions} containing 35K unaligned
questions, 50K unaligned passages, and 3K aligned question-passage pairs.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Consistent Accelerated Inference via Confident Adaptive Transformers. (arXiv:2104.08803v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08803">
<div class="article-summary-box-inner">
<span><p>We develop a novel approach for confidently accelerating inference in the
large and expensive multilayer Transformers that are now ubiquitous in natural
language processing (NLP). Amortized or approximate computational methods
increase efficiency, but can come with unpredictable performance costs. In this
work, we present CATs -- Confident Adaptive Transformers -- in which we
simultaneously increase computational efficiency, while guaranteeing a
specifiable degree of consistency with the original model with high confidence.
Our method trains additional prediction heads on top of intermediate layers,
and dynamically decides when to stop allocating computational effort to each
input using a meta consistency classifier. To calibrate our early prediction
stopping rule, we formulate a unique extension of conformal prediction. We
demonstrate the effectiveness of this approach on four classification and
regression tasks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Flexible Generation of Natural Language Deductions. (arXiv:2104.08825v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08825">
<div class="article-summary-box-inner">
<span><p>An interpretable system for open-domain reasoning needs to express its
reasoning process in a transparent form. Natural language is an attractive
representation for this purpose -- it is both highly expressive and easy for
humans to understand. However, manipulating natural language statements in
logically consistent ways is hard: models must cope with variation in how
meaning is expressed while remaining precise. In this paper, we describe
ParaPattern, a method for building models to generate deductive inferences from
diverse natural language inputs without direct human supervision. We train
BART-based models (Lewis et al., 2020) to generate the result of applying a
particular logical operation to one or more premise statements. Crucially, we
develop a largely automated pipeline for constructing suitable training
examples from Wikipedia. We evaluate our models using out-of-domain sentence
compositions from the QASC (Khot et al., 2020) and EntailmentBank (Dalvi et
al., 2021) datasets as well as targeted perturbation sets. Our results show
that our models are substantially more accurate and flexible than baseline
systems. ParaPattern achieves 85% validity on examples of the 'substitution'
operation from EntailmentBank without the use of any in-domain training data,
matching the performance of a model fine-tuned for EntailmentBank. The full
source code for our method is publicly available.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. (arXiv:2104.08836v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2104.08836">
<div class="article-summary-box-inner">
<span><p>Multimodal pre-training with text, layout, and image has achieved SOTA
performance for visually-rich document understanding tasks recently, which
demonstrates the great potential for joint learning across different
modalities. In this paper, we present LayoutXLM, a multimodal pre-trained model
for multilingual document understanding, which aims to bridge the language
barriers for visually-rich document understanding. To accurately evaluate
LayoutXLM, we also introduce a multilingual form understanding benchmark
dataset named XFUND, which includes form understanding samples in 7 languages
(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and
key-value pairs are manually labeled for each language. Experiment results show
that the LayoutXLM model has significantly outperformed the existing SOTA
cross-lingual pre-trained models on the XFUND dataset. The pre-trained
LayoutXLM model and the XFUND dataset are publicly available at
https://aka.ms/layoutxlm.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Teacher-Student MixIT for Unsupervised and Semi-supervised Speech Separation. (arXiv:2106.07843v3 [cs.SD] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2106.07843">
<div class="article-summary-box-inner">
<span><p>In this paper, we introduce a novel semi-supervised learning framework for
end-to-end speech separation. The proposed method first uses mixtures of
unseparated sources and the mixture invariant training (MixIT) criterion to
train a teacher model. The teacher model then estimates separated sources that
are used to train a student model with standard permutation invariant training
(PIT). The student model can be fine-tuned with supervised data, i.e., paired
artificial mixtures and clean speech sources, and further improved via model
distillation. Experiments with single and multi channel mixtures show that the
teacher-student training resolves the over-separation problem observed in the
original MixIT method. Further, the semisupervised performance is comparable to
a fully-supervised separation system trained using ten times the amount of
supervised data.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v4 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2107.01198">
<div class="article-summary-box-inner">
<span><p>In this work, we present to the NLP community, and to the wider research
community as a whole, an application for the diachronic analysis of research
corpora. We open source an easy-to-use tool coined: DRIFT, which allows
researchers to track research trends and development over the years. The
analysis methods are collated from well-cited research works, with a few of our
own methods added for good measure. Succinctly put, some of the analysis
methods are: keyword extraction, word clouds, predicting
declining/stagnant/growing trends using Productivity, tracking bi-grams using
Acceleration plots, finding the Semantic Drift of words, tracking trends using
similarity, etc. To demonstrate the utility and efficacy of our tool, we
perform a case study on the cs.CL corpus of the arXiv repository and draw
inferences from the analysis methods. The toolkit and the associated code are
available here: https://github.com/rajaswa/DRIFT.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">SynCoBERT: Syntax-Guided Multi-Modal Contrastive Pre-Training for Code Representation. (arXiv:2108.04556v3 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.04556">
<div class="article-summary-box-inner">
<span><p>Code representation learning, which aims to encode the semantics of source
code into distributed vectors, plays an important role in recent
deep-learning-based models for code intelligence. Recently, many pre-trained
language models for source code (e.g., CuBERT and CodeBERT) have been proposed
to model the context of code and serve as a basis for downstream code
intelligence tasks such as code search, code clone detection, and program
translation. Current approaches typically consider the source code as a plain
sequence of tokens, or inject the structure information (e.g., AST and
data-flow) into the sequential model pre-training. To further explore the
properties of programming languages, this paper proposes SynCoBERT, a
syntax-guided multi-modal contrastive pre-training approach for better code
representations. Specially, we design two novel pre-training objectives
originating from the symbolic and syntactic properties of source code, i.e.,
Identifier Prediction (IP) and AST Edge Prediction (TEP), which are designed to
predict identifiers, and edges between two nodes of AST, respectively.
Meanwhile, to exploit the complementary information in semantically equivalent
modalities (i.e., code, comment, AST) of the code, we propose a multi-modal
contrastive learning strategy to maximize the mutual information among
different modalities. Extensive experiments on four downstream tasks related to
code intelligence show that SynCoBERT advances the state-of-the-art with the
same pre-training corpus and model size.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v7 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12202">
<div class="article-summary-box-inner">
<span><p>In joint entity and relation extraction, existing work either sequentially
encode task-specific features, leading to an imbalance in inter-task feature
interaction where features extracted later have no direct contact with those
that come first. Or they encode entity features and relation features in a
parallel manner, meaning that feature representation learning for each task is
largely independent of each other except for input sharing. We propose a
partition filter network to model two-way interaction between tasks properly,
where feature encoding is decomposed into two steps: partition and filter. In
our encoder, we leverage two gates: entity and relation gate, to segment
neurons into two task partitions and one shared partition. The shared partition
represents inter-task information valuable to both tasks and is evenly shared
across two tasks to ensure proper two-way interaction. The task partitions
represent intra-task information and are formed through concerted efforts of
both gates, making sure that encoding of task-specific features is dependent
upon each other. Experiment results on six public datasets show that our model
performs significantly better than previous approaches. In addition, contrary
to what previous work has claimed, our auxiliary experiments suggest that
relation prediction is contributory to named entity prediction in a
non-negligible way. The source code can be found at
https://github.com/Coopercoppers/PFN.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Automatic Text Evaluation through the Lens of Wasserstein Barycenters. (arXiv:2108.12463v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12463">
<div class="article-summary-box-inner">
<span><p>A new metric \texttt{BaryScore} to evaluate text generation based on deep
contextualized embeddings e.g., BERT, Roberta, ELMo) is introduced. This metric
is motivated by a new framework relying on optimal transport tools, i.e.,
Wasserstein distance and barycenter. By modelling the layer output of deep
contextualized embeddings as a probability distribution rather than by a vector
embedding; this framework provides a natural way to aggregate the different
outputs through the Wasserstein space topology. In addition, it provides
theoretical grounds to our metric and offers an alternative to available
solutions e.g., MoverScore and BertScore). Numerical evaluation is performed on
four different tasks: machine translation, summarization, data2text generation
and image captioning. Our results show that \texttt{BaryScore} outperforms
other BERT based metrics and exhibits more consistent behaviour in particular
for text summarization.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Code-switched inspired losses for generic spoken dialog representations. (arXiv:2108.12465v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12465">
<div class="article-summary-box-inner">
<span><p>Spoken dialog systems need to be able to handle both multiple languages and
multilinguality inside a conversation (\textit{e.g} in case of code-switching).
In this work, we introduce new pretraining losses tailored to learn
multilingual spoken dialog representations. The goal of these losses is to
expose the model to code-switched language. To scale up training, we
automatically build a pretraining corpus composed of multilingual conversations
in five different languages (French, Italian, English, German and Spanish) from
\texttt{OpenSubtitles}, a huge multilingual corpus composed of 24.3G tokens. We
test the generic representations on \texttt{MIAM}, a new benchmark composed of
five dialog act corpora on the same aforementioned languages as well as on two
novel multilingual downstream tasks (\textit{i.e} multilingual mask utterance
retrieval and multilingual inconsistency identification). Our experiments show
that our new code switched-inspired losses achieve a better performance in both
monolingual and multilingual settings.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Multiplex Graph Neural Network for Extractive Text Summarization. (arXiv:2108.12870v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2108.12870">
<div class="article-summary-box-inner">
<span><p>Extractive text summarization aims at extracting the most representative
sentences from a given document as its summary. To extract a good summary from
a long text document, sentence embedding plays an important role. Recent
studies have leveraged graph neural networks to capture the inter-sentential
relationship (e.g., the discourse graph) to learn contextual sentence
embedding. However, those approaches neither consider multiple types of
inter-sentential relationships (e.g., semantic similarity &amp; natural
connection), nor model intra-sentential relationships (e.g, semantic &amp;
syntactic relationship among words). To address these problems, we propose a
novel Multiplex Graph Convolutional Network (Multi-GCN) to jointly model
different types of relationships among sentences and words. Based on Multi-GCN,
we propose a Multiplex Graph Summarization (Multi-GraS) model for extractive
text summarization. Finally, we evaluate the proposed models on the
CNN/DailyMail benchmark dataset to demonstrate the effectiveness of our method.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER. (arXiv:2109.00720v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00720">
<div class="article-summary-box-inner">
<span><p>Most existing NER methods rely on extensive labeled data for model training,
which struggles in the low-resource scenarios with limited training data.
Recently, prompt-tuning methods for pre-trained language models have achieved
remarkable performance in few-shot learning by exploiting prompts as task
guidance to reduce the gap between training progress and downstream tuning.
Inspired by prompt learning, we propose a novel lightweight generative
framework with prompt-guided attention for low-resource NER (LightNER).
Specifically, we construct the semantic-aware answer space of entity categories
for prompt learning to generate the entity span sequence and entity categories
without any label-specific classifiers. We further propose prompt-guided
attention by incorporating continuous prompts into the self-attention layer to
re-modulate the attention and adapt pre-trained weights. Note that we only tune
those continuous prompts with the whole parameter of the pre-trained language
model fixed, thus, making our approach lightweight and flexible for
low-resource scenarios and can better transfer knowledge across domains.
Experimental results show that LightNER can obtain comparable performance in
the standard supervised setting and outperform strong baselines in low-resource
settings by tuning only a small part of the parameters.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Improving Multimodal fusion via Mutual Dependency Maximisation. (arXiv:2109.00922v2 [cs.LG] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.00922">
<div class="article-summary-box-inner">
<span><p>Multimodal sentiment analysis is a trending area of research, and the
multimodal fusion is one of its most active topic. Acknowledging humans
communicate through a variety of channels (i.e visual, acoustic, linguistic),
multimodal systems aim at integrating different unimodal representations into a
synthetic one. So far, a consequent effort has been made on developing complex
architectures allowing the fusion of these modalities. However, such systems
are mainly trained by minimising simple losses such as $L_1$ or cross-entropy.
In this work, we investigate unexplored penalties and propose a set of new
objectives that measure the dependency between modalities. We demonstrate that
our new penalties lead to a consistent improvement (up to $4.3$ on accuracy)
across a large variety of state-of-the-art models on two well-known sentiment
analysis datasets: \texttt{CMU-MOSI} and \texttt{CMU-MOSEI}. Our method not
only achieves a new SOTA on both datasets but also produces representations
that are more robust to modality drops. Finally, a by-product of our methods
includes a statistical network which can be used to interpret the high
dimensional representations learnt by the model.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">WhyAct: Identifying Action Reasons in Lifestyle Vlogs. (arXiv:2109.02747v2 [cs.CV] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.02747">
<div class="article-summary-box-inner">
<span><p>We aim to automatically identify human action reasons in online videos. We
focus on the widespread genre of lifestyle vlogs, in which people perform
actions while verbally describing them. We introduce and make publicly
available the WhyAct dataset, consisting of 1,077 visual actions manually
annotated with their reasons. We describe a multimodal model that leverages
visual and textual information to automatically infer the reasons corresponding
to an action presented in the video.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03158">
<div class="article-summary-box-inner">
<span><p>An individual's variation in writing style is often a function of both social
and personal attributes. While structured social variation has been extensively
studied, e.g., gender based variation, far less is known about how to
characterize individual styles due to their idiosyncratic nature. We introduce
a new approach to studying idiolects through a massive cross-author comparison
to identify and encode stylistic features. The neural model achieves strong
performance at authorship identification on short texts and through an
analogy-based probing task, showing that the learned representations exhibit
surprising regularities that encode qualitative and quantitative shifts of
idiolectal styles. Through text perturbation, we quantify the relative
contributions of different linguistic elements to idiolectal variation.
Furthermore, we provide a description of idiolects through measuring inter- and
intra-author variation, showing that variation in idiolects is often
distinctive yet consistent.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">How much pretraining data do language models need to learn syntax?. (arXiv:2109.03160v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03160">
<div class="article-summary-box-inner">
<span><p>Transformers-based pretrained language models achieve outstanding results in
many well-known NLU benchmarks. However, while pretraining methods are very
convenient, they are expensive in terms of time and resources. This calls for a
study of the impact of pretraining data size on the knowledge of the models. We
explore this impact on the syntactic capabilities of RoBERTa, using models
trained on incremental sizes of raw text data. First, we use syntactic
structural probes to determine whether models pretrained on more data encode a
higher amount of syntactic information. Second, we perform a targeted syntactic
evaluation to analyze the impact of pretraining data size on the syntactic
generalization performance of the models. Third, we compare the performance of
the different models on three downstream applications: part-of-speech tagging,
dependency parsing and paraphrase identification. We complement our study with
an analysis of the cost-benefit trade-off of training such models. Our
experiments show that while models pretrained on more data encode more
syntactic knowledge and perform better on downstream applications, they do not
always offer a better performance across the different syntactic phenomena and
come at a higher financial and environmental cost.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">It is AI's Turn to Ask Human a Question: Question and Answer Pair Generation for Children Storybooks in FairytaleQA Dataset. (arXiv:2109.03423v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03423">
<div class="article-summary-box-inner">
<span><p>Existing question answering (QA) datasets are created mainly for the
application of having AI to be able to answer questions asked by humans. But in
educational applications, teachers and parents sometimes may not know what
questions they should ask a child that can maximize their language learning
results. With a newly released book QA dataset (FairytaleQA), which educational
experts labeled on 46 fairytale storybooks for early childhood readers, we
developed an automated QA generation model architecture for this novel
application. Our model (1) extracts candidate answers from a given storybook
passage through carefully designed heuristics based on a pedagogical framework;
(2) generates appropriate questions corresponding to each extracted answer
using a language model; and, (3) uses another QA model to rank top QA-pairs.
Automatic and human evaluations show that our model outperforms baselines. We
also demonstrate that our method can help with the scarcity issue of the
children's book QA dataset via data augmentation on 200 unlabeled storybooks.
</p></span>
</div>
</a>
</details>
</article>
<article>
<details class="article-expander">
<summary class="article-expander__title">ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v2 [cs.CL] UPDATED)</summary>
<a class="article-summary-link article-summary-box-outer" href="http://arxiv.org/abs/2109.03438">
<div class="article-summary-box-inner">
<span><p>In the last few years, open-domain question answering (ODQA) has advanced
rapidly due to the development of deep learning techniques and the availability
of large-scale QA datasets. However, the current datasets are essentially
designed for synchronic document collections (e.g., Wikipedia). Temporal news
collections such as long-term news archives spanning several decades, are
rarely used in training the models despite they are quite valuable for our
society. In order to foster the research in the field of ODQA on such
historical collections, we present ArchivalQA, a large question answering
dataset consisting of 1,067,056 question-answer pairs which is designed for
temporal news QA. In addition, we create four subparts of our dataset based on
the question difficulty levels and the containment of temporal expressions,
which we believe could be useful for training or testing ODQA systems
characterized by different strengths and abilities. The novel QA
dataset-constructing framework that we introduce can be also applied to create
datasets over other types of collections.
</p></span>
</div>
</a>
</details>
</article>
</section>
</section>
</li>
</ul>
</section>
<footer>
<time id="build-timestamp" datetime="2021-09-10 01:54:26.840139362 UTC">2021-09-10 01:54:26 UTC</time>
<span><a class="footer-link" href="https://github.com/NotCraft/NotFeed"> notfeed 0.2.3</a></span>
</footer>
<script src="index.js"></script>
</body>
</html>