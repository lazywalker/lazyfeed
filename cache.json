{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-29T01:13:59.903432198Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-28 å¼€æºæ“ä½œç³»ç»Ÿå¤ä»¤è¥æœ€ç»ˆæŠ¥å‘Šä¼šå®‰æ’","link":"https://rustcc.cn/article?id=ef3dd4e8-a8e8-4fec-bc7e-75703e1117ff","description":"<h3>å¼€æºæ“ä½œç³»ç»Ÿå¤ä»¤è¥æœ€ç»ˆæŠ¥å‘Šä¼šå®‰æ’</h3>\n<p>ä¼šè®®ä¸»é¢˜ï¼šå¼€æºæ“ä½œç³»ç»Ÿå¤ä»¤è¥æœ€ç»ˆæŠ¥å‘Šä¼š\nä¼šè®®æ—¶é—´ï¼š2021/08/29 09:00-11:30 (GMT+08:00) ä¸­å›½æ ‡å‡†æ—¶é—´ - åŒ—äº¬\nç‚¹å‡»é“¾æ¥å…¥ä¼šï¼Œæˆ–æ·»åŠ è‡³ä¼šè®®åˆ—è¡¨ï¼š https://meeting.tencent.com/dm/Mp7T1h5zeQOk?rs=25\nä¼šè®® IDï¼š635 194 989</p>\n<p>ä¸‹é¢æ˜¯9ä½å…¨ç¨‹å‚ä¸å¤ä»¤è¥æ´»åŠ¨åŒå­¦çš„æŠ¥å‘Šé¡ºåºã€‚æ¯äººæŠ¥å‘Šæ—¶é—´æœ€é•¿15åˆ†é’Ÿã€‚</p>\n<ol>\n<li>æ¨äº‘æ« ç‹æ¶› Rustsbiçš„å“ªå’å¼€å‘ç‰ˆç§»æ¤</li>\n<li>å…°é™ˆæ˜• zCoreå›¾å½¢æ”¯æŒ</li>\n<li>éƒ½ç§‰ç”² å®¹å™¨æŠ€æœ¯å­¦ä¹ </li>\n<li>è–›æ½‡å· RVM çš„ RISC-V æ”¯æŒ</li>\n<li>é™ˆä¹ å…±äº«è°ƒåº¦å™¨</li>\n<li>å´éå‡¡ åŸºäºç”¨æˆ·æ€ä¸­æ–­çš„å¼‚æ­¥ç³»ç»Ÿè°ƒç”¨è®¾è®¡ä¸å®ç°</li>\n<li>å½­æ·³æ¯… é™ˆå¿—æ‰¬ åŸºäºrCore-Tutorialçš„æ€§èƒ½åˆ†æè½¯ä»¶å®ç°</li>\n</ol>\n<h3>crates.liveï¼šå¯è§†åŒ– Rust crates ä¾èµ–é¡¹</h3>\n<p>crates.live æ˜¯æ¥è‡ª crates.io çš„ Rust crates çš„ä¾èµ–å¯è§†åŒ–å·¥å…·ã€‚ å®ƒæ˜¾ç¤ºäº† Rust cratesï¼ˆåŒ…ï¼‰çš„ä¾èµ–æ ‘ã€‚åŠŸèƒ½åŒ…æ‹¬ï¼š</p>\n<ul>\n<li>ä¾èµ–è§£æï¼Œ crates.live å¼•æ“é€šè¿‡åŒ¹é…ä¾èµ–ç‰ˆæœ¬æ¥å®Œæˆå®Œæ•´çš„ä¾èµ–è§£æã€‚</li>\n<li>äº¤äº’å¼å›¾è¡¨ï¼Œå¸¦æœ‰æ ‡è®°çš„æ¿æ¡ç®±çš„å¯ç¼©æ”¾äº¤äº’å¼å›¾è¡¨ã€‚</li>\n<li>å›¾åƒå¯¼å‡ºï¼Œ å°†å›¾å½¢å¯¼å‡ºä¸º PNGã€‚</li>\n<li>å¼€æ”¾ APIï¼šï¼ˆå³å°†æ¨å‡ºï¼‰GraphQL APIã€‚</li>\n</ul>\n<p>crates.live ä½¿ç”¨äº†ä¸€å †æŠ€æœ¯æ¡†æ¶ï¼ŒæŠ€æœ¯æ ˆåŒ…æ‹¬ï¼š</p>\n<ul>\n<li>Rustï¼Œ crates.live åç«¯å’Œçˆ¬è™«æ˜¯ç”¨ Rust å’Œå¼€æº Rust åº“å¼€å‘çš„ã€‚</li>\n<li>GraphQlï¼Œ WASM é©±åŠ¨çš„ GraphQL æœåŠ¡å™¨ã€‚</li>\n<li>React/Bulmaï¼Œ å‰ç«¯åº“ã€‚</li>\n<li>Terraformï¼Œ å¸®åŠ©å¯åŠ¨å’Œç»´æŠ¤æˆ‘ä»¬çš„åŸºç¡€è®¾æ–½ã€‚</li>\n<li>Cloudflareï¼Œ Cloudflare å·¥ä½œäººå‘˜è¿è¡Œ WASM åç«¯ã€‚</li>\n</ul>\n<p>å¦‚æœåœ¨ä½¿ç”¨æ­¤åº”ç”¨ç¨‹åºæ—¶æœ‰ä»»ä½•ç–‘é—®ã€å»ºè®®æˆ–é—®é¢˜ï¼› å¯ä»¥é€šè¿‡ contact@crates.live è”ç³»ã€‚ crates.live ç”± Abid Omar å¼€å‘ï¼Œå¯é€šè¿‡ contact@omarabid.com è”ç³»ã€‚</p>\n<p><a href=\"https://crates.live/\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼šhttps://crates.live/</p>\n<h3>Obakeï¼Œç‰ˆæœ¬åŒ–æ•°æ®ç»“æ„</h3>\n<p>Obake æ˜¯ä¸€ä¸ªç”¨äºå£°æ˜å’Œç»´æŠ¤ç‰ˆæœ¬åŒ–æ•°æ®ç»“æ„çš„è¿‡ç¨‹å®ã€‚ â€œobakeâ€è¿™ä¸ªåå­—å–è‡ªæ—¥è¯­â€œãŠåŒ–ã‘ï¼ˆãŠã°ã‘ï¼‰â€ï¼Œè¿™æ˜¯æ—¥æœ¬æ°‘é—´ä¼ è¯´ä¸­ä¸€ç±»ä¼šå˜å½¢çš„è¶…è‡ªç„¶ç”Ÿç‰©ã€‚</p>\n<p>åœ¨å¼€å‘åº”ç”¨ç¨‹åºæ—¶ï¼Œé…ç½®æ ¼å¼å’Œå†…éƒ¨æ•°æ®ç»“æ„é€šå¸¸ä¼šåœ¨ç‰ˆæœ¬ä¹‹é—´æ¼”å˜ã€‚ ç„¶è€Œï¼Œä¿æŒè¿™äº›ç‰ˆæœ¬ä¹‹é—´çš„å‘åå…¼å®¹æ€§éœ€è¦å£°æ˜å’Œç»´æŠ¤é—ç•™æ ¼å¼çš„æ•°æ®ç»“æ„å’Œç”¨äºåœ¨å®ƒä»¬ä¹‹é—´è¿ç§»çš„ä»£ç ã€‚ Obake çš„ç›®æ ‡æ˜¯è®©è¿™ä¸ªè¿‡ç¨‹å˜å¾—è½»æ¾ã€‚</p>\n<pre><code>#[obake::versioned]                 // create a versioned data-structure\n#[obake(version(\"0.1.0\"))]          // declare some versions\n#[obake(version(\"0.2.0\"))]\n#[derive(PartialEq, Eq, Hash)]      // additional attributes are applied to all versions\nstruct Foo {\n    #[obake(cfg(\"0.1.0\"))]          // enable fields for specific versions with\n    foo: String,                    // semantic version constraints\n   \n    #[obake(cfg(\"&gt;=0.2, &lt;=0.3.0\"))] // any semantic version constraint can appear in\n    bar: u32,                       // a `cfg` attribute \n   \n    #[obake(cfg(\"0.1.0\"))]          // multiple `cfg` attributes are treated as a\n    #[obake(cfg(\"&gt;=0.3\"))]          // disjunction over version constraints\n    baz: char,\n}\n\n// describe migrations between versions using the `From` trait\n// and an automatically generated type-level macro for referring to\n// specific versions of `Foo`\nimpl From&lt;Foo![\"0.1.0\"]&gt; for Foo![\"0.2.0\"] {\n    fn from(foo: Foo![\"0.1.0\"]) -&gt; Self {\n        Self { bar: 0 }\n    }\n}\n\n// an enumeration of all versions of `Foo` is accessed using the\n// `obake::Versioned` trait:\nlet versioned_example: &lt;Foo as obake::Versioned&gt;::Versioned = unimplemented!();\n\n// this enumeration implements `Into&lt;Foo&gt;`, where `Foo` is the latest declared\n// version of `Foo` (in this case, `Foo![\"0.2.0\"]`)\nlet example: Foo = versioned_example.into();\n</code></pre>\n<p>Github<a href=\"https://github.com/doctorn/obake\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼šhttps://github.com/doctorn/obake</p>\n<h3>icedï¼Œè·¨å¹³å° GUI åº“</h3>\n<p>icedï¼ŒRust çš„è·¨å¹³å° GUI åº“ï¼Œä¸“æ³¨äºç®€å•æ€§å’Œç±»å‹å®‰å…¨ã€‚ çµæ„Ÿæ¥è‡ª<a href=\"https://elm-lang.org/\" rel=\"noopener noreferrer\">Elm</a>ã€‚</p>\n<p><img src=\"https://raw.githubusercontent.com/hecrj/iced/master/docs/graphs/ecosystem.png\" alt=\"eco\"></p>\n<p>Github<a href=\"https://github.com/hecrj/iced/\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼šhttps://github.com/hecrj/iced/</p>\n<p>ç¤ºä¾‹ï¼šhttps://github.com/hecrj/iced/tree/master/examples</p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ <a href=\"https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929\" rel=\"noopener noreferrer\">æ´‹èŠ‹</a></p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-28 15:42:07","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rust æ—¥æŠ¥ã€‘2021-8-27 Rudra Rust çš„å†…å­˜å®‰å…¨å’Œæœªå®šä¹‰è¡Œä¸ºæ£€æµ‹å·¥å…·","link":"https://rustcc.cn/article?id=ce7eb559-fdda-45d7-a53e-293af787a813","description":"<h4>Rudra Rust çš„å†…å­˜å®‰å…¨å’Œæœªå®šä¹‰è¡Œä¸ºæ£€æµ‹å·¥å…·</h4>\n<p>Rudra æ˜¯ä¸€ä¸ªé™æ€åˆ†æå™¨ï¼Œç”¨äºæ£€æµ‹ Rust ç¨‹åºä¸­å¸¸è§çš„æœªå®šä¹‰è¡Œä¸ºã€‚å®ƒèƒ½å¤Ÿåˆ†æå•ä¸ª Rust åŒ…ä»¥åŠ crates.io ä¸Šçš„æ‰€æœ‰åŒ…ã€‚Rudra åŠå…¶ç›¸å…³è®ºæ–‡å°†åœ¨ Proceedings of the 28th ACM Symposium on Operating Systems Principles 2021 (SOSP '21) ä¸Šå‘è¡¨ã€‚</p>\n<ul>\n<li>https://github.com/sslab-gatech/Rudra#readme</li>\n</ul>\n<h4>nom 7.0 ç‰ˆæœ¬å‘å¸ƒ</h4>\n<p>nom æ˜¯ä¸€ä¸ªç”¨ Rust ç¼–å†™çš„è§£æå™¨ç»„åˆåº“ã€‚å®ƒçš„ç›®æ ‡æ˜¯æä¾›å·¥å…·æ¥æ„å»ºå®‰å…¨çš„è§£æå™¨ï¼Œè€Œä¸ä¼šå½±å“é€Ÿåº¦æˆ–å†…å­˜æ¶ˆè€—ã€‚ä¸ºæ­¤ï¼Œå®ƒå¹¿æ³›ä½¿ç”¨ Rust çš„å¼ºç±»å‹å’Œå†…å­˜å®‰å…¨æ¥ç”Ÿæˆå¿«é€Ÿä¸”æ­£ç¡®çš„è§£æå™¨ï¼Œå¹¶æä¾›å‡½æ•°ã€å®å’Œç‰¹å¾æ¥æŠ½è±¡å¤§éƒ¨åˆ†å®¹æ˜“å‡ºé”™çš„ç®¡é“ã€‚ç›®å‰7.0å·²ç»å‘å¸ƒ</p>\n<ul>\n<li>https://crates.io/crates/nom</li>\n</ul>\n<h4>egui 0.14 ç‰ˆæœ¬å‘å¸ƒ</h4>\n<p>egui æ˜¯ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„çº¯ Rust å›¾å½¢ç”¨æˆ·ç•Œé¢ã€‚egui å¯ä»¥åœ¨ Web ä¸Šã€æœ¬æœºä¸Šä»¥åŠæ‚¨æœ€å–œæ¬¢çš„æ¸¸æˆå¼•æ“ä¸­è¿è¡Œã€‚egui æ—¨åœ¨æˆä¸ºæœ€å®¹æ˜“ä½¿ç”¨çš„ Rust GUI åº“ï¼Œä»¥åŠåœ¨ Rust ä¸­åˆ¶ä½œ Web åº”ç”¨ç¨‹åºçš„æœ€ç®€å•æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨ä»»ä½•å¯ä»¥ç»˜åˆ¶çº¹ç†ä¸‰è§’å½¢çš„åœ°æ–¹ä½¿ç”¨ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥è½»æ¾åœ°å°†å…¶é›†æˆåˆ°æ‚¨é€‰æ‹©çš„æ¸¸æˆå¼•æ“ä¸­ã€‚</p>\n<ul>\n<li>æ¼”ç¤ºæ–‡æ¡£ï¼šhttps://emilk.github.io/egui/</li>\n<li>https://github.com/emilk/egui</li>\n</ul>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ åŒ—çº¬27åº¦ï¼Œä¾¯ç››é‘«</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-27 14:27:47","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å¼€æºé¡¹ç›®xiuç™»ä¸Šäº†GitHub rust trendingæ¦œ","link":"https://rustcc.cn/article?id=86c83d9a-8370-42cf-8993-ef15af6932c4","description":"<p><a href=\"https://github.com/harlanc/xiu\" rel=\"noopener noreferrer\">https://github.com/harlanc/xiu</a></p>\n<p><a href=\"https://github.com/trending/rust?since=daily\" rel=\"noopener noreferrer\">https://github.com/trending/rust?since=daily</a></p>\n<p>æ„Ÿè°¢å¤§å®¶çš„æ”¯æŒï¼ï¼</p>\n<p>PSï¼š</p>\n<p>å‰ä¸‰åæœ‰ä¸¤ä¸ªéƒ½åœ¨è®ºå›é‡Œå‘è¿‡ï¼Œè¿™ä¸ªè®ºå›æœ‰ç‚¹ç‹ ï¼Œå“ˆå“ˆ</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-27 10:43:48","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Salvo - ä¸€ä¸ªç®€å•çš„ Web åç«¯æ¡†æ¶","link":"https://rustcc.cn/article?id=e5dc5be9-b1ab-488f-8944-dd7cd97b0128","description":"<h2>ä¸ºä»€ä¹ˆè¦å†™è¿™ä¸ªæ¡†æ¶</h2>\n<p>å› ä¸ºæˆ‘ç¬¨ï¼Œæ— æ³•å­¦ä¼šä½¿ç”¨ actix-web ç­‰ç°å­˜çš„æ¡†æ¶ã€‚å½“æˆ‘æƒ³æŠŠä»¥å‰çš„ go çš„ web æœåŠ¡ä½¿ç”¨ rust å®ç°æ—¶ï¼Œä¸€çœ¼çœ‹å»ï¼Œä¼¼ä¹æ¯ä¸ªæ¡†æ¶éƒ½æ¯” go é‡Œå­˜åœ¨æ¡†æ¶å¤æ‚, æœ¬æ¥ Rust çš„å­¦ä¹ æ›²çº¿å°±å¤Ÿé™¡å³­çš„äº†, åˆä½•è‹¦æŠŠ Web æ¡†æ¶æ•´å¾—é‚£ä¹ˆå¤æ‚?</p>\n<h2>å¦‚ä½•åšåˆ°è¶³å¤Ÿç®€å•</h2>\n<p>å¾ˆå¤šåº•å±‚çš„å®ç° Hyper éƒ½å·²ç»å®ç°ï¼Œæ‰€ä»¥ï¼Œä¸€èˆ¬éœ€æ±‚ï¼ŒåŸºäº Hyper å®ç°åº”è¯¥æ²¡æœ‰é”™ã€‚Salvo ä¹Ÿæ˜¯ä¸€æ ·ã€‚ æ ¸å¿ƒåŠŸèƒ½æ˜¯æä¾›è¿˜ç”¨ç®€å•çš„APIï¼Œä»¥åŠä¸€ä¸ªåŠŸèƒ½å¼ºå¤§å¹¶ä¸”çµæ´»çš„è·¯ç”±ç³»ç»Ÿã€‚</p>\n<p>Salvo é‡Œç»Ÿä¸€äº† Handler å’Œ Middleware. Middleware å°±æ˜¯ Handler. é€šè¿‡è·¯ç”±çš„ before æˆ–è€… after æ·»åŠ åˆ° Router ä¸Šã€‚æœ¬è´¨ä¸Š, Middleware å’Œ Handler éƒ½æ˜¯å¤„ç† Request è¯·æ±‚ï¼Œå¹¶ä¸”å¯èƒ½å‘ Response å†™å…¥æ•°æ®ã€‚è€Œ Handler æ¥æ”¶çš„å‚æ•°æ˜¯ Request, Depot, Response ä¸‰ä¸ª, å…¶ä¸­ Depot ç”¨äºå­˜å‚¨è¯·æ±‚å¤„ç†è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ•°æ®. ä¸ºæ–¹ä¾¿ä¹¦å†™, åœ¨ç”¨ä¸ç€çš„æƒ…å†µä¸‹å¯ä»¥çœç•¥æ‰æŸäº›å‚æ•°.</p>\n<pre><code>use Salvo::prelude::*;\n\n#[fn_handler]\nasync fn hello_world(_req: &amp;mut Request, _depot: &amp;mut Depot, res: &amp;mut Response) {\n    res.render_plain_text(\"Hello World\");\n}\n#[fn_handler]\nasync fn hello_world2(res: &amp;mut Response) {\n    res.render_plain_text(\"Hello World\");\n}\n</code></pre>\n<p>å¦å¤–è·¯ç”±ç³»ç»Ÿæä¾›çš„ API ä¹Ÿæ˜¯æå…¶ç®€å•çš„, ä½†æ˜¯, åŠŸèƒ½å´æ˜¯å¼ºå¤§çš„. æ­£å¸¸ä½¿ç”¨éœ€æ±‚ä¸‹, åŸºæœ¬ä¸Šå°±æ˜¯åªå…³æ³¨ Router ä¸€ä¸ªç±»å‹å³å¯.</p>\n<h3>è·¯ç”±ç³»ç»Ÿ</h3>\n<p>æˆ‘è‡ªå·±æ„Ÿè§‰è·¯ç”±ç³»ç»Ÿæ˜¯è·Ÿå…¶ä»–çš„æ¡†æ¶ä¸å¤ªä¸€æ ·çš„. Router å¯ä»¥å†™å¹³ï¼Œä¹Ÿå¯ä»¥å†™æˆæ ‘çŠ¶ã€‚è¿™é‡ŒåŒºä¸šåŠ¡é€»è¾‘æ ‘ä¸è®¿é—®ç›®å½•æ ‘ã€‚ä¸šåŠ¡é€»è¾‘æ ‘æ˜¯æ ¹æ®ä¸šåŠ¡é€»è¾‘éœ€æ±‚ï¼Œåˆ’åˆ† router ç»“æ„ï¼Œå½¢æˆ router æ ‘ï¼Œå®ƒä¸ä¸€å®šä¸è®¿é—®ç›®å½•æ ‘ä¸€è‡´ã€‚</p>\n<p>æ­£å¸¸æƒ…å†µä¸‹æˆ‘ä»¬æ˜¯è¿™æ ·å†™è·¯ç”±çš„ï¼š</p>\n<pre><code>Router::new().path(\"articles\").get(list_articles).post(create_article);\nRouter::new()\n    .path(\"articles/&lt;id&gt;\")\n    .get(show_article)\n    .patch(edit_article)\n    .delete(delete_article);\n</code></pre>\n<p>å¾€å¾€æŸ¥çœ‹æ–‡ç« å’Œæ–‡ç« åˆ—è¡¨æ˜¯ä¸éœ€è¦ç”¨æˆ·ç™»å½•çš„, ä½†æ˜¯åˆ›å»º, ç¼–è¾‘, åˆ é™¤æ–‡ç« ç­‰éœ€è¦ç”¨æˆ·ç™»å½•è®¤è¯æƒé™æ‰å¯ä»¥. Salvo ä¸­æ”¯æŒåµŒå¥—çš„è·¯ç”±ç³»ç»Ÿå¯ä»¥å¾ˆå¥½åœ°æ»¡è¶³è¿™ç§éœ€æ±‚. æˆ‘ä»¬å¯ä»¥æŠŠä¸éœ€è¦ç”¨æˆ·ç™»å½•çš„è·¯ç”±å†™åˆ°ä¸€èµ·ï¼š</p>\n<pre><code>Router::new()\n    .path(\"articles\")\n    .get(list_articles)\n    .push(Router::new().path(\"&lt;id&gt;\").get(show_article));\n</code></pre>\n<p>ç„¶åæŠŠéœ€è¦ç”¨æˆ·ç™»å½•çš„è·¯ç”±å†™åˆ°ä¸€èµ·ï¼Œ å¹¶ä¸”ä½¿ç”¨ç›¸åº”çš„ä¸­é—´ä»¶éªŒè¯ç”¨æˆ·æ˜¯å¦ç™»å½•ï¼š</p>\n<pre><code>Router::new()\n    .path(\"articles\")\n    .before(auth_check)\n    .post(list_articles)\n    .push(Router::new().path(\"&lt;id&gt;\").patch(edit_article).delete(delete_article));\n</code></pre>\n<p>è™½ç„¶è¿™ä¸¤ä¸ªè·¯ç”±éƒ½æœ‰è¿™åŒæ ·çš„ <code>path(\"articles\")</code>, ç„¶è€Œå®ƒä»¬ä¾ç„¶å¯ä»¥è¢«åŒæ—¶æ·»åŠ åˆ°åŒä¸€ä¸ªçˆ¶è·¯ç”±, æ‰€ä»¥æœ€åçš„è·¯ç”±é•¿æˆäº†è¿™ä¸ªæ ·å­:</p>\n<pre><code>Router::new()\n    .push(\n        Router::new()\n            .path(\"articles\")\n            .get(list_articles)\n            .push(Router::new().path(\"&lt;id&gt;\").get(show_article)),\n    )\n    .push(\n        Router::new()\n            .path(\"articles\")\n            .before(auth_check)\n            .post(list_articles)\n            .push(Router::new().path(\"&lt;id&gt;\").patch(edit_article).delete(delete_article)),\n    );\n</code></pre>\n<p><code>&lt;id&gt;</code>åŒ¹é…äº†è·¯å¾„ä¸­çš„ä¸€ä¸ªç‰‡æ®µ, æ­£å¸¸æƒ…å†µä¸‹æ–‡ç« çš„ <code>id</code> åªæ˜¯ä¸€ä¸ªæ•°å­—, è¿™æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é™åˆ¶ <code>id</code> çš„åŒ¹é…è§„åˆ™, <code>r\"&lt;id:/\\d+/&gt;\"</code>.</p>\n<p>æ›´å¤šä¿¡æ¯å¯ä»¥æŸ¥çœ‹ç½‘ç«™ https://salvo.rs</p>\n<p>æºç åœ°å€: https://github.com/salvo-rs/salvo</p>\n<p>éå¸¸æ¬¢è¿å¤§å®¶ä¸ºé¡¹ç›®è´¡çŒ®åŠ›é‡ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•ä¸ºé¡¹ç›®ä½œå‡ºè´¡çŒ®:</p>\n<ul>\n<li>åœ¨ issue ä¸­æäº¤åŠŸèƒ½éœ€æ±‚å’Œ bug report;</li>\n<li>åœ¨ issues æˆ–è€… require feedback ä¸‹ç•™ä¸‹è‡ªå·±çš„æ„è§;</li>\n<li>é€šè¿‡ pull requests æäº¤ä»£ç ;</li>\n<li>åœ¨åšå®¢æˆ–è€…æŠ€æœ¯å¹³å°å‘è¡¨ Salvo ç›¸å…³çš„æŠ€æœ¯æ–‡ç« ã€‚</li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-27 00:23:31","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"[å·²è§£å†³]println! ä¸¥é‡æ‹–å»¶æ•ˆèƒ½ï¼Œä»…åˆ—å°ä¸€è¡Œ","link":"https://rustcc.cn/article?id=ab0d06cb-d33d-4e18-b7b3-0b3e889f7b11","description":"<p>å½“æŠŠcallå‡½æ•°æ³¨è§£åï¼Œæˆ–æ˜¯æ³¨è§£println! éƒ½å¯ä»¥å¿«é€Ÿè¿è¡Œã€‚</p>\n<p>åœ¨ https://play.rust-lang.org/ ä¸Šæœ‰æ—¶å€™å¯ä»¥ \"\"ä½¿ç”¨println! \"\" è€Œä¸”ä¾ç„¶ç¼–è¯‘çš„å¾ˆå¿«ï¼Œæœ‰æ—¶å€™åˆ™ä¸è¡Œï¼Œæˆ‘è‡ªå·±æœ¬åœ°ç”µè„‘éƒ½ä¸è¡Œã€‚</p>\n<p>è¿™æ•ˆèƒ½å·®äº†åä¸‡å…«åƒé‡Œï¼Œè¯·å¤§å®¶å¸®å¿™ï¼Œæ–°æ‰‹æ€»æ˜¯åœ¨ println! è·Œå‘ã€‚</p>\n<p>è¿™è¾¹ä½¿ç”¨ <code>cargo run --release</code> ç¼–è¯‘</p>\n<pre><code>use std::time::{Duration, Instant};\n\nstruct Struct {\n    a: String,\n    b: bool,\n}\ntrait Dyn {}\nimpl Dyn for Struct {}\n\nfn main() {\n    let start = Instant::now();\n    let mut count = 0;\n    let count_end = 100_000_000i64;\n\n    while count &lt;= count_end {\n        let m: Box&lt;Struct&gt; = Box::new(Struct {\n            b: false,\n            a: \"str\".to_string(),\n        });\n        if count == count_end {\n            call();               // ---- è¿™å„¿\n            m.b;\n            m.a;\n        }\n        count += 1;\n    }\n\n    let duration = start.elapsed();\n    println!(\"Time: {:?}\", duration);\n}\n\nfn call(){\n    println!(\"run call()\\n\");     // ---- é‡ç‚¹åœ¨è¿™å„¿ï¼Œæ³¨è§£åå˜è¶…å¿«\n}\n</code></pre>\n<p>Time:</p>\n<table>\n<thead>\n<tr>\n<th align=\"right\">ğŸ˜«ä½¿ç”¨println!</th>\n<th align=\"right\">ğŸ˜„æ³¨è§£//println!</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"right\">12.863911s</td>\n<td align=\"right\">2.8486ms</td>\n</tr>\n<tr>\n<td align=\"right\">13.2101748s</td>\n<td align=\"right\">2.4661ms</td>\n</tr>\n<tr>\n<td align=\"right\">13.5353751s</td>\n<td align=\"right\">2.0433ms</td>\n</tr>\n<tr>\n<td align=\"right\">13.4852107s</td>\n<td align=\"right\">1.7869ms</td>\n</tr>\n<tr>\n<td align=\"right\">â€”â€”â€”â€”â€”â€”â€”â€”</td>\n<td align=\"right\">â€”â€”â€”â€”â€”â€”â€”â€”</td>\n</tr>\n</tbody>\n</table>\n<hr>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-26 16:17:11","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rust æ—¥æŠ¥ã€‘2021-8-26 Pin,Unpinä¸ºä»€ä¹ˆRustéœ€è¦å®ƒä»¬","link":"https://rustcc.cn/article?id=59d42687-69e5-4d20-ab8a-e296404fba92","description":"<h3>Pin,Unpinä¸ºä»€ä¹ˆRustéœ€è¦å®ƒä»¬</h3>\n<p>åˆæ˜¯ä¸€ç¯‡è®²<code>Pin</code>çš„blogï¼Œæ˜¯ä½œè€…æœ¬äººåœ¨å­¦ä¹ Rustå¼‚æ­¥è¿‡ç¨‹ä¸­åšçš„ä¸€äº›æ€»ç»“å’Œç†è§£ï¼Œæ–¹ä¾¿å¤§å®¶åœ¨å­¦ä¹ å¼‚æ­¥æ—¶é‡åˆ°ç›¸å…³ç–‘æƒ‘å¯ä»¥æŸ¥é˜…ã€‚</p>\n<p><a href=\"https://blog.adamchalmers.com/pin-unpin/\" rel=\"noopener noreferrer\">Read More</a>: https://blog.adamchalmers.com/pin-unpin/</p>\n<h3><code>Typing the technical interview</code>ä»Haskellç¿»è¯‘åˆ°Rust</h3>\n<p><code>Typing the technical interview</code>æ˜¯ä¸€ç¯‡å°†è®¡ç®—æœºçŸ¥è¯†æ‹Ÿä½œé­”æ³•çš„å°è¯´ï¼Ÿé‰´äºå°ç¼–å­¦è¯†æœ‰é™ï¼Œå¯¹è¿™ç¯‡blogä¸æ˜¯å¾ˆäº†è§£ï¼Œå¦‚æœ‰å¯¹è¿™ç¯‡Blogç†Ÿæ‚‰çš„å°ä¼™ä¼´ï¼Œå¯ä»¥å¸®å¿™ä»‹ç»ä¸€ä¸‹ã€‚åŸæ–‡æåˆ°çš„ç›¸å…³ä»£ç éƒ½æ˜¯ä½¿ç”¨Haskellå†™çš„ï¼Œç°åœ¨ç¤¾åŒºé‡Œæœ‰äººå°†å…¶ç”¨Rusté‡æ–°å®ç°äº†ä¸€éï¼š</p>\n<p><a href=\"https://github.com/insou22/typing-the-technical-interview-rust/\" rel=\"noopener noreferrer\">Github</a>: https://github.com/insou22/typing-the-technical-interview-rust/</p>\n<p>åŒæ—¶ï¼Œå¦‚æœå¯¹è¿™ç¯‡åŸæ–‡æ„Ÿå…´è¶£çš„ï¼Œé“¾æ¥ä¹Ÿåœ¨è¿™é‡Œï¼š</p>\n<p><a href=\"https://aphyr.com/posts/342-typing-the-technical-interview\" rel=\"noopener noreferrer\">Read More</a>: https://aphyr.com/posts/342-typing-the-technical-interview</p>\n<h3>å…³äºFutureså’Œè¿è¡Œæ—¶å¦‚ä½•å·¥ä½œçš„å¿ƒæ™ºæ¨¡å‹</h3>\n<blockquote>\n<p>è¿™ä¸€éƒ¨åˆ†çš„ä¸»è¦ç›®æ ‡æ˜¯å»ºç«‹ä¸€ä¸ªé«˜å±‚æ¬¡çš„å¿ƒç†æ¨¡å‹ï¼Œè¯´æ˜æˆ‘ä»¬åœ¨å‰ä¸€ç« ä¸­è¯»åˆ°çš„ä¸åŒéƒ¨åˆ†æ˜¯å¦‚ä½•ä¸€èµ·å·¥ä½œçš„ã€‚æˆ‘å¸Œæœ›è¿™å°†ä½¿æˆ‘ä»¬åœ¨æ¥ä¸‹æ¥çš„å‡ ç« ä¸­æ·±å…¥ç ”ç©¶ç‰¹è´¨å¯¹è±¡å’Œç”Ÿæˆå™¨ç­‰ä¸»é¢˜ä¹‹å‰ï¼Œæ›´å®¹æ˜“ç†è§£é«˜å±‚æ¬¡çš„æ¦‚å¿µã€‚</p>\n</blockquote>\n<blockquote>\n<p>è¿™å¹¶ä¸æ˜¯åˆ›å»ºä¸€ä¸ªå¼‚æ­¥ç³»ç»Ÿæ¨¡å‹çš„å”¯ä¸€æ–¹æ³•ï¼Œå› ä¸ºæˆ‘ä»¬è¦å¯¹è¿è¡Œæ—¶çš„å…·ä½“æƒ…å†µè¿›è¡Œå‡è®¾ï¼Œè€Œè¿™äº›æƒ…å†µå¯èƒ½ä¼šæœ‰å¾ˆå¤§çš„ä¸åŒã€‚è¿™æ˜¯æˆ‘è®¤ä¸ºæœ€å®¹æ˜“å»ºç«‹çš„æ–¹å¼ï¼Œè€Œä¸”å¯¹äºç†è§£ä½ åœ¨å¼‚æ­¥ç”Ÿæ€ç³»ç»Ÿä¸­å‘ç°çš„å¾ˆå¤šçœŸå®çš„å®ç°ä¹Ÿå¾ˆæœ‰æ„ä¹‰ã€‚</p>\n</blockquote>\n<blockquote>\n<p>æœ€åï¼Œè¯·æ³¨æ„ï¼Œç”±äºéœ€è¦ç®€æ´æ˜äº†ï¼Œä»£ç æœ¬èº«æ˜¯ \"å‡çš„\"ã€‚</p>\n</blockquote>\n<p><a href=\"https://cfsamson.github.io/books-futures-explained/2_a_mental_model_for_futures.html\" rel=\"noopener noreferrer\">Read More</a>: https://cfsamson.github.io/books-futures-explained/2_a_mental_model_for_futures.html</p>\n<p>From æ—¥æŠ¥å°ç»„ Cupnfish</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rust.cc è®ºå›: æ”¯æŒ rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRust è¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-26 14:30:26","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€å…¨èŒè¿œç¨‹ã€‘30k-50k/ç¡…è°·åˆåˆ›å…¬å¸æ‹›/åµŒå…¥å¼å¼€å‘/ä¸­æ–‡å‹å¥½","link":"https://rustcc.cn/article?id=f8bcde1a-4455-455e-ad89-d1e507132c57","description":"<p>å…¬å¸ç®€ä»‹\næˆ‘ä»¬æ˜¯ä¸€å®¶æ€»éƒ¨ä½äºç¡…è°·çš„åˆåˆ›å…¬å¸ï¼Œå…¬å¸çš„æ ¸å¿ƒå¼€å‘äººå‘˜æ¥è‡ªäºGoogle, äºšé©¬é€Šï¼Œvmwareç­‰ä¸€çº¿ä¼ä¸šï¼ŒæŠ€æœ¯å®åŠ›é›„åšã€‚</p>\n<p>å²—ä½è¦æ±‚\n1ã€æœ¬ç§‘ä»¥ä¸Šå­¦å†ï¼Œä¸“ä¸šä¸é™ï¼›çƒ­çˆ±ç¼–ç¨‹ï¼Œä¸æ€•éº»çƒ¦ï¼Œå…·æœ‰æ­»ç£•ç²¾ç¥ï¼›</p>\n<p>2ã€æ·±å…¥ç†è§£Linuxæ“ä½œå†…æ ¸çš„ä¸€ä¸ªæˆ–å¤šä¸ªå­ç³»ç»Ÿï¼Œè­¬å¦‚è¿›ç¨‹ç®¡ç†ã€å†…å­˜ç®¡ç†ã€è®¾å¤‡ç®¡ç†ã€ç½‘ç»œå­ç³»ç»Ÿï¼›</p>\n<p>3ã€æ·±å…¥å‚ä¸è¿‡åµŒå…¥å¼äº§å“å¼€å‘çš„æ•´ä¸ªç”Ÿå‘½å‘¨æœŸï¼ŒåŒ…æ‹¬ä½†ä¸å±€é™äºBootloader, Kernel, driver, åº”ç”¨å±‚ï¼›</p>\n<p>4ã€æ‰å®çš„Cè¯­è¨€ç¼–ç¨‹å’Œè°ƒè¯•åŠŸåº•ï¼Œä¸°å¯Œçš„åº”ç”¨å±‚å¼€å‘ç»éªŒï¼Œå¯¹å†…å­˜æ³„æ¼å’Œæ®µé”™è¯¯å¤„ç†æœ‰è¿‡ä¸°å¯Œçš„å¤„ç†ç»éªŒï¼›</p>\n<p>5ã€æœ‰ç‹¬ç«‹å¼€å‘æˆ–è€…ä¸»å¯¼å¼€å‘é¡¹ç›®ç»éªŒçš„åº”è˜è€…ä¼˜å…ˆè€ƒè™‘ï¼›æœ‰ç‹¬ç«‹ä½œå“æ¥å±•ç¤ºè‡ªå·±æ°´å¹³çš„åº”è˜è€…ä¼˜å…ˆè€ƒè™‘ã€‚</p>\n<p>é™¤äº†å·¥ä½œæ‰€éœ€çš„å¿…è¦æŠ€èƒ½ä¹‹å¤–ï¼Œæˆ‘ä»¬è¿˜å¸Œæœ›æ‚¨æ˜¯ä¸€ä¸ªæœ‰è´£ä»»å¿ƒå’Œå¥½å¥‡å¿ƒçš„å¼€å‘äººå‘˜ï¼Œèƒ½è·Ÿå…¬å¸ä¸€èµ·å¿«é€Ÿæˆé•¿ã€‚</p>\n<p>èŒä½è¦æ±‚\nåœ¨å›¢é˜Ÿè´Ÿè´£äººçš„å¸¦é¢†å‚ä¸å…¬å¸è€äº§å“çš„ç»´æŠ¤å’Œæ–°äº§å“çš„ç ”å‘ã€‚</p>\n<p>è–ªèµ„ç¦åˆ©\n1ã€æœˆè–ª30k-50kï¼›</p>\n<p>2ã€å…¥èŒæ»¡ä¸€å¹´ï¼Œè¡¨ç°åˆæ ¼è€…å¯ä»¥è·å¾—å…¬å¸çš„è‚¡ç¥¨æˆ–æœŸæƒï¼›</p>\n<p>3ã€ä¼˜ç§€è€…æä¾›ç§»æ°‘ç¾å›½ï¼ŒåŠ æ‹¿å¤§çš„æœºä¼šã€‚</p>\n<p>å·¥ä½œæ–¹å¼\n1ã€è¿œç¨‹åŠå…¬</p>\n<p>2ã€å·¥ä½œæ—¶é—´ï¼šæ— å›ºå®šæ—¶é—´ï¼Œå·¥ä½œå®Œæˆåè‡ªç”±å®‰æ’</p>\n<p>3ã€å¾ˆå°‘æœ‰è·¨æ—¶åŒºçš„ä¼šè®®ï¼Œé™¤ç‰¹æ®Šã€ç´§æ€¥å·¥ä½œä»»åŠ¡å¯¹æ¥å¤–</p>\n<p>4ã€å·¥ä½œä¼šè®®ä¸»è¦ä¸ºä¸­æ–‡äº¤æµ</p>\n<p>å·¥ä½œè¯­è¨€\n1ã€ä¸­æ–‡ä¸ºä¸»ï¼Œè‹±æ–‡ä¸ºè¾…ï¼›</p>\n<p>2ã€ä¸è¦æ±‚å¾ˆå¼ºçš„å¬è¯´ï¼Œåªè¦æ±‚è¯»å†™èƒ½åŠ›åŠæ ¼ã€‚</p>\n<p>å½•ç”¨æµç¨‹\n1ã€æ”¶åˆ°ç®€å†åï¼Œå°†å®‰æ’1åˆ°3è½®ç”µè¯è¯­éŸ³é¢è¯•ï¼ˆä¼˜ç§€è€…ä¸€è½®é¢è¯•å³å¯ï¼‰ï¼›</p>\n<p>2ã€ç”µè¯è¯­éŸ³é¢è¯•ç»“æŸåï¼Œå°†è¿›å…¥è¯•ç”¨æœŸï¼ˆå…¨é¢å·¥èµ„ï¼‰ï¼›</p>\n<p>3ã€è¯•ç”¨æœŸç»“æŸåï¼Œæ­£å¼å¼€å§‹å·¥ä½œã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-26 11:18:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust traitå¥—å¨ƒå®ç°,","link":"https://rustcc.cn/article?id=bff10963-a6a9-4cc7-a3ce-4cfde731c72e","description":"<pre><code>impl&lt;'c, C: ?Sized + Completer&gt; Completer for &amp;'c C {\n    type Candidate = C::Candidate;\n\n    fn complete(\n        &amp;self,\n        line: &amp;str,\n        pos: usize,\n        ctx: &amp;Context&lt;'_&gt;,\n    ) -&gt; Result&lt;(usize, Vec&lt;Self::Candidate&gt;)&gt; {\n        (**self).complete(line, pos, ctx)\n    }\n\n    fn update(&amp;self, line: &amp;mut LineBuffer, start: usize, elected: &amp;str) {\n        (**self).update(line, start, elected)\n    }\n}\n</code></pre>\n<p>è¿™ç§å¥—å¨ƒå®ç°traitçš„æ–¹å¼å¦‚ä½•ç†è§£å•Šï¼Œ å¸ˆå‚…ä»¬æœ‰äº†è§£çš„å—</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-26 11:05:42","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"è¿œç¨‹åŠå…¬ï¼Œä¸é™åœ°åŸŸï¼Œç¼´çº³ç¤¾ä¿å…¬ç§¯é‡‘ï¼Œå‘¨æœ«åŒä¼‘ï¼Œå‘Šåˆ« 996ï¼Œæ‹’ç» 007ï¼ŒNervina Labs æ¬¢è¿ä½ ï¼","link":"https://rustcc.cn/article?id=a4789334-4646-4a33-86a2-21bc65a50824","description":"<p>Rustå¼€å‘å·¥ç¨‹å¸ˆ\nå²—ä½èŒè´£ï¼š\n1ã€è´Ÿè´£æ™ºèƒ½åˆçº¦çš„å¼€å‘åŠè®¾è®¡ï¼›\n2ã€è´Ÿè´£åŒºå—é“¾ä¸šåŠ¡ç³»ç»Ÿåˆ†æä¸è®¾è®¡å·¥ä½œï¼›\n3ã€è´Ÿè´£æ™ºèƒ½åˆçº¦ä»£ç æµ‹è¯•ã€è¿è¡Œå’Œç»´æŠ¤ã€‚</p>\n<p>ä»»èŒè¦æ±‚ï¼š\n1ã€è®¡ç®—æœºç›¸å…³ä¸“ä¸šæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼Œ3å¹´ä»¥ä¸Šå·¥ä½œç»éªŒï¼›\n2ã€ç†Ÿç»ƒæŒæ¡ C/C++ã€Rust ç­‰ç³»ç»Ÿå¼€å‘è¯­è¨€è‡³å°‘ä¸€ç§ï¼Œè‡³å°‘æœ‰è¿‡ä¸¤å¹´ç›¸å…³å¼€å‘ç»éªŒï¼›\n3ã€å¯¹æ•°æ®ç»“æ„å’Œç®—æ³•ï¼Œå¯¹å¯†ç å­¦ï¼Œå®‰å…¨åè®®å’ŒåŠ å¯†ç®—æ³•æœ‰ç ”ç©¶è€…ä¼˜å…ˆï¼›\n4ã€ä¼˜ç§€çš„è‹±è¯­æ–‡æ¡£æ’°å†™ä¸é˜…è¯»èƒ½åŠ›è€…ä¼˜å…ˆï¼›\n5ã€äº†è§£åŒºå—é“¾ï¼Œæœ‰åˆçº¦å¼€å‘ç»éªŒæ›´ä½³ã€‚\nå›¢é˜Ÿä»‹ç»ï¼šä¸ºä»€ä¹ˆåŠ å…¥æˆ‘ä»¬\n1ã€100%è¿œç¨‹å·¥ä½œï¼Œä½ å¯ä»¥baseåœ¨å…¨çƒä»»ä½•ä½ æƒ³å¾…çš„åœ°æ–¹ï¼›</p>\n<p>2ã€æ¯å¹´è‡³å°‘5å¤©ä»¥ä¸Šçš„å¹´å‡ï¼Œ2æ¬¡æ—…æ¸¸å·¥ä½œçš„æœºä¼šï¼›</p>\n<p>3ã€å…¨å‘˜é«˜æ¸©è¡¥è´´+ç”µè„‘è¡¥åŠ©+ç½‘ç»œåŠ é€Ÿè¡¥åŠ©</p>\n<p>4ã€äº”é™©ä¸€é‡‘+å…¥èŒå¤§ç¤¼åŒ…</p>\n<p>å…¬å¸äº§å“\nç®€æ´æœ‰è¶£çš„äº§å“ä»‹ç»ï¼Œèƒ½è®©ç”¨æˆ·æœ€å¿«é€Ÿåº¦äº†è§£å…¬å¸ä¸šåŠ¡ã€‚æŠŠè‡ªå®¶ä¼˜ç§€çš„äº§å“å±•ç¤ºå‡ºæ¥å¸å¼•äººæ‰å›´è§‚å§ï¼</p>\n<p>å…¬å¸ä»‹ç»\nèŠ‚ç‚¹äº’ä¿¡ç”±å‰ Nervos (https://www.nervos.org) æ ¸å¿ƒåº”ç”¨å¼€å‘è€…å‘èµ·ã€‚æˆ‘ä»¬çš„ç†æƒ³æ˜¯é€šè¿‡æŠ€æœ¯å’Œå•†ä¸šåŠªåŠ›ï¼Œè®©åŒºå—é“¾æŠ€æœ¯å°½æ—©è½åœ°ï¼Œè®©æ™®é€šç”¨æˆ·ä¹Ÿèƒ½å¤Ÿäº«å—åŒºå—é“¾æŠ€æœ¯å¸¦æ¥çš„ä»·å€¼ã€‚æˆ‘ä»¬å°†ä¸å¹¿å¤§ä¼ ç»Ÿäº’è”ç½‘å…¬å¸åˆä½œï¼Œåœ¨ç‰ˆæƒã€ç‰©æƒã€æ•°å­—èº«ä»½ç­‰é¢†åŸŸæ‰“é€ å¼€æ”¾äº’ä¿¡å…±äº«äº’é€šçš„ä»·å€¼ç½‘ç»œå¹³å°ï¼Œå¸®åŠ©ä¼ ç»Ÿå¹³å°çš„ç”¨æˆ·å°†è‡ªå·±çš„èµ„äº§ã€ä¿¡æ¯ã€æƒç›Šåœ¨æ›´å¼€æ”¾çš„åŒºå—é“¾å¹³å°ä¸Šå®ç°å•†ä¸šæ¨¡å¼å‡çº§ã€‚</p>\n<p>å¯¹å¾…äººæ‰ï¼Œæˆ‘ä»¬æœ‰3ä¸ªå…³é”®è¯ï¼šå¼€æ”¾ã€è‡ªé©±ã€æ¶Œç°ã€‚æˆ‘ä»¬ä¸€ç›´å¥‰è¡Œå¼€æ”¾å’Œå¼€æºçš„ç²¾ç¥ï¼Œåšä¿¡é€æ˜æ˜¯ä¿¡ä»»çš„åŸºç¡€ï¼Œå¼€æºæ˜¯åŒºå—é“¾çš„åŸºçŸ³ï¼Œæ‰€æœ‰çš„é¡¹ç›®ä»£ç å‡åœ¨Githubå¼€æºã€‚æˆ‘ä»¬100%è¿œç¨‹å·¥ä½œï¼Œä½ å¯ä»¥ base åœ¨å…¨çƒä»»ä½•ä¸€ä¸ªä½ æƒ³å¾…çš„åœ°æ–¹ã€‚æˆ‘ä»¬é¼“åŠ±å‘˜å·¥å¯¹è‡ªæˆ‘è¿›è¡Œç®¡ç†ï¼Œä¸ºåŒºå—é“¾ç”Ÿæ€æ·»ç –åŠ ç“¦ã€‚ä½ è®¤ä¸ºæ•´ä¸ªåŒºå—é“¾ç”Ÿæ€ç¼ºä»€ä¹ˆï¼Œä½ å¯ä»¥æå‡ºæ–¹æ¡ˆã€é¢„ç®—å’Œæ‹›è˜éœ€æ±‚ï¼Œå…¬å¸å†…éƒ¨è®¨è®ºé€šè¿‡åå¯ä»¥ç»™ä½ èµ„æºè®©ä½ å»å®ç°ã€‚</p>\n<p>ä½œä¸ºå›¢é˜Ÿçš„ä¼ ç»Ÿï¼Œæˆ‘ä»¬æ¯å¹´è®¡åˆ’æœ‰ä¸¤æ¬¡å°é—­å¼€å‘çš„â€œå›¢å»ºâ€æ´»åŠ¨ï¼Œä¸€èˆ¬ä¼šé€‰åœ¨æ­å·ã€é’å²›ç­‰é£æ™¯é¥®é£Ÿä¿±ä½³çš„åŸå¸‚ï¼ŒåŒ…ä¸‹ä¸€ä¸ªæ°‘å®¿æˆ–è€…åˆ«å¢…ï¼Œç”¨ä¸€å‘¨å·¦å³çš„æ—¶å€™ä¾›å¤§å®¶å­¦ä¹ äº¤æµã€‚å¦‚æœä½ å–œæ¬¢è¿™æ ·çš„å·¥ä½œæ–¹å¼ï¼Œæ¬¢è¿åŠ å…¥æˆ‘ä»¬ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-26 07:28:05","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"æ¥è¯´è¯´rusté‡Œä½ æœ€ä¸å–œæ¬¢çš„éƒ¨åˆ†æ˜¯å“ªäº›","link":"https://rustcc.cn/article?id=495cd0ad-4c0b-46db-8585-a2a0de1cb43d","description":"<p>å…ˆå£°æ˜ï¼Œä¸æ˜¯é»‘RUSTå“¦ï¼Œåªæ˜¯è¯´è¯´RUSTä¸­æœ€ä¸è®¨å–œçš„ä¸€äº›ä¸œè¥¿è€Œå·²ã€‚\nå¯¹æˆ‘ä¸ªäººæ¥è¯´ï¼Œæœ€è®¨åŒRUSTçš„å®åŠŸèƒ½ï¼Œä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºå®åŠŸèƒ½è®©RUSTçš„å„ç§åº“é‡Œå‡ºç°äº†å„ç§â€œæ–¹è¨€â€ï¼ŒRUSTçš„å®åŠŸèƒ½å¾ˆå¼ºå¤§è¿™ä¸ªä¸å‡ï¼Œä½†æ˜¯è¿™ä¸ªåŠŸèƒ½çš„å‡ºç°ï¼Œä½¿å¾—æœ¬æ¥å°±åºæ‚çš„RUSTè¿›ä¸€æ­¥å‡ºç°äº†æ›´å¤šæ ¹æœ¬å°±ä¸åœ¨RUSTè¯­è¨€é‡Œé¢çš„â€œæ–¹è¨€â€è¯­æ³•ï¼Œä¸åŒçš„åŒ…é‡Œä¸åŒçš„æ–¹è¨€ä¸åŒçš„ç”¨æ³•ï¼Œè¿™ä¼šå¯¼è‡´RUSTçš„è¿›ä¸€æ­¥çš„ç¢ç‰‡åŒ–ï¼Œè¿IDEéƒ½æ— æ³•æœ‰æ•ˆè¯†åˆ«å®åŠé‡Œé¢çš„è¯­å¥ï¼ŒRUSTæœ¬æ¥å°±ç¹æ‚éš¾å­¦ï¼Œå†…å®¹ä¼—å¤šï¼Œå®åˆè¿›ä¸€æ­¥åŠ å‰§äº†è¿™ç§æƒ…å†µï¼Œä½¿å¾—é˜…è¯»RUSTä»£ç ä¼šè¿›ä¸€æ­¥ä¸ç›´è§‚ï¼Œæ‰€ä»¥æˆ‘æœ€è®¨åŒRUSTçš„å®ã€‚\næ¯”å¦‚è¯´åœ¨tokioé‡Œï¼Œä¼šæœ‰è¿™æ ·çš„è¯­æ³•</p>\n<p>#[tokio::main]\npub async fn main() -&gt; Result&lt;()&gt; {\nOk(())\n}</p>\n<p>æƒ³è¿™æ ·çš„è¯­å¥éœ€è¦çœ‹tokioçš„æ–‡æ¡£æ‰èƒ½çŸ¥é“å®ƒè¾“å‡ºçš„å®Œå…¨ä½“å¤§æ¦‚æ˜¯å•¥ï¼Ÿå¥½ä¸å¥½ï¼ŸæŒºå¥½ï¼Œèƒ½å°‘è¾“å…¥å‡ è¡Œä»£ç ã€‚å¥½ä¸å¥½ï¼Ÿä¸å¥½ï¼Œæˆ‘å¾—å…ˆç†è§£å®ï¼Œå†ç»“åˆtokioå¾—æ–‡æ¡£ä»£ç ç¤ºä¾‹ï¼Œæ‰èƒ½ç¡®åˆ‡å¾—çŸ¥é“å…¶ä»£è¡¨å¾—å®Œæ•´å«ä¹‰ã€‚\næ€»ä½“ä¸Šæ¥è¯´ï¼Œæˆ‘è®¤ä¸ºRUSTå¾—å®åŠŸèƒ½æ—¢æ˜¯ä¸€ä¸ªç¥å™¨ï¼Œå´ä¹Ÿæ˜¯ä¸€ä¸ªå·¨å‘ï¼Œå¾ˆè®¨åŒã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-26 04:04:57","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</h3>\n<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´8æœˆ29æ—¥ 20:00-21:00</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»:</strong>  è®²åˆ° Rust ä½¿ç”¨ Future å¼‚æ­¥ç¼–ç¨‹ï¼Œå°±ä¸å¾—ä¸è¯´ futures å’Œ tokio è¿™ä¸¤ä¸ª crateï¼Œå…¶å®æ ‡å‡†åº“ä¸­çš„ futureï¼Œä»¥åŠ async/await å°±æ˜¯ä» futures åº“ä¸­æ•´åˆè¿›æ ‡å‡†åº“çš„, Tokio æ‹¥æœ‰æå¿«çš„æ€§èƒ½ï¼Œæ˜¯å¤§éƒ¨åˆ†ç³»ç»Ÿå¼‚æ­¥å¤„ç†çš„é€‰æ‹©ï¼Œå…¶æ„å»ºäº future ä¹‹ä¸Šã€‚Future æ˜¯  Rust å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒåŸºç¡€ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<p>1ã€ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥.</p>\n<p>2ã€ç†è§£å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>\n<p>3ã€Future ç¼–ç¨‹æ¨¡å‹è®²è§£.</p>\n<p>4ã€å¸¦é¢†å¤§å®¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ future , å†æ¬¡å¸®å¿™å¤§å®¶ç†è§£</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-19 -- Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</h3>\n<p>å·²ç»åœ¨ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªcrate ä¸Šæµ‹è¯•äº†ç‰ˆæœ¬è¿ç§»,å¹¶ä¸”å°†æµ‹è¯•æ‰€æœ‰å…¬å…±çš„ crateã€‚</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>å¼‚æ­¥å¼•æ“ C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D æ¸¸æˆå¼•æ“</h3>\n<ul>\n<li><strong>PCï¼ˆWindowsã€Linuxã€macOSï¼‰å’Œ Web (WebAssembly)</strong> æ”¯æŒã€‚</li>\n<li><strong>å»¶è¿Ÿç€è‰²</strong></li>\n<li><strong>å†…ç½®ä¿å­˜/åŠ è½½</strong></li>\n<li><strong>ç‹¬ç«‹åœºæ™¯ç¼–è¾‘å™¨</strong></li>\n<li><strong>é«˜çº§ç‰©ç†æ¨¡å‹</strong></li>\n<li><strong>åˆ†å±‚æ¨¡å‹èµ„æº</strong></li>\n<li><strong>å‡ ä½•å®ä¾‹åŒ–</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ å†°å±±ä¸Šçš„ mook &amp;&amp; æŒºè‚¥</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾: é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Šé€šè¿‡Datafuseç†è§£å…¨é“¾è·¯è·Ÿè¸ªã€‹| Vol. 4</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ22æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong> æ•°æ®åº“ç³»ç»Ÿä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚ï¼Œåºå¤§çš„ç³»ç»Ÿã€‚ç‰¹åˆ«æ˜¯åœ¨è°ƒè¯•å’Œè§‚å¯ŸSQLæ‰§è¡Œï¼Œå¤šçº¿ç¨‹ä»»åŠ¡åˆ‡æ¢ï¼Œå› ä¸ºæ²¡æœ‰å†…å­˜è°ƒç”¨æˆ–å †æ ˆè·Ÿè¸ªï¼Œè¿™ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¿½è¸ªçš„ç”±æ¥ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°å¤šè¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ªä¸ºæè¿°å’Œåˆ†æè·¨è¿›ç¨‹äº‹åŠ¡æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚Google Dapper(Dapper: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿé“¾è·¯è¿½è¸ªåŸºç¡€è®¾æ–½)è®ºæ–‡(å„tracerçš„åŸºç¡€)ä¸­æè¿°äº†åˆ†å¸ƒå¼è¿½è¸ªçš„ä¸€äº›ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­ç¨³æ€é—®é¢˜ã€åˆ†å¸ƒå¼åˆ†æã€èµ„æºå±æ€§å’Œå¾®æœåŠ¡çš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ã€‚</p>\n<p>æœ¬æ¬¡å…¬å¼€è¯¾é€š Google çš„ OpenTraceing ä»‹ç»ï¼Œç»“åˆRustçš„ tokio-rs/tracing ä½¿ç”¨ï¼Œæœ€ç»ˆç»“åˆ Datafuse é¡¹ç›®ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹å¤§å‹åº”ç”¨çš„å…¨é“¾è·¯è·Ÿè¸ªåˆ†æè¿‡ç¨‹ã€‚</p>\n<p>å…³äºDatafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ol>\n<li>\n<p>ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»ŸOpenTracingåŠåº”ç”¨åœºæ™¯</p>\n</li>\n<li>\n<p>ä»‹ç» tokio-rs/tracing åŠåœ¨ç¨‹åºå¼€å‘ä¸­çš„ä½œç”¨</p>\n</li>\n<li>\n<p>ä¸ºä»€ä¹ˆéœ€è¦tokio-rs/tracingåº“</p>\n</li>\n<li>\n<p>æ¼”ç¤ºDatafuseé¡¹ç›®ä¸­tokio-rs/tracingçš„ä½¿ç”¨</p>\n</li>\n</ol>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"è®ºå›githubè´¦æˆ·æ— æ³•ç™»å½•è§£å†³ç¬”è®°","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>æœ‰åæ˜ è¿™ä¸¤å¤©githubè´¦æˆ·æ— æ³•ç™»å½•äº†ã€‚</p>\n<p>æŠ¥è¿™ä¸ªé”™ï¼š</p>\n<pre><code>get github user info err\n</code></pre>\n<p>æŸ¥äº†å‡ ä¸ªåœ°æ–¹ï¼š</p>\n<ol>\n<li>ä»£ç æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼šOk</li>\n<li>httpsä»£ç†æ˜¯å¦æ­£å¸¸ï¼šOk</li>\n<li>æ£€æŸ¥äº†githubè¿”å›æ—¥å¿—ï¼Œå‘ç°æ˜¯ï¼š</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>è¿›å…¥è¿™ä¸ªåœ°å€ä¸€çœ‹ï¼š<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>åŸæ¥2020å¹´2æœˆå°±å·²ç»è¯´äº†ï¼Œè¦æ”¹è¦æ”¹ã€‚ä¸è¿‡æˆ‘ç¡®å®æ²¡ç•™æ„åˆ°è¿™ä¸ªä¿¡æ¯ã€‚ï¼šï¼ˆ</p>\n<p>æ„æ€å°±æ˜¯è¯´access_tokenä¸è¦æ”¾åœ¨queryå‚æ•°ä¸­ï¼Œè€Œæ˜¯è¦æ”¾åœ¨headeré‡Œé¢ã€‚ç…§å®ƒè¯´çš„ï¼Œæ”¹äº†åå°±å¥½äº†ã€‚</p>\n<p>ç‰¹æ­¤è®°å½•ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust çš„ Future ä¸ Javascript çš„ Promise åŠŸèƒ½å¯¹ç…§å‚è€ƒ","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>çš„<code>Future</code>ä¸<code>Javascript</code>çš„<code>Promise</code>åŠŸèƒ½å¯¹ç…§å‚è€ƒ</h1>\n<p>å­¦ä¹ æ–°é²œæŠ€æœ¯æ—¶ï¼Œæˆ‘æ€»æ˜¯ä¼šä¹ æƒ¯æ€§å‘æ›¾ç»ç†Ÿæ‚‰çš„å†…å®¹ä¸Šé ï¼Œç”šè‡³å¥—ç”¨ç°æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¿™æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œå¯¹ç…§<code>Javascript - Promise/A+ API</code>æ¥è®°å¿†ä¸€éƒ¨åˆ†<code>Rust Future</code>å¸¸ç”¨<code>API</code>ã€‚</p>\n<blockquote>\n<p>æ³¨æ„ï¼šæ‰€æœ‰çš„<code>Rust - Future</code>æ“ä½œéƒ½æ˜¯ä»¥<code>.await</code>ç»“å°¾çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œä¸åŒäº<code>Javascript - Promise/A+</code>ï¼Œ<code>Rust - Future</code>æ˜¯æƒ°æ€§çš„ã€‚åªæœ‰è¢«<code>.await</code>æŒ‡ä»¤æ¿€æ´»åï¼Œåœ¨<code>Rust - Future</code>å†…å°è£…çš„æ“ä½œæ‰ä¼šè¢«çœŸæ­£åœ°æ‰§è¡Œã€‚</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">æè¿°</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* ä»€ä¹ˆéƒ½ä¸åš */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll ä¸èƒ½è¢«ç”¨æ¥æ„é€ åŒ…å«äº†å¼‚æ­¥æ“ä½œçš„ Future å®ä¾‹ï¼Œå› ä¸ºã€å›è°ƒé—­åŒ…ã€‘å†…çš„ã€å¯ä¿®æ”¹å¼•ç”¨ã€‘&amp;mut Context&lt;'_&gt; ä¸èƒ½è¢«  ï¼ˆ1ï¼‰è·¨çº¿ç¨‹ä¼ é€’  ï¼ˆ2ï¼‰ä¼ é€’å‡ºé—­åŒ…ä½œç”¨åŸŸ2. task::spawn_blocking() ã€å›è°ƒé—­åŒ…ã€‘è¾“å…¥å‚æ•°å†…çš„ thread::sleep() ä¸æ˜¯é˜»å¡è¿è¡Œ task::spawn_blocking() çš„ä¸»çº¿ç¨‹ï¼Œè€Œæ˜¯é˜»å¡ä»ã€é˜»å¡ä»»åŠ¡çº¿ç¨‹æ± ã€‘ä¸­åˆ†é…æ¥è¿è¡Œé˜»å¡ä»»åŠ¡çš„ã€å·¥ä½œçº¿ç¨‹ã€‘ã€‚</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. æœ‰ä¸€ä¸ª promise/future å¤±è´¥å°±æ•´ä½“æ€§åœ°å¤±è´¥ã€‚2. try_join æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future çš„æˆåŠŸä¸å¤±è´¥ç»“æœéƒ½æ”¶é›†2. è¿”å›ç»“æœï¼š(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. ä»…åªæ”¶é›†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ promise/future2. try_race æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. æ”¶é›†ç¬¬ä¸€ä¸ªç»“æŸçš„ promise/futureï¼Œæ— è®ºå®ƒæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯å¤±è´¥æ”¶åœºã€‚2. è¿”å›ç»“æœï¼šT</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼š</strong>ã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹</p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ15æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong></p>\n<p>å¦‚æœæƒ³ç”¨ Rust å¼€å‘å¤§å‹ç›®ï¼Œæˆ–è€…å­¦ä¹ å¤§å‹é¡¹ç›®ä»£ç ï¼Œç‰¹åˆ«æ˜¯æ¡†æ¶çº§åˆ«çš„é¡¹ç›®ï¼Œé‚£ä¹ˆ Rust çš„å®æœºåˆ¶è‚¯å®šæ˜¯ä¸€ä¸ªå¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚ ä¾‹å¦‚ datafuse ä¸­çš„ä¸€äº›é…ç½®ç®¡ç†ï¼š\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>è¿™å°±æ˜¯é€šè¿‡å®å®ç°é…ç½®çš„ç»Ÿä¸€è¡Œä¸ºï¼Œä»£ç å‚è€ƒï¼š\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust è¯­è¨€å¼ºå¤§çš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¯ä»¥åˆ›å»ºå’Œåˆ©ç”¨å®ï¼Œä¸è¿‡åˆ›å»ºå®çœ‹èµ·æ¥æŒºå¤æ‚ï¼Œå¸¸å¸¸ä»¤åˆšæ¥è§¦ Rust çš„å¼€å‘è€…ç”Ÿç•æƒ§ã€‚ åœ¨æœ¬æ¬¡å…¬å¼€è¯¾ä¸­å¸®åŠ©ä½ ç†è§£ Rust Macro çš„åŸºæœ¬åŸç†ï¼Œå­¦ä¹ å¦‚ä½•åˆ›è‡ªå·²çš„ Rust å®ï¼Œä»¥åŠæŸ¥çœ‹æºç å­¦ä¹ å®çš„å®ç°ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ul>\n<li>ä»€ä¹ˆæ˜¯ Rust å®</li>\n<li>ä»€ä¹ˆæ˜¯å®è¿è¡ŒåŸç†</li>\n<li>å¦‚ä½•åˆ›å»º Rust å®è¿‡ç¨‹</li>\n<li>é˜…è¯» datafuse é¡¹ç›®æºç ï¼Œ å­¦ä¹ é¡¹ç›®ä¸­å®çš„å®ç°</li>\n</ul>\n<p><strong>è®²å¸ˆä»‹ç»</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šç†è§£Rustçš„æ‰€æœ‰æƒ| Vol 2","link":"https://rustcc.cn/article?id=c107b830-9fe1-43dd-94a3-9efcd5544205","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼šã€Šç†è§£Rustæ‰€æœ‰æƒã€‹</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š2021å¹´8æœˆ8æ—¥ 20:30-21:30</strong></p>\n<p><strong>å˜‰å®¾è®²å¸ˆï¼š è‹æ—</strong></p>\n<p><strong>å˜‰å®¾ä»‹ç»ï¼š</strong></p>\n<p>Rustä¸­æ–‡ç¤¾åŒºæˆå‘˜ï¼Œå¤šç‚¹DmallæŠ€æœ¯Leaderï¼Œå‰æŠ˜800äº’è”ç½‘ç ”å‘å›¢é˜Ÿè´Ÿè´£äººã€10ä½™å¹´ä¸€çº¿ç ”å‘ç»éªŒã€‚å…·æœ‰å¤šå¹´çš„è½¯ä»¶å¼€å‘ç»éªŒ, ç†Ÿç»ƒRubyã€Javaã€Rustç­‰å¼€å‘è¯­è¨€, åŒæ—¶ä¹Ÿå‚ä¸è¿‡Rustä¸­æ–‡ç¤¾åŒºæ—¥æŠ¥ç»´æŠ¤å·¥ä½œã€‚</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»</strong></p>\n<p>æœ¬æ¬¡è¯¾ç¨‹é€šè¿‡10ä¸ªå·¦å³çš„å°ä¾‹å­ï¼Œå¸¦å¤§å®¶ç†è§£ä¸€ä¸‹Rustçš„æ‰€æœ‰æƒï¼ŒRustå¼•ç”¨å’Œå€Ÿç”¨ï¼ŒRustå˜é‡å…‹éš†å’Œå¤åˆ¶çš„ç†å¿µã€‚</p>\n<p><strong>å‚åŠ è¯¾ç¨‹</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/Rust-pbc-1.jpg\" alt=\"\"></p>\n<p><strong>è¯¾ç¨‹è§„åˆ’</strong></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloudé¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-08 02:04:00","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"æ•°æ®è¡¨ Timestamp æ—¥æœŸ Serialize","link":"https://rustcc.cn/article?id=2ff8a69e-59bb-4502-87c0-c3416ffae8a0","description":"<p>ä¸»è¦å‚è€ƒï¼š<a href=\"https://github.com/rustcc/forustm\" rel=\"noopener noreferrer\">Rustccç½‘ç«™æºç åº“</a></p>\n<p>åœ¨å¤„ç†æ•°æ®è¡¨ä¸­æ—¥æœŸç›¸å…³æ•°æ®æ—¶ï¼ŒSeralizeåºåˆ—åŒ–ç›¸å…³æ“ä½œä¼šæŠ¥é”™ï¼Œæç¤º DateTime å­—æ®µä¸è¯†åˆ«ï¼Œ\næŸ¥äº† rustcc æºç æ‰å‘ç°ä¾èµ–ä¸­éœ€è¦å¼€å¯ç›¸åº”çš„featureã€‚ç‰¹æ­¤è®°å½•ã€‚</p>\n<h2>1.ä¾èµ–çš„åº“ï¼š</h2>\n<pre><code>[dependencies]\n# æ—¥æœŸæ—¶é—´å¤„ç† éœ€è¦å¼€å¯ serde ç‰¹å¾ æ”¯æŒåºåˆ—åŒ–\nchrono = { version = \"0.4.19\", features = [\"serde\"] }\n\n# æ•°æ®åº“ORM\ndiesel = { version = \"1.4.4\", features = [\"postgres\", \"chrono\", \"uuid\", \"r2d2\"] }\ndotenv = \"0.15.0\"\nserde = { version = \"1.0.127\", features = [\"derive\"] }\nserde_json = \"1.0.66\"\nuuid = { version = \"0.8.2\", features = [\"serde\", \"v4\"] }\n</code></pre>\n<h2>2.åˆ›å»ºæ•°æ®è¡¨</h2>\n<pre><code>CREATE TABLE characters (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(128) UNIQUE NOT NULL,\n    age INTEGER NOT NULL DEFAULT 0,\n    friends VARCHAR NOT NULL DEFAULT '',\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\n)\n</code></pre>\n<h2>3.æ•°æ®è¡¨å¯¹åº”çš„ model</h2>\n<pre><code>use chrono::{NaiveDateTime};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Queryable, Serialize, Deserialize, Debug)]\npub struct Characters {\n    pub id: i32,\n    pub name: String,\n    pub age: i32,\n    pub friends: String,\n    // è¿™é‡Œçš„ NaiveDateTime æ—¥æœŸæ ¼å¼åºåˆ—åŒ–éœ€è¦å¼€å¯ç›¸å…³ features\n    pub created_at: NaiveDateTime,\n}\n</code></pre>\n<h2>4.è·å–æ•°æ®</h2>\n<pre><code>use db::schema::characters;\nuse db::{get_connection};\nuse db::models::{Characters, NewCharacter};\nuse db::schema::characters::dsl::*;\nuse diesel::QueryDsl;\nuse diesel::prelude::*;\n\nfn main() {\n    let conn = get_connection();\n\n    // æŸ¥è¯¢å¹´é¾„å¤§äº30çš„10æ¡æ•°æ®\n    let arr: Vec&lt;Characters&gt; = characters.filter(characters::age.gt(30))\n        .limit(10)\n        .load::&lt;Characters&gt;(&amp;conn)\n        .expect(\"Loading Error\");\n\n    let date_arr = arr.iter()\n        .map(|item| {\n\t    // æ•°æ®æ ¼å¼åŒ–\n            let t = item.created_at.format(\"%Y-%m-%d %H:%M:%S\").to_string();\n            println!(\"{} {}\", item.name, t);\n            t\n        })\n        .collect::&lt;Vec&lt;String&gt;&gt;();\n}\n</code></pre>\n<p>è¾“å‡ºç»“æœç±»ä¼¼ï¼š</p>\n<pre><code>Box 2021-08-05 09:39:34\nBobe 2021-08-05 09:39:34\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-08 01:40:35","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Cargo workspace config","link":"https://rustcc.cn/article?id=c3dcce30-1fc0-4819-8992-142365c7e21c","description":"<p><a href=\"https://kaisery.github.io/trpl-zh-cn/ch14-03-cargo-workspaces.html\" rel=\"noopener noreferrer\">Workspace æ–‡æ¡£é“¾æ¥</a></p>\n<h2>ç›®å½•ç»“æ„</h2>\n<pre><code>workspace-test/\n    Cargo.toml\n    db/\n        src/\n            bin/\n                init.rs\n        Cargo.tml\n</code></pre>\n<h2>workspace</h2>\n<p>workspace-test/Cargo.toml</p>\n<pre><code>[workspace]\nmembers = [\"db\"]\ndefault-member = \"db\"\n</code></pre>\n<h2>å­é¡¹ç›®</h2>\n<p>workspace-test/db/Cargo.toml</p>\n<pre><code>[package]\nname = \"db\"\nversion = \"0.1.0\"\nedition = \"2018\"\n\n[dependencies]\n\n# å¯é€‰çš„å¯æ‰§è¡Œæ–‡ä»¶é…ç½®\n# [[bin]]\n# name = \"init\"\n# path = \"src/bin/init.rs\"\n</code></pre>\n<h2>æ“ä½œ</h2>\n<pre><code># è¿è¡Œ init\ncargo run --bin init\n# -p æŒ‡å®šé¡¹ç›®\ncargo run -p db --bin init\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-04 09:54:31","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰","link":"https://rustcc.cn/article?id=120035c3-944d-4a79-9b3a-8390697a6e13","description":"<h1><code>Rust</code>å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰</h1>\n<p>ä¸åŒäº<code>javascript</code>çš„<code>new Promise((resolve, reject) =&gt; {...})</code>æ„é€ å³è¿è¡Œï¼Œ<code>Rust</code>ä¸­çš„<code>Future</code>æ˜¯Â·æƒ°æ€§Â·çŠ¶æ€æœºã€‚è¿™ä½“ç°ä¸ºï¼š</p>\n<ol>\n<li>ã€è°ƒç”¨å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€æ‰§è¡Œå¼‚æ­¥å—ã€‘ä»…åªæ„é€ ä¸€ä¸ª<code>Future trait object</code>ã€‚</li>\n<li>å› ä¸º<code>Future</code>æ˜¯æƒ°æ€§çŠ¶æ€æœºï¼Œæ‰€ä»¥å®ƒä¸ä¼šè‡ªåŠ¨æ‰§è¡Œã€å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€å¼‚æ­¥å—ã€‘å†…çš„ä»»ä½•ä¸€è¡Œä»£ç  --- æ­¤ç‚¹ä¸<code>javascript</code>çš„Â·æ´»æ€§Â·çŠ¶æ€æœºå®Œå…¨ä¸åŒã€‚ç›¸åï¼Œéœ€è¦äººå·¥æ¿€æ´»è§¦å‘ã€‚</li>\n<li>äººå·¥å¯åŠ¨<code>Future</code>è¿è¡Œï¼Œåˆåˆ†ä¸ºä¸¤ä¸ªåœºæ™¯çš„ä¸¤ç§æƒ…å†µï¼š\n<ol>\n<li>\n<p>å·²ç»åœ¨<code>async fn</code>å†…ï¼Œ<code>Future.await</code>æ¿€æ´»ã€‚ä½†ï¼ŒåŒæ—¶<strong>é˜»å¡</strong>å½“å‰å¼‚æ­¥ç¨‹åºæ‰§è¡Œæµã€‚</p>\n</li>\n<li>\n<p>åœ¨<code>async fn</code>å¤–ï¼Œéœ€è¦å€ŸåŠ©ç”±ã€è¿è¡Œæ—¶ã€‘æä¾›çš„ã€æ‰§è¡Œå™¨ã€‘ã€‚å°±<code>async-std</code>åº“è€Œè¨€ï¼Œæœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š</p>\n<ol>\n<li><code>task::block_on(Future)</code> æ‰§è¡Œ<code>Future</code>ä¸”é˜»å¡å½“å‰çº¿ç¨‹ç›´åˆ°<code>Future</code>è¢«å®Œæˆã€‚</li>\n<li><code>task::spawn(Future)</code>ä»…æ‰§è¡Œ<code>Future</code>å’Œä¸é˜»å¡å½“å‰çº¿ç¨‹ã€‚</li>\n</ol>\n<p>æ— è®ºé€‰æ‹©ä¸Šé¢å“ªç§æ–¹å¼ï¼Œè‹¥åœ¨<code>Future</code>æ‰§è¡ŒæœŸé—´å‡ºç°äº†<code>panic</code>ï¼Œå…¶éƒ½ä¼šç»ˆæ­¢ï¼ˆ<code>abort</code>ï¼‰æ­£åœ¨å…±äº«åŒä¸€ä¸ªæ‰§è¡Œçº¿ç¨‹ï¼ˆ<code>thread</code>ï¼‰çš„æ‰€æœ‰<code>task</code>ï¼ˆÂ·æ— æ ˆÂ·åç¨‹ï¼‰çš„è¿è¡Œã€‚</p>\n</li>\n</ol>\n</li>\n</ol>\n<p>é¢˜å¤–è¯ï¼Œ</p>\n<ol>\n<li>ç»¿è‰²çº¿ç¨‹æ˜¯Â·æœ‰æ ˆÂ·åç¨‹ï¼›å¼‚æ­¥å‡½æ•°ä¸å¼‚æ­¥å—æ˜¯Â·æ— æ ˆÂ·åç¨‹ã€‚</li>\n<li>åœ¨<code>async-std</code>åº“çš„è¯æ±‡è¡¨å†…ï¼Œåç¨‹è¢«ç§°ä½œ<code>task</code>è€Œä¸æ˜¯æƒ¯ä¾‹çš„<code>coroutine</code>ã€‚</li>\n<li><code>task::spawn(Future)</code>ä¹Ÿèƒ½è¢«ä½¿ç”¨äº<code>async fn</code>æˆ–<code>async {...}</code>å†…ã€‚å®ƒè¢«ç”¨æ¥ä»£æ›¿<code>.await</code>æŒ‡ä»¤ï¼Œä»¥<strong>éé˜»å¡</strong><code>async fn</code>æˆ–<code>async {...}</code>çš„æ–¹å¼ï¼Œæ¿€æ´»ä¸æ‰§è¡Œä¸€ä¸ª<code>Future</code>å®ä¾‹ã€‚</li>\n</ol>\n<h2>ä¾‹ç¨‹</h2>\n<pre><code>async fn accept_loop(addr: impl ToSocketAddrs) -&gt; Result&lt;()&gt; {\n    // 1. TcpListener::bind(addr) è¿”å› Future\n    // 2. .await äº Future å–å¾— Result&lt;T, E&gt;\n    // 3. Result&lt;T, E&gt;? å†æ‹¿å¾— Ok&lt;T&gt; ä¸­çš„ T\n    let listener = TcpListener::bind(addr).await?; // å¼‚æ­¥å‡½æ•°å†…çš„äººå·¥å¯åŠ¨ Future\n    let mut incoming = listener.incoming();\n    // å› ä¸ºæ²¡æœ‰ä»è¯­è¨€å±‚é¢æ”¯æŒ async for loopï¼Œæ‰€ä»¥ while loop + Iterator&lt;Item = T&gt; æ¥æ¨¡æ‹Ÿä¹‹ã€‚\n    while let Some(stream) = incoming.next().await {\n        // TODO\n    }\n    Ok(())\n}\nfn main() {\n    let fut = accept_loop(\"127.0.0.1:8080\");\n    task::block_on(fut); // å¼‚æ­¥å‡½æ•°å¤–çš„äººå·¥å¯åŠ¨ Future\n}\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-03 00:01:43","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-08-27T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"With One Voice: Composing a Travel Voice Assistant from Re-purposed Models. (arXiv:2108.11463v1 [eess.AS])","link":"http://arxiv.org/abs/2108.11463","description":"<p>Voice assistants provide users a new way of interacting with digital\nproducts, allowing them to retrieve information and complete tasks with an\nincreased sense of control and flexibility. Such products are comprised of\nseveral machine learning models, like Speech-to-Text transcription, Named\nEntity Recognition and Resolution, and Text Classification. Building a voice\nassistant from scratch takes the prolonged efforts of several teams\nconstructing numerous models and orchestrating between components. Alternatives\nsuch as using third-party vendors or re-purposing existing models may be\nconsidered to shorten time-to-market and development costs. However, each\noption has its benefits and drawbacks. We present key insights from building a\nvoice search assistant for Booking.com search and recommendation system. Our\npaper compares the achieved performance and development efforts in dedicated\ntailor-made solutions against existing re-purposed models. We share and discuss\nour data-driven decisions about implementation trade-offs and their estimated\noutcomes in hindsight, showing that a fully functional machine learning product\ncan be built from existing models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Poran_S/0/1/0/all/0/1\">Shachaf Poran</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Amsalem_G/0/1/0/all/0/1\">Gil Amsalem</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Beka_A/0/1/0/all/0/1\">Amit Beka</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Goldenberg_D/0/1/0/all/0/1\">Dmitri Goldenberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Attention in Machine Reading Comprehension. (arXiv:2108.11574v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11574","description":"<p>Achieving human-level performance on some of Machine Reading Comprehension\n(MRC) datasets is no longer challenging with the help of powerful Pre-trained\nLanguage Models (PLMs). However, the internal mechanism of these artifacts\nstill remains unclear, placing an obstacle for further understanding these\nmodels. This paper focuses on conducting a series of analytical experiments to\nexamine the relations between the multi-head self-attention and the final\nperformance, trying to analyze the potential explainability in PLM-based MRC\nmodels. We perform quantitative analyses on SQuAD (English) and CMRC 2018\n(Chinese), two span-extraction MRC datasets, on top of BERT, ALBERT, and\nELECTRA in various aspects. We discover that {\\em passage-to-question} and {\\em\npassage understanding} attentions are the most important ones, showing strong\ncorrelations to the final performance than other parts. Through visualizations\nand case studies, we also observe several general findings on the attention\nmaps, which could be helpful to understand how these models solve the\nquestions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1\">Yiming Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei-Nan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1\">Wanxiang Che</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AVATAR: A Parallel Corpus for Java-Python Program Translation. (arXiv:2108.11590v1 [cs.SE])","link":"http://arxiv.org/abs/2108.11590","description":"<p>Program translation refers to migrating source code from one programming\nlanguage to another. It has a tremendous practical value in software\ndevelopment as porting software across different languages is time-consuming\nand costly. Automating program translation is of paramount importance in\nsoftware migration, and recently researchers explored unsupervised approaches\ndue to the unavailability of parallel corpora. However, the availability of\npre-trained language models for programming languages enable supervised\nfine-tuning with a small amount of labeled examples. In this work, we present a\ncorpus of 8,475 programming problems and their solutions written in two popular\nlanguages, Java and Python. We collect the dataset from competitive programming\nsites, online platforms, and open source repositories. We present several\nbaselines, including models trained from scratch or pre-trained on large-scale\nsource code collection and fine-tuned on our proposed dataset. Experiment\nresults show that while the models perform relatively well in terms of the\nlexical match, they lack in generating code that is accurate in terms of syntax\nand data-flow match.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tushar_M/0/1/0/all/0/1\">Md Golam Rahman Tushar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutReader: Pre-training of Text and Layout for Reading Order Detection. (arXiv:2108.11591v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11591","description":"<p>Reading order detection is the cornerstone to understanding visually-rich\ndocuments (e.g., receipts and forms). Unfortunately, no existing work took\nadvantage of advanced deep learning models because it is too laborious to\nannotate a large enough dataset. We observe that the reading order of WORD\ndocuments is embedded in their XML metadata; meanwhile, it is easy to convert\nWORD documents to PDFs or images. Therefore, in an automated manner, we\nconstruct ReadingBank, a benchmark dataset that contains reading order, text,\nand layout information for 500,000 document images covering a wide spectrum of\ndocument types. This first-ever large-scale dataset unleashes the power of deep\nneural networks for reading order detection. Specifically, our proposed\nLayoutReader captures the text and layout information for reading order\nprediction using the seq2seq model. It performs almost perfectly in reading\norder detection and significantly improves both open-source and commercial OCR\nengines in ordering text lines in their results in our experiments. We will\nrelease the dataset and model at \\url{https://aka.ms/readingbank}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmented Code Generation and Summarization. (arXiv:2108.11601v1 [cs.SE])","link":"http://arxiv.org/abs/2108.11601","description":"<p>Software developers write a lot of source code and documentation during\nsoftware development. Intrinsically, developers often recall parts of source\ncode or code summaries that they had written in the past while implementing\nsoftware or documenting them. To mimic developers' code or summary generation\nbehavior, we propose a retrieval augmented framework, \\tool, that retrieves\nrelevant code or summaries from a retrieval database and provides them as a\nsupplement to code generation or summarization models. \\tool has a couple of\nuniqueness. First, it extends the state-of-the-art dense retrieval technique to\nsearch for relevant code or summaries. Second, it can work with retrieval\ndatabases that include unimodal (only code or natural language description) or\nbimodal instances (code-description pairs). We conduct experiments and\nextensive analysis on two benchmark datasets of code generation and\nsummarization in Java and Python, and the promising results endorse the\neffectiveness of our proposed retrieval augmented framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Negative Sampling for Unlabeled Entity Problem in Named Entity Recognition. (arXiv:2108.11607v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11607","description":"<p>In many situations (e.g., distant supervision), unlabeled entity problem\nseriously degrades the performances of named entity recognition (NER) models.\nRecently, this issue has been well addressed by a notable approach based on\nnegative sampling. In this work, we perform two studies along this direction.\nFirstly, we analyze why negative sampling succeeds both theoretically and\nempirically. Based on the observation that named entities are highly sparse in\ndatasets, we show a theoretical guarantee that, for a long sentence, the\nprobability of containing no unlabeled entities in sampled negatives is high.\nMissampling tests on synthetic datasets have verified our guarantee in\npractice. Secondly, to mine hard negatives and further reduce missampling\nrates, we propose a weighted and adaptive sampling distribution for negative\nsampling. Experiments on synthetic datasets and well-annotated datasets show\nthat our method significantly improves negative sampling in robustness and\neffectiveness. We also have achieved new state-of-the-art results on real-world\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoMPM: Context Modeling with Speaker's Pre-trained Memory Tracking for Emotion Recognition in Conversation. (arXiv:2108.11626v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11626","description":"<p>As the use of interactive machines grow, the task of Emotion Recognition in\nConversation (ERC) became more important. If the machine generated sentences\nreflect emotion, more human-like sympathetic conversations are possible. Since\nemotion recognition in conversation is inaccurate if the previous utterances\nare not taken into account, many studies reflect the dialogue context to\nimprove the performances. We introduce CoMPM, a context embedding module (CoM)\ncombined with a pre-trained memory module (PM) that tracks memory of the\nspeaker's previous utterances within the context, and show that the pre-trained\nmemory significantly improves the final accuracy of emotion recognition. We\nexperimented on both the multi-party datasets (MELD, EmoryNLP) and the\ndyadic-party datasets (IEMOCAP, DailyDialog), showing that our approach achieve\ncompetitive performance on all datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooin Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable End-to-End Training of Knowledge Graph-Enhanced Aspect Embedding for Aspect Level Sentiment Analysis. (arXiv:2108.11656v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11656","description":"<p>Aspect level sentiment classification (ALSC) is a difficult problem with\nstate-of-the-art models showing less than 80% macro-F1 score on benchmark\ndatasets. Existing models do not incorporate information on aspect-aspect\nrelations in knowledge graphs (KGs), e.g. DBpedia. Two main challenges stem\nfrom inaccurate disambiguation of aspects to KG entities, and the inability to\nlearn aspect representations from the large KGs in joint training with ALSC\nmodels.\n</p>\n<p>We propose a two-level global-local entity embedding scheme that allows\nefficient joint training of KG-based aspect embeddings and ALSC models. A novel\nincorrect disambiguation detection technique addresses the problem of\ninaccuracy in aspect disambiguation. The proposed methods show a consistent\nimprovement of $2.5 - 4.1$ percentage points, over the recent BERT-based\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_S/0/1/0/all/0/1\">Sk Mainul Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1\">Sourangshu Bhattacharya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Technological Approaches to Detecting Online Disinformation and Manipulation. (arXiv:2108.11669v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11669","description":"<p>The move of propaganda and disinformation to the online environment is\npossible thanks to the fact that within the last decade, digital information\nchannels radically increased in popularity as a news source. The main advantage\nof such media lies in the speed of information creation and dissemination.\nThis, on the other hand, inevitably adds pressure, accelerating editorial work,\nfact-checking, and the scrutiny of source credibility. In this chapter, an\noverview of computer-supported approaches to detecting disinformation and\nmanipulative techniques based on several criteria is presented. We concentrate\non the technical aspects of automatic methods which support fact-checking,\ntopic identification, text style analysis, or message filtering on social media\nchannels. Most of the techniques employ artificial intelligence and machine\nlearning with feature extraction combining available information resources. The\nfollowing text firstly specifies the tasks related to computer detection of\nmanipulation and disinformation spreading. The second section presents concrete\nmethods of solving the tasks of the analysis, and the third sections enlists\ncurrent verification and benchmarking datasets published and used in this area\nfor evaluation and comparison.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Horak_A/0/1/0/all/0/1\">Ale&#x161; Hor&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baisa_V/0/1/0/all/0/1\">V&#xed;t Baisa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Herman_O/0/1/0/all/0/1\">Ond&#x159;ej Herman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Why Intermediate-Task Fine-Tuning Works. (arXiv:2108.11696v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11696","description":"<p>Supplementary Training on Intermediate Labeled-data Tasks (STILTs) is a\nwidely applied technique, which first fine-tunes the pretrained language models\non an intermediate task before on the target task of interest. While STILTs is\nable to further improve the performance of pretrained language models, it is\nstill unclear why and when it works. Previous research shows that those\nintermediate tasks involving complex inference, such as commonsense reasoning,\nwork especially well for RoBERTa. In this paper, we discover that the\nimprovement from an intermediate task could be orthogonal to it containing\nreasoning or other complex skills -- a simple real-fake discrimination task\nsynthesized by GPT2 can benefit diverse target tasks. We conduct extensive\nexperiments to study the impact of different factors on STILTs. These findings\nsuggest rethinking the role of intermediate fine-tuning in the STILTs pipeline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_T/0/1/0/all/0/1\">Ting-Yun Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1\">Chi-Jen Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data Augmentation for Low-Resource Named Entity Recognition Using Backtranslation. (arXiv:2108.11703v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11703","description":"<p>The state of art natural language processing systems relies on sizable\ntraining datasets to achieve high performance. Lack of such datasets in the\nspecialized low resource domains lead to suboptimal performance. In this work,\nwe adapt backtranslation to generate high quality and linguistically diverse\nsynthetic data for low-resource named entity recognition. We perform\nexperiments on two datasets from the materials science (MaSciP) and biomedical\ndomains (S800). The empirical results demonstrate the effectiveness of our\nproposed augmentation strategy, particularly in the low-resource scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yaseen_U/0/1/0/all/0/1\">Usama Yaseen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Langer_S/0/1/0/all/0/1\">Stefan Langer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Statutory Article Retrieval Dataset in French. (arXiv:2108.11792v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11792","description":"<p>Statutory article retrieval is the task of automatically retrieving law\narticles relevant to a legal question. While recent advances in natural\nlanguage processing have sparked considerable interest in many legal tasks,\nstatutory article retrieval remains primarily untouched due to the scarcity of\nlarge-scale and high-quality annotated datasets. To address this bottleneck, we\nintroduce the Belgian Statutory Article Retrieval Dataset (BSARD), which\nconsists of 1,100+ French native legal questions labeled by experienced jurists\nwith relevant articles from a corpus of 22,600+ Belgian law articles. Using\nBSARD, we benchmark several unsupervised information retrieval methods based on\nterm weighting and pooled embeddings. Our best performing baseline achieves\n50.8% R@100, which is promising for the feasibility of the task and indicates\nthat there is still substantial room for improvement. By the specificity of the\ndata domain and addressed task, BSARD presents a unique challenge problem for\nfuture research on legal information retrieval.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Louis_A/0/1/0/all/0/1\">Antoine Louis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spanakis_G/0/1/0/all/0/1\">Gerasimos Spanakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dijck_G/0/1/0/all/0/1\">Gijs Van Dijck</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-tuning Pretrained Language Models with Label Attention for Explainable Biomedical Text Classification. (arXiv:2108.11809v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11809","description":"<p>The massive growth of digital biomedical data is making biomedical text\nindexing and classification increasingly important. Accordingly, previous\nresearch has devised numerous techniques ranging from rule-based systems to\ndeep neural networks, with most focusing on feedforward, convolutional or\nrecurrent neural architectures. More recently, fine-tuned transformers-based\npretrained models (PTMs) have demonstrated superior performance in many natural\nlanguage processing tasks. However, the direct use of PTMs in the biomedical\ndomain is only limited to the target documents, ignoring the rich semantic\ninformation in the label descriptions. In this paper, we develop an improved\nlabel attention-based architecture to inject semantic label description into\nthe fine-tuning process of PTMs. Results on two public medical datasets show\nthat the proposed fine-tuning scheme outperforms the conventionally fine-tuned\nPTMs and prior state-of-the-art models. Furthermore, we show that fine-tuning\nwith the label attention mechanism is interpretable in the interpretability\nstudy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1\">Bruce Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shaoxiong Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Computational Approach to Measure Empathy and Theory-of-Mind from Written Texts. (arXiv:2108.11810v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11810","description":"<p>Theory-of-mind (ToM), a human ability to infer the intentions and thoughts of\nothers, is an essential part of empathetic experiences. We provide here the\nframework for using NLP models to measure ToM expressed in written texts. For\nthis purpose, we introduce ToM-Diary, a crowdsourced 18,238 diaries with 74,014\nKorean sentences annotated with different ToM levels. Each diary was annotated\nwith ToM levels by trained psychology students and reviewed by selected\npsychology experts. The annotators first divided the diaries based on whether\nthey mentioned other people: self-focused and other-focused. Examples of\nself-focused sentences are \"I am feeling good\". The other-focused sentences\nwere further classified into different levels. These levels differ by whether\nthe writer 1) mentions the presence of others without inferring their mental\nstate(e.g., I saw a man walking down the street), 2) fails to take the\nperspective of others (e.g., I don't understand why they refuse to wear masks),\nor 3) successfully takes the perspective of others (It must have been hard for\nthem to continue working). We tested whether state-of-the-art transformer-based\nmodels (e.g., BERT) could predict underlying ToM levels in sentences. We found\nthat BERT more successfully detected self-focused sentences than other-focused\nones. Sentences that successfully take the perspective of others (the highest\nToM level) were the most difficult to predict. Our study suggests a promising\ndirection for large-scale and computational approaches for identifying the\nability of authors to empathize and take the perspective of others. The dataset\nis at [URL](https://github.com/humanfactorspsych/covid19-tom-empathy-diary)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Yoon Kyung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1\">Inju Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jae Eun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1\">Yoonwon Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hahn_S/0/1/0/all/0/1\">Sowon Hahn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts. (arXiv:2108.11830v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11830","description":"<p>Dialogue models trained on human conversations inadvertently learn to\ngenerate offensive responses. Moreover, models can insult anyone by agreeing\nwith an offensive context. To understand the dynamics of contextually offensive\nlanguage, we study the stance of dialogue model responses in offensive Reddit\nconversations. Specifically, we crowd-annotate ToxiChat, a new dataset of 2,000\nReddit threads and model responses labeled with offensive language and stance.\nOur analysis reveals that 42% of user responses agree with toxic comments; 3x\ntheir agreement with safe comments (13%). Pre-trained transformer-based\nclassifiers fine-tuned on our dataset achieve 0.71 F1 for offensive labels and\n0.53 Macro-F1 for stance labels. Finally, we analyze some existing controllable\ntext generation (CTG) methods to mitigate the contextual offensive behavior of\ndialogue models. Compared to the baseline, our best CTG model obtains a 19%\nreduction in agreement with offensive context and 29% fewer offensive\nresponses. This highlights the need for future work to characterize and analyze\nmore forms of inappropriate behavior in dialogue models to help make them\nsafer. Our code and corpus are available at\nhttps://github.com/abaheti95/ToxiChat .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baheti_A/0/1/0/all/0/1\">Ashutosh Baheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Alleviating Exposure Bias via Contrastive Learning for Abstractive Text Summarization. (arXiv:2108.11846v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11846","description":"<p>Encoder-decoder models have achieved remarkable success in abstractive text\nsummarization, which aims to compress one or more documents into a shorter\nversion without the loss of the essential content. Unfortunately, these models\nmostly suffer a discrepancy between training and inference, i.e., the exposure\nbias problem. During the training stage, with teacher forcing these models are\noptimized to maximize the likelihood of the gold summary given the gold summary\ntokens as input to the decoder, while at inference the given tokens are\nreplaced by the generated tokens. Consequently, low-quality summaries are very\nlikely to be generated. To remedy this problem, we propose to leverage\ncontrastive learning to decrease the likelihood of these low-quality summaries,\nand meanwhile increase the likelihood of the gold summary. Since our solution\nexpands the states that the model perceives during training, we expect that the\nexposure bias problem can be alleviated. We experimentally demonstrate that our\nmethod effectively improves the performance of the state-of-the-art model on\ndifferent datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shichao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wenjie Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Realistic Study of Auto-regressive Language Models for Named Entity Typing and Recognition. (arXiv:2108.11857v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11857","description":"<p>Despite impressive results of language models for named entity recognition\n(NER), their generalization to varied textual genres, a growing entity type\nset, and new entities remains a challenge. Collecting thousands of annotations\nin each new case for training or fine-tuning is expensive and time-consuming.\nIn contrast, humans can easily identify named entities given some simple\ninstructions. Inspired by this, we challenge the reliance on large datasets and\nstudy pre-trained language models for NER in a meta-learning setup. First, we\ntest named entity typing (NET) in a zero-shot transfer scenario. Then, we\nperform NER by giving few examples at inference. We propose a method to select\nseen and rare / unseen names when having access only to the pre-trained model\nand report results on these groups. The results show: auto-regressive language\nmodels as meta-learners can perform NET and NER fairly well especially for\nregular or seen names; name irregularity when often present for a certain\nentity type can become an effective exploitable cue; names with words foreign\nto the model have the most negative impact on results; the model seems to rely\nmore on name than context cues in few-shot NER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Epure_E/0/1/0/all/0/1\">Elena V. Epure</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hennequin_R/0/1/0/all/0/1\">Romain Hennequin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Automated Fact-Checking. (arXiv:2108.11896v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11896","description":"<p>Fact-checking has become increasingly important due to the speed with which\nboth information and misinformation can spread in the modern media ecosystem.\nTherefore, researchers have been exploring how fact-checking can be automated,\nusing techniques based on natural language processing, machine learning,\nknowledge representation, and databases to automatically predict the veracity\nof claims. In this paper, we survey automated fact-checking stemming from\nnatural language processing, and discuss its connections to related tasks and\ndisciplines. In this process, we present an overview of existing datasets and\nmodels, aiming to unify the various definitions given and identify common\nconcepts. Finally, we highlight challenges for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schlichtkrull_M/0/1/0/all/0/1\">Michael Schlichtkrull</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Similar Scenes arouse Similar Emotions: Parallel Data Augmentation for Stylized Image Captioning. (arXiv:2108.11912v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11912","description":"<p>Stylized image captioning systems aim to generate a caption not only\nsemantically related to a given image but also consistent with a given style\ndescription. One of the biggest challenges with this task is the lack of\nsufficient paired stylized data. Many studies focus on unsupervised approaches,\nwithout considering from the perspective of data augmentation. We begin with\nthe observation that people may recall similar emotions when they are in\nsimilar scenes, and often express similar emotions with similar style phrases,\nwhich underpins our data augmentation idea. In this paper, we propose a novel\nExtract-Retrieve-Generate data augmentation framework to extract style phrases\nfrom small-scale stylized sentences and graft them to large-scale factual\ncaptions. First, we design the emotional signal extractor to extract style\nphrases from small-scale stylized sentences. Second, we construct the plugable\nmulti-modal scene retriever to retrieve scenes represented with pairs of an\nimage and its stylized caption, which are similar to the query image or caption\nin the large-scale factual data. In the end, based on the style phrases of\nsimilar scenes and the factual description of the current scene, we build the\nemotion-aware caption generator to generate fluent and diversified stylized\ncaptions for the current scene. Extensive experimental results show that our\nframework can alleviate the data scarcity problem effectively. It also\nsignificantly boosts the performance of several existing image captioning\nmodels in both supervised and unsupervised settings, which outperforms the\nstate-of-the-art stylized image captioning methods in terms of both sentence\nrelevance and stylishness by a substantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">Guodun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1\">Yuchen Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zehao Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yin Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HAN: Higher-order Attention Network for Spoken Language Understanding. (arXiv:2108.11916v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11916","description":"<p>Spoken Language Understanding (SLU), including intent detection and slot\nfilling, is a core component in human-computer interaction. The natural\nattributes of the relationship among the two subtasks make higher requirements\non fine-grained feature interaction, i.e., the token-level intent features and\nslot features. Previous works mainly focus on jointly modeling the relationship\nbetween the two subtasks with attention-based models, while ignoring the\nexploration of attention order. In this paper, we propose to replace the\nconventional attention with our proposed Bilinear attention block and show that\nthe introduced Higher-order Attention Network (HAN) brings improvement for the\nSLU task. Importantly, we conduct wide analysis to explore the effectiveness\nbrought from the higher-order attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Dongsheng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zhiqi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Machine Learning for Mediation in Armed Conflicts. (arXiv:2108.11942v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11942","description":"<p>Today's conflicts are becoming increasingly complex, fluid and fragmented,\noften involving a host of national and international actors with multiple and\noften divergent interests. This development poses significant challenges for\nconflict mediation, as mediators struggle to make sense of conflict dynamics,\nsuch as the range of conflict parties and the evolution of their political\npositions, the distinction between relevant and less relevant actors in peace\nmaking, or the identification of key conflict issues and their interdependence.\nInternational peace efforts appear increasingly ill-equipped to successfully\naddress these challenges. While technology is being increasingly used in a\nrange of conflict related fields, such as conflict predicting or information\ngathering, less attention has been given to how technology can contribute to\nconflict mediation. This case study is the first to apply state-of-the-art\nmachine learning technologies to data from an ongoing mediation process. Using\ndialogue transcripts from peace negotiations in Yemen, this study shows how\nmachine-learning tools can effectively support international mediators by\nmanaging knowledge and offering additional conflict analysis tools to assess\ncomplex information. Apart from illustrating the potential of machine learning\ntools in conflict mediation, the paper also emphasises the importance of\ninterdisciplinary and participatory research design for the development of\ncontext-sensitive and targeted tools and to ensure meaningful and responsible\nimplementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arana_Catania_M/0/1/0/all/0/1\">M. Arana-Catania</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lier_F/0/1/0/all/0/1\">F.A. Van Lier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Procter_R/0/1/0/all/0/1\">Rob Procter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Position-Invariant Truecasing with a Word-and-Character Hierarchical Recurrent Neural Network. (arXiv:2108.11943v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11943","description":"<p>Truecasing is the task of restoring the correct case (uppercase or lowercase)\nof noisy text generated either by an automatic system for speech recognition or\nmachine translation or by humans. It improves the performance of downstream NLP\ntasks such as named entity recognition and language modeling. We propose a\nfast, accurate and compact two-level hierarchical word-and-character-based\nrecurrent neural network model, the first of its kind for this problem. Using\nsequence distillation, we also address the problem of truecasing while ignoring\ntoken positions in the sentence, i.e. in a position-invariant manner.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1\">You-Chi Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1\">Shankar Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Mingqing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mathews_R/0/1/0/all/0/1\">Rajiv Mathews</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SASRA: Semantically-aware Spatio-temporal Reasoning Agent for Vision-and-Language Navigation in Continuous Environments. (arXiv:2108.11945v1 [cs.RO])","link":"http://arxiv.org/abs/2108.11945","description":"<p>This paper presents a novel approach for the Vision-and-Language Navigation\n(VLN) task in continuous 3D environments, which requires an autonomous agent to\nfollow natural language instructions in unseen environments. Existing\nend-to-end learning-based VLN methods struggle at this task as they focus\nmostly on utilizing raw visual observations and lack the semantic\nspatio-temporal reasoning capabilities which is crucial in generalizing to new\nenvironments. In this regard, we present a hybrid transformer-recurrence model\nwhich focuses on combining classical semantic mapping techniques with a\nlearning-based method. Our method creates a temporal semantic memory by\nbuilding a top-down local ego-centric semantic map and performs cross-modal\ngrounding to align map and language modalities to enable effective learning of\nVLN policy. Empirical results in a photo-realistic long-horizon simulation\nenvironment show that the proposed approach outperforms a variety of\nstate-of-the-art methods and baselines with over 22% relative improvement in\nSPL in prior unseen environments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Irshad_M/0/1/0/all/0/1\">Muhammad Zubair Irshad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mithun_N/0/1/0/all/0/1\">Niluthpol Chowdhury Mithun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seymour_Z/0/1/0/all/0/1\">Zachary Seymour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_H/0/1/0/all/0/1\">Han-Pang Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samarasekera_S/0/1/0/all/0/1\">Supun Samarasekera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1\">Rakesh Kumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SAUCE: Truncated Sparse Document Signature Bit-Vectors for Fast Web-Scale Corpus Expansion. (arXiv:2108.11948v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11948","description":"<p>Recent advances in text representation have shown that training on large\namounts of text is crucial for natural language understanding. However, models\ntrained without predefined notions of topical interest typically require\ncareful fine-tuning when transferred to specialized domains. When a sufficient\namount of within-domain text may not be available, expanding a seed corpus of\nrelevant documents from large-scale web data poses several challenges. First,\ncorpus expansion requires scoring and ranking each document in the collection,\nan operation that can quickly become computationally expensive as the web\ncorpora size grows. Relying on dense vector spaces and pairwise similarity adds\nto the computational expense. Secondly, as the domain concept becomes more\nnuanced, capturing the long tail of domain-specific rare terms becomes\nnon-trivial, especially under limited seed corpora scenarios.\n</p>\n<p>In this paper, we consider the problem of fast approximate corpus expansion\ngiven a small seed corpus with a few relevant documents as a query, with the\ngoal of capturing the long tail of a domain-specific set of concept terms. To\nefficiently collect large-scale domain-specific corpora with limited relevance\nfeedback, we propose a novel truncated sparse document bit-vector\nrepresentation, termed Signature Assisted Unsupervised Corpus Expansion\n(SAUCE). Experimental results show that SAUCE can reduce the computational\nburden while ensuring high within-domain lexical coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahed_M/0/1/0/all/0/1\">Muntasir Wahed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gruhl_D/0/1/0/all/0/1\">Daniel Gruhl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alba_A/0/1/0/all/0/1\">Alfredo Alba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gentile_A/0/1/0/all/0/1\">Anna Lisa Gentile</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ristoski_P/0/1/0/all/0/1\">Petar Ristoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deluca_C/0/1/0/all/0/1\">Chad Deluca</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Welch_S/0/1/0/all/0/1\">Steve Welch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lourentzou_I/0/1/0/all/0/1\">Ismini Lourentzou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Weisfeiler-Leman in the BAMBOO: Novel AMR Graph Metrics and a Benchmark for AMR Graph Similarity. (arXiv:2108.11949v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11949","description":"<p>Several metrics have been proposed for assessing the similarity of (abstract)\nmeaning representations (AMRs), but little is known about how they relate to\nhuman similarity ratings. Moreover, the current metrics have complementary\nstrengths and weaknesses: some emphasize speed, while others make the alignment\nof graph structures explicit, at the price of a costly alignment step.\n</p>\n<p>In this work we propose new Weisfeiler-Leman AMR similarity metrics that\nunify the strengths of previous metrics, while mitigating their weaknesses.\nSpecifically, our new metrics are able to match contextualized substructures\nand induce n:m alignments between their nodes. Furthermore, we introduce a\nBenchmark for AMR Metrics based on Overt Objectives (BAMBOO), the first\nbenchmark to support empirical assessment of graph-based MR similarity metrics.\nBAMBOO maximizes the interpretability of results by defining multiple overt\nobjectives that range from sentence similarity objectives to stress tests that\nprobe a metric's robustness against meaning-altering and meaning-preserving\ngraph transformations. We show the benefits of BAMBOO by profiling previous\nmetrics and our own metrics. Results indicate that our novel metrics may serve\nas a strong baseline for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Opitz_J/0/1/0/all/0/1\">Juri Opitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Daza_A/0/1/0/all/0/1\">Angel Daza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_A/0/1/0/all/0/1\">Anette Frank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LocTex: Learning Data-Efficient Visual Representations from Localized Textual Supervision. (arXiv:2108.11950v1 [cs.CV])","link":"http://arxiv.org/abs/2108.11950","description":"<p>Computer vision tasks such as object detection and semantic/instance\nsegmentation rely on the painstaking annotation of large training datasets. In\nthis paper, we propose LocTex that takes advantage of the low-cost localized\ntextual annotations (i.e., captions and synchronized mouse-over gestures) to\nreduce the annotation effort. We introduce a contrastive pre-training framework\nbetween images and captions and propose to supervise the cross-modal attention\nmap with rendered mouse traces to provide coarse localization signals. Our\nlearned visual features capture rich semantics (from free-form captions) and\naccurate localization (from mouse traces), which are very effective when\ntransferred to various downstream vision tasks. Compared with ImageNet\nsupervised pre-training, LocTex can reduce the size of the pre-training dataset\nby 10x or the target dataset by 2x while achieving comparable or even improved\nperformance on COCO instance segmentation. When provided with the same amount\nof annotations, LocTex achieves around 4% higher accuracy than the previous\nstate-of-the-art \"vision+language\" pre-training approach on the task of PASCAL\nVOC image classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhijian Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stent_S/0/1/0/all/0/1\">Simon Stent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jie Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gideon_J/0/1/0/all/0/1\">John Gideon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Song Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SUMBT+LaRL: Effective Multi-domain End-to-end Neural Task-oriented Dialog System. (arXiv:2009.10447v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.10447","description":"<p>The recent advent of neural approaches for developing each dialog component\nin task-oriented dialog systems has remarkably improved, yet optimizing the\noverall system performance remains a challenge. Besides, previous research on\nmodeling complicated multi-domain goal-oriented dialogs in end-to-end fashion\nhas been limited. In this paper, we present an effective multi-domain\nend-to-end trainable neural dialog system SUMBT+LaRL that incorporates two\nprevious strong models and facilitates them to be fully differentiable.\nSpecifically, the SUMBT+ estimates user-acts as well as dialog belief states,\nand the LaRL models latent system action spaces and generates responses given\nthe estimated contexts. We emphasize that the training framework of three steps\nsignificantly and stably increase dialog success rates: separately pretraining\nthe SUMBT+ and LaRL, fine-tuning the entire system, and then reinforcement\nlearning of dialog policy. We also introduce new reward criteria of\nreinforcement learning for dialog policy training. Then, we discuss\nexperimental results depending on the reward criteria and different dialog\nevaluation methods. Consequently, our model achieved the new state-of-the-art\nsuccess rate of 85.4% on corpus-based evaluation, and a comparable success rate\nof 81.40% on simulator-based evaluation provided by the DSTC8 challenge. To our\nbest knowledge, our work is the first comprehensive study of a modularized E2E\nmulti-domain dialog system that learning from each component to the entire\ndialog policy for task success.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jo_S/0/1/0/all/0/1\">Seokhwan Jo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyungJun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1\">Sangkeun Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Tae-Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Adversarial Learning for Cross-Lingual Word Embeddings. (arXiv:2010.08432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.08432","description":"<p>Generative adversarial networks (GANs) have succeeded in inducing\ncross-lingual word embeddings -- maps of matching words across languages --\nwithout supervision. Despite these successes, GANs' performance for the\ndifficult case of distant languages is still not satisfactory. These\nlimitations have been explained by GANs' incorrect assumption that source and\ntarget embedding spaces are related by a single linear mapping and are\napproximately isomorphic. We assume instead that, especially across distant\nlanguages, the mapping is only piece-wise linear, and propose a\nmulti-adversarial learning method. This novel method induces the seed\ncross-lingual dictionary through multiple mappings, each induced to fit the\nmapping for one subspace. Our experiments on unsupervised bilingual lexicon\ninduction show that this method improves performance over previous\nsingle-mapping methods, especially for distant languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haozhou Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Merlo_P/0/1/0/all/0/1\">Paola Merlo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ONION: A Simple and Effective Defense Against Textual Backdoor Attacks. (arXiv:2011.10369v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.10369","description":"<p>Backdoor attacks are a kind of emergent training-time threat to deep neural\nnetworks (DNNs). They can manipulate the output of DNNs and possess high\ninsidiousness. In the field of natural language processing, some attack methods\nhave been proposed and achieve very high attack success rates on multiple\npopular models. Nevertheless, there are few studies on defending against\ntextual backdoor attacks. In this paper, we propose a simple and effective\ntextual backdoor defense named ONION, which is based on outlier word detection\nand, to the best of our knowledge, is the first method that can handle all the\ntextual backdoor attack situations. Experiments demonstrate the effectiveness\nof our model in defending BiLSTM and BERT against five different backdoor\nattacks. All the code and data will be released to facilitate future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qi_F/0/1/0/all/0/1\">Fanchao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yangyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1\">Mukai Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1\">Yuan Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Transformer-Based Generation of Radiology Reports. (arXiv:2102.09777v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.09777","description":"<p>Inspired by Curriculum Learning, we propose a consecutive (i.e.\nimage-to-text-to-text) generation framework where we divide the problem of\nradiology report generation into two steps. Contrary to generating the full\nradiology report from the image at once, the model generates global concepts\nfrom the image in the first step and then reforms them into finer and coherent\ntexts using transformer-based architecture. We follow the transformer-based\nsequence-to-sequence paradigm at each step. We improve upon the\nstate-of-the-art on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nooralahzadeh_F/0/1/0/all/0/1\">Farhad Nooralahzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_N/0/1/0/all/0/1\">Nicolas Perez Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frauenfelder_T/0/1/0/all/0/1\">Thomas Frauenfelder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujimoto_K/0/1/0/all/0/1\">Koji Fujimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1\">Michael Krauthammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Diversity of Neural Text Generation via Inverse Probability Weighting. (arXiv:2103.07649v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07649","description":"<p>The neural text generation suffers from the text degeneration issue such as\nrepetition. Traditional stochastic sampling methods only focus on truncating\nthe unreliable \"tail\" of the distribution, and do not address the \"head\" part,\nwhich we show might contain tedious or even repetitive candidates with high\nprobability that lead to repetition loops. They also do not consider the issue\nthat human text does not always favor high-probability words. Inspired by\nthese, in this work we propose a heuristic sampling method. We propose to use\ninterquartile range of the predicted distribution to determine the \"head\" part,\nthen permutate and rescale the \"head\" with inverse probability. This aims at\ndecreasing the probability for the tedious and possibly repetitive candidates\nwith higher probability, and increasing the probability for the rational but\nmore surprising candidates with lower probability. The proposed algorithm\nprovides a reasonable permutation on the predicted distribution which enhances\ndiversity without compromising rationality of the distribution. We use\npre-trained language model to compare our algorithm with traditional methods.\nResults show that our algorithm can effectively increase the diversity of\ngenerated samples while achieving close resemblance to human text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiafeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04670","description":"<p>Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kristy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Data-QuestEval: A Referenceless Metric for Data-to-Text Semantic Evaluation. (arXiv:2104.07555v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07555","description":"<p>QuestEval is a reference-less metric used in text-to-text tasks, that\ncompares the generated summaries directly to the source text, by automatically\nasking and answering questions. Its adaptation to Data-to-Text tasks is not\nstraightforward, as it requires multimodal Question Generation and Answering\nsystems on the considered tasks, which are seldom available. To this purpose,\nwe propose a method to build synthetic multimodal corpora enabling to train\nmultimodal components for a data-QuestEval metric. The resulting metric is\nreference-less and multimodal; it obtains state-of-the-art correlations with\nhuman judgment on the WebNLG and WikiBio benchmarks. We make data-QuestEval's\ncode and models available for reproducibility purpose, as part of the QuestEval\nproject.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rebuffel_C/0/1/0/all/0/1\">Cl&#xe9;ment Rebuffel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soulier_L/0/1/0/all/0/1\">Laure Soulier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lamprier_S/0/1/0/all/0/1\">Sylvain Lamprier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scoutheeten_G/0/1/0/all/0/1\">Geoffrey Scoutheeten</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1\">Patrick Gallinari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Power of Saturated Transformers: A View from Circuit Complexity. (arXiv:2106.16213v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.16213","description":"<p>Transformers have become a standard architecture for many NLP problems. This\nhas motivated theoretically analyzing their capabilities as models of language,\nin order to understand what makes them successful, and what their potential\nweaknesses might be. Recent work has shown that transformers with hard\nattention are quite limited in capacity, and in fact can be simulated by\nconstant-depth circuits. However, hard attention is a restrictive assumption,\nwhich may complicate the relevance of these results for practical transformers.\nIn this work, we analyze the circuit complexity of transformers with saturated\nattention: a generalization of hard attention that more closely captures the\nattention patterns learnable in practical transformers. We show that saturated\ntransformers transcend the limitations of hard-attention transformers. With\nsome minor assumptions, we prove that the number of bits needed to represent a\nsaturated transformer memory vector is $O(\\log n)$, which implies saturated\ntransformers can be simulated by log-depth circuits. Thus, the jump from hard\nto saturated attention can be understood as increasing the transformer's\neffective circuit depth by a factor of $O(\\log n)$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01198","description":"<p>In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence-T5: Scalable Sentence Encoders from Pre-trained Text-to-Text Models. (arXiv:2108.08877v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08877","description":"<p>We provide the first exploration of text-to-text transformers (T5) sentence\nembeddings. Sentence embeddings are broadly useful for language processing\ntasks. While T5 achieves impressive performance on language tasks cast as\nsequence-to-sequence mapping problems, it is unclear how to produce sentence\nembeddings from encoder-decoder models. We investigate three methods for\nextracting T5 sentence embeddings: two utilize only the T5 encoder and one uses\nthe full T5 encoder-decoder model. Our encoder-only models outperforms\nBERT-based sentence embeddings on both transfer tasks and semantic textual\nsimilarity (STS). Our encoder-decoder method achieves further improvement on\nSTS. Scaling up T5 from millions to billions of parameters is found to produce\nconsistent improvements on downstream tasks. Finally, we introduce a two-stage\ncontrastive learning approach that achieves a new state-of-art on STS using\nsentence embeddings, outperforming both Sentence BERT and SimCSE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_J/0/1/0/all/0/1\">Jianmo Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abrego_G/0/1/0/all/0/1\">Gustavo Hern&#xe1;ndez &#xc1;brego</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1\">Ji Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hall_K/0/1/0/all/0/1\">Keith B. Hall</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recurrent multiple shared layers in Depth for Neural Machine Translation. (arXiv:2108.10417v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10417","description":"<p>Learning deeper models is usually a simple and effective approach to improve\nmodel performance, but deeper models have larger model parameters and are more\ndifficult to train. To get a deeper model, simply stacking more layers of the\nmodel seems to work well, but previous works have claimed that it cannot\nbenefit the model. We propose to train a deeper model with recurrent mechanism,\nwhich loops the encoder and decoder blocks of Transformer in the depth\ndirection. To address the increasing of model parameters, we choose to share\nparameters in different recursive moments. We conduct our experiments on WMT16\nEnglish-to-German and WMT14 English-to-France translation tasks, our model\noutperforms the shallow Transformer-Base/Big baseline by 0.35, 1.45 BLEU\npoints, which is 27.23% of Transformer-Big model parameters. Compared to the\ndeep Transformer(20-layer encoder, 6-layer decoder), our model has similar\nmodel performance and infer speed, but our model parameters are 54.72% of the\nformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1\">GuoLiang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-26T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}