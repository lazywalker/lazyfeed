{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-13T04:21:40.360459939Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"【招聘  杭州，上海】Rust开发工程师（30K-50K）","link":"https://rustcc.cn/article?id=4f8da484-d0fc-4f56-88ca-c19e7ea32b5a","description":"<p>【岗位职责】</p>\n<ol>\n<li>负责分布式计算及存储系统的高可扩展后端系统，服务和API；</li>\n<li>设计高性能、高可靠性的服务，建立快速、稳定、安全的后端代码&nbsp;；</li>\n<li>为其他开发人员提供指导，参与算法设计和实现。</li>\n<li>负责设计和优化协议、弱网通信、存储、网络并发、并行计算、加密以及安全等；</li>\n<li>保证工程质量和开发效率。</li>\n<li>设计和维护性能测试用例；</li>\n</ol>\n<p>【岗位要求】</p>\n<ol>\n<li>计算机或者相关专业本科以上学历，两年以上相关工作经验</li>\n<li>技术扎实，熟悉Rust语言编程</li>\n<li>理解ownership, trait, async等语言机制。</li>\n<li>熟练使用tokio。熟练使用rust常用库</li>\n<li>有丰富的多线程应用和平台构建经验，可熟练构建稳定、高效率和安全的代码&nbsp;；</li>\n<li>有强烈的上进心和求知欲，善于学习和运用新知识，善于沟通和逻辑表达，有强烈的团队意识和执行力。</li>\n<li>熟悉Linux下多线程/多进程编程模型，进程间通讯，消息事件通知，同步/异步。</li>\n<li>熟悉Linux下内存管理机制，低延迟、高并发无锁化编程。</li>\n</ol>\n<p>【特别备注】</p>\n<ol>\n<li>了解安全加密相关算法者优先&nbsp;；</li>\n<li>有丰富的c++、python编程经验者优先</li>\n<li>参与大型系统的开发，并成功部署、广泛应用者优先；</li>\n<li>熟悉大数据、机器学习框架，如:spark，flink, tensorflow者优先。</li>\n</ol>\n<p>【工作地点】\nbase1: 杭州市西湖区中电万谷园区\nbase2: 上海市浦东新区前滩东方广场一期\n杭州上海均有岗位。</p>\n<p>联系方式：朝歌13732914991（微信同号） 邮箱：zhaoge@fudata.cn</p>\n<p>【公司介绍】\n上海富数科技有限公司 简称“富数科技”，是国内领先的金融AI和安全计算技术领跑者，核心团队来自CapitalOne，Alibaba和IBM，公司自2016年成立以来受国内顶级风投青睐，已完成C轮融资。富数科技坚持以“以数据安全驱动人工智能”，依托于安全计算和机器学习AI技术，助力金融和各行业机构组织提高智能风控、营销和运营的效率，实现数据合规安全地融合计算和价值流通。</p>\n<p>富数科技是中国通信标准化协会会员、工信部信通院大数据安全及流通标准组成员、安全多方计算标准参与方，为行业规范标准制定贡献创新技术成果。富数科技结合最新密码学和区块链技术研发创新，其安全计算和联邦学习开创性地采用“松弛迭代法”，在智能合约、ML算法优化、代码编译和计算硬件芯片融合方面改善性能，在同等条件下实现了收敛速度的大幅提升，精度和准确度损失低于1%，速度较行业水平提高了3倍。</p>\n<p>富数科技致力于驱动安全可信的人工智能科技与各行业场景的深度融合赋能，在兼顾隐私保护下发挥大数据的商业价值。富数科技自2017年投入数据安全计算领域研发创新，拥有多项专利发明和软著，并与国内外金融机构和科研机构（上海交大等）联合研发和推动工程化商业化落地。富数科技安全计算解决方案已经落地在智能风控、智能营销、监管和科研统计分析、异业或同业数据安全融合计算等场景，目前已在银行、持牌消金、政务、医疗、运营商等领域积累上百案例，在安全的机器学习领域具有突出的领先优势。</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-12 15:57:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust 日报】2021-09-12 Rust 的 Logging 推荐","link":"https://rustcc.cn/article?id=91b91a60-cbe9-4f8a-a5ec-8825da34457b","description":"<h3>Rust 的 Logging 推荐</h3>\n<p>内容整理自 Reddit 的讨论：<a href=\"https://www.reddit.com/r/rust/comments/pmdh6a/what_is_the_current_recommendation_for_logging_in/\" rel=\"noopener noreferrer\">What is the current recommendation for logging in Rust? : rust</a>。</p>\n<p>问题简述：除了标准的 <code>log</code>，还有不少选择：<code>env_logger</code>，<code>tracing</code>，<code>slog</code>，<code>simplelog</code> 等等，最佳实践是什么？</p>\n<p>来自 <a href=\"https://www.reddit.com/user/Koxiaet/\" rel=\"noopener noreferrer\">Koxiaet</a> 的答复：通常有两类与日志相关的 crate：日志接口和日志消费者。接口提供了想要记录某些东西时调用的函数，消费者处理将结构化日志数据格式化到某个地方（stderr 或文件）。两个主要的接口是 <code>log</code> 和 <code>tracing</code>，后者功能更强大因为它支持结构化日志记录，但前者更普遍。还有另一个结构化日志接口 slog，比 <code>tracing</code> 更古老但用的较少。每个日志接口都有自己生态系统，可以根据自己的需要选择。如果在写一个库，<code>log</code> 是个不错的选择，因为所有的日志记录接口都与它兼容。但如果你确实需要结构化日志记录，则可以改用 <code>tracing</code>，这取决于你的需求，比如你是需要写到文件还是只是终端。</p>\n<p>其他网友的推荐：</p>\n<ul>\n<li>File Logging：<a href=\"https://github.com/emabee/flexi_logger\" rel=\"noopener noreferrer\">emabee/flexi_logger: A flexible logger for rust programs that can write to stderr or to log files</a>。（来自 cfsamson）</li>\n<li><code>tracing</code> 的接口：<a href=\"https://docs.rs/tracing-log/0.1.2/tracing_log/\" rel=\"noopener noreferrer\">tracing_log - Rust</a>，有多个同时操作交错日志消息时特别方便，可以按某些属性对它们进行分组并单独查看它们。（来自 class_two_perversion）</li>\n<li><a href=\"https://github.com/estk/log4rs\" rel=\"noopener noreferrer\">estk/log4rs: A highly configurable logging framework for Rust</a>，log4rs 是一个高度可配置的日志框架，以 Java 的 Logback 和 log4j 库为模型。通过 Yaml 配置，到 sdout 和文件，带有文件大小限制选项，还可以配置不同级别的日志。（来自 tms102）</li>\n<li><a href=\"https://crates.io/crates/tracing-appender\" rel=\"noopener noreferrer\">tracing-appender - crates.io: Rust Package Registry</a>，推荐者所知道的唯一线程外日志记录解决方案，不仅适用于异步应用程序。（来自 Pand9）</li>\n<li><a href=\"https://github.com/daboross/fern\" rel=\"noopener noreferrer\">daboross/fern: Simple, efficient logging for Rust</a>，像 Python 的 <code>logging</code> 和 JS 的 <code>Winston</code>。（来自 RapBeautician）</li>\n</ul>\n<h3>Rust 全栈</h3>\n<p>本文是一篇博客翻译，来自：<a href=\"https://www.justinm.one/blog/2021/09/11/fullstackrust/\" rel=\"noopener noreferrer\">Full Stack Rust - Blog</a>。</p>\n<p>一年前，我的首选语言如下：</p>\n<ul>\n<li>Python 用于高级代码快速原型设计，或用于需要第三方功能的代码</li>\n<li>C/C++ 用于长期的 low-level 项目</li>\n</ul>\n<p>当时只听过 Rust 并简单使用过，我的经验来自用 Rust 写了一个处理大文件（&gt;4GB）的事务并从中挖掘一些统计信息的小工具。我用了一个库将文件映射到内存，缤瑞按照顺序对其进行分析。有一些很酷的概念，比如编译器静态地强制内存映射在它被取消映射后无法访问——如果你不小心，C++ 中可能就会发生这种错误。</p>\n<p>不过当时并没有真正吸引我，因为那只是一个小新奇。当我向 <a href=\"https://github.com/DrChat/pdblister\" rel=\"noopener noreferrer\">pdblister</a> 添加新功能以并行获取数千个 PDB 文件时诀窍来了。由于 GIL，在 CPython 中几乎不可能，而在 C/C++ 中做到不面临并行错误是极其困难的。然而 Rust 让这变得容易。我添加了 tokio 驱动的异步，使用 <code>tokio::spawn</code> 生成新任务来下载 PDB，并修复了编译器报的错误，它可以正常工作了。Rust 编译器输出一个二进制文件，它可以在任何地方运行，没有运行时依赖。</p>\n<p><strong>取代 Python</strong></p>\n<p>这是第一点，Rust 是 Python 作为中长期工具语言的绝佳替代品。Python 的好处是庞大的库和生态系统，通过 pip 可以直接拿到，想要快速制作与 API 交互的原型，可以使用 <code>requests</code>，只要 <code>import requests</code> 就可以使用了。Rust 的 <code>reqwest\t</code> 也是如此，只要输入 <code>cargo add reqwest</code> 就可以在代码中使用它。</p>\n<p>然而当进入更长期的生命周期时，Python 就显示出劣势，<code>requests</code> 是程序的依赖，用户需要后去后才能使用。此外，由于弱类型和错误处理能力（与 Rust 比），Python 变得更加劣势。这一点上，我可以使用 Rust 比使用 Python 更快地编写原型工具，并且我可以自信地知道我的工具比等效的 Python 更易于维护且寿命更长。但是，对于短期工具，Python 可能仍然更好，因为它不需要启动项目即可在 VSCode 中获得智能感知支持。 Rust 的 cargo-script 接近将 Rust 推入脚本语言的领域，但不幸的是，我还没有在 VSCode 中找到与之集成的插件。</p>\n<p><strong>取代 C</strong></p>\n<p>Rust 也是 C 的直接替代品，它在各方面都更好，并且可以与遗留 C 代码原生互操作以进行增量替换。Rust 最大的改进是生态系统：如上所述，利用 Rust 生态中已有的库是很容易的。如果你从未使用过 C，那很幸运，实际上 C 中使用高级功能的最佳方法是自己写。</p>\n<p>C 生态系统是支离破碎的，而且很脆弱。ABI 或构建系统没有一致的标准：</p>\n<ul>\n<li>由于缺乏 ABI 一致性，你不能跨平台或操作系统使用相同的二进制文件。  所以你必须从源代码构建。</li>\n<li>由于缺乏一致的构建系统，你不能简单地和应用程序一起构建 C 库，必须修补或重写要使其与你的库兼容的库的构建系统。</li>\n<li>C 库很少跨平台兼容，因为它们缺乏可以依赖的共享抽象。</li>\n</ul>\n<p>然后还有 Rust 最特色的安全改进——我就不展开了。但根据我的经验 - 安全性在很大程度上是一种工具，可以让第三方库开发人员更容易强迫我正确使用他们的库，这是 C 库不能做的事情。</p>\n<p><strong>全栈 Rust</strong></p>\n<p>总而言之，在过去的一年中，我一直在堆栈的所有部分使用 Rust，而我之前使用过其他语言。我已经使用 Rust 来实现引导加载程序：<a href=\"https://github.com/xenia-project/xell-rs\" rel=\"noopener noreferrer\">xenia-project/xell-rs: Xell Bootloader, rewritten in Rust because ¯_(ツ)_/¯，</a>我已经使用它通过 <a href=\"https://github.com/DrChat/pdblister\" rel=\"noopener noreferrer\">pdblister</a> 和 <a href=\"https://github.com/panamax-rs/panamax\" rel=\"noopener noreferrer\">panamax</a> 中的高级 HTTP/HTTPS 和其他技术来镜像文件。我利用并贡献了优秀的 <a href=\"https://github.com/DrChat/gdbstub\" rel=\"noopener noreferrer\">gdbstub</a> 库，用于控制由自定义 VMM 运行的 VM。这些项目都是在堆栈的不同级别完成的，而 Rust 非常适合所有级别。  我已经开始在我的个人项目中专门使用 Rust，并在适合的时候推动它在我的工作中使用。</p>\n<h3>tagged_cell：快速、可初始化和线程安全的静态变量</h3>\n<p>通过 <code>TaggedCell</code> 和 <code>Tag</code> 类型实现，为了安全操作，<code>TaggedCell</code> 的每个实例都必须是唯一的。然后必须通过 <code>TaggedCell::init ()</code> 初始化 <code>TaggedCell</code>，它使用用户提供的函数或闭包初始化底层数据，然后返回一个特殊的零大小的 <code>Init&lt;Tag&gt;</code> 用于访问 Cell 的数据。为了确保每个单元格使用唯一的标签类型，<code>tagged_cell!</code> 提供宏。该宏根据变量的名称创建一个新的标记类型，并将其应用到声明中。</p>\n<pre><code>use tagged_cell::tagged_cell;\ntagged_cell!{\n   static BAR: TaggedCell&lt;Vec&lt;usize&gt;, _&gt; = TaggedCell::new();\n}\n\nlet tag = BAR.init(|| vec![0, 10, 20]);\nlet vec = BAR.get(tag);\n\nassert_eq!(vec[2], 20);\n</code></pre>\n<p>为了允许跨线程使用，只有第一次调用 <code>TaggedCell::init</code> 才会初始化 Cell 的数据。所有未来的 <code>TaggedCell::init</code> 调用都将返回一个新标签。未确定哪个线程将初始化 Cell 的数据。</p>\n<pre><code>use std::thread;\nuse tagged_cell::tagged_cell;\n\ntagged_cell!{\n    static TABLE: TaggedCell&lt;Vec&lt;usize&gt;, _&gt; = TaggedCell::new();\n}\n\nthread::spawn(move || {\n    let tag = TABLE.init(|| vec![0, 10, 20]);\n    let table = TABLE.get(tag);\n    assert_eq!(table[2], 20);\n});\n\nthread::spawn(move || {\n    let tag = TABLE.init(|| vec![0, 10, 20]);\n    let table = TABLE.get(tag);\n    assert_eq!(table[1], 10);\n});\n</code></pre>\n<p>GitHub：<a href=\"https://github.com/Dasch0/tagged_cell\" rel=\"noopener noreferrer\">Dasch0/tagged_cell: Fast, initializable, and thread safe static variables</a></p>\n<h3>ukanren-rs：µKanren 的 Rust 实现</h3>\n<p>µKanren 是一种轻量级关系编程语言</p>\n<ul>\n<li>原始的 Schema 实现在这里：<a href=\"https://github.com/jasonhemann/microKanren\" rel=\"noopener noreferrer\">jasonhemann/microKanren: The implementation of microKanren, a featherweight relational programming language</a></li>\n<li>相关参考：<a href=\"http://minikanren.org/\" rel=\"noopener noreferrer\">miniKanren.org</a></li>\n</ul>\n<pre><code>use ukanren::*;\n\nfn appendo(first: Value, second: Value, out: Value) -&gt; BoxedGoal&lt;impl Iterator&lt;Item = State&gt;&gt; {\n    eq(&amp;first, &amp;())\n        .and(eq(&amp;second, &amp;out))\n        .or(fresh(move |a: Value, d: Value, res: Value| {\n            eq(&amp;(a.clone(), d.clone()), &amp;first)\n                .and(eq(&amp;(a.clone(), res.clone()), &amp;out))\n                .and(appendo(d.clone(), second.clone(), res))\n        }))\n        .boxed()\n}\n\nlet goal = fresh(|x, y| appendo(x, y, [1, 2, 3, 4, 5].to_value()));\nassert_eq!(\n    goal.run(2).collect::&lt;Vec&lt;_&gt;&gt;(),\n    vec![\n        state![(), [1, 2, 3, 4, 5]],\n        state![[1], [2, 3, 4, 5]],\n        state![[1, 2], [3, 4, 5]],\n        state![[1, 2, 3], [4, 5]],\n        state![[1, 2, 3, 4], [5]],\n        state![[1, 2, 3, 4, 5], ()],\n    ],\n);\n</code></pre>\n<p>GitHub：<a href=\"https://github.com/ekzhang/ukanren-rs\" rel=\"noopener noreferrer\">ekzhang/ukanren-rs: Rust implementation of µKanren, a featherweight relational programming language.</a></p>\n<h3>rust-counter-strings：快速定位字符串位置</h3>\n<p>字符串中的每个星号都出现在由紧接前面的数字指定的位置。因此，29 后面的星号是该字符串中的第 29 个字符。可以在任何地方砍掉字符串的末尾，并且确切地知道它在哪里被剪掉了。比如不用数就知道字符串 <code>2*4*6*8*11*14*17*2</code> 正好有 18 个字符。当处理 50 万个字符时会比较省事。</p>\n<pre><code>$ ./rust-counter-strings 50\n# 2*4*6*8*11*14*17*20*23*26*29*32*35*38*41*44*47*50*\n</code></pre>\n<p>这就是个小工具，代码也只有几十行。</p>\n<p>GitHub：<a href=\"https://github.com/thomaschaplin/rust-counter-strings\" rel=\"noopener noreferrer\">thomaschaplin/rust-counter-strings: 🧵 Generate self-describing strings of a given length to help aid software testing</a></p>\n<hr>\n<p>From 日报小组 长琴</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc 论坛：支持 rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust 语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-12 14:30:38","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-09-11 Tabled 发布v0.3, bma-benchmark, ferros, Veloren发布v0.11","link":"https://rustcc.cn/article?id=db077b1a-5af6-4fb2-b065-ff8be974fd62","description":"<h3>Tabled 发布v0.3</h3>\n<p>Tabled 是一个易于使用的库，用于美化 Rust 结构和枚举的输出。</p>\n<p>Github<a href=\"https://github.com/zhiburt/tabled\" rel=\"noopener noreferrer\">链接</a>，https://github.com/zhiburt/tabled</p>\n<h3>bma-benchmark 一个友好的基准测试工具</h3>\n<p>使用 <code>bma_benchmark</code></p>\n<pre><code>#[macro_use]\nextern crate bma_benchmark;\n\nuse std::sync::Mutex;\n\nlet n = 100_000_000;\nlet mutex = Mutex::new(0);\nbenchmark_start!();\nfor _ in 0..n {\n    let _a = mutex.lock().unwrap();\n}\nbenchmark_print!(n);\n</code></pre>\n<p>使用宏 <code>benchmark!</code></p>\n<pre><code>#[macro_use]\nextern crate bma_benchmark;\n\nuse std::sync::Mutex;\n\nlet mutex = Mutex::new(0);\nbenchmark!(100_000_000, {\n    let _a = mutex.lock().unwrap();\n    });\n</code></pre>\n<p><img src=\"https://raw.githubusercontent.com/alttch/bma-benchmark/main/simple.png\" alt=\"结果\"></p>\n<p>Crate <a href=\"https://crates.io/crates/bma-benchmark\" rel=\"noopener noreferrer\">链接</a>，https://crates.io/crates/bma-benchmark</p>\n<h3>ferros</h3>\n<p>seL4 是一个用于构建操作系统和嵌入式程序的工具包，这个开源项目是使 Rust 中的 seL4 编程变得更好。</p>\n<p>以下代码演练假定使用示例 sel4_start 库执行 selfe，并介绍了 ferros 的某些方面。</p>\n<pre><code>use selfe_sys;\nuse ferros::alloc::{self, micro_alloc, smart_alloc};\nuse ferros::userland::{root_cnode, BootInfo};\n\n// The raw boot info is provided by the sel4_start library\nlet raw_boot_info: &amp;'static selfe_sys::seL4_BootInfo = unsafe { &amp;*sel4_start::BOOTINFO };\n\n\n// Utility for finding and claiming `Untyped` instances supplied by the boot info.\nlet mut allocator = micro_alloc::Allocator::bootstrap(&amp;raw_boot_info)?;\nlet initial_untyped = allocator\n    .get_untyped::&lt;U20&gt;() // The size of the Untyped instance, as bits\n    .expect(\"Couldn't find an untyped instance of the desired size\");\n\n// Create the top-level CNode wrapper with type-level-tracked remaining slot capacity\nlet (root_cnode, local_slots) = root_cnode(&amp;raw_boot_info);\n\n// Once we have an initial Untyped instance, memory distribution from it\n// can be tracked with compile-time checks. The smart_alloc macro synthesizes\n// the allocation code, and the capacity bounds are statically verified by\n// the type checker. The effect is that you can write 'slots' in the macro body \n// anywhere you need some slots, and you'll get the right number allocated\n// with type inference. A reference to 'ut' does the same for untyped memory. \nsmart_alloc!(|slots from local_slots, ut from uts| {\n\n    // Create a page table seL4 kernel object and return a capability pointer to it.\n    // Here we use a variable binding type annotation and Rust's type system can figure out\n    // if it can allocate a large enough Untyped instance and enough cnode slots\n    // to represent this particular kernel object.\n    let example_page_table: LocalCap&lt;UnmappedPageTable&gt; = retype(ut, slots)?;\n\n    // Create a resource-tracking wrapper around the raw boot info to assist in\n    // virtual memory related operations.\n    let boot_info  = BootInfo::wrap(raw_boot_info, ut, slots);\n    let (root_page_table, boot_info) = boot_info.map_page_table(root_page_table)?;\n});\n\n</code></pre>\n<p>Github<a href=\"https://github.com/auxoncorp/ferros\" rel=\"noopener noreferrer\">链接</a>，https://github.com/auxoncorp/ferros</p>\n<h3>Veloren发布v0.11</h3>\n<p>今天，Veloren 发布了 0.11。 这个版本已经制作了 3 个月，其一大重点是让世界各地的战斗更具活力。这是以新的地点系统的形式出现，以及 NPC 和生物如何与世界互动。</p>\n<p>要了解还有哪些新功能！请继续阅读 V0.11 变更日志<a href=\"https://veloren.net/release-0-11/\" rel=\"noopener noreferrer\">链接</a>，https://veloren.net/release-0-11/</p>\n<hr>\n<p>From 日报小组 <a href=\"https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929\" rel=\"noopener noreferrer\">洋芋</a></p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 15:33:08","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"cargo fix 如何自动修复warning？","link":"https://rustcc.cn/article?id=0d747849-2064-4ba2-a725-2fe64f6ab556","description":"<p>代码中有些外面copy过来的enum，导致很多的“should have an upper camel case name”warning。就是编码风格的问题，可是<code>cargo fix</code>没有办法按照rust给的建议自动帮我改掉。。。。</p>\n<p>有办法吗？</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 14:31:22","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"20K～80K/月 高薪招募区块链开发工程师","link":"https://rustcc.cn/article?id=af23774f-9378-4b81-8d01-e0ce37159b00","description":"<p>区块链开发工程师招聘</p>\n<p>Bochsler Finance是一家总部位于瑞士的金融集团，专注于互联网科技和金融科技领域的投资和资产管理，投资和孵化了Dfinity, Origyn等优质项目，旗下公司包括对冲基金公司，财富管理公司，交易所，Token基金等。与Dfinity，Coinlist，Polychain有紧密合作关系。</p>\n<p>Bochsler Finance上海分公司正在筹建中。Bochsler将与DFINITY以及Origyn紧密合作，共同进行互联网计算机的生态建设，技术合作开发，Dfinity生态项目咨询，孵化等。</p>\n<p>现招募以下职位：区块链实习/初级/中级/高级开发工程师</p>\n<p>职位要求\n•  在1-2月培训后，能掌握Dfinity技术开发，能与国内外团队合作，共同开发Dfinity生态项目。\n•  熟练掌握 JAVA/Rust/C/C++/MOTOKO 中的一种或多种语言，有 Rust 实际开发经验者优先\n•  熟悉以太坊，BSC，NEO，DOT智能合约，DEX，DeFi，NFT开发经验者优先\n•  具有扎实的应用密码学知识者优先(非对称加密、零知识证明、多方安全计算等)\n•  工作务实，不浮躁，有责任感和远大理想\n•  工作地点：上海中山公园附近，暂时不接受远程办公。</p>\n<p>薪酬范围和公司福利\n月薪20K～80K，优秀者工资面议，开发项目的通证提成，鼓励公司内部创业以及项目孵化，有机会参与顶级项目私募，加薪和职务上升通道畅通,与Dfinity/Origyn高级工程师交流学习。</p>\n<p>联系方式：hu@bochslerfinance.com\n微信：woniu2010woniu</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 10:48:26","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"一个优雅的过滤方式","link":"https://rustcc.cn/article?id=0a205b13-bd71-4ceb-8bda-f1977d9ce926","description":"<p>项目地址：https://github.com/Spxg/filter</p>\n<p>使用 filter-macros，然后对一个枚举体实现 Filter Trait，填上判断规则（rules），就可以用了。</p>\n<pre><code>#[add_fields]\n#[derive(BitAnd, BitOr, OpUnitTrait)]\nenum NumType {\n    Odd,\n    Even,\n    DivByThree,\n    DivByFour,\n    DivByFive,\n    IsMagicNum(i32),\n}\n\nimpl Filter for NumType {\n    type Item = i32;\n\n    fn rules(&amp;self, item: &amp;Self::Item) -&gt; bool {\n        match self {\n            NumType::Odd =&gt; item % 2 != 0,\n            NumType::Even =&gt; item % 2 == 0,\n            NumType::DivByThree =&gt; item % 3 == 0,\n            NumType::DivByFour =&gt; item % 4 == 0,\n            NumType::DivByFive =&gt; item % 5 == 0,\n            NumType::IsMagicNum(num) =&gt; item == num,\n            _ =&gt; false,\n        }\n    }\n}\n\nfn main() {\n    let nums = vec![1, 2, 3, 4, 5, 6, 9, 12, 15, 16, 20, 22, 24, 1024];\n    let test = NumType::Odd\n        | NumType::Even &amp; NumType::DivByThree &amp; NumType::DivByFour\n        | NumType::DivByFive;\n    let result = nums\n        .clone()\n        .into_iter()\n        .filter(test.ref_one_filter())\n        .collect::&lt;Vec&lt;_&gt;&gt;();\n    assert_eq!(vec![1, 3, 5, 9, 12, 15, 20, 24], result);\n\n    let test = NumType::IsMagicNum(1024);\n    assert!(test.self_filter()(1024));\n}\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 03:42:20","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"在vscode中安装了rust-analyzer插件，保存代码无法自动格式化","link":"https://rustcc.cn/article?id=017fbf6c-8032-4f6f-8ea3-fa98d7d1d133","description":"<p>在vscode中安装了rust-analyzer插件，保存代码无法自动格式化\n每次都要手动运行<code>cargo fmt</code>命令才可以，请问大家有什么办法解决这个问题吗？</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 02:14:36","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Mutex 区别","link":"https://rustcc.cn/article?id=c7557d0d-ccb9-4276-a984-78dc6d9a1091","description":"<p>Rust 标准库中有 Mutex，tokio 库中也有 Mutex，这些不同的互斥锁有不同的用法，但是具体有什么区别？</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 01:59:51","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"非凸科技 ｜招聘内推公告","link":"https://rustcc.cn/article?id=07f3c034-3b48-4f41-bb02-cdda2f8c25f9","description":"<p>一、公司介绍</p>\n<p>上海非凸智能科技有限公司（简称“非凸科技”）成立于2018年， 是一家国内领先的智能算法和交易系统服务公司。</p>\n<p>公司以“科技赋能交易，打造全球领先的交易效率管理专家”为愿景，专注于智能算法交易领域的研究和开发，目前已开发出一系列产品：非凸智能算法V4.0、非凸智能交易系统V3.0、非凸智能“T+0”策略V1.0。服务于业内知名客户中泰证券、华鑫证券、华泰证券等。</p>\n<p>公司团队来自知名投资机构及交易系统服务商，拥有丰富的产业经验和技术能力，潜心打造智能算法交易平台，在原有基础上全面升级到互联网新一代技术架构，结合机器学习等新兴技术，逐步完成各类交易算法的研发迭代，正持续为券商、量化私募等众多大型金融机构提供优质的算法服务。</p>\n<p>二、内推奖励</p>\n<p>奖励20,000元/位</p>\n<p>三、奖励机制</p>\n<p>分两个阶段获得推荐奖励：\n阶段1:候选人入职满2周，获得50%奖励；\n阶段2:候选人入职满半年，获得另50%奖励；\n如果候选人没有入职半年，则不发放阶段2的奖励。</p>\n<p>四、内推优势：</p>\n<p>候选人进入内推通道，不仅可以从海量简历中脱颖而出，还可以免笔试直通面试！</p>\n<p>五、9月内推岗位：</p>\n<p>Rust开发工程师\n【岗位职责】\n1.设计并开发基于RUST的高性能，低时延算法交易系统；\n2.设计并开发数据处理平台，监控运维平台；\n3.设计并开发面向客户的高可用交易工具等；\n4.设计并开发策略相关的回测平台。\n【岗位要求】\n1.本科及以上学历（985优先）。\n2.编程基础扎实，具有良好的计算机理论基础；\n3.熟练掌握Linux操作，性能分析，具备Rust/C++/Java/Go丰富开发经验，熟悉常用的设计模式，有分布式相关经验加分；\n4.有研发高性能，低时延系统经验加分；\n5.对技术充满热情，思考深入。自我驱动，能快速学习新鲜事物。</p>\n<p>六、内推方式</p>\n<p>1.提交简历方式：\n·投递邮箱：recruit@ft.tech\n·邮件主题：[应聘人]+[应聘岗位]+[推荐人]\n·重复推荐以收到简历时间顺序为准</p>\n<p>2.如果简历通过，安排笔试/面试</p>\n<p>还在等什么？快加入非凸内推吧！</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-10 14:19:52","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-09-10 Rust 1.55 稳定版发布","link":"https://rustcc.cn/article?id=c36da117-d84e-42a0-9304-b02fe6f27df1","description":"<h1>Rust 1.55 稳定版发布</h1>\n<p>Rust 1.55 是 2021 Edition 正式发布之前的倒数第二个版本，此版本主要更新如下：</p>\n<ul>\n<li>range 操作符支持半开语法</li>\n</ul>\n<pre><code>match x as u32 {\n      0 =&gt; println!(\"zero!\"),\n      1.. =&gt; println!(\"positive number!\"),\n}\n</code></pre>\n<ul>\n<li>极大的提高浮点数的解析速度</li>\n</ul>\n<p>采用新的 Eisel-Lemire 算法解析浮点数，不仅在速度上有很大提升，之前一些解析失败的边缘 case 也全都修复了</p>\n<ul>\n<li>\n<p>cargo 构建的时候不再重复报 Error</p>\n</li>\n<li>\n<p>细化 io::ErrorKind</p>\n</li>\n</ul>\n<p>使用 ErrorKind::Other 来区分标准库之外产生的 io Error，标准库之内不会再产生 ErrorKind::Other 的 Error。标准库未来暂未分类的 Error 类型，统一使用 Uncategorized。</p>\n<ul>\n<li>数组增加 map() 方法</li>\n</ul>\n<p>数组的 map() 方法返回的依然是数组，对数组来说更方便！</p>\n<ul>\n<li>稳定 ops::ControlFlow</li>\n</ul>\n<p>这是未来要稳定的 <code>try trait v2</code> 的一部分。</p>\n<pre><code>pub enum ControlFlow&lt;B, C = ()&gt; {\n    Continue(C),\n    Break(B),\n}\n</code></pre>\n<p>标准库里面很多地方都用到了这个类型，很有用。</p>\n<ul>\n<li><code>cargo clippy --fix</code> 可以自动帮你修复 clippy 的警告</li>\n</ul>\n<p>这个非常赞！</p>\n<p>链接：<a href=\"https://blog.rust-lang.org/2021/09/09/Rust-1.55.0.html\" rel=\"noopener noreferrer\">https://blog.rust-lang.org/2021/09/09/Rust-1.55.0.html</a></p>\n<h1>纪念 Anna Harren</h1>\n<p>Anna Harren 是第一个将 Rust 的 ::&lt;&gt; 语法命名为 Turbofish 的人。</p>\n<p>Rust 官方发布 1.55 的时候在博客里还特意纪念了最近刚去世的 Anna Harren。</p>\n<p>链接：<a href=\"https://twitter.com/garblefart/status/627886036211900416\" rel=\"noopener noreferrer\">https://twitter.com/garblefart/status/627886036211900416</a></p>\n<h1>Rustaceans 准则</h1>\n<p>Niko 发布了一篇博客，总结出 Rustaceans 的准则，比如其中提到：</p>\n<p><strong>Rust empowers by being…</strong></p>\n<ul>\n<li>⚙️ Reliable: “if it compiles, it works”</li>\n<li>🐎 Performant: “idiomatic code runs efficiently”</li>\n<li>🥰 Supportive: “the language, tools, and community are here to help”</li>\n<li>🧩 Productive: “a little effort does a lot of work”</li>\n<li>🔧 Transparent: “you can predict and control low-level details”</li>\n<li>🤸 Versatile: “you can do anything with Rust”</li>\n</ul>\n<p><strong>How to Rustacean</strong></p>\n<ul>\n<li>💖 Be kind and considerate</li>\n<li>✨ Bring joy to the user</li>\n<li>👋 Show up</li>\n<li>🔭 Recognize others’ knowledge</li>\n<li>🔁 Start somewhere</li>\n<li>✅ Follow through</li>\n<li>🤝 Pay it forward</li>\n<li>🎁 Trust and delegate</li>\n</ul>\n<p>链接：<a href=\"https://smallcultfollowing.com/babysteps//blog/2021/09/08/rustacean-principles/\" rel=\"noopener noreferrer\">https://smallcultfollowing.com/babysteps//blog/2021/09/08/rustacean-principles/</a></p>\n<p>--\nFrom 日报小组 Folyd, 侯盛鑫</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=c36da117-d84e-42a0-9304-b02fe6f27df1\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-10 12:38:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了","link":"https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b","description":"<p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>\n<ol>\n<li>认识面向基础架构语言</li>\n<li>理解 Rust 所有权</li>\n<li>通过实战理解 Rust 宏</li>\n<li>通过 Datafuse 理解全链路跟踪</li>\n<li>Rust 异步编程入门 Future Part 1</li>\n<li>Rust 异步编程入门 Future Part 2</li>\n</ol>\n<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。\n本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>\n<hr>\n<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>\n<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>\n<p>分享讲师： 董泽润</p>\n<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现\n讲师介绍：\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<hr>\n<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>\n<p>分享时间：  周日晚上2021-09-12 20:00-21:00</p>\n<p>分享讲师： 苏林</p>\n<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>\n<ol>\n<li>利用 Tokio 实现一个 Mini Http server</li>\n<li>基于 Http server提供内容动态的 API 网关</li>\n<li>利用 Redis 实现对 API 网关加速</li>\n<li>学习 Rust RPC 调用，实现微服务调用</li>\n</ol>\n<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<p>Rust 异步编程入门 Future Part 1   | Vol. 5\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<p>Rust 异步编程入门 Future Part 2  | Vol. 6\nhttps://www.bilibili.com/video/bv1oy4y1G7jC</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 02:23:16","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust 学习随笔","link":"https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8","description":"<h1>切换镜像源</h1>\n<p>crm =&gt; https://github.com/wtklbm/crm</p>\n<p>常用命令就是 <code>crm best</code></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 14:35:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"pretree 补全文档发布了,再次谢谢大神的指点终于入门了。","link":"https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827","description":"<h1>Pretree</h1>\n<p>pretree is a package for storing and querying routing rules with prefix tree .</p>\n<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>\n<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>\n<p>Inspired by <a href=\"https://github.com/obity/pretree\" rel=\"noopener noreferrer\">obity/pretree</a> (golang)</p>\n<h1>Doc</h1>\n<p>See this document at <a href=\"https://docs.rs/pretree\" rel=\"noopener noreferrer\">API documentation</a></p>\n<h1>Install</h1>\n<p>Add the following line to your Cargo.toml file:</p>\n<pre><code>pretree = \"1.0.0\"\n</code></pre>\n<h1>Example</h1>\n<pre><code>use pretree::Pretree;\nlet mut p = Pretree::new();\np.store(\"GET\",\"account/{id}/info/:name\");\np.store(\"GET\",\"account/:id/login\");\np.store(\"GET\",\"account/{id}\");\np.store(\"GET\",\"bacteria/count_number_by_month\");\nlet (ok,rule,vars) = p.query(\"GET\",\"account/929239\");\nprintln!(\"ok:{} rule:{} vars:{:#?}\",ok,rule,vars);\n\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 09:37:30","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6","link":"https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a","description":"<h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>\n<p><strong>课程时间:</strong>  2021年9月5日 20:00-21:00</p>\n<p><strong>课程介绍:</strong>  上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识,  Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>\n<h3>课程大纲</h3>\n<p>1、回顾 Rust 异步编程模型.</p>\n<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>\n<p>3、Tokio 介绍.</p>\n<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>\n<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/\nRust 异步编程入门 Future Part 1  回放地址：\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 08:40:15","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"公开课：《 Rust 异步编程入门 Future 》|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>\n<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>\n<p><strong>课程介绍:</strong>  讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是  Rust 异步编程的核心基础。</p>\n<h3>课程大纲</h3>\n<p>1、为什么需要异步.</p>\n<p>2、理解异步编程模型.</p>\n<p>3、Future 编程模型讲解.</p>\n<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>\n<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>异步引擎 C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D 游戏引擎</h3>\n<ul>\n<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>\n<li><strong>延迟着色</strong></li>\n<li><strong>内置保存/加载</strong></li>\n<li><strong>独立场景编辑器</strong></li>\n<li><strong>高级物理模型</strong></li>\n<li><strong>分层模型资源</strong></li>\n<li><strong>几何实例化</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>\n<p><strong>课程时间：</strong>  2021年8月22日 20:30-21:30</p>\n<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>\n<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>\n<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>课程大纲</h3>\n<ol>\n<li>\n<p>什么是分布式追踪系统OpenTracing及应用场景</p>\n</li>\n<li>\n<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>\n</li>\n<li>\n<p>为什么需要tokio-rs/tracing库</p>\n</li>\n<li>\n<p>演示Datafuse项目中tokio-rs/tracing的使用</p>\n</li>\n</ol>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>课程中苏林老师推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"论坛github账户无法登录解决笔记","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>有反映这两天github账户无法登录了。</p>\n<p>报这个错：</p>\n<pre><code>get github user info err\n</code></pre>\n<p>查了几个地方：</p>\n<ol>\n<li>代码是否运行正常：Ok</li>\n<li>https代理是否正常：Ok</li>\n<li>检查了github返回日志，发现是：</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>进入这个地址一看：<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>\n<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>\n<p>特此记录。</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 的 Future 与 Javascript 的 Promise 功能对照参考","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>\n<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>\n<blockquote>\n<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* 什么都不做 */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被  （1）跨线程传递  （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust公开课：《通过实战理解 Rust 宏》| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>\n<p><strong>课程时间：</strong>  2021年8月15日 20:30-21:30</p>\n<p><strong>课程介绍：</strong></p>\n<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>这就是通过宏实现配置的统一行为，代码参考：\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>\n<h3>课程大纲</h3>\n<ul>\n<li>什么是 Rust 宏</li>\n<li>什么是宏运行原理</li>\n<li>如何创建 Rust 宏过程</li>\n<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>\n</ul>\n<p><strong>讲师介绍</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>课程中苏林老师推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-09-13T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Semantic Parsing in Task-Oriented Dialog with Recursive Insertion-based Encoder. (arXiv:2109.04500v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04500","description":"<p>We introduce a Recursive INsertion-based Encoder (RINE), a novel approach for\nsemantic parsing in task-oriented dialog. Our model consists of an encoder\nnetwork that incrementally builds the semantic parse tree by predicting the\nnon-terminal label and its positions in the linearized tree. At the generation\ntime, the model constructs the semantic parse tree by recursively inserting the\npredicted non-terminal labels at the predicted positions until termination.\nRINE achieves state-of-the-art exact match accuracy on low- and high-resource\nversions of the conversational semantic parsing benchmark TOP (Gupta et al.,\n2018; Chen et al., 2020), outperforming strong sequence-to-sequence models and\ntransition-based parsers. We also show that our model design is applicable to\nnested named entity recognition task, where it performs on par with\nstate-of-the-art approach designed for that task. Finally, we demonstrate that\nour approach is 2-3.5 times faster than the sequence-to-sequence model at\ninference time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mansimov_E/0/1/0/all/0/1\">Elman Mansimov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Filling the Gaps in Ancient Akkadian Texts: A Masked Language Modelling Approach. (arXiv:2109.04513v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04513","description":"<p>We present models which complete missing text given transliterations of\nancient Mesopotamian documents, originally written on cuneiform clay tablets\n(2500 BCE - 100 CE). Due to the tablets' deterioration, scholars often rely on\ncontextual cues to manually fill in missing parts in the text in a subjective\nand time-consuming process. We identify that this challenge can be formulated\nas a masked language modelling task, used mostly as a pretraining objective for\ncontextualized language models. Following, we develop several architectures\nfocusing on the Akkadian language, the lingua franca of the time. We find that\ndespite data scarcity (1M tokens) we can achieve state of the art performance\non missing tokens prediction (89% hit@5) using a greedy decoding scheme and\npretraining on data from other languages and different time periods. Finally,\nwe conduct human evaluations showing the applicability of our models in\nassisting experts to transcribe texts in extinct languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saret_B/0/1/0/all/0/1\">Benny Saret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yehudai_A/0/1/0/all/0/1\">Asaf Yehudai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horowitz_W/0/1/0/all/0/1\">Wayne Horowitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wasserman_N/0/1/0/all/0/1\">Nathan Wasserman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Morality Frames in Political Tweets using Relational Learning. (arXiv:2109.04535v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04535","description":"<p>Extracting moral sentiment from text is a vital component in understanding\npublic opinion, social movements, and policy decisions. The Moral Foundation\nTheory identifies five moral foundations, each associated with a positive and\nnegative polarity. However, moral sentiment is often motivated by its targets,\nwhich can correspond to individuals or collective entities. In this paper, we\nintroduce morality frames, a representation framework for organizing moral\nattitudes directed at different entities, and come up with a novel and\nhigh-quality annotated dataset of tweets written by US politicians. Then, we\npropose a relational learning model to predict moral attitudes towards entities\nand moral foundations jointly. We do qualitative and quantitative evaluations,\nshowing that moral sentiment towards entities differs highly across political\nideologies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1\">Shamik Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pacheco_M/0/1/0/all/0/1\">Maria Leonor Pacheco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generic resources are what you need: Style transfer tasks without task-specific parallel training data. (arXiv:2109.04543v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04543","description":"<p>Style transfer aims to rewrite a source text in a different target style\nwhile preserving its content. We propose a novel approach to this task that\nleverages generic resources, and without using any task-specific parallel\n(source-target) data outperforms existing unsupervised approaches on the two\nmost popular style transfer tasks: formality transfer and polarity swap. In\npractice, we adopt a multi-step procedure which builds on a generic pre-trained\nsequence-to-sequence model (BART). First, we strengthen the model's ability to\nrewrite by further pre-training BART on both an existing collection of generic\nparaphrases, as well as on synthetic pairs created using a general-purpose\nlexical resource. Second, through an iterative back-translation approach, we\ntrain two models, each in a transfer direction, so that they can provide each\nother with synthetically generated pairs, dynamically in the training process.\nLastly, we let our best reresulting model generate static synthetic pairs to be\nused in a supervised training regime. Besides methodology and state-of-the-art\nresults, a core contribution of this work is a reflection on the nature of the\ntwo tasks we address, and how their differences are highlighted by their\nresponse to our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lai_H/0/1/0/all/0/1\">Huiyuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toral_A/0/1/0/all/0/1\">Antonio Toral</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nissim_M/0/1/0/all/0/1\">Malvina Nissim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Math Word Problem Generation with Mathematical Consistency and Problem Context Constraints. (arXiv:2109.04546v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04546","description":"<p>We study the problem of generating arithmetic math word problems (MWPs) given\na math equation that specifies the mathematical computation and a context that\nspecifies the problem scenario. Existing approaches are prone to generating\nMWPs that are either mathematically invalid or have unsatisfactory language\nquality. They also either ignore the context or require manual specification of\na problem template, which compromises the diversity of the generated MWPs. In\nthis paper, we develop a novel MWP generation approach that leverages i)\npre-trained language models and a context keyword selection model to improve\nthe language quality of the generated MWPs and ii) an equation consistency\nconstraint for math equations to improve the mathematical validity of the\ngenerated MWPs. Extensive quantitative and qualitative experiments on three\nreal-world MWP datasets demonstrate the superior performance of our approach\ncompared to various baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zichao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_A/0/1/0/all/0/1\">Andrew S. Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1\">Richard G. Baraniuk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeDyT: A General Framework for Multi-Step Event Forecasting via Sequence Modeling on Dynamic Entity Embeddings. (arXiv:2109.04550v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04550","description":"<p>Temporal Knowledge Graphs store events in the form of subjects, relations,\nobjects, and timestamps which are often represented by dynamic heterogeneous\ngraphs. Event forecasting is a critical and challenging task in Temporal\nKnowledge Graph reasoning that predicts the subject or object of an event in\nthe future. To obtain temporal embeddings multi-step away in the future,\nexisting methods learn generative models that capture the joint distribution of\nthe observed events. To reduce the high computation costs, these methods rely\non unrealistic assumptions of independence and approximations in training and\ninference. In this work, we propose SeDyT, a discriminative framework that\nperforms sequence modeling on the dynamic entity embeddings to solve the\nmulti-step event forecasting problem. SeDyT consists of two components: a\nTemporal Graph Neural Network that generates dynamic entity embeddings in the\npast and a sequence model that predicts the entity embeddings in the future.\nCompared with the generative models, SeDyT does not rely on any heuristic-based\nprobability model and has low computation complexity in both training and\ninference. SeDyT is compatible with most Temporal Graph Neural Networks and\nsequence models. We also design an efficient training method that trains the\ntwo components in one gradient descent propagation. We evaluate the performance\nof SeDyT on five popular datasets. By combining temporal Graph Neural Network\nmodels and sequence models, SeDyT achieves an average of 2.4% MRR improvement\nwhen not using the validation set and more than 10% MRR improvement when using\nthe validation set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1\">Hongkuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Orme_Rogers_J/0/1/0/all/0/1\">James Orme-Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kannan_R/0/1/0/all/0/1\">Rajgopal Kannan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasanna_V/0/1/0/all/0/1\">Viktor Prasanna</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SPECTRA: Sparse Structured Text Rationalization. (arXiv:2109.04552v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04552","description":"<p>Selective rationalization aims to produce decisions along with rationales\n(e.g., text highlights or word alignments between two sentences). Commonly,\nrationales are modeled as stochastic binary masks, requiring sampling-based\ngradient estimators, which complicates training and requires careful\nhyperparameter tuning. Sparse attention mechanisms are a deterministic\nalternative, but they lack a way to regularize the rationale extraction (e.g.,\nto control the sparsity of a text highlight or the number of alignments). In\nthis paper, we present a unified framework for deterministic extraction of\nstructured explanations via constrained inference on a factor graph, forming a\ndifferentiable layer. Our approach greatly eases training and rationale\nregularization, generally outperforming previous work on what comes to\nperformance and plausibility of the extracted rationales. We further provide a\ncomparative study of stochastic and deterministic methods for rationale\nextraction for classification and natural language inference tasks, jointly\nassessing their predictive power, quality of the explanations, and model\nvariability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guerreiro_N/0/1/0/all/0/1\">Nuno Miguel Guerreiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subword Mapping and Anchoring across Languages. (arXiv:2109.04556v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04556","description":"<p>State-of-the-art multilingual systems rely on shared vocabularies that\nsufficiently cover all considered languages. To this end, a simple and\nfrequently used approach makes use of subword vocabularies constructed jointly\nover several languages. We hypothesize that such vocabularies are suboptimal\ndue to false positives (identical subwords with different meanings across\nlanguages) and false negatives (different subwords with similar meanings). To\naddress these issues, we propose Subword Mapping and Anchoring across Languages\n(SMALA), a method to construct bilingual subword vocabularies. SMALA extracts\nsubword alignments using an unsupervised state-of-the-art mapping technique and\nuses them to create cross-lingual anchors based on subword similarities. We\ndemonstrate the benefits of SMALA for cross-lingual natural language inference\n(XNLI), where it improves zero-shot transfer to an unseen language without\ntask-specific data, but only by sharing subword embeddings. Moreover, in neural\nmachine translation, we show that joint subword vocabularies obtained with\nSMALA lead to higher BLEU scores on sentences that contain many false positives\nand false negatives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1\">Giorgos Vernikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popescu_Belis_A/0/1/0/all/0/1\">Andrei Popescu-Belis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TIAGE: A Benchmark for Topic-Shift Aware Dialog Modeling. (arXiv:2109.04562v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04562","description":"<p>Human conversations naturally evolve around different topics and fluently\nmove between them. In research on dialog systems, the ability to actively and\nsmoothly transition to new topics is often ignored. In this paper we introduce\nTIAGE, a new topic-shift aware dialog benchmark constructed utilizing human\nannotations on topic shifts. Based on TIAGE, we introduce three tasks to\ninvestigate different scenarios of topic-shift modeling in dialog settings:\ntopic-shift detection, topic-shift triggered response generation and\ntopic-aware dialog generation. Experiments on these tasks show that the\ntopic-shift signals in TIAGE are useful for topic-shift response generation. On\nthe other hand, dialog systems still struggle to decide when to change topic.\nThis indicates further research is needed in topic-shift aware dialog modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1\">Huiyuan Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copestake_A/0/1/0/all/0/1\">Ann Copestake</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speechformer: Reducing Information Loss in Direct Speech Translation. (arXiv:2109.04574v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04574","description":"<p>Transformer-based models have gained increasing popularity achieving\nstate-of-the-art performance in many research fields including speech\ntranslation. However, Transformer's quadratic complexity with respect to the\ninput sequence length prevents its adoption as is with audio signals, which are\ntypically represented by long sequences. Current solutions resort to an initial\nsub-optimal compression based on a fixed sampling of raw audio features.\nTherefore, potentially useful linguistic information is not accessible to\nhigher-level layers in the architecture. To solve this issue, we propose\nSpeechformer, an architecture that, thanks to reduced memory usage in the\nattention layers, avoids the initial lossy compression and aggregates\ninformation only at a higher level according to more informed linguistic\ncriteria. Experiments on three language pairs (en-&gt;de/es/nl) show the efficacy\nof our solution, with gains of up to 0.8 BLEU on the standard MuST-C corpus and\nof up to 4.0 BLEU in a low resource scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papi_S/0/1/0/all/0/1\">Sara Papi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gaido_M/0/1/0/all/0/1\">Marco Gaido</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1\">Matteo Negri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turchi_M/0/1/0/all/0/1\">Marco Turchi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph-Based Decoding for Task Oriented Semantic Parsing. (arXiv:2109.04587v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04587","description":"<p>The dominant paradigm for semantic parsing in recent years is to formulate\nparsing as a sequence-to-sequence task, generating predictions with\nauto-regressive sequence decoders. In this work, we explore an alternative\nparadigm. We formulate semantic parsing as a dependency parsing task, applying\ngraph-based decoding techniques developed for syntactic parsing. We compare\nvarious decoding techniques given the same pre-trained Transformer encoder on\nthe TOP dataset, including settings where training data is limited or contains\nonly partially-annotated examples. We find that our graph-based approach is\ncompetitive with sequence decoders on the standard setting, and offers\nsignificant improvements in data efficiency and settings where\npartially-annotated data is available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cole_J/0/1/0/all/0/1\">Jeremy R. Cole</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Nanjiang Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pasupat_P/0/1/0/all/0/1\">Panupong Pasupat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1\">Luheng He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaw_P/0/1/0/all/0/1\">Peter Shaw</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERT, mBERT, or BiBERT? A Study on Contextualized Embeddings for Neural Machine Translation. (arXiv:2109.04588v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04588","description":"<p>The success of bidirectional encoders using masked language models, such as\nBERT, on numerous natural language processing tasks has prompted researchers to\nattempt to incorporate these pre-trained models into neural machine translation\n(NMT) systems. However, proposed methods for incorporating pre-trained models\nare non-trivial and mainly focus on BERT, which lacks a comparison of the\nimpact that other pre-trained models may have on translation performance. In\nthis paper, we demonstrate that simply using the output (contextualized\nembeddings) of a tailored and suitable bilingual pre-trained language model\n(dubbed BiBERT) as the input of the NMT encoder achieves state-of-the-art\ntranslation performance. Moreover, we also propose a stochastic layer selection\napproach and a concept of dual-directional translation model to ensure the\nsufficient utilization of contextualized embeddings. In the case of without\nusing back translation, our best models achieve BLEU scores of 30.45 for En-&gt;De\nand 38.61 for De-&gt;En on the IWSLT'14 dataset, and 31.26 for En-&gt;De and 34.94\nfor De-&gt;En on the WMT'14 dataset, which exceeds all published numbers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Large-Scale Study of Machine Translation in the Turkic Languages. (arXiv:2109.04593v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04593","description":"<p>Recent advances in neural machine translation (NMT) have pushed the quality\nof machine translation systems to the point where they are becoming widely\nadopted to build competitive systems. However, there is still a large number of\nlanguages that are yet to reap the benefits of NMT. In this paper, we provide\nthe first large-scale case study of the practical application of MT in the\nTurkic language family in order to realize the gains of NMT for Turkic\nlanguages under high-resource to extremely low-resource scenarios. In addition\nto presenting an extensive analysis that identifies the bottlenecks towards\nbuilding competitive systems to ameliorate data scarcity, our study has several\nkey contributions, including, i) a large parallel corpus covering 22 Turkic\nlanguages consisting of common public datasets in combination with new datasets\nof approximately 2 million parallel sentences, ii) bilingual baselines for 26\nlanguage pairs, iii) novel high-quality test sets in three different\ntranslation domains and iv) human evaluation scores. All models, scripts, and\ndata will be released to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1\">Jamshidbek Mirzakhalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Anoop Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kariev_S/0/1/0/all/0/1\">Sherzod Kariev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyers_F/0/1/0/all/0/1\">Francis Tyers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abduraufov_O/0/1/0/all/0/1\">Otabek Abduraufov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajili_M/0/1/0/all/0/1\">Mammad Hajili</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_S/0/1/0/all/0/1\">Sardana Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khaytbaev_A/0/1/0/all/0/1\">Abror Khaytbaev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laverghetta_A/0/1/0/all/0/1\">Antonio Laverghetta Jr.</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moydinboyev_B/0/1/0/all/0/1\">Behzodbek Moydinboyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Onal_E/0/1/0/all/0/1\">Esra Onal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulatova_S/0/1/0/all/0/1\">Shaxnoza Pulatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1\">Ahsan Wahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappan_S/0/1/0/all/0/1\">Sriram Chellappan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EVOQUER: Enhancing Temporal Grounding with Video-Pivoted BackQuery Generation. (arXiv:2109.04600v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04600","description":"<p>Temporal grounding aims to predict a time interval of a video clip\ncorresponding to a natural language query input. In this work, we present\nEVOQUER, a temporal grounding framework incorporating an existing text-to-video\ngrounding model and a video-assisted query generation network. Given a query\nand an untrimmed video, the temporal grounding model predicts the target\ninterval, and the predicted video clip is fed into a video translation task by\ngenerating a simplified version of the input query. EVOQUER forms closed-loop\nlearning by incorporating loss functions from both temporal grounding and query\ngeneration serving as feedback. Our experiments on two widely used datasets,\nCharades-STA and ActivityNet, show that EVOQUER achieves promising improvements\nby 1.05 and 1.31 at R@0.7. We also discuss how the query generation task could\nfacilitate error analysis by explaining temporal grounding model behavior.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yanjun Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lulu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jason Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huayan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Augmenting BERT-style Models with Predictive Coding to Improve Discourse-level Representations. (arXiv:2109.04602v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04602","description":"<p>Current language models are usually trained using a self-supervised scheme,\nwhere the main focus is learning representations at the word or sentence level.\nHowever, there has been limited progress in generating useful discourse-level\nrepresentations. In this work, we propose to use ideas from predictive coding\ntheory to augment BERT-style language models with a mechanism that allows them\nto learn suitable discourse-level representations. As a result, our proposed\napproach is able to predict future sentences using explicit top-down\nconnections that operate at the intermediate layers of the network. By\nexperimenting with benchmarks designed to evaluate discourse-related knowledge\nusing pre-trained sentence representations, we demonstrate that our approach\nimproves performance in 6 out of 11 tasks by excelling in discourse\nrelationship detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Araujo_V/0/1/0/all/0/1\">Vladimir Araujo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villa_A/0/1/0/all/0/1\">Andr&#xe9;s Villa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendoza_M/0/1/0/all/0/1\">Marcelo Mendoza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Alvaro Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks. (arXiv:2109.04604v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04604","description":"<p>The general goal of text simplification (TS) is to reduce text complexity for\nhuman consumption. This paper investigates another potential use of neural TS:\nassisting machines performing natural language processing (NLP) tasks. We\nevaluate the use of neural TS in two ways: simplifying input texts at\nprediction time and augmenting data to provide machines with additional\ninformation during training. We demonstrate that the latter scenario provides\npositive effects on machine performance on two separate datasets. In\nparticular, the latter use of TS improves the performances of LSTM (1.82-1.98%)\nand SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,\nreal-world relation extraction task. Further, the same setting yields\nimprovements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT\ntext classifier on MNLI, a practical natural language inference dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1\">Hoang Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization. (arXiv:2109.04607v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04607","description":"<p>We present IndoBERTweet, the first large-scale pretrained model for\nIndonesian Twitter that is trained by extending a monolingually-trained\nIndonesian BERT model with additive domain-specific vocabulary. We focus in\nparticular on efficient model adaptation under vocabulary mismatch, and\nbenchmark different ways of initializing the BERT embedding layer for new word\ntypes. We find that initializing with the average BERT subword embedding makes\npretraining five times faster, and is more effective than proposed methods for\nvocabulary adaptation in terms of extrinsic evaluation over seven Twitter-based\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1\">Fajri Koto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lau_J/0/1/0/all/0/1\">Jey Han Lau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Exploratory Study on Long Dialogue Summarization: What Works and What's Next. (arXiv:2109.04609v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04609","description":"<p>Dialogue summarization helps readers capture salient information from long\nconversations in meetings, interviews, and TV series. However, real-world\ndialogues pose a great challenge to current summarization models, as the\ndialogue length typically exceeds the input limits imposed by recent\ntransformer-based pre-trained models, and the interactive nature of dialogues\nmakes relevant information more context-dependent and sparsely distributed than\nnews articles. In this work, we perform a comprehensive study on long dialogue\nsummarization by investigating three strategies to deal with the lengthy input\nproblem and locate relevant information: (1) extended transformer models such\nas Longformer, (2) retrieve-then-summarize pipeline models with several\ndialogue utterance retrieval methods, and (3) hierarchical dialogue encoding\nmodels such as HMNet. Our experimental results on three long dialogue datasets\n(QMSum, MediaSum, SummScreen) show that the retrieve-then-summarize pipeline\nmodels yield the best performance. We also demonstrate that the summary quality\ncan be further improved with a stronger retrieval model and pretraining on\nproper external summarization datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deb_B/0/1/0/all/0/1\">Budhaditya Deb</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Celikyilmaz_A/0/1/0/all/0/1\">Asli Celikyilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-driven Segment Selection for Ranking Long Documents. (arXiv:2109.04611v1 [cs.IR])","link":"http://arxiv.org/abs/2109.04611","description":"<p>Transformer-based rankers have shown state-of-the-art performance. However,\ntheir self-attention operation is mostly unable to process long sequences. One\nof the common approaches to train these rankers is to heuristically select some\nsegments of each document, such as the first segment, as training data.\nHowever, these segments may not contain the query-related parts of documents.\nTo address this problem, we propose query-driven segment selection from long\ndocuments to build training data. The segment selector provides relevant\nsamples with more accurate labels and non-relevant samples which are harder to\nbe predicted. The experimental results show that the basic BERT-based ranker\ntrained with the proposed segment selector significantly outperforms that\ntrained by the heuristically selected segments, and performs equally to the\nstate-of-the-art model with localized self-attention that can process longer\ninput sequences. Our findings open up new direction to design efficient\ntransformer-based rankers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Youngwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rahimi_R/0/1/0/all/0/1\">Razieh Rahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonab_H/0/1/0/all/0/1\">Hamed Bonab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rule-based Morphological Inflection Improves Neural Terminology Translation. (arXiv:2109.04620v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04620","description":"<p>Current approaches to incorporating terminology constraints in machine\ntranslation (MT) typically assume that the constraint terms are provided in\ntheir correct morphological forms. This limits their application to real-world\nscenarios where constraint terms are provided as lemmas. In this paper, we\nintroduce a modular framework for incorporating lemma constraints in neural MT\n(NMT) in which linguistic knowledge and diverse types of NMT models can be\nflexibly applied. It is based on a novel cross-lingual inflection module that\ninflects the target lemma constraints based on the source context. We explore\nlinguistically motivated rule-based and data-driven neural-based inflection\nmodules and design English-German health and English-Lithuanian news test\nsuites to evaluate them in domain adaptation and low-resource MT settings.\nResults show that our rule-based inflection module helps NMT models incorporate\nlemma constraints more accurately than a neural module and outperforms the\nexisting end-to-end approach with lower training costs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Weijia Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carpuat_M/0/1/0/all/0/1\">Marine Carpuat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CINS: Comprehensive Instruction for Few-shot Learning in Task-orientedDialog Systems. (arXiv:2109.04645v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04645","description":"<p>As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema(definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5)is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What Changes Can Large-scale Language Models Bring? Intensive Study on HyperCLOVA: Billions-scale Korean Generative Pretrained Transformers. (arXiv:2109.04650v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04650","description":"<p>GPT-3 shows remarkable in-context learning ability of large-scale language\nmodels (LMs) trained on hundreds of billion scale data. Here we address some\nremaining issues less reported by the GPT-3 paper, such as a non-English LM,\nthe performances of different sized models, and the effect of recently\nintroduced prompt optimization on in-context learning. To achieve this, we\nintroduce HyperCLOVA, a Korean variant of 82B GPT-3 trained on a Korean-centric\ncorpus of 560B tokens. Enhanced by our Korean-specific tokenization, HyperCLOVA\nwith our training configuration shows state-of-the-art in-context zero-shot and\nfew-shot learning performances on various downstream tasks in Korean. Also, we\nshow the performance benefits of prompt-based learning and demonstrate how it\ncan be integrated into the prompt engineering pipeline. Then we discuss the\npossibility of materializing the No Code AI paradigm by providing AI\nprototyping capabilities to non-experts of ML by introducing HyperCLOVA studio,\nan interactive prompt engineering interface. Lastly, we demonstrate the\npotential of our methods with three successful in-house applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Boseop Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">HyoungSeok Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-Woo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1\">Gichang Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kwak_D/0/1/0/all/0/1\">Donghyun Kwak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_D/0/1/0/all/0/1\">Dong Hyeon Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungju Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seonhoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1\">Dongpil Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Heungsub Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1\">Minyoung Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjae Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1\">Minsub Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1\">Suk Hyun Ko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1\">Taeyong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jinuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1\">Soyoung Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ryu_N/0/1/0/all/0/1\">Na-Hyeon Ryu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoo_K/0/1/0/all/0/1\">Kang Min Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Minsuk Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suh_S/0/1/0/all/0/1\">Soobin Suh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+In_S/0/1/0/all/0/1\">Sookyo In</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Jinseong Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kyungduk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1\">Hiun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1\">Jisu Jeong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yeo_Y/0/1/0/all/0/1\">Yong Goo Yeo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ham_D/0/1/0/all/0/1\">Donghoon Ham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1\">Dongju Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1\">Min Young Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1\">Jaewook Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_I/0/1/0/all/0/1\">Inho Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1\">Jung-Woo Ha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_W/0/1/0/all/0/1\">Woomyoung Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sung_N/0/1/0/all/0/1\">Nako Sung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Predicting emergent linguistic compositions through time: Syntactic frame extension via multimodal chaining. (arXiv:2109.04652v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04652","description":"<p>Natural language relies on a finite lexicon to express an unbounded set of\nemerging ideas. One result of this tension is the formation of new\ncompositions, such that existing linguistic units can be combined with emerging\nitems into novel expressions. We develop a framework that exploits the\ncognitive mechanisms of chaining and multimodal knowledge to predict emergent\ncompositional expressions through time. We present the syntactic frame\nextension model (SFEM) that draws on the theory of chaining and knowledge from\n\"percept\", \"concept\", and \"language\" to infer how verbs extend their frames to\nform new compositions with existing and novel nouns. We evaluate SFEM\nrigorously on the 1) modalities of knowledge and 2) categorization models of\nchaining, in a syntactically parsed English corpus over the past 150 years. We\nshow that multimodal SFEM predicts newly emerged verb syntax and arguments\nsubstantially better than competing models using purely linguistic or unimodal\nknowledge. We find support for an exemplar view of chaining as opposed to a\nprototype view and reveal how the joint approach of multimodal chaining may be\nfundamental to the creation of literal and figurative language uses including\nmetaphor and metonymy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1\">Lei Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Developing a Multilingual and Code-Mixed Visual Question Answering System by Knowledge Distillation. (arXiv:2109.04653v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04653","description":"<p>Pre-trained language-vision models have shown remarkable performance on the\nvisual question answering (VQA) task. However, most pre-trained models are\ntrained by only considering monolingual learning, especially the resource-rich\nlanguage like English. Training such models for multilingual setups demand high\ncomputing resources and multilingual language-vision dataset which hinders\ntheir application in practice. To alleviate these challenges, we propose a\nknowledge distillation approach to extend an English language-vision model\n(teacher) into an equally effective multilingual and code-mixed model\n(student). Unlike the existing knowledge distillation methods, which only use\nthe output from the last layer of the teacher network for distillation, our\nstudent model learns and imitates the teacher from multiple intermediate layers\n(language and vision encoders) with appropriately designed distillation\nobjectives for incremental knowledge extraction. We also create the large-scale\nmultilingual and code-mixed VQA dataset in eleven different language setups\nconsidering the multiple Indian and European languages. Experimental results\nand in-depth analysis show the effectiveness of the proposed VQA model over the\npre-trained language-vision models on eleven diverse language setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_H/0/1/0/all/0/1\">Humair Raj Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1\">Deepak Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ekbal_A/0/1/0/all/0/1\">Asif Ekbal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Zero-Shot Dialogue State Tracking via Cross-Task Transfer. (arXiv:2109.04655v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04655","description":"<p>Zero-shot transfer learning for dialogue state tracking (DST) enables us to\nhandle a variety of task-oriented dialogue domains without the expense of\ncollecting in-domain data. In this work, we propose to transfer the\n\\textit{cross-task} knowledge from general question answering (QA) corpora for\nthe zero-shot DST task. Specifically, we propose TransferQA, a transferable\ngenerative QA model that seamlessly combines extractive QA and multi-choice QA\nvia a text-to-text transformer framework, and tracks both categorical slots and\nnon-categorical slots in DST. In addition, we introduce two effective ways to\nconstruct unanswerable questions, namely, negative question sampling and\ncontext truncation, which enable our model to handle \"none\" value slots in the\nzero-shot DST setting. The extensive experiments show that our approaches\nsubstantially improve the existing zero-shot and few-shot results on MultiWoz.\nMoreover, compared to the fully trained baseline on the Schema-Guided Dialogue\ndataset, our approach shows better generalization ability in unseen domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhaojiang Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1\">Seungwhan Moon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Crook_P/0/1/0/all/0/1\">Paul Crook</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zhenpeng Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhiguang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhou Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1\">Eunjoon Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subba_R/0/1/0/all/0/1\">Rajen Subba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Euphemistic Phrase Detection by Masked Language Model. (arXiv:2109.04666v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04666","description":"<p>It is a well-known approach for fringe groups and organizations to use\neuphemisms -- ordinary-sounding and innocent-looking words with a secret\nmeaning -- to conceal what they are discussing. For instance, drug dealers\noften use \"pot\" for marijuana and \"avocado\" for heroin. From a social media\ncontent moderation perspective, though recent advances in NLP have enabled the\nautomatic detection of such single-word euphemisms, no existing work is capable\nof automatically detecting multi-word euphemisms, such as \"blue dream\"\n(marijuana) and \"black tar\" (heroin). Our paper tackles the problem of\neuphemistic phrase detection without human effort for the first time, as far as\nwe are aware. We first perform phrase mining on a raw text corpus (e.g., social\nmedia posts) to extract quality phrases. Then, we utilize word embedding\nsimilarities to select a set of euphemistic phrase candidates. Finally, we rank\nthose candidates by a masked language model -- SpanBERT. Compared to strong\nbaselines, we report 20-50% higher detection accuracies using our algorithm for\ndetecting euphemistic phrases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1\">Wanzheng Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhat_S/0/1/0/all/0/1\">Suma Bhat</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Numeracy Learning Ability of a Text-to-Text Transfer Model. (arXiv:2109.04672v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04672","description":"<p>The transformer-based pre-trained language models have been tremendously\nsuccessful in most of the conventional NLP tasks. But they often struggle in\nthose tasks where numerical understanding is required. Some possible reasons\ncan be the tokenizers and pre-training objectives which are not specifically\ndesigned to learn and preserve numeracy. Here we investigate the ability of\ntext-to-text transfer learning model (T5), which has outperformed its\npredecessors in the conventional NLP tasks, to learn numeracy. We consider four\nnumeracy tasks: numeration, magnitude order prediction, finding minimum and\nmaximum in a series, and sorting. We find that, although T5 models perform\nreasonably well in the interpolation setting, they struggle considerably in the\nextrapolation setting across all four tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1\">Kuntal Kumar Pal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baral_C/0/1/0/all/0/1\">Chitta Baral</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DIALKI: Knowledge Identification in Conversational Systems through Dialogue-Document Contextualization. (arXiv:2109.04673v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04673","description":"<p>Identifying relevant knowledge to be used in conversational systems that are\ngrounded in long documents is critical to effective response generation. We\nintroduce a knowledge identification model that leverages the document\nstructure to provide dialogue-contextualized passage encodings and better\nlocate knowledge relevant to the conversation. An auxiliary loss captures the\nhistory of dialogue-document connections. We demonstrate the effectiveness of\nour model on two document-grounded conversational datasets and provide analyses\nshowing generalization to unseen documents and long dialogue contexts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1\">Zeqiu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1\">Bo-Ru Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1\">Mari Ostendorf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating Self-Contained and Summary-Centric Question Answer Pairs via Differentiable Reward Imitation Learning. (arXiv:2109.04689v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04689","description":"<p>Motivated by suggested question generation in conversational news\nrecommendation systems, we propose a model for generating question-answer pairs\n(QA pairs) with self-contained, summary-centric questions and\nlength-constrained, article-summarizing answers. We begin by collecting a new\ndataset of news articles with questions as titles and pairing them with\nsummaries of varying length. This dataset is used to learn a QA pair generation\nmodel producing summaries as answers that balance brevity with sufficiency\njointly with their corresponding questions. We then reinforce the QA pair\ngeneration process with a differentiable reward function to mitigate exposure\nbias, a common problem in natural language generation. Both automatic metrics\nand human evaluation demonstrate these QA pairs successfully capture the\ncentral gists of the articles and achieve high answer accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Li Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Small_K/0/1/0/all/0/1\">Kevin Small</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Atluri_S/0/1/0/all/0/1\">Sandeep Atluri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EfficientCLIP: Efficient Cross-Modal Pre-training by Ensemble Confident Learning and Language Modeling. (arXiv:2109.04699v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04699","description":"<p>While large scale pre-training has achieved great achievements in bridging\nthe gap between vision and language, it still faces several challenges. First,\nthe cost for pre-training is expensive. Second, there is no efficient way to\nhandle the data noise which degrades model performance. Third, previous methods\nonly leverage limited image-text paired data, while ignoring richer\nsingle-modal data, which may result in poor generalization to single-modal\ndownstream tasks. In this work, we propose an EfficientCLIP method via Ensemble\nConfident Learning to obtain a less noisy data subset. Extra rich non-paired\nsingle-modal text data is used for boosting the generalization of text branch.\nWe achieve the state-of-the-art performance on Chinese cross-modal retrieval\ntasks with only 1/10 training resources compared to CLIP and WenLan, while\nshowing excellent generalization to single-modal tasks, including text\nretrieval and text classification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haofan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1\">Jincan Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1\">Weijia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Debing Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Heterogeneous Graph Neural Networks for Keyphrase Generation. (arXiv:2109.04703v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04703","description":"<p>The encoder-decoder framework achieves state-of-the-art results in keyphrase\ngeneration (KG) tasks by predicting both present keyphrases that appear in the\nsource document and absent keyphrases that do not. However, relying solely on\nthe source document can result in generating uncontrollable and inaccurate\nabsent keyphrases. To address these problems, we propose a novel graph-based\nmethod that can capture explicit knowledge from related references. Our model\nfirst retrieves some document-keyphrases pairs similar to the source document\nfrom a pre-defined index as references. Then a heterogeneous graph is\nconstructed to capture relationships of different granularities between the\nsource document and its references. To guide the decoding process, a\nhierarchical attention and copy mechanism is introduced, which directly copies\nappropriate words from both the source document and its references based on\ntheir relevance and significance. The experimental results on multiple KG\nbenchmarks show that the proposed model achieves significant improvements\nagainst other baseline models, especially with regard to the absent keyphrase\nprediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1\">Ruijian Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Zero-shot Neural Machine Translation: From a Perspective of Latent Variables. (arXiv:2109.04705v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04705","description":"<p>Zero-shot translation, directly translating between language pairs unseen in\ntraining, is a promising capability of multilingual neural machine translation\n(NMT). However, it usually suffers from capturing spurious correlations between\nthe output language and language invariant semantics due to the maximum\nlikelihood training objective, leading to poor transfer performance on\nzero-shot translation. In this paper, we introduce a denoising autoencoder\nobjective based on pivot language into traditional training objective to\nimprove the translation accuracy on zero-shot directions. The theoretical\nanalysis from the perspective of latent variables shows that our approach\nactually implicitly maximizes the probability distributions for zero-shot\ndirections. On two benchmark machine translation datasets, we demonstrate that\nthe proposed method is able to effectively eliminate the spurious correlations\nand significantly outperforms state-of-the-art methods with a remarkable\nperformance. Our code is available at https://github.com/Victorwz/zs-nmt-dae.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yichao Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Aware Meta-learning for Low-Resource Text Classification. (arXiv:2109.04707v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04707","description":"<p>Meta-learning has achieved great success in leveraging the historical learned\nknowledge to facilitate the learning process of the new task. However, merely\nlearning the knowledge from the historical tasks, adopted by current\nmeta-learning algorithms, may not generalize well to testing tasks when they\nare not well-supported by training tasks. This paper studies a low-resource\ntext classification problem and bridges the gap between meta-training and\nmeta-testing tasks by leveraging the external knowledge bases. Specifically, we\npropose KGML to introduce additional representation for each sentence learned\nfrom the extracted sentence-specific knowledge graph. The extensive experiments\non three datasets demonstrate the effectiveness of KGML under both supervised\nadaptation and unsupervised adaptation settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1\">Huaxiu Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yingxin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Shedivat_M/0/1/0/all/0/1\">Maruan Al-Shedivat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Terminology Integration for COVID-19 and other Emerging Domains. (arXiv:2109.04708v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04708","description":"<p>The majority of language domains require prudent use of terminology to ensure\nclarity and adequacy of information conveyed. While the correct use of\nterminology for some languages and domains can be achieved by adapting\ngeneral-purpose MT systems on large volumes of in-domain parallel data, such\nquantities of domain-specific data are seldom available for less-resourced\nlanguages and niche domains. Furthermore, as exemplified by COVID-19 recently,\nno domain-specific parallel data is readily available for emerging domains.\nHowever, the gravity of this recent calamity created a high demand for reliable\ntranslation of critical information regarding pandemic and infection\nprevention. This work is part of WMT2021 Shared Task: Machine Translation using\nTerminologies, where we describe Tilde MT systems that are capable of dynamic\nterminology integration at the time of translation. Our systems achieve up to\n94% COVID-19 term use accuracy on the test set of the EN-FR language pair\nwithout having access to any form of in-domain information during system\ntraining. We conclude our work with a broader discussion considering the Shared\nTask itself and terminology translation in MT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bergmanis_T/0/1/0/all/0/1\">Toms Bergmanis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinnis_M/0/1/0/all/0/1\">M&#x101;rcis Pinnis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pre-train or Annotate? Domain Adaptation with a Constrained Budget. (arXiv:2109.04711v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04711","description":"<p>Recent work has demonstrated that pre-training in-domain language models can\nboost performance when adapting to a new domain. However, the costs associated\nwith pre-training raise an important question: given a fixed budget, what steps\nshould an NLP practitioner take to maximize performance? In this paper, we\nstudy domain adaptation under budget constraints, and approach it as a customer\nchoice problem between data annotation and pre-training. Specifically, we\nmeasure the annotation cost of three procedural text datasets and the\npre-training cost of three in-domain language models. Then we evaluate the\nutility of different combinations of pre-training and data annotation under\nvarying budget constraints to assess which combination strategy works best. We\nfind that, for small budgets, spending all funds on annotation leads to the\nbest performance; once the budget becomes large enough, a combination of data\nannotation and in-domain pre-training works more optimally. We therefore\nsuggest that task-specific data annotation should be part of an economical\nstrategy when adapting an NLP model to a new domain.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1\">Fan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing Methods for Multi-label Text Classification with Long-Tailed Class Distribution. (arXiv:2109.04712v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04712","description":"<p>Multi-label text classification is a challenging task because it requires\ncapturing label dependencies. It becomes even more challenging when class\ndistribution is long-tailed. Resampling and re-weighting are common approaches\nused for addressing the class imbalance problem, however, they are not\neffective when there is label dependency besides class imbalance because they\nresult in oversampling of common labels. Here, we introduce the application of\nbalancing loss functions for multi-label text classification. We perform\nexperiments on a general domain dataset with 90 labels (Reuters-21578) and a\ndomain-specific dataset from PubMed with 18211 labels. We find that a\ndistribution-balanced loss function, which inherently addresses both the class\nimbalance and label linkage problems, outperforms commonly used loss functions.\nDistribution balancing methods have been successfully used in the image\nrecognition field. Here, we show their effectiveness in natural language\nprocessing. Source code is available at\nhttps://github.com/blessu/BalancedLossNLP.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yi Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giledereli_B/0/1/0/all/0/1\">Buse Giledereli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koksal_A/0/1/0/all/0/1\">Abdullatif K&#xf6;ksal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozgur_A/0/1/0/all/0/1\">Arzucan &#xd6;zg&#xfc;r</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ozkirimli_E/0/1/0/all/0/1\">Elif Ozkirimli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages. (arXiv:2109.04715v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04715","description":"<p>Reproducible benchmarks are crucial in driving progress of machine\ntranslation research. However, existing machine translation benchmarks have\nbeen mostly limited to high-resource or well-represented languages. Despite an\nincreasing interest in low-resource machine translation, there are no\nstandardized reproducible benchmarks for many African languages, many of which\nare used by millions of speakers but have less digitized textual data. To\ntackle these challenges, we propose AfroMT, a standardized, clean, and\nreproducible machine translation benchmark for eight widely spoken African\nlanguages. We also develop a suite of analysis tools for system diagnosis\ntaking into account the unique properties of these languages. Furthermore, we\nexplore the newly considered case of low-resource focused pretraining and\ndevelop two novel data augmentation-based strategies, leveraging word-level\nalignment information and pseudo-monolingual data for pretraining multilingual\nsequence-to-sequence models. We demonstrate significant improvements when\npretraining on 11 languages, with gains of up to 2 BLEU points over strong\nbaselines. We also show gains of up to 12 BLEU points over cross-lingual\ntransfer baselines in data-constrained scenarios. All code and pretrained\nmodels will be released as further steps towards larger reproducible benchmarks\nfor African languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1\">Junjie Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AutoTriggER: Named Entity Recognition with Auxiliary Trigger Extraction. (arXiv:2109.04726v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04726","description":"<p>Deep neural models for low-resource named entity recognition (NER) have shown\nimpressive results by leveraging distant super-vision or other meta-level\ninformation (e.g. explanation). However, the costs of acquiring such additional\ninformation are generally prohibitive, especially in domains where existing\nresources (e.g. databases to be used for distant supervision) may not exist. In\nthis paper, we present a novel two-stage framework (AutoTriggER) to improve NER\nperformance by automatically generating and leveraging \"entity triggers\" which\nare essentially human-readable clues in the text that can help guide the model\nto make better decisions. Thus, the framework is able to both create and\nleverage auxiliary supervision by itself. Through experiments on three\nwell-studied NER datasets, we show that our automatically extracted triggers\nare well-matched to human triggers, and AutoTriggER improves performance over a\nRoBERTa-CRFarchitecture by nearly 0.5 F1 points on average and much more in a\nlow resource setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-Ho Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Selvam_R/0/1/0/all/0/1\">Ravi Kiran Selvam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarwar_S/0/1/0/all/0/1\">Sheikh Muhammad Sarwar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_M/0/1/0/all/0/1\">Mahak Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1\">James Allan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Simple and Effective Method To Eliminate the Self Language Bias in Multilingual Representations. (arXiv:2109.04727v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04727","description":"<p>Language agnostic and semantic-language information isolation is an emerging\nresearch direction for multilingual representations models. We explore this\nproblem from a novel angle of geometric algebra and semantic space. A simple\nbut highly effective method \"Language Information Removal (LIR)\" factors out\nlanguage identity information from semantic related components in multilingual\nrepresentations pre-trained on multi-monolingual data. A post-training and\nmodel-agnostic method, LIR only uses simple linear operations, e.g. matrix\nfactorization and orthogonal projection. LIR reveals that for weak-alignment\nmultilingual systems, the principal components of semantic spaces primarily\nencodes language identity information. We first evaluate the LIR on a\ncross-lingual question answer retrieval task (LAReQA), which requires the\nstrong alignment for the multilingual embedding space. Experiment shows that\nLIR is highly effectively on this task, yielding almost 100% relative\nimprovement in MAP for weak-alignment models. We then evaluate the LIR on\nAmazon Reviews and XEVAL dataset, with the observation that removing language\ninformation is able to improve the cross-lingual transfer performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darve_E/0/1/0/all/0/1\">Eric Darve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Assessing the Reliability of Word Embedding Gender Bias Measures. (arXiv:2109.04732v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04732","description":"<p>Various measures have been proposed to quantify human-like social biases in\nword embeddings. However, bias scores based on these measures can suffer from\nmeasurement error. One indication of measurement quality is reliability,\nconcerning the extent to which a measure produces consistent results. In this\npaper, we assess three types of reliability of word embedding gender bias\nmeasures, namely test-retest reliability, inter-rater consistency and internal\nconsistency. Specifically, we investigate the consistency of bias scores across\ndifferent choices of random seeds, scoring rules and words. Furthermore, we\nanalyse the effects of various factors on these measures' reliability scores.\nOur findings inform better design of word embedding gender bias measures.\nMoreover, we urge researchers to be more critical about the application of such\nmeasures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yupei Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1\">Qixiang Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Genre as Weak Supervision for Cross-lingual Dependency Parsing. (arXiv:2109.04733v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04733","description":"<p>Recent work has shown that monolingual masked language models learn to\nrepresent data-driven notions of language variation which can be used for\ndomain-targeted training data selection. Dataset genre labels are already\nfrequently available, yet remain largely unexplored in cross-lingual setups. We\nharness this genre metadata as a weak supervision signal for targeted data\nselection in zero-shot dependency parsing. Specifically, we project\ntreebank-level genre information to the finer-grained sentence level, with the\ngoal to amplify information implicitly stored in unsupervised contextualized\nrepresentations. We demonstrate that genre is recoverable from multilingual\ncontextual embeddings and that it provides an effective signal for training\ndata selection in cross-lingual, zero-shot scenarios. For 12 low-resource\nlanguage treebanks, six of which are test-only, our genre-specific methods\nsignificantly outperform competitive baselines as well as recent\nembedding-based methods for data selection. Moreover, genre-based data\nselection provides new state-of-the-art results for three of these target\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Muller_Eberstein_M/0/1/0/all/0/1\">Max M&#xfc;ller-Eberstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goot_R/0/1/0/all/0/1\">Rob van der Goot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Fine-tuning Affect the Geometry of Embedding Space: A Case Study on Isotropy. (arXiv:2109.04740v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04740","description":"<p>It is widely accepted that fine-tuning pre-trained language models usually\nbrings about performance improvements in downstream tasks. However, there are\nlimited studies on the reasons behind this effectiveness, particularly from the\nviewpoint of structural changes in the embedding space. Trying to fill this\ngap, in this paper, we analyze the extent to which the isotropy of the\nembedding space changes after fine-tuning. We demonstrate that, even though\nisotropy is a desirable geometrical property, fine-tuning does not necessarily\nresult in isotropy enhancements. Moreover, local structures in pre-trained\ncontextual word representations (CWRs), such as those encoding token types or\nfrequency, undergo a massive change during fine-tuning. Our experiments show\ndramatic growth in the number of elongated directions in the embedding space,\nwhich, in contrast to pre-trained CWRs, carry the essential linguistic\nknowledge in the fine-tuned embedding space, making existing isotropy\nenhancement methods ineffective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajaee_S/0/1/0/all/0/1\">Sara Rajaee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dual-State Capsule Networks for Text Classification. (arXiv:2109.04762v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04762","description":"<p>Text classification systems based on contextual embeddings are not viable\noptions for many of the low resource languages. On the other hand, recently\nintroduced capsule networks have shown performance in par with these text\nclassification models. Thus, they could be considered as a viable alternative\nfor text classification for languages that do not have pre-trained contextual\nembedding models. However, current capsule networks depend upon spatial\npatterns without considering the sequential features of the text. They are also\nsub-optimal in capturing the context-level information in longer sequences.\nThis paper presents a novel Dual-State Capsule (DS-Caps) network-based\ntechnique for text classification, which is optimized to mitigate these issues.\nTwo varieties of states, namely sentence-level and word-level, are integrated\nwith capsule layers to capture deeper context-level information for language\nmodeling. The dynamic routing process among capsules was also optimized using\nthe context-level information obtained through sentence-level states. The\nDS-Caps networks outperform the existing capsule network architectures for\nmultiple datasets, particularly for tasks with longer sequences of text. We\nalso demonstrate the superiority of DS-Caps in text classification for a low\nresource language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Demotte_P/0/1/0/all/0/1\">Piyumal Demotte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranathunga_S/0/1/0/all/0/1\">Surangika Ranathunga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Strong Baseline for Query Efficient Attacks in a Black Box Setting. (arXiv:2109.04775v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04775","description":"<p>Existing black box search methods have achieved high success rate in\ngenerating adversarial attacks against NLP models. However, such search methods\nare inefficient as they do not consider the amount of queries required to\ngenerate adversarial attacks. Also, prior attacks do not maintain a consistent\nsearch space while comparing different search methods. In this paper, we\npropose a query efficient attack strategy to generate plausible adversarial\nexamples on text classification and entailment tasks. Our attack jointly\nleverages attention mechanism and locality sensitive hashing (LSH) to reduce\nthe query count. We demonstrate the efficacy of our approach by comparing our\nattack with four baselines across three different search spaces. Further, we\nbenchmark our results across the same search space used in prior attacks. In\ncomparison to attacks proposed, on an average, we are able to reduce the query\ncount by 75% across all datasets and target models. We also demonstrate that\nour attack achieves a higher success rate when compared to prior attacks in a\nlimited query setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Maheshwary_R/0/1/0/all/0/1\">Rishabh Maheshwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maheshwary_S/0/1/0/all/0/1\">Saket Maheshwary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pudi_V/0/1/0/all/0/1\">Vikram Pudi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multilingual Translation by Representation and Gradient Regularization. (arXiv:2109.04778v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04778","description":"<p>Multilingual Neural Machine Translation (NMT) enables one model to serve all\ntranslation directions, including ones that are unseen during training, i.e.\nzero-shot translation. Despite being theoretically attractive, current models\noften produce low quality translations -- commonly failing to even produce\noutputs in the right target language. In this work, we observe that off-target\ntranslation is dominant even in strong multilingual systems, trained on massive\nmultilingual corpora. To address this issue, we propose a joint approach to\nregularize NMT models at both representation-level and gradient-level. At the\nrepresentation level, we leverage an auxiliary target language prediction task\nto regularize decoder outputs to retain information about the target language.\nAt the gradient level, we leverage a small amount of direct data (in thousands\nof sentence pairs) to regularize model gradients. Our results demonstrate that\nour approach is highly effective in both reducing off-target translation\noccurrences and improving zero-shot translation performance by +5.59 and +10.38\nBLEU on WMT and OPUS datasets respectively. Moreover, experiments show that our\nmethod also works well when the small amount of direct data is not available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yilin Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eriguchi_A/0/1/0/all/0/1\">Akiko Eriguchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muzio_A/0/1/0/all/0/1\">Alexandre Muzio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tadepalli_P/0/1/0/all/0/1\">Prasad Tadepalli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Stefan Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hassan_H/0/1/0/all/0/1\">Hany Hassan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoR: Read-over-Read for Long Document Machine Reading Comprehension. (arXiv:2109.04780v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04780","description":"<p>Transformer-based pre-trained models, such as BERT, have achieved remarkable\nresults on machine reading comprehension. However, due to the constraint of\nencoding length (e.g., 512 WordPiece tokens), a long document is usually split\ninto multiple chunks that are independently read. It results in the reading\nfield being limited to individual chunks without information collaboration for\nlong document machine reading comprehension. To address this problem, we\npropose RoR, a read-over-read method, which expands the reading field from\nchunk to document. Specifically, RoR includes a chunk reader and a document\nreader. The former first predicts a set of regional answers for each chunk,\nwhich are then compacted into a highly-condensed version of the original\ndocument, guaranteeing to be encoded once. The latter further predicts the\nglobal answers from this condensed document. Eventually, a voting strategy is\nutilized to aggregate and rerank the regional and global answers for final\nprediction. Extensive experiments on two benchmarks QuAC and TriviaQA\ndemonstrate the effectiveness of RoR for long document reading. Notably, RoR\nranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of\nsubmission (May 17th, 2021).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Attention Channel Combinator Frontend for End-to-End Multichannel Far-field Speech Recognition. (arXiv:2109.04783v1 [cs.SD])","link":"http://arxiv.org/abs/2109.04783","description":"<p>When a sufficiently large far-field training data is presented, jointly\noptimizing a multichannel frontend and an end-to-end (E2E) Automatic Speech\nRecognition (ASR) backend shows promising results. Recent literature has shown\ntraditional beamformer designs, such as MVDR (Minimum Variance Distortionless\nResponse) or fixed beamformers can be successfully integrated as the frontend\ninto an E2E ASR system with learnable parameters. In this work, we propose the\nself-attention channel combinator (SACC) ASR frontend, which leverages the\nself-attention mechanism to combine multichannel audio signals in the magnitude\nspectral domain. Experiments conducted on a multichannel playback test data\nshows that the SACC achieved a 9.3% WERR compared to a state-of-the-art fixed\nbeamformer-based frontend, both jointly optimized with a ContextNet-based ASR\nbackend. We also demonstrate the connection between the SACC and the\ntraditional beamformers, and analyze the intermediate outputs of the SACC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1\">Rong Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Quillen_C/0/1/0/all/0/1\">Carl Quillen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_D/0/1/0/all/0/1\">Dushyant Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goderre_A/0/1/0/all/0/1\">Andrew Goderre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lainez_J/0/1/0/all/0/1\">Jos&#xe9; La&#xed;nez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milanovic_L/0/1/0/all/0/1\">Ljubomir Milanovi&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exophoric Pronoun Resolution in Dialogues with Topic Regularization. (arXiv:2109.04787v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04787","description":"<p>Resolving pronouns to their referents has long been studied as a fundamental\nnatural language understanding problem. Previous works on pronoun coreference\nresolution (PCR) mostly focus on resolving pronouns to mentions in text while\nignoring the exophoric scenario. Exophoric pronouns are common in daily\ncommunications, where speakers may directly use pronouns to refer to some\nobjects present in the environment without introducing the objects first.\nAlthough such objects are not mentioned in the dialogue text, they can often be\ndisambiguated by the general topics of the dialogue. Motivated by this, we\npropose to jointly leverage the local context and global topics of dialogues to\nsolve the out-of-text PCR problem. Extensive experiments demonstrate the\neffectiveness of adding topic regularization for resolving exophoric pronouns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1\">Xintong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1\">Yangqiu Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Changshui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1\">Kun Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into BERT. (arXiv:2109.04810v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04810","description":"<p>Infusing factual knowledge into pre-trained models is fundamental for many\nknowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions\n(MoP), an infusion approach that can handle a very large knowledge graph (KG)\nby partitioning it into smaller sub-graphs and infusing their specific\nknowledge into various BERT models using lightweight adapters. To leverage the\noverall factual knowledge for a target task, these sub-graph adapters are\nfurther fine-tuned along with the underlying BERT through a mixture layer. We\nevaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on\nsix downstream tasks (inc. NLI, QA, Classification), and the results show that\nour MoP consistently enhances the underlying BERTs in task performance, and\nachieves new SOTA performances on five evaluated datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clark_T/0/1/0/all/0/1\">Thomas Hikaru Clark</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shareghi_E/0/1/0/all/0/1\">Ehsan Shareghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does It Capture STEL? A Modular, Similarity-based Linguistic Style Evaluation Framework. (arXiv:2109.04817v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04817","description":"<p>Style is an integral part of natural language. However, evaluation methods\nfor style measures are rare, often task-specific and usually do not control for\ncontent. We propose the modular, fine-grained and content-controlled\nsimilarity-based STyle EvaLuation framework (STEL) to test the performance of\nany model that can compare two sentences on style. We illustrate STEL with two\ngeneral dimensions of style (formal/informal and simple/complex) as well as two\nspecific characteristics of style (contrac'tion and numb3r substitution). We\nfind that BERT-based methods outperform simple versions of commonly used style\nmeasures like 3-grams, punctuation frequency and LIWC-based approaches. We\ninvite the addition of further tasks and task instances to STEL and hope to\nfacilitate the improvement of style-sensitive measures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wegmann_A/0/1/0/all/0/1\">Anna Wegmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1\">Dong Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Artificial Text Detection via Examining the Topology of Attention Maps. (arXiv:2109.04825v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04825","description":"<p>The impressive capabilities of recent generative models to create texts that\nare challenging to distinguish from the human-written ones can be misused for\ngenerating fake news, product reviews, and even abusive content. Despite the\nprominent performance of existing methods for artificial text detection, they\nstill lack interpretability and robustness towards unseen models. To this end,\nwe propose three novel types of interpretable topological features for this\ntask based on Topological Data Analysis (TDA) which is currently understudied\nin the field of NLP. We empirically show that the features derived from the\nBERT model outperform count- and neural-based baselines up to 10\\% on three\ncommon datasets, and tend to be the most robust towards unseen GPT-style\ngeneration models as opposed to existing methods. The probing analysis of the\nfeatures reveals their sensitivity to the surface and syntactic properties. The\nresults demonstrate that TDA is a promising line with respect to NLP tasks,\nspecifically the ones that incorporate surface and structural information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kushnareva_L/0/1/0/all/0/1\">Laida Kushnareva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cherniavskii_D/0/1/0/all/0/1\">Daniil Cherniavskii</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1\">Vladislav Mikhailov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artemova_E/0/1/0/all/0/1\">Ekaterina Artemova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barannikov_S/0/1/0/all/0/1\">Serguei Barannikov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1\">Alexander Bernstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovskaya_I/0/1/0/all/0/1\">Irina Piontkovskaya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piontkovski_D/0/1/0/all/0/1\">Dmitri Piontkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1\">Evgeny Burnaev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking It All: Generating Contextualized Questions for any Semantic Role. (arXiv:2109.04832v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04832","description":"<p>Asking questions about a situation is an inherent step towards understanding\nit. To this end, we introduce the task of role question generation, which,\ngiven a predicate mention and a passage, requires producing a set of questions\nasking about all possible semantic roles of the predicate. We develop a\ntwo-stage model for this task, which first produces a context-independent\nquestion prototype for each role and then revises it to be contextually\nappropriate for the passage. Unlike most existing approaches to question\ngeneration, our approach does not require conditioning on existing answers in\nthe text. Instead, we condition on the type of information to inquire about,\nregardless of whether the answer appears explicitly in the text, could be\ninferred from it, or should be sought elsewhere. Our evaluation demonstrates\nthat we generate diverse and well-formed questions for a large, broad-coverage\nontology of predicates and roles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1\">Valentina Pyatkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roit_P/0/1/0/all/0/1\">Paul Roit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Michael_J/0/1/0/all/0/1\">Julian Michael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsarfaty_R/0/1/0/all/0/1\">Reut Tsarfaty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model. (arXiv:2109.04834v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04834","description":"<p>Multi-turn response selection models have recently shown comparable\nperformance to humans in several benchmark datasets. However, in the real\nenvironment, these models often have weaknesses, such as making incorrect\npredictions based heavily on superficial patterns without a comprehensive\nunderstanding of the context. For example, these models often give a high score\nto the wrong response candidate containing several keywords related to the\ncontext but using the inconsistent tense. In this study, we analyze the\nweaknesses of the open-domain Korean Multi-turn response selection models and\npublish an adversarial dataset to evaluate these weaknesses. We also suggest a\nstrategy to build a robust model in this adversarial environment.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kijong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seojin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1\">Wooin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Joosung Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dong-hun Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FR-Detect: A Multi-Modal Framework for Early Fake News Detection on Social Media Using Publishers Features. (arXiv:2109.04835v1 [cs.SI])","link":"http://arxiv.org/abs/2109.04835","description":"<p>In recent years, with the expansion of the Internet and attractive social\nmedia infrastructures, people prefer to follow the news through these media.\nDespite the many advantages of these media in the news field, the lack of any\ncontrol and verification mechanism has led to the spread of fake news, as one\nof the most important threats to democracy, economy, journalism and freedom of\nexpression. Designing and using automatic methods to detect fake news on social\nmedia has become a significant challenge. In this paper, we examine the\npublishers' role in detecting fake news on social media. We also suggest a high\naccurate multi-modal framework, namely FR-Detect, using user-related and\ncontent-related features with early detection capability. For this purpose, two\nnew user-related features, namely Activity Credibility and Influence, have been\nintroduced for publishers. Furthermore, a sentence-level convolutional neural\nnetwork is provided to combine these features with latent textual content\nfeatures properly. Experimental results have shown that the publishers'\nfeatures can improve the performance of content-based models by up to 13% and\n29% in accuracy and F1-score, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jarrahi_A/0/1/0/all/0/1\">Ali Jarrahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Safari_L/0/1/0/all/0/1\">Leila Safari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Block Pruning For Faster Transformers. (arXiv:2109.04838v1 [cs.LG])","link":"http://arxiv.org/abs/2109.04838","description":"<p>Pre-training has improved model accuracy for both classification and\ngeneration tasks at the cost of introducing much larger and slower models.\nPruning methods have proven to be an effective way of reducing model size,\nwhereas distillation methods are proven for speeding up inference. We introduce\na block pruning approach targeting both small and fast models. Our approach\nextends structured methods by considering blocks of any size and integrates\nthis structure into the movement pruning paradigm for fine-tuning. We find that\nthis approach learns to prune out full components of the underlying model, such\nas attention heads. Experiments consider classification and generation tasks,\nyielding among other results a pruned model that is a 2.4x faster, 74% smaller\nBERT on SQuAD v1, with a 1% drop on F1, competitive both with distilled models\nin speed and pruned models in size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lagunas_F/0/1/0/all/0/1\">Fran&#xe7;ois Lagunas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlaix_E/0/1/0/all/0/1\">Ella Charlaix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanh_V/0/1/0/all/0/1\">Victor Sanh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active learning for reducing labeling effort in text classification tasks. (arXiv:2109.04847v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04847","description":"<p>Labeling data can be an expensive task as it is usually performed manually by\ndomain experts. This is cumbersome for deep learning, as it is dependent on\nlarge labeled datasets. Active learning (AL) is a paradigm that aims to reduce\nlabeling effort by only using the data which the used model deems most\ninformative. Little research has been done on AL in a text classification\nsetting and next to none has involved the more recent, state-of-the-art NLP\nmodels. Here, we present an empirical study that compares different\nuncertainty-based algorithms with BERT$_{base}$ as the used classifier. We\nevaluate the algorithms on two NLP classification datasets: Stanford Sentiment\nTreebank and KvK-Frontpages. Additionally, we explore heuristics that aim to\nsolve presupposed problems of uncertainty-based AL; namely, that it is\nunscalable and that it is prone to selecting outliers. Furthermore, we explore\nthe influence of the query-pool size on the performance of AL. Whereas it was\nfound that the proposed heuristics for AL did not improve performance of AL;\nour results show that using uncertainty-based AL with BERT$_{base}$ outperforms\nrandom sampling of data. This difference in performance can decrease as the\nquery-pool size gets larger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacobs_P/0/1/0/all/0/1\">Pieter Floris Jacobs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wenniger_G/0/1/0/all/0/1\">Gideon Maillette de Buy Wenniger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wiering_M/0/1/0/all/0/1\">Marco Wiering</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schomaker_L/0/1/0/all/0/1\">Lambert Schomaker</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CoPHE: A Count-Preserving Hierarchical Evaluation Metric in Large-Scale Multi-Label Text Classification. (arXiv:2109.04853v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04853","description":"<p>Large-Scale Multi-Label Text Classification (LMTC) includes tasks with\nhierarchical label spaces, such as automatic assignment of ICD-9 codes to\ndischarge summaries. Performance of models in prior art is evaluated with\nstandard precision, recall, and F1 measures without regard for the rich\nhierarchical structure. In this work we argue for hierarchical evaluation of\nthe predictions of neural LMTC models. With the example of the ICD-9 ontology\nwe describe a structural issue in the representation of the structured label\nspace in prior art, and propose an alternative representation based on the\ndepth of the ontology. We propose a set of metrics for hierarchical evaluation\nusing the depth-based representation. We compare the evaluation scores from the\nproposed metrics with previously used metrics on prior art LMTC models for\nICD-9 coding in MIMIC-III. We also propose further avenues of research\ninvolving the proposed ontological representation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Falis_M/0/1/0/all/0/1\">Mat&#xfa;&#x161; Falis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1\">Hang Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alex_B/0/1/0/all/0/1\">Beatrice Alex</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Studying word order through iterative shuffling. (arXiv:2109.04867v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04867","description":"<p>As neural language models approach human performance on NLP benchmark tasks,\ntheir advances are widely seen as evidence of an increasingly complex\nunderstanding of syntax. This view rests upon a hypothesis that has not yet\nbeen empirically tested: that word order encodes meaning essential to\nperforming these tasks. We refute this hypothesis in many cases: in the GLUE\nsuite and in various genres of English text, the words in a sentence or phrase\ncan rarely be permuted to form a phrase carrying substantially different\ninformation. Our surprising result relies on inference by iterative shuffling\n(IBIS), a novel, efficient procedure that finds the ordering of a bag of words\nhaving the highest likelihood under a fixed language model. IBIS can use any\nblack-box model without additional training and is superior to existing word\nordering algorithms. Coalescing our findings, we discuss how shuffling\ninference procedures such as IBIS can benefit language modeling and constrained\ngeneration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Malkin_N/0/1/0/all/0/1\">Nikolay Malkin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lanka_S/0/1/0/all/0/1\">Sameera Lanka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1\">Pranav Goel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jojic_N/0/1/0/all/0/1\">Nebojsa Jojic</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiAzterTest: a Multilingual Analyzer on Multiple Levels of Language for Readability Assessment. (arXiv:2109.04870v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04870","description":"<p>Readability assessment is the task of determining how difficult or easy a\ntext is or which level/grade it has. Traditionally, language dependent\nreadability formula have been used, but these formulae take few text\ncharacteristics into account. However, Natural Language Processing (NLP) tools\nthat assess the complexity of texts are able to measure more different features\nand can be adapted to different languages. In this paper, we present the\nMultiAzterTest tool: (i) an open source NLP tool which analyzes texts on over\n125 measures of cohesion,language, and readability for English, Spanish and\nBasque, but whose architecture is designed to easily adapt other languages;\n(ii) readability assessment classifiers that improve the performance of\nCoh-Metrix in English, Coh-Metrix-Esp in Spanish and ErreXail in Basque; iii) a\nweb tool. MultiAzterTest obtains 90.09 % in accuracy when classifying into\nthree reading levels (elementary, intermediate, and advanced) in English and\n95.50 % in Basque and 90 % in Spanish when classifying into two reading levels\n(simple and complex) using a SMO classifier. Using cross-lingual features,\nMultiAzterTest also obtains competitive results above all in a complex vs\nsimple distinction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bengoetxea_K/0/1/0/all/0/1\">Kepa Bengoetxea</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Dios_I/0/1/0/all/0/1\">Itziar Gonzalez-Dios</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Integrating Approaches to Word Representation. (arXiv:2109.04876v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04876","description":"<p>The problem of representing the atomic elements of language in modern neural\nlearning systems is one of the central challenges of the field of natural\nlanguage processing. I present a survey of the distributional, compositional,\nand relational approaches to addressing this task, and discuss various means of\nintegrating them into systems, with special emphasis on the word level and the\nout-of-vocabulary phenomenon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pinter_Y/0/1/0/all/0/1\">Yuval Pinter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Test Time Adapter Ensembling for Low-resource Language Varieties. (arXiv:2109.04877v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04877","description":"<p>Adapters are light-weight modules that allow parameter-efficient fine-tuning\nof pretrained models. Specialized language and task adapters have recently been\nproposed to facilitate cross-lingual transfer of multilingual pretrained models\n(Pfeiffer et al., 2020b). However, this approach requires training a separate\nlanguage adapter for every language one wishes to support, which can be\nimpractical for languages with limited data. An intuitive solution is to use a\nrelated language adapter for the new language variety, but we observe that this\nsolution can lead to sub-optimal performance. In this paper, we aim to improve\nthe robustness of language adapters to uncovered languages without training new\nadapters. We find that ensembling multiple existing language adapters makes the\nfine-tuned model significantly more robust to other language varieties not\nincluded in these adapters. Building upon this observation, we propose Entropy\nMinimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble\nweights of the pretrained language adapters for each test sentence by\nminimizing the entropy of its predictions. Experiments on three diverse groups\nof language varieties show that our method leads to significant improvements on\nboth named entity recognition and part-of-speech tagging across all languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document-level Entity-based Extraction as Template Generation. (arXiv:2109.04901v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04901","description":"<p>Document-level entity-based extraction (EE), aiming at extracting\nentity-centric information such as entity roles and entity relations, is key to\nautomatic knowledge acquisition from text corpora for various domains. Most\ndocument-level EE systems build extractive models, which struggle to model\nlong-term dependencies among entities at the document level. To address this\nissue, we propose a generative framework for two document-level EE tasks:\nrole-filler entity extraction (REE) and relation extraction (RE). We first\nformulate them as a template generation problem, allowing models to efficiently\ncapture cross-entity dependencies, exploit label semantics, and avoid the\nexponential computation complexity of identifying N-ary relations. A novel\ncross-attention guided copy mechanism, TopK Copy, is incorporated into a\npre-trained sequence-to-sequence model to enhance the capabilities of\nidentifying key information in the input document. Experiments done on the\nMUC-4 and SciREX dataset show new state-of-the-art results on REE (+3.26%),\nbinary RE (+4.8%), and 4-ary RE (+2.7%) in F1 score.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kung-Hsiang Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1\">Sam Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ReasonBERT: Pre-trained to Reason with Distant Supervision. (arXiv:2109.04912v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04912","description":"<p>We present ReasonBert, a pre-training method that augments language models\nwith the ability to reason over long-range relations and multiple, possibly\nhybrid contexts. Unlike existing pre-training methods that only harvest\nlearning signals from local contexts of naturally occurring texts, we propose a\ngeneralized notion of distant supervision to automatically connect multiple\npieces of text and tables to create pre-training examples that require\nlong-range reasoning. Different types of reasoning are simulated, including\nintersecting multiple pieces of evidence, bridging from one piece of evidence\nto another, and detecting unanswerable cases. We conduct a comprehensive\nevaluation on a variety of extractive question answering datasets ranging from\nsingle-hop to multi-hop and from text-only to table-only to hybrid that require\nvarious reasoning capabilities and show that ReasonBert achieves remarkable\nimprovement over an array of strong baselines. Few-shot experiments further\ndemonstrate that our pre-training method substantially improves sample\nefficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1\">Xiang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lees_A/0/1/0/all/0/1\">Alyssa Lees</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">You Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1\">Cong Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmoWOZ: A Large-Scale Corpus and Labelling Scheme for Emotion in Task-Oriented Dialogue Systems. (arXiv:2109.04919v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04919","description":"<p>The ability to recognise emotions lends a conversational artificial\nintelligence a human touch. While emotions in chit-chat dialogues have received\nsubstantial attention, emotions in task-oriented dialogues have been largely\noverlooked despite having an equally important role, such as to signal failure\nor success. Existing emotion-annotated task-oriented corpora are limited in\nsize, label richness, and public availability, creating a bottleneck for\ndownstream tasks. To lay a foundation for studies on emotions in task-oriented\ndialogues, we introduce EmoWOZ, a large-scale manually emotion-annotated corpus\nof task-oriented dialogues. EmoWOZ is based on MultiWOZ, a multi-domain\ntask-oriented dialogue dataset. It contains more than 11K dialogues with more\nthan 83K emotion annotations of user utterances. In addition to Wizzard-of-Oz\ndialogues from MultiWOZ, we collect human-machine dialogues within the same set\nof domains to sufficiently cover the space of various emotions that can happen\nduring the lifetime of a data-driven dialogue system. To the best of our\nknowledge, this is the first large-scale open-source corpus of its kind. We\npropose a novel emotion labelling scheme, which is tailored to task-oriented\ndialogues. We report a set of experimental results to show the usability of\nthis corpus for emotion recognition and state tracking in task-oriented\ndialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Shutong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lubis_N/0/1/0/all/0/1\">Nurul Lubis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geishauser_C/0/1/0/all/0/1\">Christian Geishauser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1\">Hsien-chin Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heck_M/0/1/0/all/0/1\">Michael Heck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niekerk_C/0/1/0/all/0/1\">Carel van Niekerk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gasic_M/0/1/0/all/0/1\">Milica Ga&#x161;i&#x107;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Examining Cross-lingual Contextual Embeddings with Orthogonal Structural Probes. (arXiv:2109.04921v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04921","description":"<p>State-of-the-art contextual embeddings are obtained from large language\nmodels available only for a few languages. For others, we need to learn\nrepresentations using a multilingual model. There is an ongoing debate on\nwhether multilingual embeddings can be aligned in a space shared across many\nlanguages. The novel Orthogonal Structural Probe (Limisiewicz and Mare\\v{c}ek,\n2021) allows us to answer this question for specific linguistic features and\nlearn a projection based only on mono-lingual annotated datasets. We evaluate\nsyntactic (UD) and lexical (WordNet) structural information encoded inmBERT's\ncontextual representations for nine diverse languages. We observe that for\nlanguages closely related to English, no transformation is needed. The\nevaluated information is encoded in a shared cross-lingual embedding space. For\nother languages, it is beneficial to apply orthogonal transformation learned\nseparately for each language. We successfully apply our findings to zero-shot\nand few-shot cross-lingual parsing.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1\">Tomasz Limisiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1\">David Mare&#x10d;ek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond the Tip of the Iceberg: Assessing Coherence of Text Classifiers. (arXiv:2109.04922v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04922","description":"<p>As large-scale, pre-trained language models achieve human-level and\nsuperhuman accuracy on existing language understanding tasks, statistical bias\nin benchmark data and probing studies have recently called into question their\ntrue capabilities. For a more informative evaluation than accuracy on text\nclassification tasks can offer, we propose evaluating systems through a novel\nmeasure of prediction coherence. We apply our framework to two existing\nlanguage understanding benchmarks with different properties to demonstrate its\nversatility. Our experimental results show that this evaluation framework,\nalthough simple in ideas and implementation, is a quick, effective, and\nversatile measure to provide insight into the coherence of machines'\npredictions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars. (arXiv:2109.04939v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04939","description":"<p>In computational linguistics, it has been shown that hierarchical structures\nmake language models (LMs) more human-like. However, the previous literature\nhas been agnostic about a parsing strategy of the hierarchical models. In this\npaper, we investigated whether hierarchical structures make LMs more\nhuman-like, and if so, which parsing strategy is most cognitively plausible. In\norder to address this question, we evaluated three LMs against human reading\ntimes in Japanese with head-final left-branching structures: Long Short-Term\nMemory (LSTM) as a sequential model and Recurrent Neural Network Grammars\n(RNNGs) with top-down and left-corner parsing strategies as hierarchical\nmodels. Our computational modeling demonstrated that left-corner RNNGs\noutperformed top-down RNNGs and LSTM, suggesting that hierarchical and\nleft-corner architectures are more cognitively plausible than top-down or\nsequential architectures. In addition, the relationships between the cognitive\nplausibility and (i) perplexity, (ii) parsing, and (iii) beam size will also be\ndiscussed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_R/0/1/0/all/0/1\">Ryo Yoshida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noji_H/0/1/0/all/0/1\">Hiroshi Noji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oseki_Y/0/1/0/all/0/1\">Yohei Oseki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tiered Reasoning for Intuitive Physics: Toward Verifiable Commonsense Language Understanding. (arXiv:2109.04947v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04947","description":"<p>Large-scale, pre-trained language models (LMs) have achieved human-level\nperformance on a breadth of language understanding tasks. However, evaluations\nonly based on end task performance shed little light on machines' true ability\nin language understanding and reasoning. In this paper, we highlight the\nimportance of evaluating the underlying reasoning process in addition to end\nperformance. Toward this goal, we introduce Tiered Reasoning for Intuitive\nPhysics (TRIP), a novel commonsense reasoning dataset with dense annotations\nthat enable multi-tiered evaluation of machines' reasoning process. Our\nempirical results show that while large LMs can achieve high end performance,\nthey struggle to support their predictions with valid supporting evidence. The\nTRIP dataset and our baseline results will motivate verifiable evaluation of\ncommonsense reasoning and facilitate future research toward developing better\nlanguage understanding and reasoning models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Storks_S/0/1/0/all/0/1\">Shane Storks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1\">Qiaozi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yichi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"We went to look for meaning and all we got were these lousy representations: aspects of meaning representation for computational semantics. (arXiv:2109.04949v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04949","description":"<p>In this paper we examine different meaning representations that are commonly\nused in different natural language applications today and discuss their limits,\nboth in terms of the aspects of the natural language meaning they are modelling\nand in terms of the aspects of the application for which they are used.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dobnik_S/0/1/0/all/0/1\">Simon Dobnik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cooper_R/0/1/0/all/0/1\">Robin Cooper</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ek_A/0/1/0/all/0/1\">Adam Ek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noble_B/0/1/0/all/0/1\">Bill Noble</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Larsson_S/0/1/0/all/0/1\">Staffan Larsson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ilinykh_N/0/1/0/all/0/1\">Nikolai Ilinykh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maraev_V/0/1/0/all/0/1\">Vladislav Maraev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Somashekarappa_V/0/1/0/all/0/1\">Vidya Somashekarappa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Pretraining for Summarization Require Knowledge Transfer?. (arXiv:2109.04953v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04953","description":"<p>Pretraining techniques leveraging enormous datasets have driven recent\nadvances in text summarization. While folk explanations suggest that knowledge\ntransfer accounts for pretraining's benefits, little is known about why it\nworks or what makes a pretraining task or dataset suitable. In this paper, we\nchallenge the knowledge transfer story, showing that pretraining on documents\nconsisting of character n-grams selected at random, we can nearly match the\nperformance of models pretrained on real corpora. This work holds the promise\nof eliminating upstream corpora, which may alleviate some concerns over\noffensive language, bias, and copyright issues. To see whether the small\nresidual benefit of using real data could be accounted for by the structure of\nthe pretraining task, we design several tasks motivated by a qualitative study\nof summarization corpora. However, these tasks confer no appreciable benefit,\nleaving open the possibility of a small role for knowledge transfer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishna_K/0/1/0/all/0/1\">Kundan Krishna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bigham_J/0/1/0/all/0/1\">Jeffrey Bigham</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1\">Zachary C. Lipton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controlled Neural Sentence-Level Reframing of News Articles. (arXiv:2109.04957v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04957","description":"<p>Framing a news article means to portray the reported event from a specific\nperspective, e.g., from an economic or a health perspective. Reframing means to\nchange this perspective. Depending on the audience or the submessage, reframing\ncan become necessary to achieve the desired effect on the readers. Reframing is\nrelated to adapting style and sentiment, which can be tackled with neural text\ngeneration techniques. However, it is more challenging since changing a frame\nrequires rewriting entire sentences rather than single phrases. In this paper,\nwe study how to computationally reframe sentences in news articles while\nmaintaining their coherence to the context. We treat reframing as a\nsentence-level fill-in-the-blank task for which we train neural models on an\nexisting media frame corpus. To guide the training, we propose three\nstrategies: framed-language pretraining, named-entity preservation, and\nadversarial learning. We evaluate respective models automatically and manually\nfor topic consistency, coherence, and successful reframing. Our results\nindicate that generating properly-framed text works well but with tradeoffs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei-Fan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Khatib_K/0/1/0/all/0/1\">Khalid Al-Khatib</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stein_B/0/1/0/all/0/1\">Benno Stein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wachsmuth_H/0/1/0/all/0/1\">Henning Wachsmuth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LAViTeR: Learning Aligned Visual and Textual Representations Assisted by Image and Caption Generation. (arXiv:2109.04993v1 [cs.CV])","link":"http://arxiv.org/abs/2109.04993","description":"<p>Pre-training visual and textual representations from large-scale image-text\npairs is becoming a standard approach for many downstream vision-language\ntasks. The transformer-based models learn inter and intra-modal attention\nthrough a list of self-supervised learning tasks. This paper proposes LAViTeR,\na novel architecture for visual and textual representation learning. The main\nmodule, Visual Textual Alignment (VTA) will be assisted by two auxiliary tasks,\nGAN-based image synthesis and Image Captioning. We also propose a new\nevaluation metric measuring the similarity between the learnt visual and\ntextual embedding. The experimental results on two public datasets, CUB and\nMS-COCO, demonstrate superior visual and textual representation alignment in\nthe joint feature embedding space\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shaikh_M/0/1/0/all/0/1\">Mohammad Abuzar Shaikh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1\">Zhanghexuan Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moukheiber_D/0/1/0/all/0/1\">Dana Moukheiber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srihari_S/0/1/0/all/0/1\">Sargur Srihari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1\">Mingchen Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Topic-Aware Contrastive Learning for Abstractive Dialogue Summarization. (arXiv:2109.04994v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04994","description":"<p>Unlike well-structured text, such as news reports and encyclopedia articles,\ndialogue content often comes from two or more interlocutors, exchanging\ninformation with each other. In such a scenario, the topic of a conversation\ncan vary upon progression and the key information for a certain topic is often\nscattered across multiple utterances of different speakers, which poses\nchallenges to abstractly summarize dialogues. To capture the various topic\ninformation of a conversation and outline salient facts for the captured\ntopics, this work proposes two topic-aware contrastive learning objectives,\nnamely coherence detection and sub-summary generation objectives, which are\nexpected to implicitly model the topic change and handle information scattering\nchallenges for the dialogue summarization task. The proposed contrastive\nobjectives are framed as auxiliary tasks for the primary dialogue summarization\ntask, united via an alternative parameter updating strategy. Extensive\nexperiments on benchmark datasets demonstrate that the proposed simple method\nsignificantly outperforms strong baselines and achieves new state-of-the-art\nperformance. The code and trained models are publicly available via\n\\href{https://github.com/Junpliu/ConDigSum}{https://github.com/Junpliu/ConDigSum}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Junpeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yanyan Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hainan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1\">Zhuoye Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1\">Caixia Yuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaojie Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Box Embeddings: An open-source library for representation learning using geometric structures. (arXiv:2109.04997v1 [cs.CL])","link":"http://arxiv.org/abs/2109.04997","description":"<p>A major factor contributing to the success of modern representation learning\nis the ease of performing various vector operations. Recently, objects with\ngeometric structures (eg. distributions, complex or hyperbolic vectors, or\nregions such as cones, disks, or boxes) have been explored for their\nalternative inductive biases and additional representational capacities. In\nthis work, we introduce Box Embeddings, a Python library that enables\nresearchers to easily apply and extend probabilistic box embeddings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chheda_T/0/1/0/all/0/1\">Tejas Chheda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Purujit Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1\">Trang Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1\">Dhruvesh Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boratko_M/0/1/0/all/0/1\">Michael Boratko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dasgupta_S/0/1/0/all/0/1\">Shib Sankar Dasgupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training. (arXiv:2109.05003v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05003","description":"<p>We study the problem of training named entity recognition (NER) models using\nonly distantly-labeled data, which can be automatically obtained by matching\nentity mentions in the raw text with entity types in a knowledge base. The\nbiggest challenge of distantly-supervised NER is that the distant supervision\nmay induce incomplete and noisy labels, rendering the straightforward\napplication of supervised learning ineffective. In this paper, we propose (1) a\nnoise-robust learning scheme comprised of a new loss function and a noisy label\nremoval step, for training NER models on distantly-labeled data, and (2) a\nself-training method that uses contextualized augmentations created by\npre-trained language models to improve the generalization ability of the NER\nmodel. On three benchmark datasets, our method achieves superior performance,\noutperforming existing distantly-supervised NER models by significant margins.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yu Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jiaxin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BiSECT: Learning to Split and Rephrase Sentences with Bitexts. (arXiv:2109.05006v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05006","description":"<p>An important task in NLP applications such as sentence simplification is the\nability to take a long, complex sentence and split it into shorter sentences,\nrephrasing as necessary. We introduce a novel dataset and a new model for this\n`split and rephrase' task. Our BiSECT training data consists of 1 million long\nEnglish sentences paired with shorter, meaning-equivalent English sentences. We\nobtain these by extracting 1-2 sentence alignments in bilingual parallel\ncorpora and then using machine translation to convert both sides of the corpus\ninto the same language. BiSECT contains higher quality training examples than\nprevious Split and Rephrase corpora, with sentence splits that require more\nsignificant modifications. We categorize examples in our corpus, and use these\ncategories in a novel model that allows us to target specific regions of the\ninput sentence to be split and edited. Moreover, we show that models trained on\nBiSECT can perform a wider variety of split operations and improve upon\nprevious state-of-the-art approaches in automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Joongwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kriz_R/0/1/0/all/0/1\">Reno Kriz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Machine Translation Quality and Post-Editing Performance. (arXiv:2109.05016v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05016","description":"<p>We test the natural expectation that using MT in professional translation\nsaves human processing time. The last such study was carried out by\nSanchez-Torron and Koehn (2016) with phrase-based MT, artificially reducing the\ntranslation quality. In contrast, we focus on neural MT (NMT) of high quality,\nwhich has become the state-of-the-art approach since then and also got adopted\nby most translation companies.\n</p>\n<p>Through an experimental study involving over 30 professional translators for\nEnglish -&gt; Czech translation, we examine the relationship between NMT\nperformance and post-editing time and quality. Across all models, we found that\nbetter MT systems indeed lead to fewer changes in the sentences in this\nindustry setting. The relation between system quality and post-editing time is\nhowever not straightforward and, contrary to the results on phrase-based MT,\nBLEU is definitely not a stable predictor of the time or final output quality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zouhar_V/0/1/0/all/0/1\">Vil&#xe9;m Zouhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamchyna_A/0/1/0/all/0/1\">Ale&#x161; Tamchyna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Popel_M/0/1/0/all/0/1\">Martin Popel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dimensional Emotion Detection from Categorical Emotion. (arXiv:1911.02499v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1911.02499","description":"<p>We present a model to predict fine-grained emotions along the continuous\ndimensions of valence, arousal, and dominance (VAD) with a corpus with\ncategorical emotion annotations. Our model is trained by minimizing the EMD\n(Earth Mover's Distance) loss between the predicted VAD score distribution and\nthe categorical emotion distributions sorted along VAD, and it can\nsimultaneously classify the emotion categories and predict the VAD scores for a\ngiven sentence. We use pre-trained RoBERTa-Large and fine-tune on three\ndifferent corpora with categorical labels and evaluate on EmoBank corpus with\nVAD scores. We show that our approach reaches comparable performance to that of\nthe state-of-the-art classifiers in categorical emotion classification and\nshows significant positive correlations with the ground truth VAD scores. Also,\nfurther training with supervision of VAD labels leads to improved performance\nespecially when dataset is small. We also present examples of predictions of\nappropriate emotion words that are not part of the original annotations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungjoon Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_S/0/1/0/all/0/1\">Seonghyeon Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1\">Jaeyeol Jeon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1\">Hee Young Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Surprising Variability in Word Embedding Stability Across Languages. (arXiv:2004.14876v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.14876","description":"<p>Word embeddings are powerful representations that form the foundation of many\nnatural language processing architectures, both in English and in other\nlanguages. To gain further insight into word embeddings, we explore their\nstability (e.g., overlap between the nearest neighbors of a word in different\nembedding spaces) in diverse languages. We discuss linguistic properties that\nare related to stability, drawing out insights about correlations with\naffixing, language gender systems, and other features. This has implications\nfor embedding use, particularly in research that uses them to study language\ntrends.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Burdick_L/0/1/0/all/0/1\">Laura Burdick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms. (arXiv:2005.00782v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00782","description":"<p>Pre-trained language models (PTLMs) have achieved impressive performance on\ncommonsense inference benchmarks, but their ability to employ commonsense to\nmake robust inferences, which is crucial for effective communications with\nhumans, is debated. In the pursuit of advancing fluid human-AI communication,\nwe propose a new challenge, RICA: Robust Inference capability based on\nCommonsense Axioms, that evaluates robust commonsense inference despite textual\nperturbations. To generate data for this challenge, we develop a systematic and\nscalable procedure using commonsense knowledge bases and probe PTLMs across two\ndifferent evaluation settings. Extensive experiments on our generated probe\nsets with more than 10k statements show that PTLMs perform no better than\nrandom guessing on the zero-shot setting, are heavily impacted by statistical\nbiases, and are not robust to perturbation attacks. We also find that\nfine-tuning on similar statements offer limited gains, as PTLMs still fail to\ngeneralize to unseen inferences. Our new large-scale benchmark exposes a\nsignificant gap between PTLMs and human-level language understanding and offers\na new challenge for PTLMs to demonstrate commonsense.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1\">Rahul Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Seyeon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1\">Daniel Ho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Hard Retrieval Decoder Attention for Transformers. (arXiv:2009.14658v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.14658","description":"<p>The Transformer translation model is based on the multi-head attention\nmechanism, which can be parallelized easily. The multi-head attention network\nperforms the scaled dot-product attention function in parallel, empowering the\nmodel by jointly attending to information from different representation\nsubspaces at different positions. In this paper, we present an approach to\nlearning a hard retrieval attention where an attention head only attends to one\ntoken in the sentence rather than all tokens. The matrix multiplication between\nattention probabilities and the value sequence in the standard scaled\ndot-product attention can thus be replaced by a simple and efficient retrieval\noperation. We show that our hard retrieval attention mechanism is 1.43 times\nfaster in decoding, while preserving translation quality on a wide range of\nmachine translation tasks when used in the decoder self- and cross-attention\nnetworks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hongfei Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qiuhui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1\">Josef van Genabith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Model Selection for Cross-Lingual Transfer. (arXiv:2010.06127v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.06127","description":"<p>Transformers that are pre-trained on multilingual corpora, such as, mBERT and\nXLM-RoBERTa, have achieved impressive cross-lingual transfer capabilities. In\nthe zero-shot transfer setting, only English training data is used, and the\nfine-tuned model is evaluated on another target language. While this works\nsurprisingly well, substantial variance has been observed in target language\nperformance between different fine-tuning runs, and in the zero-shot setup, no\ntarget-language development data is available to select among multiple\nfine-tuned models. Prior work has relied on English dev data to select among\nmodels that are fine-tuned with different learning rates, number of steps and\nother hyperparameters, often resulting in suboptimal choices. In this paper, we\nshow that it is possible to select consistently better models when small\namounts of annotated data are available in auxiliary pivot languages. We\npropose a machine learning approach to model selection that uses the fine-tuned\nmodel's own internal representations to predict its cross-lingual capabilities.\nIn extensive experiments we find that this method consistently selects better\nmodels than English validation data across twenty five languages (including\neight low-resource languages), and often achieves results that are comparable\nto model selection using target language development data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effects of Parameter Norm Growth During Transformer Training: Inductive Bias from Gradient Descent. (arXiv:2010.09697v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2010.09697","description":"<p>The capacity of neural networks like the widely adopted transformer is known\nto be very high. Evidence is emerging that they learn successfully due to\ninductive bias in the training routine, typically a variant of gradient descent\n(GD). To better understand this bias, we study the tendency for transformer\nparameters to grow in magnitude ($\\ell_2$ norm) during training, and its\nimplications for the emergent representations within self attention layers.\nEmpirically, we document norm growth in the training of transformer language\nmodels, including T5 during its pretraining. As the parameters grow in\nmagnitude, we prove that the network approximates a discretized network with\nsaturated activation functions. Such \"saturated\" networks are known to have a\nreduced capacity compared to the full network family that can be described in\nterms of formal languages and automata. Our results suggest saturation is a new\ncharacterization of an inductive bias implicit in GD of particular interest for\nNLP. We leverage the emergent discrete structure in a saturated transformer to\nanalyze the role of different attention heads, finding that some focus locally\non a small number of positions, while other heads compute global averages,\nallowing counting. We believe understanding the interplay between these two\ncapabilities may shed further light on the structure of computation within\nlarge transformers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Merrill_W/0/1/0/all/0/1\">William Merrill</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramanujan_V/0/1/0/all/0/1\">Vivek Ramanujan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1\">Roy Schwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A scalable framework for learning from implicit user feedback to improve natural language understanding in large-scale conversational AI systems. (arXiv:2010.12251v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12251","description":"<p>Natural Language Understanding (NLU) is an established component within a\nconversational AI or digital assistant system, and it is responsible for\nproducing semantic understanding of a user request. We propose a scalable and\nautomatic approach for improving NLU in a large-scale conversational AI system\nby leveraging implicit user feedback, with an insight that user interaction\ndata and dialog context have rich information embedded from which user\nsatisfaction and intention can be inferred. In particular, we propose a general\ndomain-agnostic framework for curating new supervision data for improving NLU\nfrom live production traffic. With an extensive set of experiments, we show the\nresults of applying the framework and improving NLU for a large-scale\nproduction system and show its impact across 10 domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sunghyun Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Han Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1\">Ameen Patel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mudgal_S/0/1/0/all/0/1\">Sidharth Mudgal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sungjin Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Young-Bum Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsoukas_S/0/1/0/all/0/1\">Spyros Matsoukas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarikaya_R/0/1/0/all/0/1\">Ruhi Sarikaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Measuring Association Between Labels and Free-Text Rationales. (arXiv:2010.12762v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12762","description":"<p>In interpretable NLP, we require faithful rationales that reflect the model's\ndecision-making process for an explained instance. While prior work focuses on\nextractive rationales (a subset of the input words), we investigate their\nless-studied counterpart: free-text natural language rationales. We demonstrate\nthat pipelines, existing models for faithful extractive rationalization on\ninformation-extraction style tasks, do not extend as reliably to \"reasoning\"\ntasks requiring free-text rationales. We turn to models that jointly predict\nand rationalize, a class of widely used high-performance models for free-text\nrationalization whose faithfulness is not yet established. We define\nlabel-rationale association as a necessary property for faithfulness: the\ninternal mechanisms of the model producing the label and the rationale must be\nmeaningfully correlated. We propose two measurements to test this property:\nrobustness equivalence and feature importance agreement. We find that\nstate-of-the-art T5-based joint models exhibit both properties for\nrationalizing commonsense question-answering and natural language inference,\nindicating their potential for producing faithful free-text rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1\">Sarah Wiegreffe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marasovic_A/0/1/0/all/0/1\">Ana Marasovi&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COUGH: A Challenge Dataset and Models for COVID-19 FAQ Retrieval. (arXiv:2010.12800v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12800","description":"<p>We present a large, challenging dataset, COUGH, for COVID-19 FAQ retrieval.\nSimilar to a standard FAQ dataset, COUGH consists of three parts: FAQ Bank,\nQuery Bank and Relevance Set. The FAQ Bank contains ~16K FAQ items scraped from\n55 credible websites (e.g., CDC and WHO). For evaluation, we introduce Query\nBank and Relevance Set, where the former contains 1,236 human-paraphrased\nqueries while the latter contains ~32 human-annotated FAQ items for each query.\nWe analyze COUGH by testing different FAQ retrieval models built on top of BM25\nand BERT, among which the best model achieves 48.8 under P@5, indicating a\ngreat challenge presented by COUGH and encouraging future research for further\nimprovement. Our COUGH dataset is available at\nhttps://github.com/sunlab-osu/covid-faq.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinliang Frederick Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Heming Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1\">Xiang Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Simon Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1\">Huan Sun</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-supervised Relation Extraction via Incremental Meta Self-Training. (arXiv:2010.16410v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.16410","description":"<p>To alleviate human efforts from obtaining large-scale annotations,\nSemi-Supervised Relation Extraction methods aim to leverage unlabeled data in\naddition to learning from limited samples. Existing self-training methods\nsuffer from the gradual drift problem, where noisy pseudo labels on unlabeled\ndata are incorporated during training. To alleviate the noise in pseudo labels,\nwe propose a method called MetaSRE, where a Relation Label Generation Network\ngenerates quality assessment on pseudo labels by (meta) learning from the\nsuccessful and failed attempts on Relation Classification Network as an\nadditional meta-objective. To reduce the influence of noisy pseudo labels,\nMetaSRE adopts a pseudo label selection and exploitation scheme which assesses\npseudo label quality on unlabeled samples and only exploits high-quality pseudo\nlabels in a self-training fashion to incrementally augment labeled samples for\nboth robustness and accuracy. Experimental results on two public datasets\ndemonstrate the effectiveness of the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1\">Fukun Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenyao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NegatER: Unsupervised Discovery of Negatives in Commonsense Knowledge Bases. (arXiv:2011.07497v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2011.07497","description":"<p>Codifying commonsense knowledge in machines is a longstanding goal of\nartificial intelligence. Recently, much progress toward this goal has been made\nwith automatic knowledge base (KB) construction techniques. However, such\ntechniques focus primarily on the acquisition of positive (true) KB statements,\neven though negative (false) statements are often also important for\ndiscriminative reasoning over commonsense KBs. As a first step toward the\nlatter, this paper proposes NegatER, a framework that ranks potential negatives\nin commonsense KBs using a contextual language model (LM). Importantly, as most\nKBs do not contain negatives, NegatER relies only on the positive knowledge in\nthe LM and does not require ground-truth negative examples. Experiments\ndemonstrate that, compared to multiple contrastive data augmentation\napproaches, NegatER yields negatives that are more grammatical, coherent, and\ninformative -- leading to statistically significant accuracy improvements in a\nchallenging KB completion task and confirming that the positive knowledge in\nLMs can be \"re-purposed\" to generate negative knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jing Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Sentence Representation Learning with Conditional Masked Language Model. (arXiv:2012.14388v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14388","description":"<p>This paper presents a novel training method, Conditional Masked Language\nModeling (CMLM), to effectively learn sentence representations on large scale\nunlabeled corpora. CMLM integrates sentence representation learning into MLM\ntraining by conditioning on the encoded vectors of adjacent sentences. Our\nEnglish CMLM model achieves state-of-the-art performance on SentEval, even\noutperforming models learned using supervised signals. As a fully unsupervised\nlearning method, CMLM can be conveniently extended to a broad range of\nlanguages and domains. We find that a multilingual CMLM model co-trained with\nbitext retrieval (BR) and natural language inference (NLI) tasks outperforms\nthe previous state-of-the-art multilingual models by a large margin, e.g. 10%\nimprovement upon baseline models on cross-lingual semantic search. We explore\nthe same language bias of the learned representations, and propose a simple,\npost-training and model agnostic approach to remove the language identifying\ninformation from the representation while still retaining sentence semantics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Ziyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cer_D/0/1/0/all/0/1\">Daniel Cer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Law_J/0/1/0/all/0/1\">Jax Law</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darve_E/0/1/0/all/0/1\">Eric Darve</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNKs Everywhere: Adapting Multilingual Language Models to New Scripts. (arXiv:2012.15562v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15562","description":"<p>Massively multilingual language models such as multilingual BERT offer\nstate-of-the-art cross-lingual transfer performance on a range of NLP tasks.\nHowever, due to limited capacity and large differences in pretraining data\nsizes, there is a profound performance gap between resource-rich and\nresource-poor target languages. The ultimate challenge is dealing with\nunder-resourced languages not covered at all by the models and written in\nscripts unseen during pretraining. In this work, we propose a series of novel\ndata-efficient methods that enable quick and effective adaptation of pretrained\nmultilingual models to such low-resource languages and unseen scripts. Relying\non matrix factorization, our methods capitalize on the existing latent\nknowledge about multiple languages already available in the pretrained model's\nembedding matrix. Furthermore, we show that learning of the new dedicated\nembedding matrix in the target language can be improved by leveraging a small\nnumber of vocabulary items (i.e., the so-called lexically overlapping tokens)\nshared between mBERT's and target language vocabulary. Our adaptation\ntechniques offer substantial performance gains for languages with unseen\nscripts. We also demonstrate that they can yield improvements for low-resource\nlanguages written in scripts covered by the pretrained model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1\">Ivan Vuli&#x107;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1\">Sebastian Ruder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Revisiting Robust Neural Machine Translation: A Transformer Case Study. (arXiv:2012.15710v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15710","description":"<p>Transformers (Vaswani et al., 2017) have brought a remarkable improvement in\nthe performance of neural machine translation (NMT) systems but they could be\nsurprisingly vulnerable to noise. In this work, we try to investigate how noise\nbreaks Transformers and if there exist solutions to deal with such issues.\nThere is a large body of work in the NMT literature on analyzing the behavior\nof conventional models for the problem of noise but Transformers are relatively\nunderstudied in this context. Motivated by this, we introduce a novel\ndata-driven technique called Target Augmented Fine-tuning (TAFT) to incorporate\nnoise during training. This idea is comparable to the well-known fine-tuning\nstrategy. Moreover, we propose two other novel extensions to the original\nTransformer: Controlled Denoising (CD) and Dual-Channel Decoding (DCD), that\nmodify the neural architecture as well as the training process to handle noise.\nOne important characteristic of our techniques is that they only impact the\ntraining phase and do not impose any overhead at inference time. We evaluated\nour techniques to translate the English--German pair in both directions and\nobserved that our models have a higher tolerance to noise. More specifically,\nthey perform with no deterioration where up to 10% of entire test words are\ninfected by noise.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Passban_P/0/1/0/all/0/1\">Peyman Passban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saladi_P/0/1/0/all/0/1\">Puneeth S.M. Saladi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FastIF: Scalable Influence Functions for Efficient Model Interpretation and Debugging. (arXiv:2012.15781v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.15781","description":"<p>Influence functions approximate the \"influences\" of training data-points for\ntest predictions and have a wide variety of applications. Despite the\npopularity, their computational cost does not scale well with model and\ntraining data size. We present FastIF, a set of simple modifications to\ninfluence functions that significantly improves their run-time. We use\nk-Nearest Neighbors (kNN) to narrow the search space down to a subset of good\ncandidate data points, identify the configurations that best balance the\nspeed-quality trade-off in estimating the inverse Hessian-vector product, and\nintroduce a fast parallel variant. Our proposed method achieves about 80X\nspeedup while being highly correlated with the original influence values. With\nthe availability of the fast influence functions, we demonstrate their\nusefulness in four applications. First, we examine whether influential\ndata-points can \"explain\" test time behavior using the framework of\nsimulatability. Second, we visualize the influence interactions between\ntraining and test data-points. Third, we show that we can correct model errors\nby additional fine-tuning on certain influential data-points, improving the\naccuracy of a trained MultiNLI model by 2.5% on the HANS dataset. Finally, we\nexperiment with a similar setup but fine-tuning on datapoints not seen during\ntraining, improving the model accuracy by 2.8% and 1.7% on HANS and ANLI\ndatasets respectively. Overall, our fast influence functions can be efficiently\napplied to large models and datasets, and our experiments demonstrate the\npotential of influence functions in model interpretation and correcting model\nerrors. Code is available at\nhttps://github.com/salesforce/fast-influence-functions\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1\">Han Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajani_N/0/1/0/all/0/1\">Nazneen Fatema Rajani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1\">Peter Hase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing Commonsense Emergence in Few-shot Knowledge Models. (arXiv:2101.00297v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00297","description":"<p>Recently, commonsense knowledge models - pretrained language models (LM)\nfine-tuned on knowledge graph (KG) tuples - showed that considerable amounts of\ncommonsense knowledge can be encoded in the parameters of large language\nmodels. However, as parallel studies show that LMs are poor hypothesizers of\ndeclarative commonsense relationships on their own, it remains unclear whether\nthis knowledge is learned during pretraining or from fine-tuning on KG\nexamples. To investigate this question, we train commonsense knowledge models\nin few-shot settings to study the emergence of their commonsense representation\nabilities. Our results show that commonsense knowledge models can rapidly adapt\nfrom limited examples, indicating that KG fine-tuning serves to learn an\ninterface to encoded knowledge learned during pretraining. Importantly, our\nanalysis of absolute, angular, and distributional parameter changes during\nfew-shot fine-tuning provides novel insights into how this interface is\nlearned.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Da_J/0/1/0/all/0/1\">Jeff Da</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bras_R/0/1/0/all/0/1\">Ronan Le Bras</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1\">Ximing Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00433","description":"<p>Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nThis is problematic, as previous work has demonstrated possible trade-offs and\nnegative consequences to disclosive transparency, such as a confusion effect,\nwhere \"too much information\" clouds a reader's understanding of what a system\ndescription means. Disclosive transparency's subjective nature has rendered\ndeep study into these problems and their remedies difficult. To improve this\nstate of affairs, We introduce neural language model-based probabilistic\nmetrics to directly model disclosive transparency, and demonstrate that they\ncorrelate with user and expert opinions of system transparency, making them a\nvalid objective proxy. Finally, we demonstrate the use of these metrics in a\npilot study quantifying the relationships between transparency, confusion, and\nuser perceptions in a corpus of real NLP system descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generative Spoken Language Modeling from Raw Audio. (arXiv:2102.01192v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.01192","description":"<p>We introduce Generative Spoken Language Modeling, the task of learning the\nacoustic and linguistic characteristics of a language from raw audio (no text,\nno labels), and a set of metrics to automatically evaluate the learned\nrepresentations at acoustic and linguistic levels for both encoding and\ngeneration. We set up baseline systems consisting of a discrete speech encoder\n(returning pseudo-text units), a generative language model (trained on\npseudo-text), and a speech decoder (generating a waveform from pseudo-text) all\ntrained without supervision and validate the proposed metrics with human\nevaluation. Across 3 speech encoders (CPC, wav2vec 2.0, HuBERT), we find that\nthe number of discrete units (50, 100, or 200) matters in a task-dependent and\nencoder-dependent way, and that some combinations approach text-based systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Evgeny Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bolte_B/0/1/0/all/0/1\">Benjamin Bolte</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Adelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling Creative Inspiration with Fine-Grained Functional Facets of Ideas. (arXiv:2102.09761v2 [cs.HC] UPDATED)","link":"http://arxiv.org/abs/2102.09761","description":"<p>Large repositories of products, patents and scientific papers offer an\nopportunity for building systems that scour millions of ideas and help users\ndiscover inspirations. However, idea descriptions are typically in the form of\nunstructured text, lacking key structure that is required for supporting\ncreative innovation interactions. Prior work has explored idea representations\nthat were limited in expressivity, required significant manual effort from\nusers, or dependent on curated knowledge bases with poor coverage. We explore a\nnovel representation that automatically breaks up products into fine-grained\nfunctional facets capturing the purposes and mechanisms of ideas, and use it to\nsupport important creative innovation interactions: functional search for\nideas, and exploration of the design space around a focal problem by viewing\nrelated problem perspectives pooled from across many products. In user studies,\nour approach boosts the quality of creative search and inspirations,\noutperforming strong baselines by 50-60%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tamari_R/0/1/0/all/0/1\">Ronen Tamari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1\">Hyeonsu Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1\">Joel Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1\">Aniket Kittur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1\">Dafna Shahaf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01378","description":"<p>Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model's decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Cues and Error Correction for Translation Robustness. (arXiv:2103.07352v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.07352","description":"<p>Neural Machine Translation models are sensitive to noise in the input texts,\nsuch as misspelled words and ungrammatical constructions. Existing robustness\ntechniques generally fail when faced with unseen types of noise and their\nperformance degrades on clean texts. In this paper, we focus on three types of\nrealistic noise that are commonly generated by humans and introduce the idea of\nvisual context to improve translation robustness for noisy texts. In addition,\nwe describe a novel error correction training regime that can be used as an\nauxiliary task to further improve translation robustness. Experiments on\nEnglish-French and English-German translation show that both multimodal and\nerror correction components improve model robustness to noisy texts, while\nstill retaining translation quality on clean texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhenhao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_M/0/1/0/all/0/1\">Marek Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lawyers are Dishonest? Quantifying Representational Harms in Commonsense Knowledge Resources. (arXiv:2103.11320v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11320","description":"<p>Warning: this paper contains content that may be offensive or upsetting.\n</p>\n<p>Numerous natural language processing models have tried injecting commonsense\nby using the ConceptNet knowledge base to improve performance on different\ntasks. ConceptNet, however, is mostly crowdsourced from humans and may reflect\nhuman biases such as \"lawyers are dishonest.\" It is important that these biases\nare not conflated with the notion of commonsense. We study this missing yet\nimportant problem by first defining and quantifying biases in ConceptNet as two\ntypes of representational harms: overgeneralization of polarized perceptions\nand representation disparity. We find that ConceptNet contains severe biases\nand disparities across four demographic categories. In addition, we analyze two\ndownstream models that use ConceptNet as a source for commonsense knowledge and\nfind the existence of biases in those models as well. We further propose a\nfiltered-based bias-mitigation approach and examine its effectiveness. We show\nthat our mitigation approach can reduce the issues in both resource and models\nbut leads to a performance drop, leaving room for future work to build fairer\nand stronger commonsense models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1\">Ninareh Mehrabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Morstatter_F/0/1/0/all/0/1\">Fred Morstatter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galstyan_A/0/1/0/all/0/1\">Aram Galstyan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Morphosyntactic Well-formedness of Generated Texts. (arXiv:2103.16590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.16590","description":"<p>Text generation systems are ubiquitous in natural language processing\napplications. However, evaluation of these systems remains a challenge,\nespecially in multilingual settings. In this paper, we propose L'AMBRE -- a\nmetric to evaluate the morphosyntactic well-formedness of text using its\ndependency parse and morphosyntactic rules of the language. We present a way to\nautomatically extract various rules governing morphosyntax directly from\ndependency treebanks. To tackle the noisy outputs from text generation systems,\nwe propose a simple methodology to train robust parsers. We show the\neffectiveness of our metric on the task of machine translation through a\ndiachronic study of systems translating into morphologically-rich languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1\">Adithya Pratapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rijhwani_S/0/1/0/all/0/1\">Shruti Rijhwani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1\">Aditi Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mortensen_D/0/1/0/all/0/1\">David R. Mortensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Keyword Spotting in Any Language. (arXiv:2104.01454v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01454","description":"<p>We introduce a few-shot transfer learning method for keyword spotting in any\nlanguage. Leveraging open speech corpora in nine languages, we automate the\nextraction of a large multilingual keyword bank and use it to train an\nembedding model. With just five training examples, we fine-tune the embedding\nmodel for keyword spotting and achieve an average F1 score of 0.75 on keyword\nclassification for 180 new keywords unseen by the embedding model in these nine\nlanguages. This embedding model also generalizes to new languages. We achieve\nan average F1 score of 0.65 on 5-shot models for 260 keywords sampled across 13\nnew languages unseen by the embedding model. We investigate streaming accuracy\nfor our 5-shot models in two contexts: keyword spotting and keyword search.\nAcross 440 keywords in 22 languages, we achieve an average streaming keyword\nspotting accuracy of 87.4% with a false acceptance rate of 4.3%, and observe\npromising initial results on keyword search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mazumder_M/0/1/0/all/0/1\">Mark Mazumder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Banbury_C/0/1/0/all/0/1\">Colby Banbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_J/0/1/0/all/0/1\">Josh Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Warden_P/0/1/0/all/0/1\">Pete Warden</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddi_V/0/1/0/all/0/1\">Vijay Janapa Reddi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Contrastive samples via Summarization for Text Classification with limited annotations. (arXiv:2104.05094v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05094","description":"<p>Contrastive Learning has emerged as a powerful representation learning method\nand facilitates various downstream tasks especially when supervised data is\nlimited. How to construct efficient contrastive samples through data\naugmentation is key to its success. Unlike vision tasks, the data augmentation\nmethod for contrastive learning has not been investigated sufficiently in\nlanguage tasks. In this paper, we propose a novel approach to construct\ncontrastive samples for language tasks using text summarization. We use these\nsamples for supervised contrastive learning to gain better text representations\nwhich greatly benefit text classification tasks with limited annotations. To\nfurther improve the method, we mix up samples from different classes and add an\nextra regularization, named Mixsum, in addition to the cross-entropy-loss.\nExperiments on real-world text classification datasets (Amazon-5, Yelp-5, AG\nNews, and IMDb) demonstrate the effectiveness of the proposed contrastive\nlearning framework with summarization-based data augmentation and Mixsum\nregularization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yangkai Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1\">Tengfei Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1\">Lingfei Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_F/0/1/0/all/0/1\">Fangli Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuhong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1\">Shouling Ji</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WHOSe Heritage: Classification of UNESCO World Heritage \"Outstanding Universal Value\" Documents with Soft Labels. (arXiv:2104.05547v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05547","description":"<p>The UNESCO World Heritage List (WHL) includes the exceptionally valuable\ncultural and natural heritage to be preserved for mankind. Evaluating and\njustifying the Outstanding Universal Value (OUV) is essential for each site\ninscribed in the WHL, and yet a complex task, even for experts, since the\nselection criteria of OUV are not mutually exclusive. Furthermore, manual\nannotation of heritage values and attributes from multi-source textual data,\nwhich is currently dominant in heritage studies, is knowledge-demanding and\ntime-consuming, impeding systematic analysis of such authoritative documents in\nterms of their implications on heritage management. This study applies\nstate-of-the-art NLP models to build a classifier on a new dataset containing\nStatements of OUV, seeking an explainable and scalable automation tool to\nfacilitate the nomination, evaluation, research, and monitoring processes of\nWorld Heritage sites. Label smoothing is innovatively adapted to improve the\nmodel performance by adding prior inter-class relationship knowledge to\ngenerate soft labels. The study shows that the best models fine-tuned from BERT\nand ULMFiT can reach 94.3% top-3 accuracy. A human study with expert evaluation\non the model prediction shows that the models are sufficiently generalizable.\nThe study is promising to be further developed and applied in heritage research\nand practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bai_N/0/1/0/all/0/1\">Nan Bai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1\">Renqian Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nourian_P/0/1/0/all/0/1\">Pirouz Nourian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roders_A/0/1/0/all/0/1\">Ana Pereira Roders</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relational World Knowledge Representation in Contextual Language Models: A Review. (arXiv:2104.05837v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05837","description":"<p>Relational knowledge bases (KBs) are commonly used to represent world\nknowledge in machines. However, while advantageous for their high degree of\nprecision and interpretability, KBs are usually organized according to\nmanually-defined schemas, which limit their expressiveness and require\nsignificant human efforts to engineer and maintain. In this review, we take a\nnatural language processing perspective to these limitations, examining how\nthey may be addressed in part by training deep contextual language models (LMs)\nto internalize and express relational knowledge in more flexible forms. We\npropose to organize knowledge representation strategies in LMs by the level of\nKB supervision provided, from no KB supervision at all to entity- and\nrelation-level supervision. Our contributions are threefold: (1) We provide a\nhigh-level, extensible taxonomy for knowledge representation in LMs; (2) Within\nour taxonomy, we highlight notable models, evaluation tasks, and findings, in\norder to provide an up-to-date review of current knowledge representation\ncapabilities in LMs; and (3) We suggest future research directions that build\nupon the complementary aspects of LMs and KBs as knowledge representations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Safavi_T/0/1/0/all/0/1\">Tara Safavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1\">Danai Koutra</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Visual Goal-Step Inference using wikiHow. (arXiv:2104.05845v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2104.05845","description":"<p>Understanding what sequence of steps are needed to complete a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task, where a model\nis given a textual goal and must choose which of four images represents a\nplausible step towards that goal. With a new dataset harvested from wikiHow\nconsisting of 772,277 images representing human actions, we show that our task\nis challenging for state-of-the-art multimodal models. Moreover, the multimodal\nrepresentation learned from our data can be effectively transferred to other\ndatasets like HowTo100m, increasing the VGSI accuracy by 15 - 20%. Our task\nwill facilitate multimodal reasoning about procedural events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yue Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1\">Artemis Panagopoulou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yatskar_M/0/1/0/all/0/1\">Mark Yatskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lessons on Parameter Sharing across Layers in Transformers. (arXiv:2104.06022v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06022","description":"<p>We propose a parameter sharing method for Transformers (Vaswani et al.,\n2017). The proposed approach relaxes a widely used technique, which shares\nparameters for one layer with all layers such as Universal Transformers\n(Dehghani et al., 2019), to increase the efficiency in the computational time.\nWe propose three strategies: Sequence, Cycle, and Cycle (rev) to assign\nparameters to each layer. Experimental results show that the proposed\nstrategies are efficient in the parameter size and computational time.\nMoreover, we indicate that the proposed strategies are also effective in the\nconfiguration where we use many training data such as the recent WMT\ncompetition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takase_S/0/1/0/all/0/1\">Sho Takase</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Masked Language Modeling and the Distributional Hypothesis: Order Word Matters Pre-training for Little. (arXiv:2104.06644v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06644","description":"<p>A possible explanation for the impressive performance of masked language\nmodel (MLM) pre-training is that such models have learned to represent the\nsyntactic structures prevalent in classical NLP pipelines. In this paper, we\npropose a different explanation: MLMs succeed on downstream tasks almost\nentirely due to their ability to model higher-order word co-occurrence\nstatistics. To demonstrate this, we pre-train MLMs on sentences with randomly\nshuffled word order, and show that these models still achieve high accuracy\nafter fine-tuning on many downstream tasks -- including on tasks specifically\ndesigned to be challenging for models that ignore word order. Our models\nperform surprisingly well according to some parametric syntactic probes,\nindicating possible deficiencies in how we test representations for syntactic\ninformation. Overall, our results show that purely distributional information\nlargely explains the success of pre-training, and underscore the importance of\ncurating challenging evaluation datasets that require deeper linguistic\nknowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1\">Koustuv Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1\">Dieuwke Hupkes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pineau_J/0/1/0/all/0/1\">Joelle Pineau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06979","description":"<p>Learning sentence embeddings often requires a large amount of labeled data.\nHowever, for most tasks and domains, labeled data is seldom available and\ncreating it is expensive. In this work, we present a new state-of-the-art\nunsupervised method based on pre-trained Transformers and Sequential Denoising\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\nIt can achieve up to 93.1% of the performance of in-domain supervised\napproaches. Further, we show that TSDAE is a strong domain adaptation and\npre-training method for sentence embeddings, significantly outperforming other\napproaches like Masked Language Model.\n</p>\n<p>A crucial shortcoming of previous studies is the narrow evaluation: Most work\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\ndoes not require any domain knowledge. It is unclear if these proposed methods\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\nother recent approaches on four different datasets from heterogeneous domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pseudo Zero Pronoun Resolution Improves Zero Anaphora Resolution. (arXiv:2104.07425v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07425","description":"<p>Masked language models (MLMs) have contributed to drastic performance\nimprovements with regard to zero anaphora resolution (ZAR). To further improve\nthis approach, in this study, we made two proposals. The first is a new\npretraining task that trains MLMs on anaphoric relations with explicit\nsupervision, and the second proposal is a new finetuning method that remedies a\nnotorious issue, the pretrain-finetune discrepancy. Our experiments on Japanese\nZAR demonstrated that our two proposals boost the state-of-the-art performance,\nand our detailed analysis provides new insights on the remaining challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Konno_R/0/1/0/all/0/1\">Ryuto Konno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiyono_S/0/1/0/all/0/1\">Shun Kiyono</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsubayashi_Y/0/1/0/all/0/1\">Yuichiroh Matsubayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouchi_H/0/1/0/all/0/1\">Hiroki Ouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Effect of Efficient Messaging and Input Variability on Neural-Agent Iterated Language Learning. (arXiv:2104.07637v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07637","description":"<p>Natural languages display a trade-off among different strategies to convey\nsyntactic structure, such as word order or inflection. This trade-off, however,\nhas not appeared in recent simulations of iterated language learning with\nneural network agents (Chaabouni et al., 2019b). We re-evaluate this result in\nlight of three factors that play an important role in comparable experiments\nfrom the Language Evolution field: (i) speaker bias towards efficient\nmessaging, (ii) non systematic input languages, and (iii) learning bottleneck.\nOur simulations show that neural agents mainly strive to maintain the utterance\ntype distribution observed during learning, instead of developing a more\nefficient or systematic language.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lian_Y/0/1/0/all/0/1\">Yuchen Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisazza_A/0/1/0/all/0/1\">Arianna Bisazza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Verhoef_T/0/1/0/all/0/1\">Tessa Verhoef</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detect and Classify -- Joint Span Detection and Classification for Health Outcomes. (arXiv:2104.07789v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07789","description":"<p>A health outcome is a measurement or an observation used to capture and\nassess the effect of a treatment. Automatic detection of health outcomes from\ntext would undoubtedly speed up access to evidence necessary in healthcare\ndecision making. Prior work on outcome detection has modelled this task as\neither (a) a sequence labelling task, where the goal is to detect which text\nspans describe health outcomes, or (b) a classification task, where the goal is\nto classify a text into a pre-defined set of categories depending on an outcome\nthat is mentioned somewhere in that text. However, this decoupling of span\ndetection and classification is problematic from a modelling perspective and\nignores global structural correspondences between sentence-level and word-level\ninformation present in a given text. To address this, we propose a method that\nuses both word-level and sentence-level information to simultaneously perform\noutcome span detection and outcome type classification. In addition to\ninjecting contextual information to hidden vectors, we use label attention to\nappropriately weight both word and sentence level information. Experimental\nresults on several benchmark datasets for health outcome detection show that\nour proposed method consistently outperforms decoupled methods, reporting\ncompetitive results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abaho_M/0/1/0/all/0/1\">Michael Abaho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bollegala_D/0/1/0/all/0/1\">Danushka Bollegala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williamson_P/0/1/0/all/0/1\">Paula Williamson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodd_S/0/1/0/all/0/1\">Susanna Dodd</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What to Pre-Train on? Efficient Intermediate Task Selection. (arXiv:2104.08247v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08247","description":"<p>Intermediate task fine-tuning has been shown to culminate in large transfer\ngains across many NLP tasks. With an abundance of candidate datasets as well as\npre-trained language models, it has become infeasible to run the cross-product\nof all combinations to find the best transfer setting. In this work we first\nestablish that similar sequential fine-tuning gains can be achieved in adapter\nsettings, and subsequently consolidate previously proposed methods that\nefficiently identify beneficial tasks for intermediate transfer learning. We\nexperiment with a diverse set of 42 intermediate and 11 target English\nclassification, multiple choice, question answering, and sequence tagging\ntasks. Our results show that efficient embedding based methods that rely solely\non the respective datasets outperform computational expensive few-shot\nfine-tuning approaches. Our best methods achieve an average Regret@3 of less\nthan 1% across all target tasks, demonstrating that we are able to efficiently\nidentify the best datasets for intermediate training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Poth_C/0/1/0/all/0/1\">Clifton Poth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Wikily\" Supervised Neural Translation Tailored to Cross-Lingual Tasks. (arXiv:2104.08384v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08384","description":"<p>We present a simple but effective approach for leveraging Wikipedia for\nneural machine translation as well as cross-lingual tasks of image captioning\nand dependency parsing without using any direct supervision from external\nparallel data or supervised models in the target language. We show that first\nsentences and titles of linked Wikipedia pages, as well as cross-lingual image\ncaptions, are strong signals for a seed parallel data to extract bilingual\ndictionaries and cross-lingual word embeddings for mining parallel text from\nWikipedia. Our final model achieves high BLEU scores that are close to or\nsometimes higher than strong supervised baselines in low-resource languages;\ne.g. supervised BLEU of 4.0 versus 12.1 from our model in English-to-Kazakh.\nMoreover, we tailor our wikily supervised translation models to unsupervised\nimage captioning, and cross-lingual dependency parser transfer. In image\ncaptioning, we train a multi-tasking machine translation and image captioning\npipeline for Arabic and English from which the Arabic training data is a\ntranslated version of the English captioning data, using our wikily-supervised\ntranslation models. Our captioning results on Arabic are slightly better than\nthat of its supervised model. In dependency parsing, we translate a large\namount of monolingual text, and use it as artificial training data in an\nannotation projection framework. We show that our model outperforms recent work\non cross-lingual transfer of dependency parsers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rasooli_M/0/1/0/all/0/1\">Mohammad Sadegh Rasooli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijaya_D/0/1/0/all/0/1\">Derry Tanti Wijaya</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XLEnt: Mining a Large Cross-lingual Entity Dataset with Lexical-Semantic-Phonetic Word Alignment. (arXiv:2104.08597v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08597","description":"<p>Cross-lingual named-entity lexica are an important resource to multilingual\nNLP tasks such as machine translation and cross-lingual wikification. While\nknowledge bases contain a large number of entities in high-resource languages\nsuch as English and French, corresponding entities for lower-resource languages\nare often missing. To address this, we propose Lexical-Semantic-Phonetic Align\n(LSP-Align), a technique to automatically mine cross-lingual entity lexica from\nmined web data. We demonstrate LSP-Align outperforms baselines at extracting\ncross-lingual entity pairs and mine 164 million entity pairs from 120 different\nlanguages aligned with English. We release these cross-lingual entity pairs\nalong with the massively multilingual tagged named entity corpus as a resource\nto the NLP community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Renduchintala_A/0/1/0/all/0/1\">Adithya Renduchintala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-Shot Cross-Lingual Transfer Learning via Robust Training. (arXiv:2104.08645v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08645","description":"<p>Pre-trained multilingual language encoders, such as multilingual BERT and\nXLM-R, show great potential for zero-shot cross-lingual transfer. However,\nthese multilingual encoders do not precisely align words and phrases across\nlanguages. Especially, learning alignments in the multilingual embedding space\nusually requires sentence-level or word-level parallel corpora, which are\nexpensive to be obtained for low-resource languages. An alternative is to make\nthe multilingual encoders more robust; when fine-tuning the encoder using\ndownstream task, we train the encoder to tolerate noise in the contextual\nembedding spaces such that even if the representations of different languages\nare not aligned well, the model can still achieve good performance on zero-shot\ncross-lingual transfer. In this work, we propose a learning strategy for\ntraining robust models by drawing connections between adversarial examples and\nthe failure cases of zero-shot cross-lingual transfer. We adopt two widely used\nrobust training methods, adversarial training and randomized smoothing, to\ntrain the desired robust model. The experimental results demonstrate that\nrobust training improves zero-shot cross-lingual transfer on text\nclassification tasks. The improvement is more significant in the generalized\ncross-lingual transfer setting, where the pair of input sentences belong to two\ndifferent languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Dependencies and Statistical Dependence. (arXiv:2104.08685v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08685","description":"<p>Are pairs of words that tend to occur together also likely to stand in a\nlinguistic dependency? This empirical question is motivated by a long history\nof literature in cognitive science, psycholinguistics, and NLP. In this work we\ncontribute an extensive analysis of the relationship between linguistic\ndependencies and statistical dependence between words. Improving on previous\nwork, we introduce the use of large pretrained language models to compute\ncontextualized estimates of the pointwise mutual information between words\n(CPMI). For multiple models and languages, we extract dependency trees which\nmaximize CPMI, and compare to gold standard linguistic dependencies. Overall,\nwe find that CPMI dependencies achieve an unlabelled undirected attachment\nscore of at most $\\approx 0.5$. While far above chance, and consistently above\na non-contextualized PMI baseline, this score is generally comparable to a\nsimple baseline formed by connecting adjacent words. We analyze which kinds of\nlinguistic dependencies are best captured in CPMI dependencies, and also find\nmarked differences between the estimates of the large pretrained language\nmodels, illustrating how their different training schemes affect the type of\ndependencies they capture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hoover_J/0/1/0/all/0/1\">Jacob Louis Hoover</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1\">Wenyu Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+ODonnell_T/0/1/0/all/0/1\">Timothy J. O&#x27;Donnell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Keyphrase Generation with Fine-Grained Evaluation-Guided Reinforcement Learning. (arXiv:2104.08799v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08799","description":"<p>Aiming to generate a set of keyphrases, Keyphrase Generation (KG) is a\nclassical task for capturing the central idea from a given document. Based on\nSeq2Seq models, the previous reinforcement learning framework on KG tasks\nutilizes the evaluation metrics to further improve the well-trained neural\nmodels. However, these KG evaluation metrics such as $F_1@5$ and $F_1@M$ are\nonly aware of the exact correctness of predictions on phrase-level and ignore\nthe semantic similarities between similar predictions and targets, which\ninhibits the model from learning deep linguistic patterns. In response to this\nproblem, we propose a new fine-grained evaluation metric to improve the RL\nframework, which considers different granularities: token-level $F_1$ score,\nedit distance, duplication, and prediction quantities. On the whole, the new\nframework includes two reward functions: the fine-grained evaluation score and\nthe vanilla $F_1$ score. This framework helps the model identifying some\npartial match phrases which can be further optimized as the exact match ones.\nExperiments on KG benchmarks show that our proposed training framework\noutperforms the previous RL training frameworks among all evaluation scores. In\naddition, our method can effectively ease the synonym problem and generate a\nhigher quality prediction. The source code is available at\n\\url{https://github.com/xuyige/FGRL4KG}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1\">Yichao Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yige Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1\">Jiacheng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08821","description":"<p>This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances the state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We find that dropout acts as minimal data augmentation and\nremoving it leads to a representation collapse. Then, we propose a supervised\napproach, which incorporates annotated pairs from natural language inference\ndatasets into our contrastive learning framework, by using \"entailment\" pairs\nas positives and \"contradiction\" pairs as hard negatives. We evaluate SimCSE on\nstandard semantic textual similarity (STS) tasks, and our unsupervised and\nsupervised models using BERT base achieve an average of 76.3% and 81.6%\nSpearman's correlation respectively, a 4.2% and 2.2% improvement compared to\nprevious best results. We also show -- both theoretically and empirically --\nthat contrastive learning objective regularizes pre-trained embeddings'\nanisotropic space to be more uniform, and it better aligns positive pairs when\nsupervised signals are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09574","description":"<p>Humans use commonsense reasoning (CSR) implicitly to produce natural and\ncoherent responses in conversations. Aiming to close the gap between current\nresponse generation (RG) models and human communication abilities, we want to\nunderstand why RG models respond as they do by probing RG model's understanding\nof commonsense reasoning that elicits proper responses. We formalize the\nproblem by framing commonsense as a latent variable in the RG task and using\nexplanations for responses as textual form of commonsense. We collect 6k\nannotated explanations justifying responses from four dialogue datasets and ask\nhumans to verify them and propose two probing settings to evaluate RG models'\nCSR capabilities. Probing results show that models fail to capture the logical\nrelations between commonsense explanations and responses and fine-tuning on\nin-domain data and increasing model sizes do not lead to understanding of CSR\nfor RG. We hope our study motivates more research in making RG models emulate\nthe human reasoning process in pursuit of smooth human-AI communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Justin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generalising Multilingual Concept-to-Text NLG with Language Agnostic Delexicalisation. (arXiv:2105.03432v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03432","description":"<p>Concept-to-text Natural Language Generation is the task of expressing an\ninput meaning representation in natural language. Previous approaches in this\ntask have been able to generalise to rare or unseen instances by relying on a\ndelexicalisation of the input. However, this often requires that the input\nappears verbatim in the output text. This poses challenges in multilingual\nsettings, where the task expands to generate the output text in multiple\nlanguages given the same input. In this paper, we explore the application of\nmultilingual models in concept-to-text and propose Language Agnostic\nDelexicalisation, a novel delexicalisation method that uses multilingual\npretrained embeddings, and employs a character-level post-editing model to\ninflect words in their correct form during relexicalisation. Our experiments\nacross five datasets and five languages show that multilingual models\noutperform monolingual models in concept-to-text and that our framework\noutperforms previous approaches, especially for low resource languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1\">Giulio Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lampouras_G/0/1/0/all/0/1\">Gerasimos Lampouras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FNet: Mixing Tokens with Fourier Transforms. (arXiv:2105.03824v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.03824","description":"<p>We show that Transformer encoder architectures can be sped up, with limited\naccuracy costs, by replacing the self-attention sublayers with simple linear\ntransformations that \"mix\" input tokens. These linear mixers, along with\nstandard nonlinearities in feed-forward layers, prove competent at modeling\nsemantic relationships in several text classification tasks. Most surprisingly,\nwe find that replacing the self-attention sublayer in a Transformer encoder\nwith a standard, unparameterized Fourier Transform achieves 92-97% of the\naccuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on\nGPUs and 70% faster on TPUs at standard 512 input lengths. At longer input\nlengths, our FNet model is significantly faster: when compared to the\n\"efficient\" Transformers on the Long Range Arena benchmark, FNet matches the\naccuracy of the most accurate models, while outpacing the fastest models across\nall sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\nFinally, FNet has a light memory footprint and is particularly efficient at\nsmaller model sizes; for a fixed speed and accuracy budget, small FNet models\noutperform Transformer counterparts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1\">Ilya Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ontanon_S/0/1/0/all/0/1\">Santiago Ontanon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DRIFT: A Toolkit for Diachronic Analysis of Scientific Literature. (arXiv:2107.01198v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.01198","description":"<p>In this work, we present to the NLP community, and to the wider research\ncommunity as a whole, an application for the diachronic analysis of research\ncorpora. We open source an easy-to-use tool coined: DRIFT, which allows\nresearchers to track research trends and development over the years. The\nanalysis methods are collated from well-cited research works, with a few of our\nown methods added for good measure. Succinctly put, some of the analysis\nmethods are: keyword extraction, word clouds, predicting\ndeclining/stagnant/growing trends using Productivity, tracking bi-grams using\nAcceleration plots, finding the Semantic Drift of words, tracking trends using\nsimilarity, etc. To demonstrate the utility and efficacy of our tool, we\nperform a case study on the cs.CL corpus of the arXiv repository and draw\ninferences from the analysis methods. The toolkit and the associated code are\navailable here: https://github.com/rajaswa/DRIFT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Abheesht Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chhablani_G/0/1/0/all/0/1\">Gunjan Chhablani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pandey_H/0/1/0/all/0/1\">Harshit Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patil_R/0/1/0/all/0/1\">Rajaswa Patil</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Opinion Prediction with User Fingerprinting. (arXiv:2108.00270v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.00270","description":"<p>Opinion prediction is an emerging research area with diverse real-world\napplications, such as market research and situational awareness. We identify\ntwo lines of approaches to the problem of opinion prediction. One uses\ntopic-based sentiment analysis with time-series modeling, while the other uses\nstatic embedding of text. The latter approaches seek user-specific solutions by\ngenerating user fingerprints. Such approaches are useful in predicting user's\nreactions to unseen content. In this work, we propose a novel dynamic\nfingerprinting method that leverages contextual embedding of user's comments\nconditioned on relevant user's reading history. We integrate BERT variants with\na recurrent neural network to generate predictions. The results show up to 13\\%\nimprovement in micro F1-score compared to previous approaches. Experimental\nresults show novel insights that were previously unknown such as better\npredictions for an increase in dynamic history length, the impact of the nature\nof the article on performance, thereby laying the foundation for further\nresearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tumarada_K/0/1/0/all/0/1\">Kishore Tumarada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yifan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fan Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dragut_E/0/1/0/all/0/1\">Eduard Dragut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gnawali_O/0/1/0/all/0/1\">Omprakash Gnawali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1\">Arjun Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BROS: A Pre-trained Language Model Focusing on Text and Layout for Better Key Information Extraction from Documents. (arXiv:2108.04539v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04539","description":"<p>Key information extraction (KIE) from document images requires understanding\nthe contextual and spatial semantics of texts in two-dimensional (2D) space.\nMany recent studies try to solve the task by developing pre-training language\nmodels focusing on combining visual features from document images with texts\nand their layout. On the other hand, this paper tackles the problem by going\nback to the basic: effective combination of text and layout. Specifically, we\npropose a pre-trained language model, named BROS (BERT Relying On Spatiality),\nthat encodes relative positions of texts in 2D space and learns from unlabeled\ndocuments with area-masking strategy. With this optimized training scheme for\nunderstanding texts in 2D space, BROS shows comparable or better performance\ncompared to previous methods on four KIE benchmarks (FUNSD, SROIE*, CORD, and\nSciTSR) without relying on visual features. This paper also reveals two\nreal-world challenges in KIE tasks--(1) minimizing the error from incorrect\ntext ordering and (2) efficient learning from fewer downstream examples--and\ndemonstrates the superiority of BROS over previous methods. Our code will be\nopen to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bursting Scientific Filter Bubbles: Boosting Innovation via Novel Author Discovery. (arXiv:2108.05669v2 [cs.DL] UPDATED)","link":"http://arxiv.org/abs/2108.05669","description":"<p>Isolated silos of scientific research and the growing challenge of\ninformation overload limit awareness across the literature and hinder\ninnovation. Algorithmic curation and recommendation, which often prioritize\nrelevance, can further reinforce these informational \"filter bubbles.\" In\nresponse, we describe Bridger, a system for facilitating discovery of scholars\nand their work, to explore design tradeoffs between relevant and novel\nrecommendations. We construct a faceted representation of authors with\ninformation gleaned from their papers and inferred author personas, and use it\nto develop an approach that locates commonalities (\"bridges\") and contrasts\nbetween scientists -- retrieving partially similar authors rather than aiming\nfor strict similarity. In studies with computer science researchers, this\napproach helps users discover authors considered useful for generating novel\nresearch directions, outperforming a state-of-art neural model. In addition to\nrecommending new content, we also demonstrate an approach for displaying it in\na manner that boosts researchers' ability to understand the work of authors\nwith whom they are unfamiliar. Finally, our analysis reveals that Bridger\nconnects authors who have different citation profiles, publish in different\nvenues, and are more distant in social co-authorship networks, raising the\nprospect of bridging diverse communities and facilitating discovery.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portenoy_J/0/1/0/all/0/1\">Jason Portenoy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radensky_M/0/1/0/all/0/1\">Marissa Radensky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_J/0/1/0/all/0/1\">Jevin West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"De-identification of Unstructured Clinical Texts from Sequence to Sequence Perspective. (arXiv:2108.07971v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.07971","description":"<p>In this work, we propose a novel problem formulation for de-identification of\nunstructured clinical text. We formulate the de-identification problem as a\nsequence to sequence learning problem instead of a token classification\nproblem. Our approach is inspired by the recent state-of -the-art performance\nof sequence to sequence learning models for named entity recognition. Early\nexperimentation of our proposed approach achieved 98.91% recall rate on i2b2\ndataset. This performance is comparable to current state-of-the-art models for\nunstructured clinical text de-identification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Anjum_M/0/1/0/all/0/1\">Md Monowar Anjum</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1\">Noman Mohammed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xiaoqian Jiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Retrieval Augmented Code Generation and Summarization. (arXiv:2108.11601v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2108.11601","description":"<p>Software developers write a lot of source code and documentation during\nsoftware development. Intrinsically, developers often recall parts of source\ncode or code summaries that they had written in the past while implementing\nsoftware or documenting them. To mimic developers' code or summary generation\nbehavior, we propose a retrieval augmented framework, REDCODER, that retrieves\nrelevant code or summaries from a retrieval database and provides them as a\nsupplement to code generation or summarization models. REDCODER has a couple of\nuniqueness. First, it extends the state-of-the-art dense retrieval technique to\nsearch for relevant code or summaries. Second, it can work with retrieval\ndatabases that include unimodal (only code or natural language description) or\nbimodal instances (code-description pairs). We conduct experiments and\nextensive analysis on two benchmark datasets of code generation and\nsummarization in Java and Python, and the promising results endorse the\neffectiveness of our proposed retrieval augmented framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Parvez_M/0/1/0/all/0/1\">Md Rizwan Parvez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmad_W/0/1/0/all/0/1\">Wasi Uddin Ahmad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1\">Saikat Chakraborty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ray_B/0/1/0/all/0/1\">Baishakhi Ray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12229","description":"<p>The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications. For example, intent\nclassification in dialogue systems. The reason is that the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current methods can tackle such problems\nreliably in a realistic scenario where zero OOD training data is available. In\nthis study, we propose ProtoInfoMax, a new architecture that extends\nPrototypical Networks to simultaneously process in-domain and OOD sentences via\nMutual Information Maximization (InfoMax) objective. Experimental results show\nthat our proposed method can substantially improve performance up to 20% for\nOOD detection in low resource settings of text classification. We also show\nthat ProtoInfoMax is less prone to typical overconfidence errors of Neural\nNetworks, leading to more reliable prediction results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SummerTime: Text Summarization Toolkit for Non-experts. (arXiv:2108.12738v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12738","description":"<p>Recent advances in summarization provide models that can generate summaries\nof higher quality. Such models now exist for a number of summarization tasks,\nincluding query-based summarization, dialogue summarization, and multi-document\nsummarization. While such models and tasks are rapidly growing in the research\nfield, it has also become challenging for non-experts to keep track of them. To\nmake summarization methods more accessible to a wider audience, we develop\nSummerTime by rethinking the summarization task from the perspective of an NLP\nnon-expert. SummerTime is a complete toolkit for text summarization, including\nvarious models, datasets and evaluation metrics, for a full spectrum of\nsummarization-related tasks. SummerTime integrates with libraries designed for\nNLP researchers, and enables users with easy-to-use APIs. With SummerTime,\nusers can locate pipeline solutions and search for the best model with their\nown data, and visualize the differences, all with a few lines of code. We also\nprovide explanations for models and evaluation metrics to help users understand\nthe model behaviors and select models that best suit their needs. Our library,\nalong with a notebook demo, is available at\nhttps://github.com/Yale-LILY/SummerTime.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1\">Ansong Ni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Azerbayev_Z/0/1/0/all/0/1\">Zhangir Azerbayev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mutuma_M/0/1/0/all/0/1\">Mutethia Mutuma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1\">Troy Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yusen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Awadallah_A/0/1/0/all/0/1\">Ahmed Hassan Awadallah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1\">Dragomir Radev</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v2 [cs.PL] UPDATED)","link":"http://arxiv.org/abs/2108.12971","description":"<p>A smart contract is a program executed on a blockchain, based on which many\ncryptocurrencies are implemented, and is being used for automating\ntransactions. Due to the large amount of money that smart contracts deal with,\nthere is a surging demand for a method that can statically and formally verify\nthem.\n</p>\n<p>This article describes our type-based static verification tool HELMHOLTZ for\nMichelson, which is a statically typed stack-based language for writing smart\ncontracts that are executed on the blockchain platform Tezos. HELMHOLTZ is\ndesigned on top of our extension of Michelson's type system with refinement\ntypes. HELMHOLTZ takes a Michelson program annotated with a user-defined\nspecification written in the form of a refinement type as input; it then\ntypechecks the program against the specification based on the refinement type\nsystem, discharging the generated verification conditions with the SMT solver\nZ3. We briefly introduce our refinement type system for the core calculus\nMini-Michelson of Michelson, which incorporates the characteristic features\nsuch as compound datatypes (e.g., lists and pairs), higher-order functions, and\ninvocation of another contract. \\HELMHOLTZ{} successfully verifies several\npractical Michelson programs, including one that transfers money to an account\nand that checks a digital signature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_Y/0/1/0/all/0/1\">Yuki Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiromasa Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawata_A/0/1/0/all/0/1\">Akira Kawata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuse_J/0/1/0/all/0/1\">Jun Furuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_A/0/1/0/all/0/1\">Atsushi Igarashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Search Engine for Discovery of Scientific Challenges and Directions. (arXiv:2108.13751v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13751","description":"<p>Keeping track of scientific challenges, advances and emerging directions is a\nfundamental part of research. However, researchers face a flood of papers that\nhinders discovery of important knowledge. In biomedicine, this directly impacts\nhuman lives. To address this problem, we present a novel task of extraction and\nsearch of scientific challenges and directions, to facilitate rapid knowledge\ndiscovery. We construct and release an expert-annotated corpus of texts sampled\nfrom full-length papers, labeled with novel semantic categories that generalize\nacross many types of challenges and directions. We focus on a large corpus of\ninterdisciplinary work relating to the COVID-19 pandemic, ranging from\nbiomedicine to areas such as AI and economics. We apply a model trained on our\ndata to identify challenges and directions across the corpus and build a\ndedicated search engine. In experiments with 19 researchers and clinicians\nusing our system, we outperform a popular scientific search engine in assisting\nknowledge discovery. Finally, we show that models trained on our resource\ngeneralize to the wider biomedical domain and to AI papers, highlighting its\nbroad utility. We make our data, model and search engine publicly available.\nhttps://challenges.apps.allenai.org/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahav_D/0/1/0/all/0/1\">Dan Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_J/0/1/0/all/0/1\">Jon Saad Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shomron_N/0/1/0/all/0/1\">Noam Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Supervised Contrastive Learning for Multimodal Unreliable News Detection in COVID-19 Pandemic. (arXiv:2109.01850v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01850","description":"<p>As the digital news industry becomes the main channel of information\ndissemination, the adverse impact of fake news is explosively magnified. The\ncredibility of a news report should not be considered in isolation. Rather,\npreviously published news articles on the similar event could be used to assess\nthe credibility of a news report. Inspired by this, we propose a BERT-based\nmultimodal unreliable news detection framework, which captures both textual and\nvisual information from unreliable articles utilising the contrastive learning\nstrategy. The contrastive learner interacts with the unreliable news classifier\nto push similar credible news (or similar unreliable news) closer while moving\nnews articles with similar content but opposite credibility labels away from\neach other in the multimodal embedding space. Experimental results on a\nCOVID-19 related dataset, ReCOVery, show that our model outperforms a number of\ncompetitive baseline in unreliable news detection.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wenjia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_L/0/1/0/all/0/1\">Lin Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Don't Go Far Off: An Empirical Study on Neural Poetry Translation. (arXiv:2109.02972v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02972","description":"<p>Despite constant improvements in machine translation quality, automatic\npoetry translation remains a challenging problem due to the lack of\nopen-sourced parallel poetic corpora, and to the intrinsic complexities\ninvolved in preserving the semantics, style, and figurative nature of poetry.\nWe present an empirical investigation for poetry translation along several\ndimensions: 1) size and style of training data (poetic vs. non-poetic),\nincluding a zero-shot setup; 2) bilingual vs. multilingual learning; and 3)\nlanguage-family-specific models vs. mixed-multilingual models. To accomplish\nthis, we contribute a parallel dataset of poetry translations for several\nlanguage pairs. Our results show that multilingual fine-tuning on poetic text\nsignificantly outperforms multilingual fine-tuning on non-poetic text that is\n35X larger in size, both in terms of automatic metrics (BLEU, BERTScore) and\nhuman evaluation metrics such as faithfulness (meaning and poetic style).\nMoreover, multilingual fine-tuning on poetic data outperforms \\emph{bilingual}\nfine-tuning on poetic data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1\">Arkadiy Saakyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Idiosyncratic but not Arbitrary: Learning Idiolects in Online Registers Reveals Distinctive yet Consistent Individual Styles. (arXiv:2109.03158v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03158","description":"<p>An individual's variation in writing style is often a function of both social\nand personal attributes. While structured social variation has been extensively\nstudied, e.g., gender based variation, far less is known about how to\ncharacterize individual styles due to their idiosyncratic nature. We introduce\na new approach to studying idiolects through a massive cross-author comparison\nto identify and encode stylistic features. The neural model achieves strong\nperformance at authorship identification on short texts and through an\nanalogy-based probing task, showing that the learned representations exhibit\nsurprising regularities that encode qualitative and quantitative shifts of\nidiolectal styles. Through text perturbation, we quantify the relative\ncontributions of different linguistic elements to idiolectal variation.\nFurthermore, we provide a description of idiolects through measuring inter- and\nintra-author variation, showing that variation in idiolects is often\ndistinctive yet consistent.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jian Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1\">David Jurgens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge mining of unstructured information: application to cyber-domain. (arXiv:2109.03848v2 [cs.CR] UPDATED)","link":"http://arxiv.org/abs/2109.03848","description":"<p>Cyber intelligence is widely and abundantly available in numerous open online\nsources with reports on vulnerabilities and incidents. This constant stream of\nnoisy information requires new tools and techniques if it is to be used for the\nbenefit of analysts and investigators in various organizations. In this paper\nwe present and implement a novel knowledge graph and knowledge mining framework\nfor extracting relevant information from free-form text about incidents in the\ncyber domain. Our framework includes a machine learning based pipeline as well\nas crawling methods for generating graphs of entities, attackers and the\nrelated information with our non-technical cyber ontology. We test our\nframework on publicly available cyber incident datasets to evaluate the\naccuracy of our knowledge mining methods as well as the usefulness of the\nframework in the use of cyber analysts. Our results show analyzing the\nknowledge graph constructed using the novel framework, an analyst can infer\nadditional information from the current cyber landscape in terms of risk to\nvarious entities and the propagation of risk between industries and countries.\nExpanding the framework to accommodate more technical and operational level\ninformation can increase the accuracy and explainability of trends and risk in\nthe knowledge graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Takko_T/0/1/0/all/0/1\">Tuomas Takko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhattacharya_K/0/1/0/all/0/1\">Kunal Bhattacharya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lehto_M/0/1/0/all/0/1\">Martti Lehto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jalasvirta_P/0/1/0/all/0/1\">Pertti Jalasvirta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cederberg_A/0/1/0/all/0/1\">Aapo Cederberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaski_K/0/1/0/all/0/1\">Kimmo Kaski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Collecting a Large-Scale Gender Bias Dataset for Coreference Resolution and Machine Translation. (arXiv:2109.03858v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03858","description":"<p>Recent works have found evidence of gender bias in models of machine\ntranslation and coreference resolution using mostly synthetic diagnostic\ndatasets. While these quantify bias in a controlled experiment, they often do\nso on a small scale and consist mostly of artificial, out-of-distribution\nsentences. In this work, we find grammatical patterns indicating stereotypical\nand non-stereotypical gender-role assignments (e.g., female nurses versus male\ndancers) in corpora from three domains, resulting in a first large-scale gender\nbias dataset of 108K diverse real-world English sentences. We manually verify\nthe quality of our corpus and use it to evaluate gender bias in various\ncoreference resolution and machine translation models. We find that all tested\nmodels tend to over-rely on gender stereotypes when presented with natural\ninputs, which may be especially harmful when deployed in commercial systems.\nFinally, we show that our dataset lends itself to finetuning a coreference\nresolution model, finding it mitigates bias on a held out set. Our dataset and\nmodels are publicly available at www.github.com/SLAB-NLP/BUG. We hope they will\nspur future research into gender bias evaluation mitigation techniques in\nrealistic settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Shahar Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lazar_K/0/1/0/all/0/1\">Koren Lazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1\">Gabriel Stanovsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Speech Recognition for Low-Resource Indian Languages using Multi-Task conformer. (arXiv:2109.03969v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03969","description":"<p>Transformers have recently become very popular for sequence-to-sequence\napplications such as machine translation and speech recognition. In this work,\nwe propose a multi-task learning-based transformer model for low-resource\nmultilingual speech recognition for Indian languages. Our proposed model\nconsists of a conformer [1] encoder and two parallel transformer decoders. We\nuse a phoneme decoder (PHN-DEC) for the phoneme recognition task and a grapheme\ndecoder (GRP-DEC) to predict grapheme sequence. We consider the phoneme\nrecognition task as an auxiliary task for our multi-task learning framework. We\njointly optimize the network for both phoneme and grapheme recognition tasks\nusing Joint CTC-Attention [2] training. We use a conditional decoding scheme to\ninject the language information into the model before predicting the grapheme\nsequence. Our experiments show that our proposed approach can obtain\nsignificant improvement over previous approaches [4]. We also show that our\nconformer-based dual-decoder approach outperforms both the transformer-based\ndual-decoder approach and single decoder approach. Finally, We compare\nmonolingual ASR models with our proposed multilingual ASR approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+N_K/0/1/0/all/0/1\">Krishna D N</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer for Text Classification with Dictionary-based Heterogeneous Graph. (arXiv:2109.04400v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04400","description":"<p>In cross-lingual text classification, it is required that task-specific\ntraining data in high-resource source languages are available, where the task\nis identical to that of a low-resource target language. However, collecting\nsuch training data can be infeasible because of the labeling cost, task\ncharacteristics, and privacy concerns. This paper proposes an alternative\nsolution that uses only task-independent word embeddings of high-resource\nlanguages and bilingual dictionaries. First, we construct a dictionary-based\nheterogeneous graph (DHG) from bilingual dictionaries. This opens the\npossibility to use graph neural networks for cross-lingual transfer. The\nremaining challenge is the heterogeneity of DHG because multiple languages are\nconsidered. To address this challenge, we propose dictionary-based\nheterogeneous graph neural network (DHGNet) that effectively handles the\nheterogeneity of DHG by two-step aggregations, which are word-level and\nlanguage-level aggregations. Experimental results demonstrate that our method\noutperforms pretrained models even though it does not access to large corpora.\nFurthermore, it can perform well even though dictionaries contain many\nincorrect translations. Its robustness allows the usage of a wider range of\ndictionaries such as an automatically constructed dictionary and crowdsourced\ndictionary, which are convenient for real-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chairatanakul_N/0/1/0/all/0/1\">Nuttapong Chairatanakul</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriwatanasakdi_N/0/1/0/all/0/1\">Noppayut Sriwatanasakdi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charoenphakdee_N/0/1/0/all/0/1\">Nontawat Charoenphakdee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murata_T/0/1/0/all/0/1\">Tsuyoshi Murata</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-12T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/"}}]}]}