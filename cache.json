{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-01T01:56:19.980087076Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ã€å…¨èŒè¿œç¨‹ã€‘15K-30K/ç¡…è°·æ— ç AIå›¢é˜Ÿæ‹›/åç«¯å¼€å‘å·¥ç¨‹å¸ˆ/Java or Scala","link":"https://rustcc.cn/article?id=6c2bed71-6adf-45ac-bd60-904427c5c51c","description":"<p>å…¬å¸ç®€ä»‹\næˆ‘ä»¬æ˜¯ä¸€ä¸ªåˆšåˆšæˆç«‹åœ¨ä¸Šæµ·çš„åˆ›ä¸šå…¬å¸ã€‚åˆ›å§‹äººåœ¨ç¡…è°·ï¼Œå…¨å‘˜è¿œç¨‹ï¼Œè‡´åŠ›äºæ‰“é€ ä¸‹ä¸€ä»£æ— ç AIæ•°æ®äº§å“ï¼ŒæŠ€æœ¯æ°›å›´æµ“åšã€‚ç›®å‰å’Œå›½å†…ä¸€å®¶çŸ¥åCDPå¹³å°å‚å•†åˆç†æ¨å‡ºäº†ç¬¬ä¸€ç‰ˆæ— ä»£ç æœºå™¨å­¦ä¹ é¢„æµ‹å¹³å°ï¼Œå¸®åŠ©ä¼ä¸šå®ç°è¿è¥çš„æ™ºèƒ½åŒ–ã€‚</p>\n<p>å·¥ä½œèŒè´£\n1ã€è´Ÿè´£æœºå™¨å­¦ä¹ å¹³å°çš„åç«¯æŠ€æœ¯æ¶æ„;</p>\n<p>2ã€è´Ÿè´£æœºå™¨å­¦ä¹ å¹³å°çš„åç«¯ä»£ç å®ç°åŠå•å…ƒæµ‹è¯•ï¼›</p>\n<p>3ã€è´Ÿè´£ç¼–å†™æŠ€æœ¯è®¾è®¡æ–‡æ¡£ã€APIæ–‡æ¡£ã€‚</p>\n<p>ä»»èŒè¦æ±‚\n1ã€è®¡ç®—æœºç›¸å…³ä¸“ä¸šï¼Œæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼›</p>\n<p>2ã€ ç†Ÿç»ƒä½¿ç”¨Java æˆ– Scala å¼€å‘ï¼Œ5å¹´ä»¥ä¸Šçš„å¼€å‘ç»éªŒï¼›</p>\n<p>3ã€ç†Ÿç»ƒä½¿ç”¨SprintBootï¼Œç†Ÿæ‚‰ç›¸å…³çš„ç”Ÿæ€å’Œä½¿ç”¨æ–¹æ³•ï¼›</p>\n<p>4ã€æœ‰å¤§æ•°æ®å¤„ç†ç»éªŒä¼˜å…ˆã€‚</p>\n<p>å…³äºæ²Ÿé€š\n1ã€ä½¿ç”¨é£ä¹¦ä½œä¸ºæ²Ÿé€šå’Œæ–‡æ¡£å·¥å…·ï¼›</p>\n<p>2ã€æ¯å¤©ä¸Šåˆ 9 ç‚¹ï¼ˆå†¬ä»¤æ—¶ï¼Œå¤ä»¤æ—¶æ˜¯ ä¸Šåˆ 8:30 ï¼‰ä¼šæœ‰ç®€å•çš„åŒæ­¥ï¼›</p>\n<p>3ã€æ¯å¤©å†™æ—¥æŠ¥ï¼Œè¯´æ˜ä»»åŠ¡çš„è¿›åº¦ï¼Œä»¥åŠå‘ç°å“ªäº›é—®é¢˜å’Œéœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼›</p>\n<p>4ã€æ¯ä¸ª Sprint ä¼šèŠ±æ—¶é—´æ‹†è§£ Story å’Œåˆ†é…ä»»åŠ¡ï¼Œéœ€è¦å„è‡ªåˆ†æå‡ºå„ä¸ªéœ€æ±‚ç‚¹å’Œå…³é”®ç‚¹ï¼Œå‘ç°é£é™©å’Œä¸ç¡®å®šçš„åœ°æ–¹åŠæ—©ç¡®è®¤ã€‚</p>\n<p>è–ªèµ„å¾…é‡\n15K-30Kï¼Œå…¨èŒï¼ˆä¸æ¥å—å…¼èŒï¼‰ï¼Œæä¾›äº”é™©ä¸€é‡‘ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-01 01:52:23","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Mike Tang å¼ æ±‰ä¸œ è€æ²¹æ¡å„ä½å¤§ä½¬è¯·è¿›ï¼Œç´ æ•°å¤šçº¿ç¨‹é—®é¢˜","link":"https://rustcc.cn/article?id=b10b7a68-e2bf-42b6-a583-ef99478e50d3","description":"<p>å„ä½å¤§ä½¬ï¼š</p>\n<p>æ˜¨å¤©è…¾è®¯è§†é¢‘èŠçš„æ¯”è¾ƒå¼€å¿ƒï¼Œç•™äº†ä¸€ä¸ªå°¾å·´ã€‚æˆ‘ç°åœ¨æŠŠæˆ‘çš„å›°æƒ‘æ”¾å‡ºæ¥ï¼Œå¤§å®¶å°½æƒ…æ‹ç –ã€‚</p>\n<p>è¿™ä¸ªæ˜¯æˆ‘å†™çš„æ–‡ç« ï¼Œæ–‡ç« åˆ†äº†ä¸¤ç«  ï¼š</p>\n<p><a href=\"https://github.com/sunnyrust/rustBible/blob/master/books/6.2.md\" rel=\"noopener noreferrer\">6.2 å¤šçº¿ç¨‹â€”â€”channel</a></p>\n<p><a href=\"https://github.com/sunnyrust/rustBible/blob/master/books/6.3.md\" rel=\"noopener noreferrer\">6.3 å¤šçº¿ç¨‹â€”â€”future</a></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-01 00:56:14","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-31 Rust edition","link":"https://rustcc.cn/article?id=d6a7f8d7-4d99-4488-a933-1d70cefcd2cd","description":"<h2><code>delicate</code> åˆ†å¸ƒå¼è°ƒåº¦ç³»ç»Ÿ</h2>\n<p>å‘¨äºŒçš„å” å—‘å®¤ä¸»é¢˜\nä¸»æŒäººï¼šæ§Ÿæ©™ç‚®ç‚®\nç®€ä»‹ï¼šRustç¤¾åŒºä¹‹å‰æ²¡æœ‰æ´»è·ƒçš„åˆ†å¸ƒå¼è°ƒåº¦ç³»ç»Ÿé¡¹ç›®ï¼Œä¸ºäº†å¡«è¡¥è¿™ä¸ªç©ºç™½æˆ‘å¼€å§‹è°ƒç ”å®ç°é¡¹ç›®ï¼Œç›®å‰å·²ç»å¿«è¦å‘å¸ƒV1.1äº†ã€‚</p>\n<p>åœ¨é¡¹ç›®è®¾è®¡ä¸åº•å±‚åº“çš„å®ç°ä»smolå¥—ä»¶ä¸­è·å¾—äº†å¾ˆå¤šçµæ„Ÿï¼Œä¹Ÿä¼šç®€å•è·Ÿå¤§å®¶ä»‹ç»ä¸‹ smol &amp; tokio ä¸€äº›å„è‡ªçš„è®¾è®¡å“²å­¦ï¼Œasync-process async-io async-task ä¸€äº›æ¼‚äº®çš„ä»£ç ç‰‡æ®µã€‚</p>\n<ul>\n<li>æ–‡æ¡£åœ°å€ï¼šhttps://delicate-rs.github.io/</li>\n<li>æºç ï¼šhttps://github.com/BinChengZhao/delicate</li>\n</ul>\n<h2>datafusion-5.0.0 å‘å¸ƒ</h2>\n<p>datafusion æ˜¯åŸºäº Apache Arrow åˆ—æ ¼å¼ã€ä½¿ç”¨ Rust å®ç°çš„å¯æ‰©å±•æŸ¥è¯¢æ‰§è¡Œæ¡†æ¶ï¼Œæ”¯æŒSQL å’Œ DataFrame APIï¼Œä¹Ÿå¯ä»¥é€šè¿‡ ballista crate(ä¹Ÿå‘å¸ƒäº† v0.5.0) æ”¯æŒåˆ†å¸ƒå¼æŸ¥è¯¢</p>\n<ul>\n<li>å‘å¸ƒé¡µï¼šhttps://arrow.apache.org/blog/2021/08/18/datafusion-5.0.0/</li>\n<li>ä»“åº“ï¼š https://github.com/apache/arrow-datafusion</li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-31 14:24:01","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€åŸåˆ›ã€‘Rust tokio å¦‚ä½•ä»¥å¼‚æ­¥éé˜»å¡æ–¹å¼è¿è¡Œå¤§é‡ä»»åŠ¡","link":"https://rustcc.cn/article?id=ba4f86c6-667d-4acb-89a1-e2fb0617f524","description":"<p>tokio å®˜æ–¹ç»™äº†ä¸€ä¸ªå®Œæ•´çš„<a href=\"https://tokio.rs/tokio/topics/bridging#spawning-things-on-a-runtime\" rel=\"noopener noreferrer\">ä¾‹å­</a>ï¼šæ‰‹åŠ¨æ„å»º runtime ï¼Œåˆ©ç”¨ block_on æ¥è¿è¡Œå¤šä¸ªä»»åŠ¡ã€‚\ntokio çš„ä»»åŠ¡æ˜¯ç”± <code>tokio::spawn</code> ä¹‹ç±»çš„å‡½æ•°äº§ç”Ÿçš„ <code>JoinHandle</code> ç±»å‹ï¼Œè€Œä¸”æ˜¯ä¸ª <code>Future</code> ã€‚</p>\n<p>è€Œä¸‹é¢åˆ©ç”¨ <code>#[tokio::main]</code> å’Œ await ç¼–å†™äº†ç­‰ä»·çš„ç‰ˆæœ¬ï¼ˆä¸ºäº†ç›´è§‚å¯¹æ¯”ä»»åŠ¡å®Œæˆçš„å®é™…é¡ºåºå’Œæ€»è€—æ—¶ï¼Œæˆ‘å¯¹ sleep çš„æ—¶é—´åšäº†ä¸€äº›ç®€åŒ–ï¼‰ï¼š</p>\n<pre><code>use std::time::Instant;\nuse tokio::time::{sleep, Duration};\n\n#[tokio::main]\nasync fn main() -&gt; std::io::Result&lt;()&gt; {\n    let now = Instant::now();\n\n    let mut handles = Vec::with_capacity(10);\n    for i in 0..10 {\n        handles.push(tokio::spawn(my_bg_task(i)));\n    }\n\n    // Do something time-consuming while the background tasks execute.\n    std::thread::sleep(Duration::from_millis(120));\n    println!(\"Finished time-consuming task.\");\n\n    // Wait for all of them to complete.\n    for handle in handles {\n        handle.await?;\n    }\n\n    println!(\"æ€»è€—æ—¶ï¼š{} ms\", now.elapsed().as_millis());\n    Ok(())\n}\n\nasync fn my_bg_task(i: u64) {\n    let millis = 100;\n    println!(\"Task {} sleeping for {} ms.\", i, millis);\n    sleep(Duration::from_millis(millis)).await;\n    println!(\"Task {} stopping.\", i);\n}\n</code></pre>\n<p>è¾“å‡ºç»“æœï¼š</p>\n<pre><code>Task 0 sleeping for 100 ms.\nTask 1 sleeping for 100 ms.\nTask 2 sleeping for 100 ms.\nTask 3 sleeping for 100 ms.\nTask 4 sleeping for 100 ms.\nTask 5 sleeping for 100 ms.\nTask 6 sleeping for 100 ms.\nTask 7 sleeping for 100 ms.\nTask 8 sleeping for 100 ms.\nTask 9 sleeping for 100 ms.\nTask 9 stopping.\nTask 0 stopping.\nTask 1 stopping.\nTask 2 stopping.\nTask 3 stopping.\nTask 4 stopping.\nTask 5 stopping.\nTask 6 stopping.\nTask 7 stopping.\nTask 8 stopping.\nFinished time-consuming task.\næ€»è€—æ—¶ï¼š120 ms\n</code></pre>\n<p>å¦‚æœæŠŠä¸»çº¿ç¨‹çš„çš„ sleep æ—¶é—´æ”¹æˆ 100 msï¼š<code>std::thread::sleep(Duration::from_millis(100));</code>\nåˆ™äº§ç”Ÿä¸‹é¢çš„ç»“æœï¼š</p>\n<pre><code>Task 0 sleeping for 100 ms.\nTask 1 sleeping for 100 ms.\nTask 2 sleeping for 100 ms.\nTask 3 sleeping for 100 ms.\nTask 4 sleeping for 100 ms.\nTask 5 sleeping for 100 ms.\nTask 6 sleeping for 100 ms.\nTask 7 sleeping for 100 ms.\nTask 8 sleeping for 100 ms.\nTask 9 sleeping for 100 ms.\nFinished time-consuming task.\nTask 3 stopping.\nTask 0 stopping.\nTask 1 stopping.\nTask 2 stopping.\nTask 9 stopping.\nTask 4 stopping.\nTask 5 stopping.\nTask 6 stopping.\nTask 7 stopping.\nTask 8 stopping.\næ€»è€—æ—¶ï¼š103 ms\n</code></pre>\n<p>å¯ä»¥çœ‹åˆ°ï¼Œ<code>my_bg_task</code> å®é™…æ˜¯å¼‚æ­¥éé˜»å¡æ‰§è¡Œçš„ ğŸ‘ ï¼š</p>\n<ul>\n<li>å¼‚æ­¥ï¼šå› ä¸ºæ¯ä¸ªä»»åŠ¡ä¸å¿…ç­‰å¾…å…¶ç»“æœå°±å¯ä»¥å¼€å§‹ä¸‹ä¸€ä¸ªä»»åŠ¡ï¼Œå³ï¼›</li>\n</ul>\n<pre><code>// å¼‚æ­¥\nTask 0 sleeping for 100 ms.\nTask 1 sleeping for 100 ms.\n...\n\n// åŒæ­¥\nTask 0 sleeping for 100 ms.\nTask 0 stopping.\nTask 1 sleeping for 100 ms.\nTask 1 stopping.\n...\n</code></pre>\n<ul>\n<li>éé˜»å¡ï¼šæ¯ä¸ªä»»åŠ¡ä¹‹é—´å¯ä»¥å¿«é€Ÿåˆ‡æ¢ï¼Œä¸å¿…ç­‰å¾…å…¶ä»–ä»»åŠ¡å®Œæˆæ‰åˆ‡æ¢ï¼Œè¿™ä¸ªä¾‹å­è¡¨ç°åœ¨ï¼š\n<ul>\n<li>ä»»åŠ¡ 0-9 ä»¥ä¹±åºæ–¹å¼ stop</li>\n<li><code>Finished time-consuming task.</code> ä¸ <code>Task x stopping.</code>  çš„æ‰“å°é¡ºåºåªä¸ä»»åŠ¡å„è‡ªçš„è¿è¡Œ (sleep) æ—¶é—´æœ‰å…³ï¼Œä¸æºä»£ç çš„å£°æ˜æ‰§è¡Œé¡ºåºæ— å…³ã€‚åªæœ‰ä»»åŠ¡ä¹‹é—´å¿«é€Ÿåˆ‡æ¢æ‰èƒ½åšåˆ°è¿™ä¸€ç‚¹ã€‚å›é¡¾å®˜ç½‘çš„ä¾‹å­ï¼š10 ä¸ªä»»åŠ¡çš„ sleep æ—¶é—´çº¿æ€§é€’å‡ ï¼ˆ<code>let millis = 1000 - 50 * i;</code>ï¼‰ï¼Œä» 6 ä¸ªä»»åŠ¡å¼€å§‹å°äºä¸»çº¿ç¨‹ sleep ä»»åŠ¡çš„æ—¶é—´ï¼ˆ750 msï¼‰ï¼Œè€Œç­‰å¾… 10 ä¸ªä»»åŠ¡æ‰§è¡Œçš„è¯­å¥ <code>for handle in handles { ... }</code> æ˜¾ç„¶ä½äº <code>std::thread::sleep</code> ä¹‹åï¼Œæ‰€ä»¥ä»»åŠ¡ä¹‹é—´éé˜»å¡æ‰§è¡Œçš„è¯ï¼Œæ‰“å°ç»“æœä¸º sleep æ—¶é—´è¶ŠçŸ­çš„ä»»åŠ¡å…ˆå®Œæˆï¼Œæ—¶é—´è¶Šé•¿çš„ä»»åŠ¡åå®Œæˆï¼Œæ€»è€—æ—¶ä¸ºä»»åŠ¡ä¸­çš„æœ€é•¿è€—æ—¶ï¼š</li>\n</ul>\n</li>\n</ul>\n<pre><code>Task 0 sleeping for 1000 ms.\nTask 1 sleeping for 950 ms.\nTask 2 sleeping for 900 ms.\nTask 3 sleeping for 850 ms.\nTask 4 sleeping for 800 ms.\nTask 5 sleeping for 750 ms.\nTask 6 sleeping for 700 ms.\nTask 7 sleeping for 650 ms.\nTask 8 sleeping for 600 ms.\nTask 9 sleeping for 550 ms.\nTask 9 stopping.\nTask 8 stopping.\nTask 7 stopping.\nTask 6 stopping.\nFinished time-consuming task.\nTask 5 stopping.\nTask 4 stopping.\nTask 3 stopping.\nTask 2 stopping.\nTask 1 stopping.\nTask 0 stopping.\næ€»è€—æ—¶ï¼š1001 ms // éå¸¸å®Œç¾\n</code></pre>\n<p>ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œå¯¹äº async block/fn ä½ è‡³å°‘æœ‰ä»¥ä¸‹ä¸€äº›åšæ³•ï¼š</p>\n<ol>\n<li>å¯¹ async block/fn è°ƒç”¨ <code>.await</code> æ¥ç­‰å¾…ç»“æœï¼›</li>\n<li>å¯¹å¯åˆ—ä¸¾çš„å°‘æ•° Future è°ƒç”¨ <code>join!</code> æˆ–è€… <code>select!</code> æ¥åŒæ—¶ç­‰å¾…å¤šä¸ªç»“æœ æˆ–è€… ç­‰å¾…å¤šä¸ªåˆ†æ”¯çš„ç¬¬ä¸€ä¸ªç»“æœï¼›</li>\n<li>å¯¹å¤§é‡ Future è°ƒç”¨ <a href=\"https://docs.rs/futures/0.3.17/futures/?search=join\" rel=\"noopener noreferrer\">join</a> æˆ–è€… <a href=\"https://docs.rs/futures/0.3.17/futures/?search=select\" rel=\"noopener noreferrer\">select</a> ä¸€ç±»æ”¯æŒä¼ å…¥ Vec / iter å‚æ•°ç±»å‹çš„å‡½æ•°ï¼Œæ¯”å¦‚è¿™ä¸ªä¾‹å­ä¸­çš„ <code>for handle in handles { ... }</code> éƒ¨åˆ†å°±å¯ä»¥æ”¹å†™æˆ <code>futures::future::join_all(handles).await;</code> ï¼›</li>\n<li>æŠŠ async block/fn å˜æˆä»»åŠ¡ï¼Œç„¶åè°ƒç”¨ <code>Runtime::block_on</code> ï¼ˆç­‰ä»·åœ°ï¼Œå¯¹ä»»åŠ¡ awaitï¼‰æ¥æ‰§è¡Œè®¸å¤šä»»åŠ¡ã€‚</li>\n</ol>\n<p>å®¹æ˜“çŠ¯çš„é”™è¯¯æ˜¯ï¼Œå¸Œæœ›å¼‚æ­¥éé˜»å¡æ—¶ï¼Œå¯¹æ‰€æœ‰ async block/fn è¿›è¡Œäº† awaitï¼Œè€Œæ²¡æœ‰è¿›è¡Œä»»åŠ¡åŒ–å¤„ç†ï¼ˆå³ æŠŠ Future é€šè¿‡ spwan å‡½æ•°è½¬åŒ–æˆä»»åŠ¡ï¼‰ï¼š</p>\n<pre><code>use std::time::Instant;\nuse tokio::time::{sleep, Duration};\n\n#[tokio::main]\nasync fn main() {\n    let now = Instant::now();\n\n    let mut handles = Vec::with_capacity(10);\n    for i in 0..10 {\n        handles.push(my_bg_task(i)); // æ²¡æœ‰æŠŠ Future å˜æˆä»»åŠ¡\n    }\n\n    std::thread::sleep(Duration::from_millis(120));\n    println!(\"Finished time-consuming task.\");\n\n    for handle in handles {\n        handle.await; // è€Œä¸”æ¯ä¸ª handle å¿…é¡»æ‰§è¡Œå®Œæ‰èƒ½æ‰§è¡Œä¸‹ä¸€ä¸ª handle\n    }\n    println!(\"æ€»è€—æ—¶ï¼š{} ms\", now.elapsed().as_millis());\n}\n\nasync fn my_bg_task(i: u64) {\n    let millis = 100;\n    println!(\"Task {} sleeping for {} ms.\", i, millis);\n    sleep(Duration::from_millis(millis)).await;\n    println!(\"Task {} stopping.\", i);\n}\n</code></pre>\n<p>è¿è¡Œç»“æœï¼šåŒæ­¥é˜»å¡</p>\n<pre><code>Finished time-consuming task.\nTask 0 sleeping for 100 ms.\nTask 0 stopping.\nTask 1 sleeping for 100 ms.\nTask 1 stopping.\nTask 2 sleeping for 100 ms.\nTask 2 stopping.\nTask 3 sleeping for 100 ms.\nTask 3 stopping.\nTask 4 sleeping for 100 ms.\nTask 4 stopping.\nTask 5 sleeping for 100 ms.\nTask 5 stopping.\nTask 6 sleeping for 100 ms.\nTask 6 stopping.\nTask 7 sleeping for 100 ms.\nTask 7 stopping.\nTask 8 sleeping for 100 ms.\nTask 8 stopping.\nTask 9 sleeping for 100 ms.\nTask 9 stopping.\næ€»è€—æ—¶ï¼š1130 ms\n</code></pre>\n<hr>\n<p>æˆ–è€…åƒè¿™æ ·ï¼š</p>\n<pre><code>use std::time::Instant;\nuse tokio::time::{sleep, Duration};\n\n#[tokio::main]\nasync fn main() {\n    let now = Instant::now();\n\n    let mut handles = Vec::with_capacity(10);\n    for i in 0..10 {\n        handles.push(my_bg_task(i)); // æ²¡æœ‰æŠŠ Future å˜æˆä»»åŠ¡\n    }\n\n    std::thread::sleep(Duration::from_millis(120));\n    println!(\"Finished time-consuming task.\");\n\n    futures::future::join_all(handles).await; // ä½†æ˜¯ join_all ä¼šç­‰å¾…æ‰€æœ‰ Future å¹¶å‘æ‰§è¡Œå®Œ\n    println!(\"æ€»è€—æ—¶ï¼š{} ms\", now.elapsed().as_millis());\n}\n\nasync fn my_bg_task(i: u64) {\n    let millis = 100;\n    println!(\"Task {} sleeping for {} ms.\", i, millis);\n    sleep(Duration::from_millis(millis)).await;\n    println!(\"Task {} stopping.\", i);\n}\n</code></pre>\n<p>è¿è¡Œç»“æœï¼šå¼‚æ­¥é˜»å¡</p>\n<pre><code>Finished time-consuming task.\nTask 0 sleeping for 100 ms.\nTask 1 sleeping for 100 ms.\nTask 2 sleeping for 100 ms.\nTask 3 sleeping for 100 ms.\nTask 4 sleeping for 100 ms.\nTask 5 sleeping for 100 ms.\nTask 6 sleeping for 100 ms.\nTask 7 sleeping for 100 ms.\nTask 8 sleeping for 100 ms.\nTask 9 sleeping for 100 ms.\nTask 0 stopping.\nTask 1 stopping.\nTask 2 stopping.\nTask 3 stopping.\nTask 4 stopping.\nTask 5 stopping.\nTask 6 stopping.\nTask 7 stopping.\nTask 8 stopping.\nTask 9 stopping.\næ€»è€—æ—¶ï¼š221 ms\n</code></pre>\n<p>â€‹</p>\n<p><em>P.S. å…³äºä»£ç ä¸­ <code>std::thread::sleep</code> å’Œ <code>tokio::time::sleep</code> çš„åŒºåˆ«ï¼Œå‚è€ƒè¿™ç¯‡æ–‡ç«  <a href=\"https://ryhl.io/blog/async-what-is-blocking/\" rel=\"noopener noreferrer\">Async: What is blocking? (by Alice Ryhl)</a> ã€‚</em></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-31 12:35:12","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"[é—®é¢˜å·²è§£å†³ï¼Œéå¸¸é€‚åˆæ–°æ‰‹çœ‹]æ–°æ‰‹æ±‚åŠ©æŒ‡é’ˆçš„ä½¿ç”¨é—®é¢˜ï¼Œå†™äº†ä¸ªå‰ç¼€æ ‘ç®—æ³•ä¸€ç›´ä¸èƒ½ç¼–è¯‘æœ‰è°å¸®å¿™çœ‹ä¸€ä¸‹ã€‚è¯·é«˜æ‰‹èµæ•™ã€‚","link":"https://rustcc.cn/article?id=56e27ff0-860c-41c8-81d4-4756fffd5abc","description":"<pre><code>fn insert(mut self, url_rule: &amp;str) {\n        let mut current = self.clone(); // ä½œä¸ºæ¸¸æ ‡æŒ‡é’ˆä½¿ç”¨ï¼Œå¥½åƒæ²¡æœ‰è¾¾åˆ°æ¸¸æ ‡çš„æ•ˆæœ\n        let list = parse_path(url_rule);\n        for word in &amp;list {\n            let mut is_exist = false;\n            for n in current.child() {\n                if n.name == word.to_string() {\n                    is_exist = true;\n                    current = n.clone();\n                    break;\n                }\n            }\n\n            if is_exist {\n                continue;\n            }\n            let mut node = Tree::new(word);\n            if is_variable(word) {\n                node.is_variable = true\n            };\n            current.append_child(&amp;node);\n            current = node.clone()\n        }\n\n        current.rule = url_rule.to_string();\n        current.is_end = true;\n    }\n\n</code></pre>\n<p>ä¸Šé¢çš„currentæ¸¸æ ‡æˆ‘åº”è¯¥ç”¨ä»€ä¹ˆç±»å‹æŒ‡é’ˆï¼Œå› ä¸ºæ˜¯è‡ªå®šä¹‰ç±»å‹Treeï¼Œç»‘å®šåªèƒ½ç”¨Cloneã€‚æ€ä¹ˆç”¨å¼•ç”¨æŒ‡é’ˆä¿®æ”¹å­èŠ‚ç‚¹çš„æ•°æ®\ngolangçš„å®ç°åœ¨https://github.com/obity/pretree/blob/main/pretree.goè¿™ä¸ªæ˜¯æ²¡æœ‰é—®é¢˜çš„ã€‚rustå®åœ¨æ˜¯ä¸ä¼šå†™ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-31 08:10:17","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rustè¿›ç¨‹å…³é—­æ—¶çš„å›è°ƒé’©å­","link":"https://rustcc.cn/article?id=77a67a30-75d9-49f5-bf60-7a650477f552","description":"<p>è¯·æ•™ä¸€ä¸‹è¿›ç¨‹å…³é—­æ—¶çš„å›è°ƒå‡½æ•°æ€ä¹ˆå®ç°ï¼Ÿ</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-31 03:38:13","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å¦‚ä½•æŠŠrustç¼–è¯‘çš„å¯æ‰§è¡Œæ–‡ä»¶ä¸­çš„ä¼—å¤šæ–‡ä»¶åå»é™¤","link":"https://rustcc.cn/article?id=fdd79d8b-5b4f-471a-84ea-47f832d1f4c1","description":"<p>ä¸‹é¢æ˜¯ä»å¯æ‰§è¡Œæ–‡ä»¶ä¸­æˆªå–çš„ä¸€æ®µã€‚é‡Œé¢åŒ…å«å¤§é‡çš„æ–‡ä»¶åå’Œå­—ç¬¦ä¸²ã€‚è¿™äº›æ–‡ä»¶åå ç”¨å¾ˆå¤šç©ºé—´ã€‚è¯·é—®å¦‚ä½•ä¸è®©è¿™ä¸ªæ–‡ä»¶åç¼–è¯‘åˆ°å¯æ‰§è¡Œæ–‡ä»¶ä¸­ï¼Ÿ</p>\n<pre><code>library/core/src/fmt/mod.rs^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^A^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^B^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^C^D^D^D^D^D^@^@^@^@^@^@^@^@^@^@^@attempted to index str up to maximum usizelibrary/core/src/str/pattern.rs^@library/core/src/str/lossy.rsassertion failed: broken.is_empty()EmptyParseIntErrorInvalidDigitPosOverflowNegOverflowUtf8Errorvalid_up_toerror_len.debug_str_offsets.debug_str.debug_rnglists.debug_ranges.debug_line_str.debug_line.debug_info.debug_addr.debug_abbrevassertion failed: src.len() == dst.len()assertion failed: edge.height == self.height - 1assertion failed: idx &lt; CAPACITY/rustc/a15f484b918a4533ad633ea903ccce82910af342/library/alloc/src/collections/btree/node.rs/rustc/a15f484b918a4533ad633ea903ccce82910af342/library/alloc/src/collections/btree/map/entry.rs/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/gimli-0.23.0/src/read/abbrev.rsJimI am hello world\n^@^@^@^@^@^@^@^@^@^@^@^@^@Invalid archive member headerInvalid archive terminatorInvalid archive member sizeArchive member size is too large^@^@^@Invalid archive extended name offsetInvalid archive extended name length::@*&amp;&lt;&gt;(,/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/rustc-demangle-0.1.18/src/legacy.rs?[]::{closure#}, _-false...!f64f32usizeu64u32u16u8isizei64i32i16i8()str/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/rustc-demangle-0.1.18/src/v0.rs0x' const ; &gt;  +  = Cunsafe \" fn(punycode{.llvm.^@^@^@^@^@^@^@^@^@^@/Users/runner/.cargo/registry/src/github.com-1ecc6299db9ec823/rustc-demangle-0.1.18/src/lib.rsAccessErroruse of std::thread::current() is not possible after the thread's local data has been destroyedalready mutably borrowedcalled `Option::unwrap()` on a `None` valuelibrary/std/src/sys_common/thread_info.rsthread name may not contain interior null bytesfailed to generate unique thread ID: bitspace exhausted^@^@^@Â»Â±Â°&lt;^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@Â§Â«Âª2^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@^@called `Result::unwrap()` on an `Err` valuelibrary/std/src/sys/unix/mutex.rsOsmessageCustomerrorUnexpectedEofConnectionRefusedConnectionResetConnectionAbortedNotConnectedAddrInUseBrokenPipeAlreadyExistsWouldBlockInvalidInputInvalidDataWriteZeroInterruptedOtherstrerror_r failurelibrary/std/src/sys/unix/os.rslibrary/std/src/ffi/c_str.rslibrary/std/src/thread/mod.rscannot access a Thread Local Storage value during or after destruction^@^@^@^@^@^@rwlock maximum reader count exceededrwlock read lock would result in deadlockfatal runtime error:\nthread panicked while panicking. aborting.\nRUST_BACKTRACE&lt;unnamed&gt;formatter errorfailed to write whole bufferlibrary/std/src/io/mod.rsnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-31 03:36:54","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-30 å¦‚ä½•æ¥çœ‹å¾… unwrap","link":"https://rustcc.cn/article?id=59dad850-933e-49dd-9ab8-5370d5c77857","description":"<h1>å¦‚ä½•æ¥çœ‹å¾… unwrap</h1>\n<p><code>unwrap</code> æ–¹æ³•å¯èƒ½ä¼šè®©æ–°æ‰‹æ„Ÿåˆ°å›°æƒ‘ã€‚ä¸€äº›å»ºè®®:</p>\n<ul>\n<li>å¯ä»¥ä½¿ç”¨ Expect (&amp;str) è€Œä¸æ˜¯ unwrap() ä¸º panic æä¾›ä¸Šä¸‹æ–‡ã€‚</li>\n<li>ä½¿ç”¨ unwrap å’Œ expect ç±»ä¼¼äºæ–­è¨€ã€‚å¦‚æœä»–ä»¬ panicï¼Œé‚£åªæœ‰åœ¨ä¸å¯æŒ½å›çš„æƒ…å†µä¸‹æ‰ä¼šå‘ç”Ÿã€‚</li>\n<li>é¿å…åœ¨åº“ä»£ç ä¸­ä½¿ç”¨ã€‚</li>\n</ul>\n<p><a href=\"https://owengage.com/writing/2021-08-30-how-to-think-of-unwrap/\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<h1>singleton-cell: ä¸€ä¸ªæ›´å¼ºå¤§çš„ ghost cell æ‰©å±•</h1>\n<p>è¿™ä¸ªåº“æä¾›äº†ä¸€ä¸ªå®‰å…¨çš„ã€é›¶å¼€é”€çš„æ¥å£ï¼Œç”¨äºé€šè¿‡è®¿é—®å¦ä¸€ä¸ªå•ä¾‹ä»¤ç‰Œæ¥ä¿æŠ¤å¯¹å…±äº«æ•°æ®çš„è®¿é—®ã€‚å®ƒæ˜¯ GhostCellçš„æ‰©å±•ï¼Œé™¤äº†å“ç‰Œä»¤ç‰Œå¤–ï¼Œå®ƒè¿˜å…è®¸æ›´å¤šæ™®é€šçš„å•ä¾‹ï¼Œä½¿æ•°æ®æˆä¸ºâ€œé™æ€çš„â€</p>\n<p>è¿™ä¸ªåº“æœ¬èº«ä¹Ÿæä¾›äº†ä¸¤ä¸ªå•ä¾‹å®ç°:</p>\n<ul>\n<li>é€šè¿‡with_tokenå°†é™å®šèŒƒå›´çš„æ ‡è®°ä»¤ç‰Œä½œä¸º GhostCell</li>\n<li>é€šè¿‡new_singletonç®€å•åœ°åˆ›å»ºä¸€æ¬¡å•ä¾‹ç»“æ„</li>\n</ul>\n<p><a href=\"https://crates.io/crates/singleton-cell\" rel=\"noopener noreferrer\">crate åœ°å€</a></p>\n<h1>Learning Rust: Interfacing with C</h1>\n<p>é€šè¿‡æœ¬æ–‡å­¦ä¹ å¦‚ä½•ä½¿ç”¨ Rust è°ƒç”¨ C æ–¹æ³•ä»¥åŠå¦‚ä½•åœ¨ C ä¸­è°ƒç”¨ Rust æ–¹æ³•.</p>\n<p><a href=\"https://piware.de/post/2021-08-27-rust-and-c/\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<h1>RefineDB: Rustç¼–å†™çš„å¼ºç±»å‹æ–‡æ¡£æ•°æ®åº“</h1>\n<p>è¿è¡Œåœ¨ä»»ä½•äº‹åŠ¡æ€§ é”®å€¼å­˜å‚¨ä¸Šçš„ å¼ºç±»å‹ æ–‡æ¡£æ•°æ®åº“ã€‚</p>\n<p>ç›®å‰æ”¯æŒçš„ backends æœ‰:</p>\n<ul>\n<li>FoundationDB</li>\n<li>å•æœºéƒ¨ç½²çš„ SQLiteã€‚</li>\n<li>ä¸€ä¸ªç®€å•çš„å†…å­˜é”®å€¼å­˜å‚¨ã€‚</li>\n</ul>\n<p><a href=\"https://github.com/losfair/RefineDB\" rel=\"noopener noreferrer\">github åœ°å€</a></p>\n<p>--</p>\n<p>From æ—¥æŠ¥å°ç»„ BobQinï¼ŒFBIå°ç™½</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-30 12:46:04","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"è¿œç¨‹åŠå…¬ï¼Œä¸é™åœ°åŸŸï¼Œç¼´çº³ç¤¾ä¿å…¬ç§¯é‡‘ï¼Œå‘¨æœ«åŒä¼‘ï¼Œå‘Šåˆ« 996ï¼Œæ‹’ç» 007ï¼ŒNervina Labs æ¬¢è¿ä½ ï¼","link":"https://rustcc.cn/article?id=a90b0635-b332-4e23-9a44-eb9282f519ef","description":"<p>rustå¼€å‘å·¥ç¨‹å¸ˆ\nå²—ä½èŒè´£ï¼š1ã€è´Ÿè´£æ™ºèƒ½åˆçº¦çš„å¼€å‘åŠè®¾è®¡ï¼›2ã€è´Ÿè´£åŒºå—é“¾ä¸šåŠ¡ç³»ç»Ÿåˆ†æä¸è®¾è®¡å·¥ä½œï¼›3ã€è´Ÿè´£æ™ºèƒ½åˆçº¦ä»£ç æµ‹è¯•ã€è¿è¡Œå’Œç»´æŠ¤ã€‚ä»»èŒè¦æ±‚ï¼š1ã€è®¡ç®—æœºç›¸å…³ä¸“ä¸šæœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼Œ3å¹´ä»¥ä¸Šå·¥ä½œç»éªŒï¼›2ã€ç†Ÿç»ƒæŒæ¡ C/C++ã€Rust ç­‰ç³»ç»Ÿå¼€å‘è¯­è¨€è‡³å°‘ä¸€ç§ï¼Œè‡³å°‘æœ‰è¿‡ä¸¤å¹´ç›¸å…³å¼€å‘ç»éªŒï¼›3ã€å¯¹æ•°æ®ç»“æ„å’Œç®—æ³•ï¼Œå¯¹å¯†ç å­¦ï¼Œå®‰å…¨åè®®å’ŒåŠ å¯†ç®—æ³•æœ‰ç ”ç©¶è€…ä¼˜å…ˆï¼›4ã€ä¼˜ç§€çš„è‹±è¯­æ–‡æ¡£æ’°å†™ä¸é˜…è¯»èƒ½åŠ›è€…ä¼˜å…ˆï¼›5ã€äº†è§£åŒºå—é“¾ï¼Œæœ‰åˆçº¦å¼€å‘ç»éªŒæ›´ä½³ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-30 03:06:51","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"æ„å»ºå®‰å…¨æ˜“ç”¨çš„é“¾è¡¨","link":"https://rustcc.cn/article?id=273831e7-932d-476f-9d31-323151afb123","description":"<p>å†™äº†ä¸€ä¸ªé“¾è¡¨çš„Crateï¼Œæ„¿æ™¯æ˜¯æ„å»ºå®‰å…¨ä¸”æ˜“ç”¨çš„é“¾è¡¨ã€‚</p>\n<p>æ¬¢è¿å¤§å®¶æ¥æ‰¾èŒ¬ï¼ˆBugï¼‰æˆ–æéœ€æ±‚ :)</p>\n<p>Crate IOé“¾æ¥ï¼š<a href=\"https://crates.io/crates/cyclic_list\" rel=\"noopener noreferrer\">https://crates.io/crates/cyclic_list</a>;</p>\n<p>GitHubé“¾æ¥ï¼š<a href=\"https://github.com/whjpji/cyclic_list\" rel=\"noopener noreferrer\">https://github.com/whjpji/cyclic_list</a></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-29 15:10:34","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</h3>\n<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´8æœˆ29æ—¥ 20:00-21:00</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»:</strong>  è®²åˆ° Rust ä½¿ç”¨ Future å¼‚æ­¥ç¼–ç¨‹ï¼Œå°±ä¸å¾—ä¸è¯´ futures å’Œ tokio è¿™ä¸¤ä¸ª crateï¼Œå…¶å®æ ‡å‡†åº“ä¸­çš„ futureï¼Œä»¥åŠ async/await å°±æ˜¯ä» futures åº“ä¸­æ•´åˆè¿›æ ‡å‡†åº“çš„, Tokio æ‹¥æœ‰æå¿«çš„æ€§èƒ½ï¼Œæ˜¯å¤§éƒ¨åˆ†ç³»ç»Ÿå¼‚æ­¥å¤„ç†çš„é€‰æ‹©ï¼Œå…¶æ„å»ºäº future ä¹‹ä¸Šã€‚Future æ˜¯  Rust å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒåŸºç¡€ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<p>1ã€ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥.</p>\n<p>2ã€ç†è§£å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>\n<p>3ã€Future ç¼–ç¨‹æ¨¡å‹è®²è§£.</p>\n<p>4ã€å¸¦é¢†å¤§å®¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ future , å†æ¬¡å¸®å¿™å¤§å®¶ç†è§£</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-19 -- Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</h3>\n<p>å·²ç»åœ¨ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªcrate ä¸Šæµ‹è¯•äº†ç‰ˆæœ¬è¿ç§»,å¹¶ä¸”å°†æµ‹è¯•æ‰€æœ‰å…¬å…±çš„ crateã€‚</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>å¼‚æ­¥å¼•æ“ C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D æ¸¸æˆå¼•æ“</h3>\n<ul>\n<li><strong>PCï¼ˆWindowsã€Linuxã€macOSï¼‰å’Œ Web (WebAssembly)</strong> æ”¯æŒã€‚</li>\n<li><strong>å»¶è¿Ÿç€è‰²</strong></li>\n<li><strong>å†…ç½®ä¿å­˜/åŠ è½½</strong></li>\n<li><strong>ç‹¬ç«‹åœºæ™¯ç¼–è¾‘å™¨</strong></li>\n<li><strong>é«˜çº§ç‰©ç†æ¨¡å‹</strong></li>\n<li><strong>åˆ†å±‚æ¨¡å‹èµ„æº</strong></li>\n<li><strong>å‡ ä½•å®ä¾‹åŒ–</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ å†°å±±ä¸Šçš„ mook &amp;&amp; æŒºè‚¥</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾: é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Šé€šè¿‡Datafuseç†è§£å…¨é“¾è·¯è·Ÿè¸ªã€‹| Vol. 4</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ22æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong> æ•°æ®åº“ç³»ç»Ÿä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚ï¼Œåºå¤§çš„ç³»ç»Ÿã€‚ç‰¹åˆ«æ˜¯åœ¨è°ƒè¯•å’Œè§‚å¯ŸSQLæ‰§è¡Œï¼Œå¤šçº¿ç¨‹ä»»åŠ¡åˆ‡æ¢ï¼Œå› ä¸ºæ²¡æœ‰å†…å­˜è°ƒç”¨æˆ–å †æ ˆè·Ÿè¸ªï¼Œè¿™ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¿½è¸ªçš„ç”±æ¥ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°å¤šè¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ªä¸ºæè¿°å’Œåˆ†æè·¨è¿›ç¨‹äº‹åŠ¡æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚Google Dapper(Dapper: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿé“¾è·¯è¿½è¸ªåŸºç¡€è®¾æ–½)è®ºæ–‡(å„tracerçš„åŸºç¡€)ä¸­æè¿°äº†åˆ†å¸ƒå¼è¿½è¸ªçš„ä¸€äº›ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­ç¨³æ€é—®é¢˜ã€åˆ†å¸ƒå¼åˆ†æã€èµ„æºå±æ€§å’Œå¾®æœåŠ¡çš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ã€‚</p>\n<p>æœ¬æ¬¡å…¬å¼€è¯¾é€š Google çš„ OpenTraceing ä»‹ç»ï¼Œç»“åˆRustçš„ tokio-rs/tracing ä½¿ç”¨ï¼Œæœ€ç»ˆç»“åˆ Datafuse é¡¹ç›®ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹å¤§å‹åº”ç”¨çš„å…¨é“¾è·¯è·Ÿè¸ªåˆ†æè¿‡ç¨‹ã€‚</p>\n<p>å…³äºDatafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ol>\n<li>\n<p>ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»ŸOpenTracingåŠåº”ç”¨åœºæ™¯</p>\n</li>\n<li>\n<p>ä»‹ç» tokio-rs/tracing åŠåœ¨ç¨‹åºå¼€å‘ä¸­çš„ä½œç”¨</p>\n</li>\n<li>\n<p>ä¸ºä»€ä¹ˆéœ€è¦tokio-rs/tracingåº“</p>\n</li>\n<li>\n<p>æ¼”ç¤ºDatafuseé¡¹ç›®ä¸­tokio-rs/tracingçš„ä½¿ç”¨</p>\n</li>\n</ol>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"è®ºå›githubè´¦æˆ·æ— æ³•ç™»å½•è§£å†³ç¬”è®°","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>æœ‰åæ˜ è¿™ä¸¤å¤©githubè´¦æˆ·æ— æ³•ç™»å½•äº†ã€‚</p>\n<p>æŠ¥è¿™ä¸ªé”™ï¼š</p>\n<pre><code>get github user info err\n</code></pre>\n<p>æŸ¥äº†å‡ ä¸ªåœ°æ–¹ï¼š</p>\n<ol>\n<li>ä»£ç æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼šOk</li>\n<li>httpsä»£ç†æ˜¯å¦æ­£å¸¸ï¼šOk</li>\n<li>æ£€æŸ¥äº†githubè¿”å›æ—¥å¿—ï¼Œå‘ç°æ˜¯ï¼š</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>è¿›å…¥è¿™ä¸ªåœ°å€ä¸€çœ‹ï¼š<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>åŸæ¥2020å¹´2æœˆå°±å·²ç»è¯´äº†ï¼Œè¦æ”¹è¦æ”¹ã€‚ä¸è¿‡æˆ‘ç¡®å®æ²¡ç•™æ„åˆ°è¿™ä¸ªä¿¡æ¯ã€‚ï¼šï¼ˆ</p>\n<p>æ„æ€å°±æ˜¯è¯´access_tokenä¸è¦æ”¾åœ¨queryå‚æ•°ä¸­ï¼Œè€Œæ˜¯è¦æ”¾åœ¨headeré‡Œé¢ã€‚ç…§å®ƒè¯´çš„ï¼Œæ”¹äº†åå°±å¥½äº†ã€‚</p>\n<p>ç‰¹æ­¤è®°å½•ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust çš„ Future ä¸ Javascript çš„ Promise åŠŸèƒ½å¯¹ç…§å‚è€ƒ","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>çš„<code>Future</code>ä¸<code>Javascript</code>çš„<code>Promise</code>åŠŸèƒ½å¯¹ç…§å‚è€ƒ</h1>\n<p>å­¦ä¹ æ–°é²œæŠ€æœ¯æ—¶ï¼Œæˆ‘æ€»æ˜¯ä¼šä¹ æƒ¯æ€§å‘æ›¾ç»ç†Ÿæ‚‰çš„å†…å®¹ä¸Šé ï¼Œç”šè‡³å¥—ç”¨ç°æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¿™æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œå¯¹ç…§<code>Javascript - Promise/A+ API</code>æ¥è®°å¿†ä¸€éƒ¨åˆ†<code>Rust Future</code>å¸¸ç”¨<code>API</code>ã€‚</p>\n<blockquote>\n<p>æ³¨æ„ï¼šæ‰€æœ‰çš„<code>Rust - Future</code>æ“ä½œéƒ½æ˜¯ä»¥<code>.await</code>ç»“å°¾çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œä¸åŒäº<code>Javascript - Promise/A+</code>ï¼Œ<code>Rust - Future</code>æ˜¯æƒ°æ€§çš„ã€‚åªæœ‰è¢«<code>.await</code>æŒ‡ä»¤æ¿€æ´»åï¼Œåœ¨<code>Rust - Future</code>å†…å°è£…çš„æ“ä½œæ‰ä¼šè¢«çœŸæ­£åœ°æ‰§è¡Œã€‚</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">æè¿°</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* ä»€ä¹ˆéƒ½ä¸åš */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll ä¸èƒ½è¢«ç”¨æ¥æ„é€ åŒ…å«äº†å¼‚æ­¥æ“ä½œçš„ Future å®ä¾‹ï¼Œå› ä¸ºã€å›è°ƒé—­åŒ…ã€‘å†…çš„ã€å¯ä¿®æ”¹å¼•ç”¨ã€‘&amp;mut Context&lt;'_&gt; ä¸èƒ½è¢«  ï¼ˆ1ï¼‰è·¨çº¿ç¨‹ä¼ é€’  ï¼ˆ2ï¼‰ä¼ é€’å‡ºé—­åŒ…ä½œç”¨åŸŸ2. task::spawn_blocking() ã€å›è°ƒé—­åŒ…ã€‘è¾“å…¥å‚æ•°å†…çš„ thread::sleep() ä¸æ˜¯é˜»å¡è¿è¡Œ task::spawn_blocking() çš„ä¸»çº¿ç¨‹ï¼Œè€Œæ˜¯é˜»å¡ä»ã€é˜»å¡ä»»åŠ¡çº¿ç¨‹æ± ã€‘ä¸­åˆ†é…æ¥è¿è¡Œé˜»å¡ä»»åŠ¡çš„ã€å·¥ä½œçº¿ç¨‹ã€‘ã€‚</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. æœ‰ä¸€ä¸ª promise/future å¤±è´¥å°±æ•´ä½“æ€§åœ°å¤±è´¥ã€‚2. try_join æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future çš„æˆåŠŸä¸å¤±è´¥ç»“æœéƒ½æ”¶é›†2. è¿”å›ç»“æœï¼š(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. ä»…åªæ”¶é›†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ promise/future2. try_race æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. æ”¶é›†ç¬¬ä¸€ä¸ªç»“æŸçš„ promise/futureï¼Œæ— è®ºå®ƒæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯å¤±è´¥æ”¶åœºã€‚2. è¿”å›ç»“æœï¼šT</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼š</strong>ã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹</p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ15æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong></p>\n<p>å¦‚æœæƒ³ç”¨ Rust å¼€å‘å¤§å‹ç›®ï¼Œæˆ–è€…å­¦ä¹ å¤§å‹é¡¹ç›®ä»£ç ï¼Œç‰¹åˆ«æ˜¯æ¡†æ¶çº§åˆ«çš„é¡¹ç›®ï¼Œé‚£ä¹ˆ Rust çš„å®æœºåˆ¶è‚¯å®šæ˜¯ä¸€ä¸ªå¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚ ä¾‹å¦‚ datafuse ä¸­çš„ä¸€äº›é…ç½®ç®¡ç†ï¼š\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>è¿™å°±æ˜¯é€šè¿‡å®å®ç°é…ç½®çš„ç»Ÿä¸€è¡Œä¸ºï¼Œä»£ç å‚è€ƒï¼š\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust è¯­è¨€å¼ºå¤§çš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¯ä»¥åˆ›å»ºå’Œåˆ©ç”¨å®ï¼Œä¸è¿‡åˆ›å»ºå®çœ‹èµ·æ¥æŒºå¤æ‚ï¼Œå¸¸å¸¸ä»¤åˆšæ¥è§¦ Rust çš„å¼€å‘è€…ç”Ÿç•æƒ§ã€‚ åœ¨æœ¬æ¬¡å…¬å¼€è¯¾ä¸­å¸®åŠ©ä½ ç†è§£ Rust Macro çš„åŸºæœ¬åŸç†ï¼Œå­¦ä¹ å¦‚ä½•åˆ›è‡ªå·²çš„ Rust å®ï¼Œä»¥åŠæŸ¥çœ‹æºç å­¦ä¹ å®çš„å®ç°ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ul>\n<li>ä»€ä¹ˆæ˜¯ Rust å®</li>\n<li>ä»€ä¹ˆæ˜¯å®è¿è¡ŒåŸç†</li>\n<li>å¦‚ä½•åˆ›å»º Rust å®è¿‡ç¨‹</li>\n<li>é˜…è¯» datafuse é¡¹ç›®æºç ï¼Œ å­¦ä¹ é¡¹ç›®ä¸­å®çš„å®ç°</li>\n</ul>\n<p><strong>è®²å¸ˆä»‹ç»</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šç†è§£Rustçš„æ‰€æœ‰æƒ| Vol 2","link":"https://rustcc.cn/article?id=c107b830-9fe1-43dd-94a3-9efcd5544205","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼šã€Šç†è§£Rustæ‰€æœ‰æƒã€‹</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š2021å¹´8æœˆ8æ—¥ 20:30-21:30</strong></p>\n<p><strong>å˜‰å®¾è®²å¸ˆï¼š è‹æ—</strong></p>\n<p><strong>å˜‰å®¾ä»‹ç»ï¼š</strong></p>\n<p>Rustä¸­æ–‡ç¤¾åŒºæˆå‘˜ï¼Œå¤šç‚¹DmallæŠ€æœ¯Leaderï¼Œå‰æŠ˜800äº’è”ç½‘ç ”å‘å›¢é˜Ÿè´Ÿè´£äººã€10ä½™å¹´ä¸€çº¿ç ”å‘ç»éªŒã€‚å…·æœ‰å¤šå¹´çš„è½¯ä»¶å¼€å‘ç»éªŒ, ç†Ÿç»ƒRubyã€Javaã€Rustç­‰å¼€å‘è¯­è¨€, åŒæ—¶ä¹Ÿå‚ä¸è¿‡Rustä¸­æ–‡ç¤¾åŒºæ—¥æŠ¥ç»´æŠ¤å·¥ä½œã€‚</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»</strong></p>\n<p>æœ¬æ¬¡è¯¾ç¨‹é€šè¿‡10ä¸ªå·¦å³çš„å°ä¾‹å­ï¼Œå¸¦å¤§å®¶ç†è§£ä¸€ä¸‹Rustçš„æ‰€æœ‰æƒï¼ŒRustå¼•ç”¨å’Œå€Ÿç”¨ï¼ŒRustå˜é‡å…‹éš†å’Œå¤åˆ¶çš„ç†å¿µã€‚</p>\n<p><strong>å‚åŠ è¯¾ç¨‹</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/Rust-pbc-1.jpg\" alt=\"\"></p>\n<p><strong>è¯¾ç¨‹è§„åˆ’</strong></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloudé¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-08 02:04:00","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"æ•°æ®è¡¨ Timestamp æ—¥æœŸ Serialize","link":"https://rustcc.cn/article?id=2ff8a69e-59bb-4502-87c0-c3416ffae8a0","description":"<p>ä¸»è¦å‚è€ƒï¼š<a href=\"https://github.com/rustcc/forustm\" rel=\"noopener noreferrer\">Rustccç½‘ç«™æºç åº“</a></p>\n<p>åœ¨å¤„ç†æ•°æ®è¡¨ä¸­æ—¥æœŸç›¸å…³æ•°æ®æ—¶ï¼ŒSeralizeåºåˆ—åŒ–ç›¸å…³æ“ä½œä¼šæŠ¥é”™ï¼Œæç¤º DateTime å­—æ®µä¸è¯†åˆ«ï¼Œ\næŸ¥äº† rustcc æºç æ‰å‘ç°ä¾èµ–ä¸­éœ€è¦å¼€å¯ç›¸åº”çš„featureã€‚ç‰¹æ­¤è®°å½•ã€‚</p>\n<h2>1.ä¾èµ–çš„åº“ï¼š</h2>\n<pre><code>[dependencies]\n# æ—¥æœŸæ—¶é—´å¤„ç† éœ€è¦å¼€å¯ serde ç‰¹å¾ æ”¯æŒåºåˆ—åŒ–\nchrono = { version = \"0.4.19\", features = [\"serde\"] }\n\n# æ•°æ®åº“ORM\ndiesel = { version = \"1.4.4\", features = [\"postgres\", \"chrono\", \"uuid\", \"r2d2\"] }\ndotenv = \"0.15.0\"\nserde = { version = \"1.0.127\", features = [\"derive\"] }\nserde_json = \"1.0.66\"\nuuid = { version = \"0.8.2\", features = [\"serde\", \"v4\"] }\n</code></pre>\n<h2>2.åˆ›å»ºæ•°æ®è¡¨</h2>\n<pre><code>CREATE TABLE characters (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(128) UNIQUE NOT NULL,\n    age INTEGER NOT NULL DEFAULT 0,\n    friends VARCHAR NOT NULL DEFAULT '',\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\n)\n</code></pre>\n<h2>3.æ•°æ®è¡¨å¯¹åº”çš„ model</h2>\n<pre><code>use chrono::{NaiveDateTime};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Queryable, Serialize, Deserialize, Debug)]\npub struct Characters {\n    pub id: i32,\n    pub name: String,\n    pub age: i32,\n    pub friends: String,\n    // è¿™é‡Œçš„ NaiveDateTime æ—¥æœŸæ ¼å¼åºåˆ—åŒ–éœ€è¦å¼€å¯ç›¸å…³ features\n    pub created_at: NaiveDateTime,\n}\n</code></pre>\n<h2>4.è·å–æ•°æ®</h2>\n<pre><code>use db::schema::characters;\nuse db::{get_connection};\nuse db::models::{Characters, NewCharacter};\nuse db::schema::characters::dsl::*;\nuse diesel::QueryDsl;\nuse diesel::prelude::*;\n\nfn main() {\n    let conn = get_connection();\n\n    // æŸ¥è¯¢å¹´é¾„å¤§äº30çš„10æ¡æ•°æ®\n    let arr: Vec&lt;Characters&gt; = characters.filter(characters::age.gt(30))\n        .limit(10)\n        .load::&lt;Characters&gt;(&amp;conn)\n        .expect(\"Loading Error\");\n\n    let date_arr = arr.iter()\n        .map(|item| {\n\t    // æ•°æ®æ ¼å¼åŒ–\n            let t = item.created_at.format(\"%Y-%m-%d %H:%M:%S\").to_string();\n            println!(\"{} {}\", item.name, t);\n            t\n        })\n        .collect::&lt;Vec&lt;String&gt;&gt;();\n}\n</code></pre>\n<p>è¾“å‡ºç»“æœç±»ä¼¼ï¼š</p>\n<pre><code>Box 2021-08-05 09:39:34\nBobe 2021-08-05 09:39:34\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-08 01:40:35","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Cargo workspace config","link":"https://rustcc.cn/article?id=c3dcce30-1fc0-4819-8992-142365c7e21c","description":"<p><a href=\"https://kaisery.github.io/trpl-zh-cn/ch14-03-cargo-workspaces.html\" rel=\"noopener noreferrer\">Workspace æ–‡æ¡£é“¾æ¥</a></p>\n<h2>ç›®å½•ç»“æ„</h2>\n<pre><code>workspace-test/\n    Cargo.toml\n    db/\n        src/\n            bin/\n                init.rs\n        Cargo.tml\n</code></pre>\n<h2>workspace</h2>\n<p>workspace-test/Cargo.toml</p>\n<pre><code>[workspace]\nmembers = [\"db\"]\ndefault-member = \"db\"\n</code></pre>\n<h2>å­é¡¹ç›®</h2>\n<p>workspace-test/db/Cargo.toml</p>\n<pre><code>[package]\nname = \"db\"\nversion = \"0.1.0\"\nedition = \"2018\"\n\n[dependencies]\n\n# å¯é€‰çš„å¯æ‰§è¡Œæ–‡ä»¶é…ç½®\n# [[bin]]\n# name = \"init\"\n# path = \"src/bin/init.rs\"\n</code></pre>\n<h2>æ“ä½œ</h2>\n<pre><code># è¿è¡Œ init\ncargo run --bin init\n# -p æŒ‡å®šé¡¹ç›®\ncargo run -p db --bin init\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-04 09:54:31","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰","link":"https://rustcc.cn/article?id=120035c3-944d-4a79-9b3a-8390697a6e13","description":"<h1><code>Rust</code>å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰</h1>\n<p>ä¸åŒäº<code>javascript</code>çš„<code>new Promise((resolve, reject) =&gt; {...})</code>æ„é€ å³è¿è¡Œï¼Œ<code>Rust</code>ä¸­çš„<code>Future</code>æ˜¯Â·æƒ°æ€§Â·çŠ¶æ€æœºã€‚è¿™ä½“ç°ä¸ºï¼š</p>\n<ol>\n<li>ã€è°ƒç”¨å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€æ‰§è¡Œå¼‚æ­¥å—ã€‘ä»…åªæ„é€ ä¸€ä¸ª<code>Future trait object</code>ã€‚</li>\n<li>å› ä¸º<code>Future</code>æ˜¯æƒ°æ€§çŠ¶æ€æœºï¼Œæ‰€ä»¥å®ƒä¸ä¼šè‡ªåŠ¨æ‰§è¡Œã€å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€å¼‚æ­¥å—ã€‘å†…çš„ä»»ä½•ä¸€è¡Œä»£ç  --- æ­¤ç‚¹ä¸<code>javascript</code>çš„Â·æ´»æ€§Â·çŠ¶æ€æœºå®Œå…¨ä¸åŒã€‚ç›¸åï¼Œéœ€è¦äººå·¥æ¿€æ´»è§¦å‘ã€‚</li>\n<li>äººå·¥å¯åŠ¨<code>Future</code>è¿è¡Œï¼Œåˆåˆ†ä¸ºä¸¤ä¸ªåœºæ™¯çš„ä¸¤ç§æƒ…å†µï¼š\n<ol>\n<li>\n<p>å·²ç»åœ¨<code>async fn</code>å†…ï¼Œ<code>Future.await</code>æ¿€æ´»ã€‚ä½†ï¼ŒåŒæ—¶<strong>é˜»å¡</strong>å½“å‰å¼‚æ­¥ç¨‹åºæ‰§è¡Œæµã€‚</p>\n</li>\n<li>\n<p>åœ¨<code>async fn</code>å¤–ï¼Œéœ€è¦å€ŸåŠ©ç”±ã€è¿è¡Œæ—¶ã€‘æä¾›çš„ã€æ‰§è¡Œå™¨ã€‘ã€‚å°±<code>async-std</code>åº“è€Œè¨€ï¼Œæœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š</p>\n<ol>\n<li><code>task::block_on(Future)</code> æ‰§è¡Œ<code>Future</code>ä¸”é˜»å¡å½“å‰çº¿ç¨‹ç›´åˆ°<code>Future</code>è¢«å®Œæˆã€‚</li>\n<li><code>task::spawn(Future)</code>ä»…æ‰§è¡Œ<code>Future</code>å’Œä¸é˜»å¡å½“å‰çº¿ç¨‹ã€‚</li>\n</ol>\n<p>æ— è®ºé€‰æ‹©ä¸Šé¢å“ªç§æ–¹å¼ï¼Œè‹¥åœ¨<code>Future</code>æ‰§è¡ŒæœŸé—´å‡ºç°äº†<code>panic</code>ï¼Œå…¶éƒ½ä¼šç»ˆæ­¢ï¼ˆ<code>abort</code>ï¼‰æ­£åœ¨å…±äº«åŒä¸€ä¸ªæ‰§è¡Œçº¿ç¨‹ï¼ˆ<code>thread</code>ï¼‰çš„æ‰€æœ‰<code>task</code>ï¼ˆÂ·æ— æ ˆÂ·åç¨‹ï¼‰çš„è¿è¡Œã€‚</p>\n</li>\n</ol>\n</li>\n</ol>\n<p>é¢˜å¤–è¯ï¼Œ</p>\n<ol>\n<li>ç»¿è‰²çº¿ç¨‹æ˜¯Â·æœ‰æ ˆÂ·åç¨‹ï¼›å¼‚æ­¥å‡½æ•°ä¸å¼‚æ­¥å—æ˜¯Â·æ— æ ˆÂ·åç¨‹ã€‚</li>\n<li>åœ¨<code>async-std</code>åº“çš„è¯æ±‡è¡¨å†…ï¼Œåç¨‹è¢«ç§°ä½œ<code>task</code>è€Œä¸æ˜¯æƒ¯ä¾‹çš„<code>coroutine</code>ã€‚</li>\n<li><code>task::spawn(Future)</code>ä¹Ÿèƒ½è¢«ä½¿ç”¨äº<code>async fn</code>æˆ–<code>async {...}</code>å†…ã€‚å®ƒè¢«ç”¨æ¥ä»£æ›¿<code>.await</code>æŒ‡ä»¤ï¼Œä»¥<strong>éé˜»å¡</strong><code>async fn</code>æˆ–<code>async {...}</code>çš„æ–¹å¼ï¼Œæ¿€æ´»ä¸æ‰§è¡Œä¸€ä¸ª<code>Future</code>å®ä¾‹ã€‚</li>\n</ol>\n<h2>ä¾‹ç¨‹</h2>\n<pre><code>async fn accept_loop(addr: impl ToSocketAddrs) -&gt; Result&lt;()&gt; {\n    // 1. TcpListener::bind(addr) è¿”å› Future\n    // 2. .await äº Future å–å¾— Result&lt;T, E&gt;\n    // 3. Result&lt;T, E&gt;? å†æ‹¿å¾— Ok&lt;T&gt; ä¸­çš„ T\n    let listener = TcpListener::bind(addr).await?; // å¼‚æ­¥å‡½æ•°å†…çš„äººå·¥å¯åŠ¨ Future\n    let mut incoming = listener.incoming();\n    // å› ä¸ºæ²¡æœ‰ä»è¯­è¨€å±‚é¢æ”¯æŒ async for loopï¼Œæ‰€ä»¥ while loop + Iterator&lt;Item = T&gt; æ¥æ¨¡æ‹Ÿä¹‹ã€‚\n    while let Some(stream) = incoming.next().await {\n        // TODO\n    }\n    Ok(())\n}\nfn main() {\n    let fut = accept_loop(\"127.0.0.1:8080\");\n    task::block_on(fut); // å¼‚æ­¥å‡½æ•°å¤–çš„äººå·¥å¯åŠ¨ Future\n}\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-03 00:01:43","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-09-01T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Want To Reduce Labeling Cost? GPT-3 Can Help. (arXiv:2108.13487v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13487","description":"<p>Data annotation is a time-consuming and labor-intensive process for many NLP\ntasks. Although there exist various methods to produce pseudo data labels, they\nare often task-specific and require a decent amount of labeled data to start\nwith. Recently, the immense language model GPT-3 with 175 billion parameters\nhas achieved tremendous improvement across many few-shot learning tasks. In\nthis paper, we explore ways to leverage GPT-3 as a low-cost data labeler to\ntrain other models. We find that, to make the downstream model achieve the same\nperformance on a variety of NLU and NLG tasks, it costs 50% to 96% less to use\nlabels from GPT-3 than using labels from humans. Furthermore, we propose a\nnovel framework of combining pseudo labels from GPT-3 with human labels, which\nleads to even better performance with limited labeling budget. These results\npresent a cost-effective data labeling methodology that is generalizable to\nmany practical applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yichong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semi-Supervised Exaggeration Detection of Health Science Press Releases. (arXiv:2108.13493v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13493","description":"<p>Public trust in science depends on honest and factual communication of\nscientific papers. However, recent studies have demonstrated a tendency of news\nmedia to misrepresent scientific papers by exaggerating their findings. Given\nthis, we present a formalization of and study into the problem of exaggeration\ndetection in science communication. While there are an abundance of scientific\npapers and popular media articles written about them, very rarely do the\narticles include a direct link to the original paper, making data collection\nchallenging. We address this by curating a set of labeled press\nrelease/abstract pairs from existing expert annotated studies on exaggeration\nin press releases of scientific papers suitable for benchmarking the\nperformance of machine learning models on the task. Using limited data from\nthis and previous studies on exaggeration detection in science, we introduce\nMT-PET, a multi-task version of Pattern Exploiting Training (PET), which\nleverages knowledge from complementary cloze-style QA tasks to improve few-shot\nlearning. We demonstrate that MT-PET outperforms PET and supervised learning\nboth when data is limited, as well as when there is an abundance of data for\nthe main task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wright_D/0/1/0/all/0/1\">Dustin Wright</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConVIScope: Visual Analytics for Exploring Patient Conversations. (arXiv:2108.13514v1 [cs.HC])","link":"http://arxiv.org/abs/2108.13514","description":"<p>The proliferation of text messaging for mobile health is generating a large\namount of patient-doctor conversations that can be extremely valuable to health\ncare professionals. We present ConVIScope, a visual text analytic system that\ntightly integrates interactive visualization with natural language processing\nin analyzing patient-doctor conversations. ConVIScope was developed in\ncollaboration with healthcare professionals following a user-centered iterative\ndesign. Case studies with six domain experts suggest the potential utility of\nConVIScope and reveal lessons for further developments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Hoque_E/0/1/0/all/0/1\">Enamul Hoque</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Lester_R/0/1/0/all/0/1\">Richard Lester</a> (3), <a href=\"http://arxiv.org/find/cs/1/au:+Chau_R/0/1/0/all/0/1\">Raymond Chau</a> (3) ((1) Department of Computer Science, University of British Columbia, (2) School of Information Technology, York University, (3) Department of Medicine, University of British Columbia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Consistent Document-level Entity Linking: Joint Models for Entity Linking and Coreference Resolution. (arXiv:2108.13530v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13530","description":"<p>We consider the task of document-level entity linking (EL), where it is\nimportant to make consistent decisions for entity mentions over the full\ndocument jointly. We aim to leverage explicit \"connections\" among mentions\nwithin the document itself: we propose to join the EL task with that of\ncoreference resolution (coref). This is complementary to related works that\nexploit either (i) implicit document information (e.g., latent relations among\nentity mentions, or general language models) or (ii) connections between the\ncandidate links (e.g, as inferred from the external knowledge base).\nSpecifically, we cluster mentions that are linked via coreference, and enforce\na single EL for all of the clustered mentions together. The latter constraint\nhas the added benefit of increased coverage by joining EL candidate lists for\nthe thus clustered mentions. We formulate the coref+EL problem as a structured\nprediction task over directed trees and use a globally normalized model to\nsolve it. Experimental results on two datasets show a boost of up to +5%\nF1-score on both coref and EL tasks, compared to their standalone counterparts.\nFor a subset of hard cases, with individual mentions lacking the correct EL in\ntheir candidate entity list, we obtain a +50% increase in accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zaporojets_K/0/1/0/all/0/1\">Klim Zaporojets</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deleu_J/0/1/0/all/0/1\">Johannes Deleu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Demeester_T/0/1/0/all/0/1\">Thomas Demeester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Develder_C/0/1/0/all/0/1\">Chris Develder</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Linguistic Characterization of Divisive Topics Online: Case Studies on Contentiousness in Abortion, Climate Change, and Gun Control. (arXiv:2108.13556v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13556","description":"<p>As public discourse continues to move and grow online, conversations about\ndivisive topics on social media platforms have also increased. These divisive\ntopics prompt both contentious and non-contentious conversations. Although what\ndistinguishes these conversations, often framed as what makes these\nconversations contentious, is known in broad strokes, much less is known about\nthe linguistic signature of these conversations. Prior work has shown that\ncontentious content and structure can be a predictor for this task, however,\nmost of them have been focused on conversation in general, very specific\nevents, or complex structural analysis. Additionally, many models used in prior\nwork have lacked interpret-ability, a key factor in online moderation. Our work\nfills these gaps by focusing on conversations from highly divisive topics\n(abortion, climate change, and gun control), operationalizing a set of novel\nlinguistic and conversational characteristics and user factors, and\nincorporating them to build interpretable models. We demonstrate that such\ncharacteristics can largely improve the performance of prediction on this task,\nand also enable nuanced interpretability. Our case studies on these three\ncontentious topics suggest that certain generic linguistic characteristics are\nhighly correlated with contentiousness in conversations while others\ndemonstrate significant contextual influences on specific divisive topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Beel_J/0/1/0/all/0/1\">Jacob Beel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1\">Tong Xiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soni_S/0/1/0/all/0/1\">Sandeep Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"T3-Vis: a visual analytic framework for Training and fine-Tuning Transformers in NLP. (arXiv:2108.13587v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13587","description":"<p>Transformers are the dominant architecture in NLP, but their training and\nfine-tuning is still very challenging. In this paper, we present the design and\nimplementation of a visual analytic framework for assisting researchers in such\nprocess, by providing them with valuable insights about the model's intrinsic\nproperties and behaviours. Our framework offers an intuitive overview that\nallows the user to explore different facets of the model (e.g., hidden states,\nattention) through interactive visualization, and allows a suite of built-in\nalgorithms that compute the importance of model components and different parts\nof the input sequence. Case studies and feedback from a user focus group\nindicate that the framework is useful, and suggest several improvements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wen Xiao</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lanjun Wang</a> (2), <a href=\"http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1\">Hyeju Jang</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Carenini_G/0/1/0/all/0/1\">Giuseppe Carenini</a> (1) ((1) University of British Columbia, (2) Huawei Cananda Technologies Co. Ltd.)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Does Adversarial Fine-Tuning Benefit BERT?. (arXiv:2108.13602v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13602","description":"<p>Adversarial training (AT) is one of the most reliable methods for defending\nagainst adversarial attacks in machine learning. Variants of this method have\nbeen used as regularization mechanisms to achieve SOTA results on NLP\nbenchmarks, and they have been found to be useful for transfer learning and\ncontinual learning. We search for the reasons for the effectiveness of AT by\ncontrasting vanilla and adversarially fine-tuned BERT models. We identify\npartial preservation of BERT's syntactic abilities during fine-tuning as the\nkey to the success of AT. We observe that adversarially fine-tuned models\nremain more faithful to BERT's language modeling behavior and are more\nsensitive to the word order. As concrete examples of syntactic abilities, an\nadversarially fine-tuned model could have an advantage of up to 38% on anaphora\nagreement and up to 11% on dependency parsing. Our analysis demonstrates that\nvanilla fine-tuning oversimplifies the sentence representation by focusing\nheavily on one or a few label-indicative words. AT, however, moderates the\neffect of these influential words and encourages representational diversity.\nThis allows for a more hierarchical representation of a sentence and leads to\nthe mitigation of BERT's loss of syntactic abilities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ebrahimi_J/0/1/0/all/0/1\">Javid Ebrahimi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1\">Hao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Lingual Text Classification of Transliterated Hindi and Malayalam. (arXiv:2108.13620v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13620","description":"<p>Transliteration is very common on social media, but transliterated text is\nnot adequately handled by modern neural models for various NLP tasks. In this\nwork, we combine data augmentation approaches with a Teacher-Student training\nscheme to address this issue in a cross-lingual transfer setting for\nfine-tuning state-of-the-art pre-trained multilingual language models such as\nmBERT and XLM-R. We evaluate our method on transliterated Hindi and Malayalam,\nalso introducing new datasets for benchmarking on real-world scenarios: one on\nsentiment classification in transliterated Malayalam, and another on crisis\ntweet classification in transliterated Hindi and Malayalam (related to the 2013\nNorth India and 2018 Kerala floods). Our method yielded an average improvement\nof +5.6% on mBERT and +4.7% on XLM-R in F1 scores over their strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Krishnan_J/0/1/0/all/0/1\">Jitin Krishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1\">Antonios Anastasopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purohit_H/0/1/0/all/0/1\">Hemant Purohit</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rangwala_H/0/1/0/all/0/1\">Huzefa Rangwala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dynamic Sliding Window for Meeting Summarization. (arXiv:2108.13629v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13629","description":"<p>Recently abstractive spoken language summarization raises emerging research\ninterest, and neural sequence-to-sequence approaches have brought significant\nperformance improvement. However, summarizing long meeting transcripts remains\nchallenging. Due to the large length of source contents and targeted summaries,\nneural models are prone to be distracted on the context, and produce summaries\nwith degraded quality. Moreover, pre-trained language models with input length\nlimitations cannot be readily applied to long sequences. In this work, we first\nanalyze the linguistic characteristics of meeting transcripts on a\nrepresentative corpus, and find that the sentences comprising the summary\ncorrelate with the meeting agenda. Based on this observation, we propose a\ndynamic sliding window strategy for meeting summarization. Experimental results\nshow that performance benefit from the proposed method, and outputs obtain\nhigher factual consistency than the base model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimulLR: Simultaneous Lip Reading Transducer with Attention-Guided Adaptive Memory. (arXiv:2108.13630v1 [cs.CV])","link":"http://arxiv.org/abs/2108.13630","description":"<p>Lip reading, aiming to recognize spoken sentences according to the given\nvideo of lip movements without relying on the audio stream, has attracted great\ninterest due to its application in many scenarios. Although prior works that\nexplore lip reading have obtained salient achievements, they are all trained in\na non-simultaneous manner where the predictions are generated requiring access\nto the full video. To breakthrough this constraint, we study the task of\nsimultaneous lip reading and devise SimulLR, a simultaneous lip Reading\ntransducer with attention-guided adaptive memory from three aspects: (1) To\naddress the challenge of monotonic alignments while considering the syntactic\nstructure of the generated sentences under simultaneous setting, we build a\ntransducer-based model and design several effective training strategies\nincluding CTC pre-training, model warm-up and curriculum learning to promote\nthe training of the lip reading transducer. (2) To learn better spatio-temporal\nrepresentations for simultaneous encoder, we construct a truncated 3D\nconvolution and time-restricted self-attention layer to perform the\nframe-to-frame interaction within a video segment containing fixed number of\nframes. (3) The history information is always limited due to the storage in\nreal-time scenarios, especially for massive video data. Therefore, we devise a\nnovel attention-guided adaptive memory to organize semantic information of\nhistory segments and enhance the visual representations with acceptable\ncomputation-aware latency. The experiments show that the SimulLR achieves the\ntranslation speedup 9.10$\\times$ compared with the state-of-the-art\nnon-simultaneous methods, and also obtains competitive results, which indicates\nthe effectiveness of our proposed methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zhijie Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhou Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haoyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jinglin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Meng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaofei He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Explaining Classes through Word Attribution. (arXiv:2108.13653v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13653","description":"<p>In recent years, several methods have been proposed for explaining individual\npredictions of deep learning models, yet there has been little study of how to\naggregate these predictions to explain how such models view classes as a whole\nin text classification tasks. In this work, we propose a method for explaining\nclasses using deep learning models and the Integrated Gradients feature\nattribution technique by aggregating explanations of individual examples in\ntext classification to general descriptions of the classes. We demonstrate the\napproach on Web register (genre) classification using the XML-R model and the\nCorpus of Online Registers of English (CORE), finding that the method\nidentifies plausible and discriminative keywords characterizing all but the\nsmallest class.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ronnqvist_S/0/1/0/all/0/1\">Samuel R&#xf6;nnqvist</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Myntti_A/0/1/0/all/0/1\">Amanda Myntti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kyrolainen_A/0/1/0/all/0/1\">Aki-Juhani Kyr&#xf6;l&#xe4;inen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pyysalo_S/0/1/0/all/0/1\">Sampo Pyysalo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laippala_V/0/1/0/all/0/1\">Veronika Laippala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ginter_F/0/1/0/all/0/1\">Filip Ginter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discretized Integrated Gradients for Explaining Language Models. (arXiv:2108.13654v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13654","description":"<p>As a prominent attribution-based explanation algorithm, Integrated Gradients\n(IG) is widely adopted due to its desirable explanation axioms and the ease of\ngradient computation. It measures feature importance by averaging the model's\noutput gradient interpolated along a straight-line path in the input data\nspace. However, such straight-line interpolated points are not representative\nof text data due to the inherent discreteness of the word embedding space. This\nquestions the faithfulness of the gradients computed at the interpolated points\nand consequently, the quality of the generated explanations. Here we propose\nDiscretized Integrated Gradients (DIG), which allows effective attribution\nalong non-linear interpolation paths. We develop two interpolation strategies\nfor the discrete word embedding space that generates interpolation points that\nlie close to actual words in the embedding space, yielding more faithful\ngradient computation. We demonstrate the effectiveness of DIG over IG through\nexperimental and human evaluations on multiple sentiment classification\ndatasets. We provide the source code of DIG to encourage reproducible research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1\">Soumya Sanyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MELM: Data Augmentation with Masked Entity Language Modeling for Cross-lingual NER. (arXiv:2108.13655v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13655","description":"<p>Data augmentation for cross-lingual NER requires fine-grained control over\ntoken labels of the augmented text. Existing augmentation approach based on\nmasked language modeling may replace a labeled entity with words of a different\nclass, which makes the augmented sentence incompatible with the original label\nsequence, and thus hurts the performance.We propose a data augmentation\nframework with Masked-Entity Language Modeling (MELM) which effectively ensures\nthe replacing entities fit the original labels. Specifically, MELM linearizes\nNER labels into sentence context, and thus the fine-tuned MELM is able to\npredict masked tokens by explicitly conditioning on their labels. Our MELM is\nagnostic to the source of data to be augmented. Specifically, when MELM is\napplied to augment training data of the source language, it achieves up to 3.5%\nF1 score improvement for cross-lingual NER. When unlabeled target data is\navailable and MELM can be further applied to augment pseudo-labeled target\ndata, the performance gain reaches 5.7%. Moreover, MELM consistently\noutperforms multiple baseline methods for data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1\">Ran Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1\">Ruidan He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xin Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bing_L/0/1/0/all/0/1\">Lidong Bing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1\">Erik Cambria</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1\">Chunyan Miao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Rule Generation for Time Expression Normalization. (arXiv:2108.13658v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13658","description":"<p>The understanding of time expressions includes two sub-tasks: recognition and\nnormalization. In recent years, significant progress has been made in the\nrecognition of time expressions while research on normalization has lagged\nbehind. Existing SOTA normalization methods highly rely on rules or grammars\ndesigned by experts, which limits their performance on emerging corpora, such\nas social media texts. In this paper, we model time expression normalization as\na sequence of operations to construct the normalized temporal value, and we\npresent a novel method called ARTime, which can automatically generate\nnormalization rules from training data without expert interventions.\nSpecifically, ARTime automatically captures possible operation sequences from\nannotated data and generates normalization rules on time expressions with\ncommon surface forms. The experimental results show that ARTime can\nsignificantly surpass SOTA methods on the Tweets benchmark, and achieves\ncompetitive results with existing expert-engineered rule methods on the\nTempEval-3 benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1\">Wentao Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jianhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinmao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gray Cycles of Maximum Length Related to k-Character Substitutions. (arXiv:2108.13659v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13659","description":"<p>Given a word binary relation $\\tau$ we define a $\\tau$-Gray cycle over a\nfinite language $X$ to be a permutation $\\left(w_{[i]}\\right)_{0\\le i\\le\n|X|-1}$ of $X$ such that each word $w_i$ is an image of the previous word\n$w_{i-1}$ by $\\tau$. In that framework, we introduce the complexity measure\n$\\lambda(n)$, equal to the largest cardinality of a language $X$ having words\nof length at most $n$, and such that a $\\tau$-Gray cycle over $X$ exists. The\npresent paper is concerned with the relation $\\tau=\\sigma_k$, the so-called\n$k$-character substitution, where $(u,v)$ belongs to $\\sigma_k$ if, and only\nif, the Hamming distance of $u$ and $v$ is $k$. We compute the bound\n$\\lambda(n)$ for all cases of the alphabet cardinality and the argument $n$.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neraud_J/0/1/0/all/0/1\">Jean N&#xe9;raud</a> (LITIS, UNIROUEN)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-Oriented Dialogue System as Natural Language Generation. (arXiv:2108.13679v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13679","description":"<p>In this paper, we propose to formulate the task-oriented dialogue system as\nthe purely natural language generation task, so as to fully leverage the\nlarge-scale pre-trained models like GPT-2 and simplify complicated\ndelexicalization prepossessing. However, directly applying this method heavily\nsuffers from the dialogue entity inconsistency caused by the removal of\ndelexicalized tokens, as well as the catastrophic forgetting problem of the\npre-trained model during fine-tuning, leading to unsatisfactory performance. To\nalleviate these problems, we design a novel GPT-Adapter-CopyNet network, which\nincorporates the lightweight adapter and CopyNet modules into GPT-2 to achieve\nbetter performance on transfer learning and dialogue entity generation.\nExperimental results conducted on the DSTC8 Track 1 benchmark and MultiWOZ\ndataset demonstrate that our proposed approach significantly outperforms\nbaseline models with a remarkable performance on automatic and human\nevaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weizhi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Junliang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Faithful or Extractive? On Mitigating the Faithfulness-Abstractiveness Trade-off in Abstractive Summarization. (arXiv:2108.13684v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13684","description":"<p>Despite recent progress in abstractive summarization, systems still suffer\nfrom faithfulness errors. While prior work has proposed models that improve\nfaithfulness, it is unclear whether the improvement comes from an increased\nlevel of extractiveness of the model outputs as one naive way to improve\nfaithfulness is to make summarization models more extractive. In this work, we\npresent a framework for evaluating the effective faithfulness of summarization\nsystems, by generating a faithfulnessabstractiveness trade-off curve that\nserves as a control at different operating points on the abstractiveness\nspectrum. We then show that the Maximum Likelihood Estimation (MLE) baseline as\nwell as a recently proposed method for improving faithfulness, are both worse\nthan the control at the same level of abstractiveness. Finally, we learn a\nselector to identify the most faithful and abstractive summary for a given\ndocument, and show that this system can attain higher faithfulness scores in\nhuman evaluations while being more abstractive than the baseline system on two\ndatasets. Moreover, we show that our system is able to achieve a better\nfaithfulness-abstractiveness trade-off than the control at the same level of\nabstractiveness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ladhak_F/0/1/0/all/0/1\">Faisal Ladhak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durmus_E/0/1/0/all/0/1\">Esin Durmus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Grounded Dialogue with Reward-Driven Knowledge Selection. (arXiv:2108.13686v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13686","description":"<p>Knowledge-grounded dialogue is a task of generating a fluent and informative\nresponse based on both conversation context and a collection of external\nknowledge, in which knowledge selection plays an important role and attracts\nmore and more research interest. However, most existing models either select\nonly one knowledge or use all knowledge for responses generation. The former\nmay lose valuable information in discarded knowledge, while the latter may\nbring a lot of noise. At the same time, many approaches need to train the\nknowledge selector with knowledge labels that indicate ground-truth knowledge,\nbut these labels are difficult to obtain and require a large number of manual\nannotations. Motivated by these issues, we propose Knoformer, a dialogue\nresponse generation model based on reinforcement learning, which can\nautomatically select one or more related knowledge from the knowledge pool and\ndoes not need knowledge labels during training. Knoformer is evaluated on two\nknowledge-guided conversation datasets, and achieves state-of-the-art\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TNNT: The Named Entity Recognition Toolkit. (arXiv:2108.13700v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13700","description":"<p>Extraction of categorised named entities from text is a complex task given\nthe availability of a variety of Named Entity Recognition (NER) models and the\nunstructured information encoded in different source document formats.\nProcessing the documents to extract text, identifying suitable NER models for a\ntask, and obtaining statistical information is important in data analysis to\nmake informed decisions. This paper presents TNNT, a toolkit that automates the\nextraction of categorised named entities from unstructured information encoded\nin source documents, using diverse state-of-the-art Natural Language Processing\n(NLP) tools and NER models. TNNT integrates 21 different NER models as part of\na Knowledge Graph Construction Pipeline (KGCP) that takes a document set as\ninput and processes it based on the defined settings, applying the selected\nblocks of NER models to output the results. The toolkit generates all results\nwith an integrated summary of the extracted entities, enabling enhanced data\nanalysis to support the KGCP, and also, to aid further NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seneviratne_S/0/1/0/all/0/1\">Sandaru Seneviratne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mendez_S/0/1/0/all/0/1\">Sergio J. Rodr&#xed;guez M&#xe9;ndez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xuecheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Omran_P/0/1/0/all/0/1\">Pouya G. Omran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Taylor_K/0/1/0/all/0/1\">Kerry Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haller_A/0/1/0/all/0/1\">Armin Haller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Plan-then-Generate: Controlled Data-to-Text Generation via Planning. (arXiv:2108.13740v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13740","description":"<p>Recent developments in neural networks have led to the advance in\ndata-to-text generation. However, the lack of ability of neural models to\ncontrol the structure of generated output can be limiting in certain real-world\napplications. In this study, we propose a novel Plan-then-Generate (PlanGen)\nframework to improve the controllability of neural data-to-text models.\nExtensive experiments and analyses are conducted on two benchmark datasets,\nToTTo and WebNLG. The results show that our model is able to control both the\nintra-sentence and inter-sentence structure of the generated output.\nFurthermore, empirical comparisons against previous state-of-the-art methods\nshow that our model improves the generation quality as well as the output\ndiversity as judged by human and automatic evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vandyke_D/0/1/0/all/0/1\">David Vandyke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Sihui Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yimai Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Monolingual versus Multilingual BERTology for Vietnamese Extractive Multi-Document Summarization. (arXiv:2108.13741v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13741","description":"<p>Recent researches have demonstrated that BERT shows potential in a wide range\nof natural language processing tasks. It is adopted as an encoder for many\nstate-of-the-art automatic summarizing systems, which achieve excellent\nperformance. However, so far, there is not much work done for Vietnamese. In\nthis paper, we showcase how BERT can be implemented for extractive text\nsummarization in Vietnamese. We introduce a novel comparison between different\nmultilingual and monolingual BERT models. The experiment results indicate that\nmonolingual models produce promising results compared to other multilingual\nmodels and previous text summarizing models for Vietnamese.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Quoc_H/0/1/0/all/0/1\">Huy To Quoc</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1\">Kiet Van Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1\">Ngan Luu-Thuy Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1\">Anh Gia-Tuan Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Search Engine for Discovery of Biomedical Challenges and Directions. (arXiv:2108.13751v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13751","description":"<p>The ability to keep track of scientific challenges, advances and emerging\ndirections is a fundamental part of research. However, researchers face a flood\nof papers that hinders discovery of important knowledge. In biomedicine, this\ndirectly impacts human lives. To address this problem, we present a novel task\nof extraction and search of scientific challenges and directions, to facilitate\nrapid knowledge discovery. We construct and release an expert-annotated corpus\nof texts sampled from full-length papers, labeled with novel semantic\ncategories that generalize across many types of challenges and directions. We\nfocus on a large corpus of interdisciplinary work relating to the COVID-19\npandemic, ranging from biomedicine to areas such as AI and economics. We apply\na model trained on our data to identify challenges and directions across the\ncorpus and build a dedicated search engine for this information. In studies\nwith researchers, including those working directly on COVID-19, we outperform a\npopular scientific search engine in assisting knowledge discovery. Finally, we\nshow that models trained on our resource generalize to the wider biomedical\ndomain, highlighting its broad utility. We make our data, model and search\nengine publicly available. https://challenges.apps.allenai.org\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lahav_D/0/1/0/all/0/1\">Dan Lahav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Falcon_J/0/1/0/all/0/1\">Jon Saad Falcon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuehl_B/0/1/0/all/0/1\">Bailey Kuehl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parasa_S/0/1/0/all/0/1\">Sravanthi Parasa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shomron_N/0/1/0/all/0/1\">Noam Shomron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1\">Duen Horng Chau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel S. Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enjoy the Salience: Towards Better Transformer-based Faithful Explanations with Word Salience. (arXiv:2108.13759v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13759","description":"<p>Pretrained transformer-based models such as BERT have demonstrated\nstate-of-the-art predictive performance when adapted into a range of natural\nlanguage processing tasks. An open problem is how to improve the faithfulness\nof explanations (rationales) for the predictions of these models. In this\npaper, we hypothesize that salient information extracted a priori from the\ntraining data can complement the task-specific information learned by the model\nduring fine-tuning on a downstream task. In this way, we aim to help BERT not\nto forget assigning importance to informative input tokens when making\npredictions by proposing SaLoss; an auxiliary loss function for guiding the\nmulti-head attention mechanism during training to be close to salient\ninformation extracted a priori using TextRank. Experiments for explanation\nfaithfulness across five datasets, show that models trained with SaLoss\nconsistently provide more faithful explanations across four different feature\nattribution methods compared to vanilla BERT. Using the rationales extracted\nfrom vanilla BERT and SaLoss models to train inherently faithful classifiers,\nwe further show that the latter result in higher predictive performance in\ndownstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chrysostomou_G/0/1/0/all/0/1\">George Chrysostomou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The five Is: Key principles for interpretable and safe conversational AI. (arXiv:2108.13766v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13766","description":"<p>In this position paper, we present five key principles, namely\ninterpretability, inherent capability to explain, independent data, interactive\nlearning, and inquisitiveness, for the development of conversational AI that,\nunlike the currently popular black box approaches, is transparent and\naccountable. At present, there is a growing concern with the use of black box\nstatistical language models: While displaying impressive average performance,\nsuch systems are also prone to occasional spectacular failures, for which there\nis no clear remedy. In an effort to initiate a discussion on possible\nalternatives, we outline and exemplify how our five principles enable the\ndevelopment of conversational AI systems that are transparent and thus safer\nfor use. We also present some of the challenges inherent in the implementation\nof those principles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wahde_M/0/1/0/all/0/1\">Mattias Wahde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Virgolin_M/0/1/0/all/0/1\">Marco Virgolin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Network psychometrics and cognitive network science open new ways for detecting, understanding and tackling the complexity of math anxiety: A review. (arXiv:2108.13800v1 [cs.SI])","link":"http://arxiv.org/abs/2108.13800","description":"<p>Math anxiety is a clinical pathology impairing cognitive processing in\nmath-related contexts. Originally thought to affect only inexperienced,\nlow-achieving students, recent investigations show how math anxiety is vastly\ndiffused even among high-performing learners. This review of data-informed\nstudies outlines math anxiety as a complex system that: (i) cripples\nwell-being, self-confidence and information processing on both conscious and\nsubconscious levels, (ii) can be transmitted by social interactions, like a\npathogen, and worsened by distorted perceptions, (iii) affects roughly 20% of\nstudents in 63 out of 64 worldwide educational systems but correlates weakly\nwith academic performance, and (iv) poses a concrete threat to students'\nwell-being, computational literacy and career prospects in science. These\npatterns underline the crucial need to go beyond performance for estimating\nmath anxiety. Recent advances with network psychometrics and cognitive network\nscience provide ideal frameworks for detecting, interpreting and intervening\nupon such clinical condition. Merging education research, psychology and data\nscience, the approaches reviewed here reconstruct psychological constructs as\ncomplex systems, represented either as multivariate correlation models (e.g.\ngraph exploratory analysis) or as cognitive networks of semantic/emotional\nassociations (e.g. free association networks or forma mentis networks). Not\nonly can these interconnected networks detect otherwise hidden levels of math\nanxiety but - more crucially - they can unveil the specific layout of\ninteracting factors, e.g. key sources and targets, behind math anxiety in a\ngiven cohort. As discussed here, these network approaches open concrete ways\nfor unveiling students' perceptions, emotions and mental well-being, and can\nenable future powerful data-informed interventions untangling math anxiety.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Stella_M/0/1/0/all/0/1\">Massimo Stella</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TREND: Trigger-Enhanced Relation-Extraction Network for Dialogues. (arXiv:2108.13811v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13811","description":"<p>The goal of dialogue relation extraction (DRE) is to identify the relation\nbetween two entities in a given dialogue. During conversations, speakers may\nexpose their relations to certain entities by some clues, such evidences called\n\"triggers\". However, none of the existing work on DRE tried to detect triggers\nand leverage the information for enhancing the performance. This paper proposes\nTREND, a multi-tasking BERT-based model which learns to identify triggers for\nimproving relation extraction. The experimental results show that the proposed\nmethod achieves the state-of-the-art on the benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Po-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shang-Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Open-Domain Question Answering. (arXiv:2108.13817v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13817","description":"<p>Open-domain Question Answering (ODQA) has achieved significant results in\nterms of supervised learning manner. However, data annotation cannot also be\nirresistible for its huge demand in an open domain. Though unsupervised QA or\nunsupervised Machine Reading Comprehension (MRC) has been tried more or less,\nunsupervised ODQA has not been touched according to our best knowledge. This\npaper thus pioneers the work of unsupervised ODQA by formally introducing the\ntask and proposing a series of key data construction methods. Our exploration\nin this work inspiringly shows unsupervised ODQA can reach up to 86%\nperformance of supervised ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1\">Pengfei Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoguang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jian Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Domain Adaptation for Question Answering using Limited Text Corpora. (arXiv:2108.13854v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13854","description":"<p>Question generation has recently shown impressive results in customizing\nquestion answering (QA) systems to new domains. These approaches circumvent the\nneed for manually annotated training data from the new domain and, instead,\ngenerate synthetic question-answer pairs that are used for training. However,\nexisting methods for question generation rely on large amounts of synthetically\ngenerated datasets and costly computational resources, which render these\ntechniques widely inaccessible when the text corpora is of limited size. This\nis problematic as many niche domains rely on small text corpora, which\nnaturally restricts the amount of synthetic data that can be generated. In this\npaper, we propose a novel framework for domain adaptation called contrastive\ndomain adaptation for QA (CAQA). Specifically, CAQA combines techniques from\nquestion generation and domain-invariant learning to answer out-of-domain\nquestions in settings with limited text corpora. Here, we train a QA system on\nboth source data and generated data from the target domain with a contrastive\nadaptation loss that is incorporated in the training objective. By combining\ntechniques from question generation and domain-invariant learning, our model\nachieved considerable improvements compared to state-of-the-art baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yue_Z/0/1/0/all/0/1\">Zhenrui Yue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kratzwald_B/0/1/0/all/0/1\">Bernhard Kratzwald</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feuerriegel_S/0/1/0/all/0/1\">Stefan Feuerriegel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Beyond Model Extraction: Imitation Attack for Black-Box NLP APIs. (arXiv:2108.13873v1 [cs.CR])","link":"http://arxiv.org/abs/2108.13873","description":"<p>Machine-learning-as-a-service (MLaaS) has attracted millions of users to\ntheir outperforming sophisticated models. Although published as black-box APIs,\nthe valuable models behind these services are still vulnerable to imitation\nattacks. Recently, a series of works have demonstrated that attackers manage to\nsteal or extract the victim models. Nonetheless, none of the previous stolen\nmodels can outperform the original black-box APIs. In this work, we take the\nfirst step of showing that attackers could potentially surpass victims via\nunsupervised domain adaptation and multi-victim ensemble. Extensive experiments\non benchmark datasets and real-world APIs validate that the imitators can\nsucceed in outperforming the original black-box models. We consider this as a\nmilestone in the research of imitation attack, especially on NLP APIs, as the\nsuperior performance could influence the defense or even publishing strategy of\nAPI providers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiongkai Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xuanli He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lyu_L/0/1/0/all/0/1\">Lingjuan Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When Retriever-Reader Meets Scenario-Based Multiple-Choice Questions. (arXiv:2108.13875v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13875","description":"<p>Scenario-based question answering (SQA) requires retrieving and reading\nparagraphs from a large corpus to answer a question which is contextualized by\na long scenario description. Since a scenario contains both keyphrases for\nretrieval and much noise, retrieval for SQA is extremely difficult. Moreover,\nit can hardly be supervised due to the lack of relevance labels of paragraphs\nfor SQA. To meet the challenge, in this paper we propose a joint\nretriever-reader model called JEEVES where the retriever is implicitly\nsupervised only using QA labels via a novel word weighting mechanism. JEEVES\nsignificantly outperforms a variety of strong baselines on multiple-choice\nquestions in three SQA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1\">Zixian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1\">Ao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1\">Yulin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1\">Gong Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1\">Yuzhong Qu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Backdoor Attacks on Pre-trained Models by Layerwise Weight Poisoning. (arXiv:2108.13888v1 [cs.CR])","link":"http://arxiv.org/abs/2108.13888","description":"<p>\\textbf{P}re-\\textbf{T}rained \\textbf{M}odel\\textbf{s} have been widely\napplied and recently proved vulnerable under backdoor attacks: the released\npre-trained weights can be maliciously poisoned with certain triggers. When the\ntriggers are activated, even the fine-tuned model will predict pre-defined\nlabels, causing a security threat. These backdoors generated by the poisoning\nmethods can be erased by changing hyper-parameters during fine-tuning or\ndetected by finding the triggers. In this paper, we propose a stronger\nweight-poisoning attack method that introduces a layerwise weight poisoning\nstrategy to plant deeper backdoors; we also introduce a combinatorial trigger\nthat cannot be easily detected. The experiments on text classification tasks\nshow that previous defense methods cannot resist our weight-poisoning method,\nwhich indicates that our method can be widely applied and may provide hints for\nfuture model robustness studies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Demin Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiehang Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Like Article, Like Audience: Enforcing Multimodal Correlations for Disinformation Detection. (arXiv:2108.13892v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13892","description":"<p>User-generated content (e.g., tweets and profile descriptions) and shared\ncontent between users (e.g., news articles) reflect a user's online identity.\nThis paper investigates whether correlations between user-generated and\nuser-shared content can be leveraged for detecting disinformation in online\nnews articles. We develop a multimodal learning algorithm for disinformation\ndetection. The latent representations of news articles and user-generated\ncontent allow that during training the model is guided by the profile of users\nwho prefer content similar to the news article that is evaluated, and this\neffect is reinforced if that content is shared among different users. By only\nleveraging user information during model optimization, the model does not rely\non user profiling when predicting an article's veracity. The algorithm is\nsuccessfully applied to three widely used neural classifiers, and results are\nobtained on different datasets. Visualization techniques show that the proposed\nmodel learns feature representations of unseen news articles that better\ndiscriminate between fake and real news texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Allein_L/0/1/0/all/0/1\">Liesbeth Allein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perrotta_D/0/1/0/all/0/1\">Domenico Perrotta</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"mMARCO: A Multilingual Version of MS MARCO Passage Ranking Dataset. (arXiv:2108.13897v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13897","description":"<p>The MS MARCO ranking dataset has been widely used for training deep learning\nmodels for IR tasks, achieving considerable effectiveness on diverse zero-shot\nscenarios. However, this type of resource is scarce in other languages than\nEnglish. In this work we present mMARCO, a multilingual version of the MS MARCO\npassage ranking dataset comprising 8 languages that was created using machine\ntranslation. We evaluated mMARCO by fine-tuning mono and multilingual\nre-ranking models on it. Experimental results demonstrate that multilingual\nmodels fine-tuned on our translated dataset achieve superior effectiveness than\nmodels fine-tuned on the original English version alone. Also, our distilled\nmultilingual re-ranker is competitive with non-distilled models while having\n5.4 times fewer parameters. The translated datasets as well as fine-tuned\nmodels are available at https://github.com/unicamp-dl/mMARCO.git.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bonifacio_L/0/1/0/all/0/1\">Luiz Henrique Bonifacio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Campiotti_I/0/1/0/all/0/1\">Israel Campiotti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lotufo_R/0/1/0/all/0/1\">Roberto Lotufo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nogueira_R/0/1/0/all/0/1\">Rodrigo Nogueira</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The emojification of sentiment on social media: Collection and analysis of a longitudinal Twitter sentiment dataset. (arXiv:2108.13898v1 [cs.SI])","link":"http://arxiv.org/abs/2108.13898","description":"<p>Social media, as a means for computer-mediated communication, has been\nextensively used to study the sentiment expressed by users around events or\ntopics. There is however a gap in the longitudinal study of how sentiment\nevolved in social media over the years. To fill this gap, we develop TM-Senti,\na new large-scale, distantly supervised Twitter sentiment dataset with over 184\nmillion tweets and covering a time period of over seven years. We describe and\nassess our methodology to put together a large-scale, emoticon- and emoji-based\nlabelled sentiment analysis dataset, along with an analysis of the resulting\ndataset. Our analysis highlights interesting temporal changes, among others in\nthe increasing use of emojis over emoticons. We publicly release the dataset\nfor further research in tasks including sentiment analysis and text\nclassification of tweets. The dataset can be fully rehydrated including tweet\nmetadata and without missing tweets thanks to the archive of tweets publicly\navailable on the Internet Archive, which the dataset is based on.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1\">Wenjie Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alkhalifa_R/0/1/0/all/0/1\">Rabab Alkhalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zubiaga_A/0/1/0/all/0/1\">Arkaitz Zubiaga</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Retrieval Augmented Generation for Zero-shot Slot Filling. (arXiv:2108.13934v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13934","description":"<p>Automatically inducing high quality knowledge graphs from a given collection\nof documents still remains a challenging problem in AI. One way to make headway\nfor this problem is through advancements in a related task known as slot\nfilling. In this task, given an entity query in form of [Entity, Slot, ?], a\nsystem is asked to fill the slot by generating or extracting the missing value\nexploiting evidence extracted from relevant passage(s) in the given document\ncollection. The recent works in the field try to solve this task in an\nend-to-end fashion using retrieval-based language models. In this paper, we\npresent a novel approach to zero-shot slot filling that extends dense passage\nretrieval with hard negatives and robust training procedures for retrieval\naugmented generation models. Our model reports large improvements on both T-REx\nand zsRE slot filling datasets, improving both passage retrieval and slot value\ngeneration, and ranking at the top-1 position in the KILT leaderboard.\nMoreover, we demonstrate the robustness of our system showing its domain\nadaptation capability on a new variant of the TACRED dataset for slot filling,\nthrough a combination of zero/few-shot learning. We release the source code and\npre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Thermostat: A Large Collection of NLP Model Explanations and Analysis Tools. (arXiv:2108.13961v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13961","description":"<p>In the language domain, as in other domains, neural explainability takes an\never more important role, with feature attribution methods on the forefront.\nMany such methods require considerable computational resources and expert\nknowledge about implementation details and parameter choices. To facilitate\nresearch, we present Thermostat which consists of a large collection of model\nexplanations and accompanying analysis tools. Thermostat allows easy access to\nover 200k explanations for the decisions of prominent state-of-the-art models\nspanning across different NLP tasks, generated with multiple explainers. The\ndataset took over 10k GPU hours (&gt; one year) to compile; compute time that the\ncommunity now saves. The accompanying software tools allow to analyse\nexplanations instance-wise but also accumulatively on corpus level. Users can\ninvestigate and compare models, datasets and explainers without the need to\norchestrate implementation details. Thermostat is fully open source,\ndemocratizes explainability research in the language domain, circumvents\nredundant computations and increases comparability and replicability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feldhus_N/0/1/0/all/0/1\">Nils Feldhus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schwarzenberg_R/0/1/0/all/0/1\">Robert Schwarzenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1\">Sebastian M&#xf6;ller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v1 [cs.CL])","link":"http://arxiv.org/abs/2108.13990","description":"<p>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,\nbut how to properly use them for dialogue state tracking has not been\nsystematically investigated. In this paper, we study this problem from the\nperspectives of pre-training objectives as well as the formats of context\nrepresentations. We demonstrate that the choice of pre-training objective makes\na significant difference to the state tracking quality. In particular, we find\nthat masked span prediction is more effective than auto-regressive language\nmodeling. We also explore using Pegasus, a span prediction-based pre-training\nobjective for text summarization, for the state tracking model. We found that\npre-training for the seemingly distant summarization task works surprisingly\nwell for dialogue state tracking. In addition, we found that while recurrent\nstate context representation works also reasonably well, the model may have a\nhard time recovering from earlier mistakes. We conducted experiments on the\nMultiWOZ 2.1-2.4 data sets with consistent observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdieh_M/0/1/0/all/0/1\">Mahdis Mahdieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Generative Approach for Mitigating Structural Biases in Natural Language Inference. (arXiv:2108.14006v1 [cs.CL])","link":"http://arxiv.org/abs/2108.14006","description":"<p>Many natural language inference (NLI) datasets contain biases that allow\nmodels to perform well by only using a biased subset of the input, without\nconsidering the remainder features. For instance, models are able to make a\nclassification decision by only using the hypothesis, without learning the true\nrelationship between it and the premise. These structural biases lead\ndiscriminative models to learn unintended superficial features and to\ngeneralize poorly out of the training distribution. In this work, we\nreformulate the NLI task as a generative task, where a model is conditioned on\nthe biased subset of the input and the label and generates the remaining subset\nof the input. We show that by imposing a uniform prior, we obtain a provably\nunbiased model. Through synthetic experiments, we find that this approach is\nhighly robust to large amounts of bias. We then demonstrate empirically on two\ntypes of natural bias that this approach leads to fully unbiased models in\npractice. However, we find that generative models are difficult to train and\nthey generally perform worse than discriminative baselines. We highlight the\ndifficulty of the generative modeling task in the context of NLI as a cause for\nthis worse performance. Finally, by fine-tuning the generative model with a\ndiscriminative objective, we reduce the performance gap between the generative\nmodel and the discriminative baseline, while allowing for a small amount of\nbias.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Asael_D/0/1/0/all/0/1\">Dimion Asael</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_Z/0/1/0/all/0/1\">Zachary Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Belinkov_Y/0/1/0/all/0/1\">Yonatan Belinkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HUMBO: Bridging Response Generation and Facial Expression Synthesis. (arXiv:1905.11240v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1905.11240","description":"<p>Spoken dialogue systems that assist users to solve complex tasks such as\nmovie ticket booking have become an emerging research topic in artificial\nintelligence and natural language processing areas. With a well-designed\ndialogue system as an intelligent personal assistant, people can accomplish\ncertain tasks more easily via natural language interactions. Today there are\nseveral virtual intelligent assistants in the market; however, most systems\nonly focus on textual or vocal interaction. In this paper, we present HUMBO, a\nsystem aiming at generating dialogue responses and simultaneously synthesize\ncorresponding visual expressions on faces for better multimodal interaction.\nHUMBO can (1) let users determine the appearances of virtual assistants by a\nsingle image, and (2) generate coherent emotional utterances and facial\nexpressions on the user-provided image. This is not only a brand new research\ndirection but more importantly, an ultimate step toward more human-like virtual\nassistants.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1\">Shang-Yu Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1\">Po-Wei Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yun-Nung Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reasoning Visual Dialog with Sparse Graph Learning and Knowledge Transfer. (arXiv:2004.06698v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2004.06698","description":"<p>Visual dialog is a task of answering a sequence of questions grounded in an\nimage using the previous dialog history as context. In this paper, we study how\nto address two fundamental challenges for this task: (1) reasoning over\nunderlying semantic structures among dialog rounds and (2) identifying several\nappropriate answers to the given question. To address these challenges, we\npropose a Sparse Graph Learning (SGL) method to formulate visual dialog as a\ngraph structure learning task. SGL infers inherently sparse dialog structures\nby incorporating binary and score edges and leveraging a new structural loss\nfunction. Next, we introduce a Knowledge Transfer (KT) method that extracts the\nanswer predictions from the teacher model and uses them as pseudo labels. We\npropose KT to remedy the shortcomings of single ground-truth labels, which\nseverely limit the ability of a model to obtain multiple reasonable answers. As\na result, our proposed model significantly improves reasoning capability\ncompared to baseline methods and outperforms the state-of-the-art approaches on\nthe VisDial v1.0 dataset. The source code is available at\nhttps://github.com/gicheonkang/SGLKT-VisDial.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1\">Gi-Cheon Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1\">Junseok Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hwaran Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1\">Byoung-Tak Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jin-Hwa Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural CRF Model for Sentence Alignment in Text Simplification. (arXiv:2005.02324v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.02324","description":"<p>The success of a text simplification system heavily depends on the quality\nand quantity of complex-simple sentence pairs in the training corpus, which are\nextracted by aligning sentences between parallel articles. To evaluate and\nimprove sentence alignment quality, we create two manually annotated\nsentence-aligned datasets from two commonly used text simplification corpora,\nNewsela and Wikipedia. We propose a novel neural CRF alignment model which not\nonly leverages the sequential nature of sentences in parallel documents but\nalso utilizes a neural sentence pair model to capture semantic similarity.\nExperiments demonstrate that our proposed approach outperforms all the previous\nwork on monolingual sentence alignment task by more than 5 points in F1. We\napply our CRF aligner to construct two new text simplification datasets,\nNewsela-Auto and Wiki-Auto, which are much larger and of better quality\ncompared to the existing datasets. A Transformer-based seq2seq model trained on\nour datasets establishes a new state-of-the-art for text simplification in both\nautomatic and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1\">Chao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddela_M/0/1/0/all/0/1\">Mounica Maddela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1\">Wuwei Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1\">Yang Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1\">Wei Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Dialogue Response Selection with Hierarchical Curriculum Learning. (arXiv:2012.14756v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14756","description":"<p>We study the learning of a matching model for dialogue response selection.\nMotivated by the recent finding that models trained with random negative\nsamples are not ideal in real-world scenarios, we propose a hierarchical\ncurriculum learning framework that trains the matching model in an\n\"easy-to-difficult\" scheme. Our learning framework consists of two\ncomplementary curricula: (1) corpus-level curriculum (CC); and (2)\ninstance-level curriculum (IC). In CC, the model gradually increases its\nability in finding the matching clues between the dialogue context and a\nresponse candidate. As for IC, it progressively strengthens the model's ability\nin identifying the mismatching information between the dialogue context and a\nresponse candidate. Empirical studies on three benchmark datasets with three\nstate-of-the-art matching models demonstrate that the proposed learning\nframework significantly improves the model performance across various\nevaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1\">Deng Cai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyu Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1\">Zibo Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_S/0/1/0/all/0/1\">Simon Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yunbo Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressive Transformer-Based Generation of Radiology Reports. (arXiv:2102.09777v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.09777","description":"<p>Inspired by Curriculum Learning, we propose a consecutive (i.e.,\nimage-to-text-to-text) generation framework where we divide the problem of\nradiology report generation into two steps. Contrary to generating the full\nradiology report from the image at once, the model generates global concepts\nfrom the image in the first step and then reforms them into finer and coherent\ntexts using a transformer architecture. We follow the transformer-based\nsequence-to-sequence paradigm at each step. We improve upon the\nstate-of-the-art on two benchmark datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nooralahzadeh_F/0/1/0/all/0/1\">Farhad Nooralahzadeh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_N/0/1/0/all/0/1\">Nicolas Perez Gonzalez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frauenfelder_T/0/1/0/all/0/1\">Thomas Frauenfelder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fujimoto_K/0/1/0/all/0/1\">Koji Fujimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Krauthammer_M/0/1/0/all/0/1\">Michael Krauthammer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TSDAE: Using Transformer-based Sequential Denoising Auto-Encoder for Unsupervised Sentence Embedding Learning. (arXiv:2104.06979v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06979","description":"<p>Learning sentence embeddings often requires a large amount of labeled data.\nHowever, for most tasks and domains, labeled data is seldom available and\ncreating it is expensive. In this work, we present a new state-of-the-art\nunsupervised method based on pre-trained Transformers and Sequential Denoising\nAuto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points.\nIt can achieve up to 93.1% of the performance of in-domain supervised\napproaches. Further, we show that TSDAE is a strong domain adaptation and\npre-training method for sentence embeddings, significantly outperforming other\napproaches like Masked Language Model.\n</p>\n<p>A crucial shortcoming of previous studies is the narrow evaluation: Most work\nmainly evaluates on the single task of Semantic Textual Similarity (STS), which\ndoes not require any domain knowledge. It is unclear if these proposed methods\ngeneralize to other domains and tasks. We fill this gap and evaluate TSDAE and\nother recent approaches on four different datasets from heterogeneous domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1\">Kexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08315","description":"<p>Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n</p>\n<p>However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n</p>\n<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SimCSE: Simple Contrastive Learning of Sentence Embeddings. (arXiv:2104.08821v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08821","description":"<p>This paper presents SimCSE, a simple contrastive learning framework that\ngreatly advances the state-of-the-art sentence embeddings. We first describe an\nunsupervised approach, which takes an input sentence and predicts itself in a\ncontrastive objective, with only standard dropout used as noise. This simple\nmethod works surprisingly well, performing on par with previous supervised\ncounterparts. We hypothesize that dropout acts as minimal data augmentation and\nremoving it leads to a representation collapse. Then, we incorporate annotated\npairs from natural language inference datasets into our contrastive learning\nframework, by using \"entailment\" pairs as positives and \"contradiction\" pairs\nas hard negatives. We evaluate SimCSE on standard semantic textual similarity\n(STS) tasks, and our unsupervised and supervised models using BERT-base achieve\nan average of 76.3% and 81.6% Spearman's correlation respectively, a 4.2 and\n2.2 points improvement compared to previous best results. We also show -- both\ntheoretically and empirically -- that contrastive learning objective\nregularizes pre-trained embeddings' anisotropic space to be more uniform, and\nit better aligns positive pairs when supervised signals are available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1\">Tianyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1\">Xingcheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UniKeyphrase: A Unified Extraction and Generation Framework for Keyphrase Prediction. (arXiv:2106.04847v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.04847","description":"<p>Keyphrase Prediction (KP) task aims at predicting several keyphrases that can\nsummarize the main idea of the given document. Mainstream KP methods can be\ncategorized into purely generative approaches and integrated models with\nextraction and generation. However, these methods either ignore the diversity\namong keyphrases or only weakly capture the relation across tasks implicitly.\nIn this paper, we propose UniKeyphrase, a novel end-to-end learning framework\nthat jointly learns to extract and generate keyphrases. In UniKeyphrase,\nstacked relation layer and bag-of-words constraint are proposed to fully\nexploit the latent semantic relation between extraction and generation in the\nview of model structure and training process, respectively. Experiments on KP\nbenchmarks demonstrate that our joint approach outperforms mainstream methods\nby a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Huanqin Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1\">Wei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_D/0/1/0/all/0/1\">Dan Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Tao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1\">Feng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Di Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Data Augmentation for Text Classification. (arXiv:2107.03158v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.03158","description":"<p>Data augmentation, the artificial creation of training data for machine\nlearning by transformations, is a widely studied research field across machine\nlearning disciplines. While it is useful for increasing the generalization\ncapabilities of a model, it can also address many other challenges and\nproblems, from overcoming a limited amount of training data over regularizing\nthe objective to limiting the amount data used to protect privacy. Based on a\nprecise description of the goals and applications of data augmentation (C1) and\na taxonomy for existing works (C2), this survey is concerned with data\naugmentation methods for textual classification and aims to achieve a concise\nand comprehensive overview for researchers and practitioners (C3). Derived from\nthe taxonomy, we divided more than 100 methods into 12 different groupings and\nprovide state-of-the-art references expounding which methods are highly\npromising (C4). Finally, research perspectives that may constitute a building\nblock for future work are given (C5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bayer_M/0/1/0/all/0/1\">Markus Bayer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kaufhold_M/0/1/0/all/0/1\">Marc-Andr&#xe9; Kaufhold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reuter_C/0/1/0/all/0/1\">Christian Reuter</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Argumentative Dialogue System for COVID-19 Vaccine Information. (arXiv:2107.12079v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.12079","description":"<p>Dialogue systems are widely used in AI to support timely and interactive\ncommunication with users. We propose a general-purpose dialogue system\narchitecture that leverages computational argumentation to perform reasoning\nand provide consistent and explainable answers. We illustrate the system using\na COVID-19 vaccine information case study.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fazzinga_B/0/1/0/all/0/1\">Bettina Fazzinga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galassi_A/0/1/0/all/0/1\">Andrea Galassi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Torroni_P/0/1/0/all/0/1\">Paolo Torroni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Goal-Oriented Script Construction. (arXiv:2107.13189v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.13189","description":"<p>The knowledge of scripts, common chains of events in stereotypical scenarios,\nis a valuable asset for task-oriented natural language understanding systems.\nWe propose the Goal-Oriented Script Construction task, where a model produces a\nsequence of steps to accomplish a given goal. We pilot our task on the first\nmultilingual script learning dataset supporting 18 languages collected from\nwikiHow, a website containing half a million how-to articles. For baselines, we\nconsider both a generation-based approach using a language model and a\nretrieval-based approach by first retrieving the relevant steps from a large\ncandidate pool and then ordering them. We show that our task is practical,\nfeasible but challenging for state-of-the-art Transformer models, and that our\nmethods can be readily deployed for various other datasets and domains with\ndecent zero-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lyu_Q/0/1/0/all/0/1\">Qing Lyu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Li Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GENder-IT: An Annotated English-Italian Parallel Challenge Set for Cross-Linguistic Natural Gender Phenomena. (arXiv:2108.02854v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02854","description":"<p>Languages differ in terms of the absence or presence of gender features, the\nnumber of gender classes and whether and where gender features are explicitly\nmarked. These cross-linguistic differences can lead to ambiguities that are\ndifficult to resolve, especially for sentence-level MT systems. The\nidentification of ambiguity and its subsequent resolution is a challenging task\nfor which currently there aren't any specific resources or challenge sets\navailable. In this paper, we introduce gENder-IT, an English--Italian challenge\nset focusing on the resolution of natural gender phenomena by providing\nword-level gender tags on the English source side and multiple gender\nalternative translations, where needed, on the Italian target side.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1\">Eva Vanmassenhove</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monti_J/0/1/0/all/0/1\">Johanna Monti</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hatemoji: A Test Suite and Adversarially-Generated Dataset for Benchmarking and Detecting Emoji-based Hate. (arXiv:2108.05921v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.05921","description":"<p>Detecting online hate is a complex task, and low-performing models have\nharmful consequences when used for sensitive applications such as content\nmoderation. Emoji-based hate is a key emerging challenge for automated\ndetection. We present HatemojiCheck, a test suite of 3,930 short-form\nstatements that allows us to evaluate performance on hateful language expressed\nwith emoji. Using the test suite, we expose weaknesses in existing hate\ndetection models. To address these weaknesses, we create the HatemojiTrain\ndataset using a human-and-model-in-the-loop approach. Models trained on these\n5,912 adversarial examples perform substantially better at detecting\nemoji-based hate, while retaining strong performance on text-only hate. Both\nHatemojiCheck and HatemojiTrain are made publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kirk_H/0/1/0/all/0/1\">Hannah Rose Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vidgen_B/0/1/0/all/0/1\">Bertram Vidgen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thrush_T/0/1/0/all/0/1\">Tristan Thrush</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hale_S/0/1/0/all/0/1\">Scott A. Hale</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CushLEPOR: Customised hLEPOR Metric Using LABSE Distilled Knowledge Model to Improve Agreement with Human Judgements. (arXiv:2108.09484v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09484","description":"<p>Human evaluation has always been expensive while researchers struggle to\ntrust the automatic metrics. To address this, we propose to customise\ntraditional metrics by taking advantages of the pre-trained language models\n(PLMs) and the limited available human labelled scores. We first re-introduce\nthe hLEPOR metric factors, followed by the Python portable version we developed\nwhich achieved the automatic tuning of the weighting parameters in hLEPOR\nmetric. Then we present the customised hLEPOR (cushLEPOR) which uses LABSE\ndistilled knowledge model to improve the metric agreement with human judgements\nby automatically optimised factor weights regarding the exact MT language pairs\nthat cushLEPOR is deployed to. We also optimise cushLEPOR towards human\nevaluation data based on MQM and pSQM framework on English-German and\nChinese-English language pairs. The experimental investigations show cushLEPOR\nboosts hLEPOR performances towards better agreements to PLMs like LABSE with\nmuch lower cost, and better agreements to human evaluations including MQM and\npSQM scores, and yields much better performances than BLEU (data available at\n\\url{https://github.com/poethan/cushLEPOR}).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1\">Lifeng Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sorokina_I/0/1/0/all/0/1\">Irina Sorokina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1\">Gleb Erofeev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1\">Serge Gladkoff</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sarcasm Detection in Twitter -- Performance Impact while using Data Augmentation: Word Embeddings. (arXiv:2108.09924v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09924","description":"<p>Sarcasm is the use of words usually used to either mock or annoy someone, or\nfor humorous purposes. Sarcasm is largely used in social networks and\nmicroblogging websites, where people mock or censure in a way that makes it\ndifficult even for humans to tell if what is said is what is meant. Failure to\nidentify sarcastic utterances in Natural Language Processing applications such\nas sentiment analysis and opinion mining will confuse classification algorithms\nand generate false results. Several studies on sarcasm detection have utilized\ndifferent learning algorithms. However, most of these learning models have\nalways focused on the contents of expression only, leaving the contextual\ninformation in isolation. As a result, they failed to capture the contextual\ninformation in the sarcastic expression. Moreover, some datasets used in\nseveral studies have an unbalanced dataset which impacting the model result. In\nthis paper, we propose a contextual model for sarcasm identification in twitter\nusing RoBERTa, and augmenting the dataset by applying Global Vector\nrepresentation (GloVe) for the construction of word embedding and context\nlearning to generate more data and balancing the dataset. The effectiveness of\nthis technique is tested with various datasets and data augmentation settings.\nIn particular, we achieve performance gain by 3.2% in the iSarcasm dataset when\nusing data augmentation to increase 20% of data labeled as sarcastic, resulting\nF-score of 40.4% compared to 37.2% without data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Handoyo_A/0/1/0/all/0/1\">Alif Tri Handoyo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hidayaturrahman/0/1/0/all/0/1\">Hidayaturrahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhartono_D/0/1/0/all/0/1\">Derwin Suhartono</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions. (arXiv:2108.12189v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12189","description":"<p>This paper presents Macquarie University's participation to the BioASQ\nSynergy Task, and BioASQ9b Phase B. In each of these tasks, our participation\nfocused on the use of query-focused extractive summarisation to obtain the\nideal answers to medical questions. The Synergy Task is an end-to-end question\nanswering task on COVID-19 where systems are required to return relevant\ndocuments, snippets, and answers to a given question. Given the absence of\ntraining data, we used a query-focused summarisation system that was trained\nwith the BioASQ8b training data set and we experimented with methods to\nretrieve the documents and snippets. Considering the poor quality of the\ndocuments and snippets retrieved by our system, we observed reasonably good\nquality in the answers returned. For phase B of the BioASQ9b task, the relevant\ndocuments and snippets were already included in the test data. Our system split\nthe snippets into candidate sentences and used BERT variants under a sentence\nclassification setup. The system used the question and candidate sentence as\ninput and was trained to predict the likelihood of the candidate sentence being\npart of the ideal answer. The runs obtained either the best or second best\nROUGE-F1 results of all participants to all batches of BioASQ9b. This shows\nthat using BERT in a classification setup is a very strong baseline for the\nidentification of ideal answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molla_D/0/1/0/all/0/1\">Diego Moll&#xe1;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_U/0/1/0/all/0/1\">Urvashi Khanna</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Galat_D/0/1/0/all/0/1\">Dima Galat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vincent Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rybinski_M/0/1/0/all/0/1\">Maciej Rybinski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Table-to-Text Generation with Prototype Memory. (arXiv:2108.12516v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12516","description":"<p>Neural table-to-text generation models have achieved remarkable progress on\nan array of tasks. However, due to the data-hungry nature of neural models,\ntheir performances strongly rely on large-scale training examples, limiting\ntheir applicability in real-world applications. To address this, we propose a\nnew framework: Prototype-to-Generate (P2G), for table-to-text generation under\nthe few-shot scenario. The proposed framework utilizes the retrieved\nprototypes, which are jointly selected by an IR system and a novel prototype\nselector to help the model bridging the structural gap between tables and\ntexts. Experimental results on three benchmark datasets with three\nstate-of-the-art models demonstrate that the proposed framework significantly\nimproves the model performance across various evaluation metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1\">Yixuan Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1\">Zaiqiao Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baker_S/0/1/0/all/0/1\">Simon Baker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling the Knowledge of Large-scale Generative Models into Retrieval Models for Efficient Open-domain Conversation. (arXiv:2108.12582v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12582","description":"<p>Despite the remarkable performance of large-scale generative models in\nopen-domain conversation, they are known to be less practical for building\nreal-time conversation systems due to high latency. On the other hand,\nretrieval models could return responses with much lower latency but show\ninferior performance to the large-scale generative models since the\nconversation quality is bounded by the pre-defined response set. To take\nadvantage of both approaches, we propose a new training method called G2R\n(Generative-to-Retrieval distillation) that preserves the efficiency of a\nretrieval model while leveraging the conversational ability of a large-scale\ngenerative model by infusing the knowledge of the generative model into the\nretrieval model. G2R consists of two distinct techniques of distillation: the\ndata-level G2R augments the dialogue dataset with additional responses\ngenerated by the large-scale generative model, and the model-level G2R\ntransfers the response quality score assessed by the generative model to the\nscore of the retrieval model by the knowledge distillation loss. Through\nextensive experiments including human evaluation, we demonstrate that our\nretrieval-based conversation system trained with G2R shows a substantially\nimproved performance compared to the baseline retrieval model while showing\nsignificantly lower inference latency than the large-scale generative models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1\">Beomsu Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1\">Seokjun Seo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1\">Seungju Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Erdenee_E/0/1/0/all/0/1\">Enkhbayar Erdenee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1\">Buru Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scheduled Sampling Based on Decoding Steps for Neural Machine Translation. (arXiv:2108.12963v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12963","description":"<p>Scheduled sampling is widely used to mitigate the exposure bias problem for\nneural machine translation. Its core motivation is to simulate the inference\nscene during training by replacing ground-truth tokens with predicted tokens,\nthus bridging the gap between training and inference. However, vanilla\nscheduled sampling is merely based on training steps and equally treats all\ndecoding steps. Namely, it simulates an inference scene with uniform error\nrates, which disobeys the real inference scene, where larger decoding steps\nusually have higher error rates due to error accumulations. To alleviate the\nabove discrepancy, we propose scheduled sampling methods based on decoding\nsteps, increasing the selection chance of predicted tokens with the growth of\ndecoding steps. Consequently, we can more realistically simulate the inference\nscene during training, thus better bridging the gap between training and\ninference. Moreover, we investigate scheduled sampling based on both training\nsteps and decoding steps for further improvements. Experimentally, our\napproaches significantly outperform the Transformer baseline and vanilla\nscheduled sampling on three large-scale WMT tasks. Additionally, our approaches\nalso generalize well to the text summarization task on two popular benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Differentiable Prompt Makes Pre-trained Language Models Better Few-shot Learners. (arXiv:2108.13161v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13161","description":"<p>Large-scale pre-trained language models have contributed significantly to\nnatural language processing by demonstrating remarkable abilities as few-shot\nlearners. However, their effectiveness depends mainly on scaling the model\nparameters and prompt design, hindering their implementation in most real-world\napplications. This study proposes a novel pluggable, extensible, and efficient\napproach named DifferentiAble pRompT (DART), which can convert small language\nmodels into better few-shot learners without any prompt engineering. The main\nprinciple behind this approach involves reformulating potential natural\nlanguage processing tasks into the task of a pre-trained language model and\ndifferentially optimizing the prompt template as well as the target label with\nbackpropagation. Furthermore, the proposed approach can be: (i) Plugged to any\npre-trained language models; (ii) Extended to widespread classification tasks.\nA comprehensive evaluation of standard NLP tasks demonstrates that the proposed\napproach achieves a better few-shot performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HELMHOLTZ: A Verifier for Tezos Smart Contracts Based on Refinement Types. (arXiv:2108.12971v1 [cs.CL] CROSS LISTED)","link":"http://arxiv.org/abs/2108.12971","description":"<p>A smart contract is a program executed on a blockchain, based on which many\ncryptocurrencies are implemented, and is being used for automating\ntransactions. Due to the large amount of money that smart contracts deal with,\nthere is a surging demand for a method that can statically and formally verify\nthem.\n</p>\n<p>This article describes our type-based static verification tool HELMHOLTZ for\nMichelson, which is a statically typed stack-based language for writing smart\ncontracts that are executed on the blockchain platform Tezos. HELMHOLTZ is\ndesigned on top of our extension of Michelson's type system with refinement\ntypes. HELMHOLTZ takes a Michelson program annotated with a user-defined\nspecification written in the form of a refinement type as input; it then\ntypechecks the program against the specification based on the refinement type\nsystem, discharging the generated verification conditions with the SMT solver\nZ3. We briefly introduce our refinement type system for the core calculus\nMini-Michelson of Michelson, which incorporates the characteristic features\nsuch as compound datatypes (e.g., lists and pairs), higher-order functions, and\ninvocation of another contract. \\HELMHOLTZ{} successfully verifies several\npractical Michelson programs, including one that transfers money to an account\nand that checks a digital signature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_Y/0/1/0/all/0/1\">Yuki Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1\">Hiromasa Saito</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1\">Ran Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kawata_A/0/1/0/all/0/1\">Akira Kawata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Furuse_J/0/1/0/all/0/1\">Jun Furuse</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suenaga_K/0/1/0/all/0/1\">Kohei Suenaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Igarashi_A/0/1/0/all/0/1\">Atsushi Igarashi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-31T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","syn":"http://purl.org/rss/1.0/modules/syndication/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}