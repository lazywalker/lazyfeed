{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-09T01:54:24.218200276Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"【Rust日报】2021-09-08 -- 本月 Rust OSDev（2021 年 8 月）","link":"https://rustcc.cn/article?id=e6eaa344-4be9-4ead-9864-9f1c5dc568ad","description":"<h3>本月 Rust OSDev（2021 年 8 月）</h3>\n<h4>项目更新</h4>\n<ul>\n<li>\n<p>acpi - 包含用于解析 ACPI 表的 crate - 现代计算机的固件用于将有关硬件的信息传递给操作系统的数据结构。</p>\n<p>已将处理器 UID 和本地 APIC ID 的表示更改 <code>acpi::platform::Processor</code>为<code>u32</code>，以支持 X2APIC 用于支持更多处理器的更广泛的 ID。</p>\n</li>\n<li>\n<p>uefi-rs - 该uefi箱提供了安全，高性能包装UEFI。</p>\n</li>\n<li>\n<p>x86_64  - 提供了用于各种抽象x86_64系统，包括包装为CPU指令，访问处理器的专用寄存器，和抽象类型体系结构的特定结构，如页表和描述符表。</p>\n</li>\n<li>\n<p>bootloader - 实现了64位ELF可执行文件便于装载基于rust定制引导程序。</p>\n</li>\n<li>\n<p>multboot2 - 提供一个抽象类型multiboot2引导程序的引导信息。</p>\n</li>\n<li>\n<p>pic_8259 - 提供了用于8259个8259A可编程中断控制器（PICS）的抽象。</p>\n</li>\n</ul>\n<p>ReadMore:<a href=\"https://rust-osdev.com/this-month/2021-08/\" rel=\"noopener noreferrer\">https://rust-osdev.com/this-month/2021-08/</a></p>\n<h3>为什么 Rust 需要 Pin 和 unpin</h3>\n<ul>\n<li>什么是Futures</li>\n<li>什么是自引用类型</li>\n<li>为什么他们不安全</li>\n<li>Pin/Unpin 如何使它们安全</li>\n<li>使用 Pin/Unpin 编写复杂的futures</li>\n</ul>\n<p>ReadMore:<a href=\"https://blog.cloudflare.com/pin-and-unpin-in-rust/\" rel=\"noopener noreferrer\">https://blog.cloudflare.com/pin-and-unpin-in-rust/</a></p>\n<h3>htmlq</h3>\n<p>像 jq，但用于 HTML。使用 CSS 选择器从 HTML 文件中提取部分内容。</p>\n<p>例子:</p>\n<pre><code>$ curl --silent https://www.rust-lang.org/ | htmlq '#get-help'\n&lt;div class=\"four columns mt3 mt0-l\" id=\"get-help\"&gt;\n        &lt;h4&gt;Get help!&lt;/h4&gt;\n        &lt;ul&gt;\n          &lt;li&gt;&lt;a href=\"https://doc.rust-lang.org\"&gt;Documentation&lt;/a&gt;&lt;/li&gt;\n          &lt;li&gt;&lt;a href=\"https://users.rust-lang.org\"&gt;Ask a Question on the Users Forum&lt;/a&gt;&lt;/li&gt;\n          &lt;li&gt;&lt;a href=\"http://ping.rust-lang.org\"&gt;Check Website Status&lt;/a&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n        &lt;div class=\"languages\"&gt;\n            &lt;label class=\"hidden\" for=\"language-footer\"&gt;Language&lt;/label&gt;\n            &lt;select id=\"language-footer\"&gt;\n                &lt;option title=\"English (US)\" value=\"en-US\"&gt;English (en-US)&lt;/option&gt;\n&lt;option title=\"French\" value=\"fr\"&gt;Français (fr)&lt;/option&gt;\n&lt;option title=\"German\" value=\"de\"&gt;Deutsch (de)&lt;/option&gt;\n\n            &lt;/select&gt;\n        &lt;/div&gt;\n      &lt;/div&gt;\n</code></pre>\n<p>ReadMore:<a href=\"https://github.com/mgdm/htmlq\" rel=\"noopener noreferrer\">https://github.com/mgdm/htmlq</a></p>\n<hr>\n<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-08 15:57:38","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"刚学,对生命周期的疑问","link":"https://rustcc.cn/article?id=3582ba85-7c0f-451f-aea8-9da5d6b69905","description":"<p>中文文档中的示例 10-24\nfn main() {\nlet string1 = String::from(\"long string is long\");\nlet result;\n{\nlet string2 = String::from(\"xyz\");\nresult = longest(string1.as_str(), string2.as_str());\n}\nprintln!(\"The longest string is {}\", result);\n}\nfn longest&lt;'a&gt;(x: &amp;'a str, y: &amp;'a str) -&gt; &amp;'a str {\nif x.len() &gt; y.len() {\nx\n} else {\ny\n}\n}\n会出现<code>string2</code> does not live long enough。\n可是将\nlet string1 =\"long string is long\";\n{\nlet string2 = \"xyz\";\n//如上\n}\n改成这样却能正确输出结果</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-08 09:10:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"由于微软更新了webview2版本，tauri的程序不能打包release版本了","link":"https://rustcc.cn/article?id=6d630089-a902-4eca-a98c-86c6e72d70f6","description":"<p>rust GUI太难了</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-08 05:09:42","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust  调用c返回的动态数组","link":"https://rustcc.cn/article?id=a048c6cc-cbdd-4965-80d7-5aa0097678a7","description":"<p>C 使用malloc生成一个动态长度的数组，具体类型可知。C如何传递给rust 才能使rust知道该数组的长度和地址？</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 13:36:14","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"多地招聘 ｜安全性极高的Rust，想来学吗？","link":"https://rustcc.cn/article?id=a48abd3f-43dc-4456-bade-0f94ccfa72b4","description":"<p>非凸科技（https://ft.tech）正成为国内金融市场智能投资交易平台的引领者，在原有基础上全面升级到互联网新一代技术架构，采用Rust构建智能算法交易平台，逐步迭代，持续为券商、量化私募等众多大型金融机构提供优质的算法服务。</p>\n<p>Rust已经连续五年被开发人员评为“最为喜爱”的编程语言，因为它可以避免某些类型的内存安全错误，能从根本上改善软件漏洞的现状。</p>\n<p>Why Rust？安全、并发、实用，新时代的系统级编程语言。</p>\n<p>高性能: Rust 速度惊人且内存利用率极高。由于没有运行时和垃圾回收，它能够胜任对性能要求特别高的服务，可以在嵌入式设备上运行，还能轻松和其他语言集成。</p>\n<p>可靠性：Rust 丰富的类型系统和所有权模型保证了内存安全和线程安全，让您在编译期就能够消除各种各样的错误。</p>\n<p>生产力：Rust 拥有出色的文档、友好的编译器和清晰的错误提示信息， 还集成了一流的工具——包管理器和构建工具，智能地自动补全和类型检验的多编辑器支持， 以及自动格式化代码等。</p>\n<p>让Rust声名远播的优点还包括：提供C和C++的速度和控制能力，同时还提供了其他语言（如Go和Python）的安全性和安全性保证。MSRC将近70％的漏洞归类为内存安全问题，因此消除此类漏洞至关重要。</p>\n<p>现阶段，非凸科技正在寻找行业内优秀的Rust开发工程师，薪资福利超级优厚。关键是团队有很好的Rust开发氛围，Rust大神手把手辅导，助你从Rust新人不断升级。欢迎加入我们！</p>\n<p>公司福利：\n1.提供租房补贴；\n2.日常不间断网红零食、饮料、茶水供给；\n3.不定期组织各部门技术交流学习研讨会，分享心得，互相成长；\n4.团建活动丰富多彩，放松心情缓解疲劳。</p>\n<p>工作地点：北京、上海、成都、深圳、美国加州</p>\n<p>岗位：Rust开发工程师</p>\n<p>岗位职责：\n1.设计并开发基于RUST的高性能，低时延算法交易系统；\n2.设计并开发数据处理平台，监控运维平台；\n3.设计并开发面向客户的高可用交易工具等；\n4.设计并开发策略相关的回测平台。</p>\n<p>岗位要求：\n1.本科及以上学历（985优先）。编程基础扎实，具有良好的计算机理论基础；\n2.熟练掌握Linux操作，性能分析，具备Rust/C++/Java/Go丰富开发经验，熟悉常用的设计模式，有分布式相关经验加分；\n3.有研发高性能，低时延系统经验加分；\n4.对技术充满热情，思考深入。自我驱动，能快速学习新鲜事物。</p>\n<p>岗位薪酬：Base 30K-60K+，有期权激励</p>\n<p>投递邮箱：recruit@non-convex.com\n联系人加微信：SweeneyTodd333333</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 10:26:58","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-09-07","link":"https://rustcc.cn/article?id=fd95c63b-08d3-468a-9743-82feb56df3f2","description":"<h2>【大家的项目】 <code>Poem-openapi</code> v0.2.1 发布</h2>\n<p><a href=\"https://www.reddit.com/r/rust/comments/pjhoiu/poemopenapi_021_released/\" rel=\"noopener noreferrer\">Poem-openapi 0.2.1 released!</a></p>\n<p><strong>Poem-openapi</strong>:</p>\n<blockquote>\n<p><code>Poem-openapi</code> 使你能够方便快捷的构建符合 <code>OpenAPIv3</code>标准的应用程序接口; 通过使用过程宏来生成大量样板代码，你将有更多时间和精力来专注于实现更重要的业务逻辑。</p>\n</blockquote>\n<p><strong>功能特性</strong>:</p>\n<ul>\n<li>完全支持 <code>async/await</code>;</li>\n<li>类型安全;</li>\n<li>对<code>Rustfmt</code>用户友好 （过程宏）;</li>\n<li>最小开销;</li>\n</ul>\n<p><strong>示例</strong>:</p>\n<p>代码：</p>\n<pre><code>use poem_openapi::{payload::PlainText, OpenAPI, API};\n\nstruct Api;\n\n#[API]\nimpl Api {\n    #[oai(path = \"/\", method = \"get\")]\n    async fn index(&amp;self, #[oai(name = \"name\", in = \"query\")] name: Option&lt;String&gt;) -&gt; PlainText {\n        match name {\n            Some(name) =&gt; format!(\"hello, {}!\", name).into(),\n            None =&gt; \"hello!\".into(),\n        }\n    }\n}\n\n#[tokio::main]\nasync fn main() {\n    poem::Server::bind(\"127.0.0.1:3000\")\n        .await\n        .unwrap()\n        .run(OpenAPI::new(Api).title(\"hello World\").ui_path(\"/ui\"))\n        .await\n        .unwrap();\n}\n</code></pre>\n<p>运行：</p>\n<pre><code>[test@localhost poem-openapi]$ cargo run --example hello_world\n\n[test@localhost poem-openapi]$ curl http://localhost:3000\nhello!\n\n[test@localhost poem-openapi]$ curl http://localhost:3000\\?name\\=sunli\nhello, sunli!\n</code></pre>\n<p><strong>更多信息</strong>：</p>\n<ul>\n<li>项目地址：<a href=\"https://github.com/poem-web/poem-openapi\" rel=\"noopener noreferrer\">https://github.com/poem-web/poem-openapi</a></li>\n<li>开源声明：<a href=\"https://rustcc.cn/article?id=334551fe-a8a9-4561-aa63-11987b10e81f\" rel=\"noopener noreferrer\">Poem-openapi开源了!</a></li>\n<li>作者信息：<a href=\"https://github.com/sunli829\" rel=\"noopener noreferrer\">油条哥主页</a></li>\n</ul>\n<hr>\n<h2>Relm4 v0.1 发布</h2>\n<p><a href=\"https://aaronerhardt.github.io/blog/posts/announcing_relm4/\" rel=\"noopener noreferrer\">Announcing Relm4 v0.1</a></p>\n<p>在第一个测试版发布大约一个月后，经过无数个小时的工作，作者高兴地宣布Relm4的第一个稳定版本正式发布！</p>\n<p><strong>关于Relm4</strong>：</p>\n<p><code>Relm4</code>是一个受<code>Elm</code>启发并基于<code>gtk4-rs</code>的惯用GUI库。它是一个从头开始构建的<code>relm</code>的新版本，并且兼容<code>gtk4</code>和<code>libadwaita</code>。<code>Relm4</code>的主要目标是生产效率、灵活性、简单性和可维护性。</p>\n<p><strong>功能特性</strong></p>\n<ul>\n<li>支持<code>libadwaita</code>;</li>\n<li>配套书籍<a href=\"https://aaronerhardt.github.io/relm4-book/book/\" rel=\"noopener noreferrer\"><em>GUI development with Relm4</em></a> 已完结;</li>\n<li>新增支持非阻塞IO的消息句柄;</li>\n<li>更多的可复用组件;</li>\n<li>许多其他的改进和修复;</li>\n</ul>\n<p>完整的ChangeLog可以参见：\n<a href=\"https://github.com/AaronErhardt/relm4/blob/main/CHANGES.md\" rel=\"noopener noreferrer\">https://github.com/AaronErhardt/relm4/blob/main/CHANGES.md</a></p>\n<p><strong>更多信息</strong>：</p>\n<ul>\n<li>项目地址：<a href=\"https://github.com/AaronErhardt/relm4\" rel=\"noopener noreferrer\">https://github.com/AaronErhardt/relm4</a></li>\n<li>项目文档：<a href=\"https://aaronerhardt.github.io/docs/relm4/relm4/\" rel=\"noopener noreferrer\">https://aaronerhardt.github.io/docs/relm4/relm4/</a></li>\n<li>参考书籍：<a href=\"https://aaronerhardt.github.io/relm4-book/book/\" rel=\"noopener noreferrer\"><em>GUI development with Relm4</em></a></li>\n</ul>\n<hr>\n<h2>Ockam示例: 构建一个可以安全访问远程私有网络的通道</h2>\n<p><a href=\"https://github.com/ockam-network/ockam/tree/develop/documentation/use-cases/secure-remote-access-tunnels#readme\" rel=\"noopener noreferrer\">Build a secure access tunnel to a service in a remote private network</a></p>\n<p>Ockam是一个支持端到端加密、双向认证、网络安全的Rust和Elixir语言通信库。</p>\n<p>在本篇博文中，作者详细的介绍了如何使用<code>Ockam</code>在Rust中通过约<code>20行</code>代码来构建一个可以安全访问远程私有网络中设备的通道。</p>\n<hr>\n<h2>Skiff: 一门用Rust编写的逐渐类型化的函数式编程语言</h2>\n<p><a href=\"https://www.reddit.com/r/ProgrammingLanguages/comments/pjcewi/introducing_skiff_a_gradually_typed_functional/\" rel=\"noopener noreferrer\">Introducing Skiff, a gradually typed functional language written in Rust</a></p>\n<p>Skiff，是一门用Rust编写的逐渐类型化的函数式编程语言。所谓逐渐类型化是指作者计划下一步通过添加类型化关键字来区分完全类型函数和部分类型函数。</p>\n<p>Skiff受<code>Elm</code>/<code>Pyret</code>/<code>Python</code>语言启发，并受<code>Rust</code>/<code>Javascript</code>/<code>Typescript</code>/<code>Haskell</code>/<code>OCaml</code>/<code>Lua</code>等语言影响，当前语言功能还在持续完善中，作者提供了一个由<code>wasm!</code>驱动的<a href=\"https://skiff.paulbiberstein.me/\" rel=\"noopener noreferrer\">网页编辑器</a>可供读者学习使用，更多信息请访问项目主页的<a href=\"https://github.com/P-bibs/skiff/\" rel=\"noopener noreferrer\">Readme</a>。</p>\n<p><strong>更多信息</strong>：</p>\n<ul>\n<li>项目地址：<a href=\"https://github.com/P-bibs/skiff/\" rel=\"noopener noreferrer\">https://github.com/P-bibs/skiff/</a></li>\n<li>网页编辑器：<a href=\"https://skiff.paulbiberstein.me/\" rel=\"noopener noreferrer\">https://skiff.paulbiberstein.me/</a></li>\n</ul>\n<hr>\n<p>From 日报小组 odd-cat</p>\n<p>社区学习交流平台订阅：</p>\n<p><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rust.cc 论坛: 支持 rss</a></p>\n<p><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust 语言中文社区</a></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 10:05:41","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"40k-80k招聘全职远程区块链开发工程师","link":"https://rustcc.cn/article?id=5c42bdfd-0e97-4fc4-a494-4c9d14a5679f","description":"<p>公司简介\n我们是一家总部位于硅谷的区块链初创企业,我们跟绝大部分区块链公司不同的是，我们有顶级的产品来支撑我们的业务和发展远景。重要的事情说三遍，有产品，有产品，有产品，而且是业界领先的产品！</p>\n<p>目前我们在全球150多个国家拥有20,000+的用户，30,000+节点。</p>\n<p>目前公司已经盈利，现金贮备丰厚，正在对接业界最等级的风险投资机构，处于起飞的前夕。</p>\n<p>基本要求\n1、扎实的计算机科学基础知识；</p>\n<p>2、动手能力强，有死磕精神；</p>\n<p>3、有丰富的 Rust 开发经验；</p>\n<p>4、曾经独立完成或者主导完成过具有挑战性的项目；</p>\n<p>5、对工作有强大的责任心。</p>\n<p>加分项\n1、区块链相关数据结构与算法；</p>\n<p>2、Substrate或其他区块链节点开发经验；</p>\n<p>3、跨链、Layer 2 开发经验。</p>\n<p>薪资福利\n1、月薪40k-80k；</p>\n<p>2、入职满一年，表现合格者可以获得公司的股票或期权；</p>\n<p>3、优秀者提供移民美国、加拿大的机会。</p>\n<p>工作方式\n1、远程办公</p>\n<p>2、工作时间：无固定时间，工作完成后自由安排</p>\n<p>3、很少有跨时区的会议，除特殊、紧急工作任务对接外</p>\n<p>4、工作会议主要为中文交流</p>\n<p>工作语言\n1、英文为主，中文为辅</p>\n<p>2、英文要求读写能力合格及优秀</p>\n<p>面试流程\n1、收到简历后，将安排1到3轮电话语音面试；</p>\n<p>2、电话语音面试结束后，将进入试用期（全额工资）；</p>\n<p>3、试用期结束后，正式开始工作。</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 08:53:36","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-09-06 加快 Rust 的编译","link":"https://rustcc.cn/article?id=28141388-ce4a-4421-95b4-e2059e3b2347","description":"<h1>加快 Rust 的编译</h1>\n<p>众所周知，Rust代码编译起来很慢。但我有一种强烈的直觉，大多数Rust代码的编译速度比它本可以的要慢得多。</p>\n<p>例如, Rust 的 <code>rust-analyzer</code> CI 在 GitHub 上操作需要8分钟。这是一个相当大和复杂的项目，有20万行自己的代码和100万行依赖。</p>\n<p>跟随作者, 让我们进一步了解如何使编译时间保持在合理的范围内!</p>\n<p><a href=\"https://matklad.github.io/2021/09/04/fast-rust-builds.html\" rel=\"noopener noreferrer\">原文链接</a></p>\n<h1>async 的另外一个语法方案</h1>\n<p>作者对于 <code>async</code>的语法方案提出了完整的自己的解决方案, 非常有趣.</p>\n<p><a href=\"https://ibraheem.ca/writings/an-alternative-async-fn-syntax/\" rel=\"noopener noreferrer\">原文链接</a></p>\n<h1>Rust 插件</h1>\n<p>这是作者的第二篇关于 Rust插件的文章!\n在这里，作者将尝试编写一些 PDK (Plugin Development Kit, 插件开发工具包) 可能是什么样子的简单代码，并对在编写过程中出现的问题做一些研究。</p>\n<p><a href=\"https://nullderef.com/blog/plugin-start/\" rel=\"noopener noreferrer\">原文链接</a></p>\n<h1>sentinel-rust: Rust 版本的 sentinel</h1>\n<p><code>Sentinel</code> 是一个面向分布式服务架构的高可用流量控制组件. 现在 Rust 版本已加入</p>\n<p><a href=\"https://github.com/sentinel-group/sentinel-rust\" rel=\"noopener noreferrer\">github 地址</a></p>\n<h1><code>&lt;&lt;Programming Rust&gt;&gt;</code> 第二版电子书已上架</h1>\n<p><code>&lt;&lt;Programming Rust&gt;&gt;</code> 第二版的在线电子书现已上架.</p>\n<p><a href=\"https://www.lunaticai.com/2021/09/programming-rust-2nd-edition-pdf-github.html\" rel=\"noopener noreferrer\">原文链接</a></p>\n<p>--</p>\n<p>From 日报小组 BobQin，FBI小白</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 12:00:30","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust ffi 调用C++函数,C++函数返回 之指向智能对象的指针","link":"https://rustcc.cn/article?id=6060f204-1000-4bc2-a492-1e452c7ae9ef","description":"<p>\\stable-x86_64-pc-windows-msvc\\lib\\rustlib\\x86_64-pc-windows-msvc\\lib\\libcompiler_builtins-0f3806ca1d72c7be.rlib\" \"kernel32.lib\" \"ws2_32.lib\" \"advapi32.lib\" \"userenv.lib\" \"kernel32.lib\" \"msvcrt.lib\" \"/NXCOMPAT\" \"/LIBPATH:C:\\Users\\xiake\\.rustup\\toolchains\\stable-x86_64-pc-windows-msvc\\lib\\rustlib\\x86_64-pc-windows-msvc\\lib\" \"/OUT:D:\\other\\github\\rust_nav\\testffi\\target\\debug\\deps\\testffi.exe\" \"/OPT:REF,NOICF\" \"/DEBUG\" \"/NATVIS:C:\\Users\\xiake\\.rustup\\toolchains\\stable-x86_64-pc-windows-msvc\\lib\\rustlib\\etc\\intrinsic.natvis\" \"/NATVIS:C:\\Users\\xiake\\.rustup\\toolchains\\stable-x86_64-pc-windows-msvc\\lib\\rustlib\\etc\\liballoc.natvis\" \"/NATVIS:C:\\Users\\xiake\\.rustup\\toolchains\\stable-x86_64-pc-windows-msvc\\lib\\rustlib\\etc\\libcore.natvis\" \"/NATVIS:C:\\Users\\xiake\\.rustup\\toolchains\\stable-x86_64-pc-windows-msvc\\lib\\rustlib\\etc\\libstd.natvis\"\n= note: navapp.lib(pch.obj) : MSIL .netmodule or module compiled with /GL found; restarting link with /LTCG; add /LTCG to the link command line to improve linker performance\nCreating library D:\\other\\github\\rust_nav\\testffi\\target\\debug\\deps\\testffi.lib and object D:\\other\\github\\rust_nav\\testffi\\target\\debug\\deps\\testffi.exp\nnavapp.lib(navapp.obj) : error LNK2001: unresolved external symbol \"public: class KBEngine::SmartPointer __cdecl KBEngine::Navigation::findNavigation(class std::basic_string&lt;char,struct std::char_traits,class std::allocator &gt;)\" (?findNavigation@Navigation@KBEngine@@QEAA?AV?$SmartPointer@VNavigationHandle@KBEngine@@@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@Z)\nnavapp.lib(navapp.obj) : error LNK2001: unresolved external symbol \"public: class KBEngine::SmartPointer <em><em>cdecl KBEngine::Navigation::loadNavigation(class std::basic_string&lt;char,struct std::char_traits,class std::allocator &gt;,class std::map&lt;int,class std::basic_string&lt;char,struct std::char_traits,class std::allocator &gt;,struct std::less,class std::allocator&lt;struct std::pair&lt;int const ,class std::basic_string&lt;char,struct std::char_traits,class std::allocator &gt; &gt; &gt; &gt; const &amp;)\" (?loadNavigation@Navigation@KBEngine@@QEAA?AV?$SmartPointer@VNavigationHandle@KBEngine@@@2@V?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@AEBV?$map@HV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@U?$less@H@2@V?$allocator@U?$pair@$$CBHV?$basic_string@DU?$char_traits@D@std@@V?$allocator@D@2@@std@@@std@@@2@@5@@Z)\nnavapp.lib(navapp.obj) : error LNK2001: unresolved external symbol \"protected: static class KBEngine::Navigation * KBEngine::Singleton::singleton</em>\" (?singleton</em>@?$Singleton@VNavigation@KBEngine@@@KBEngine@@1PEAVNavigation@2@EA)\nD:\\other\\github\\rust_nav\\testffi\\target\\debug\\deps\\testffi.exe : fatal error LNK1120: 3 unresolved externals</p>\n<p>C++部分</p>\n<p>DLLEXPORT KBEngine::NavigationHandlePtr* get_nav_handle(char* resPath_);</p>\n<p>typedef SmartPointer NavigationHandlePtr;</p>\n<p>template \nclass SmartPointer : public ConstSmartPointer\n{\npublic:\ntypedef ConstSmartPointer ConstProxy;</p>\n<pre><code>SmartPointer(T* obj = 0, typename ConstProxy::REF_TAG tag = ConstProxy::NEW_REF):\nConstProxy(obj, tag)\n{\n}\n\nSmartPointer( const SmartPointer&lt;T&gt;&amp; P ) : ConstProxy( P ) { }\n\ntemplate&lt;class DerivedType&gt;\nSmartPointer( ConstSmartPointer&lt;DerivedType&gt;&amp; dt ) :\n\tConstProxy( dt.get() )\n{\n}\n</code></pre>\n<p>代码太长省略</p>\n<p>求大佬给点提示  实在找不到原因</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 07:05:51","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"有没有方法将一个usize的值转成i32的-1","link":"https://rustcc.cn/article?id=7f247165-5483-4b7e-b39b-7f2db042e5df","description":"<p>今天写leetcode的每日一题的时候突发奇想</p>\n<pre><code>impl Solution {\n    pub fn search(nums: Vec&lt;i32&gt;, target: i32) -&gt; i32 {\n        nums.binary_search(&amp;target).unwrap_or_default() as i32\n    }\n}\n</code></pre>\n<p>像这里能不能用一个unwrap_or(x) 然后转成失败返回-1</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 03:45:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了","link":"https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b","description":"<p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>\n<ol>\n<li>认识面向基础架构语言</li>\n<li>理解 Rust 所有权</li>\n<li>通过实战理解 Rust 宏</li>\n<li>通过 Datafuse 理解全链路跟踪</li>\n<li>Rust 异步编程入门 Future Part 1</li>\n<li>Rust 异步编程入门 Future Part 2</li>\n</ol>\n<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。\n本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>\n<hr>\n<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>\n<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>\n<p>分享讲师： 董泽润</p>\n<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现\n讲师介绍：\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<hr>\n<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>\n<p>分享时间：  周日晚上2021-09-12 20:00-21:00</p>\n<p>分享讲师： 苏林</p>\n<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>\n<ol>\n<li>利用 Tokio 实现一个 Mini Http server</li>\n<li>基于 Http server提供内容动态的 API 网关</li>\n<li>利用 Redis 实现对 API 网关加速</li>\n<li>学习 Rust RPC 调用，实现微服务调用</li>\n</ol>\n<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<p>Rust 异步编程入门 Future Part 1   | Vol. 5\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<p>Rust 异步编程入门 Future Part 2  | Vol. 6\nhttps://www.bilibili.com/video/bv1oy4y1G7jC</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 02:23:16","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust 学习随笔","link":"https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8","description":"<h1>切换镜像源</h1>\n<p>crm =&gt; https://github.com/wtklbm/crm</p>\n<p>常用命令就是 <code>crm best</code></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 14:35:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"pretree 补全文档发布了,再次谢谢大神的指点终于入门了。","link":"https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827","description":"<h1>Pretree</h1>\n<p>pretree is a package for storing and querying routing rules with prefix tree .</p>\n<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>\n<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>\n<p>Inspired by <a href=\"https://github.com/obity/pretree\" rel=\"noopener noreferrer\">obity/pretree</a> (golang)</p>\n<h1>Doc</h1>\n<p>See this document at <a href=\"https://docs.rs/pretree\" rel=\"noopener noreferrer\">API documentation</a></p>\n<h1>Install</h1>\n<p>Add the following line to your Cargo.toml file:</p>\n<pre><code>pretree = \"1.0.0\"\n</code></pre>\n<h1>Example</h1>\n<pre><code>use pretree::Pretree;\nlet mut p = Pretree::new();\np.store(\"GET\",\"account/{id}/info/:name\");\np.store(\"GET\",\"account/:id/login\");\np.store(\"GET\",\"account/{id}\");\np.store(\"GET\",\"bacteria/count_number_by_month\");\nlet (ok,rule,vars) = p.query(\"GET\",\"account/929239\");\nprintln!(\"ok:{} rule:{} vars:{:#?}\",ok,rule,vars);\n\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 09:37:30","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6","link":"https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a","description":"<h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>\n<p><strong>课程时间:</strong>  2021年9月5日 20:00-21:00</p>\n<p><strong>课程介绍:</strong>  上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识,  Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>\n<h3>课程大纲</h3>\n<p>1、回顾 Rust 异步编程模型.</p>\n<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>\n<p>3、Tokio 介绍.</p>\n<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>\n<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/\nRust 异步编程入门 Future Part 1  回放地址：\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 08:40:15","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"公开课：《 Rust 异步编程入门 Future 》|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>\n<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>\n<p><strong>课程介绍:</strong>  讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是  Rust 异步编程的核心基础。</p>\n<h3>课程大纲</h3>\n<p>1、为什么需要异步.</p>\n<p>2、理解异步编程模型.</p>\n<p>3、Future 编程模型讲解.</p>\n<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>\n<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>异步引擎 C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D 游戏引擎</h3>\n<ul>\n<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>\n<li><strong>延迟着色</strong></li>\n<li><strong>内置保存/加载</strong></li>\n<li><strong>独立场景编辑器</strong></li>\n<li><strong>高级物理模型</strong></li>\n<li><strong>分层模型资源</strong></li>\n<li><strong>几何实例化</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>\n<p><strong>课程时间：</strong>  2021年8月22日 20:30-21:30</p>\n<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>\n<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>\n<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>课程大纲</h3>\n<ol>\n<li>\n<p>什么是分布式追踪系统OpenTracing及应用场景</p>\n</li>\n<li>\n<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>\n</li>\n<li>\n<p>为什么需要tokio-rs/tracing库</p>\n</li>\n<li>\n<p>演示Datafuse项目中tokio-rs/tracing的使用</p>\n</li>\n</ol>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>课程中苏林老师推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"论坛github账户无法登录解决笔记","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>有反映这两天github账户无法登录了。</p>\n<p>报这个错：</p>\n<pre><code>get github user info err\n</code></pre>\n<p>查了几个地方：</p>\n<ol>\n<li>代码是否运行正常：Ok</li>\n<li>https代理是否正常：Ok</li>\n<li>检查了github返回日志，发现是：</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>进入这个地址一看：<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>\n<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>\n<p>特此记录。</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 的 Future 与 Javascript 的 Promise 功能对照参考","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>\n<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>\n<blockquote>\n<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* 什么都不做 */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被  （1）跨线程传递  （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust公开课：《通过实战理解 Rust 宏》| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>\n<p><strong>课程时间：</strong>  2021年8月15日 20:30-21:30</p>\n<p><strong>课程介绍：</strong></p>\n<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>这就是通过宏实现配置的统一行为，代码参考：\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>\n<h3>课程大纲</h3>\n<ul>\n<li>什么是 Rust 宏</li>\n<li>什么是宏运行原理</li>\n<li>如何创建 Rust 宏过程</li>\n<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>\n</ul>\n<p><strong>讲师介绍</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>课程中苏林老师推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-09-09T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Text-Free Prosody-Aware Generative Spoken Language Modeling. (arXiv:2109.03264v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03264","description":"<p>Speech pre-training has primarily demonstrated efficacy on classification\ntasks, while its capability of generating novel speech, similar to how GPT-2\ncan generate coherent paragraphs, has barely been explored. Generative Spoken\nLanguage Modeling (GSLM) (Lakhotia et al., 2021) is the only prior work\naddressing the generative aspects of speech pre-training, which replaces text\nwith discovered phone-like units for language modeling and shows the ability to\ngenerate meaningful novel sentences. Unfortunately, despite eliminating the\nneed of text, the units used in GSLM discard most of the prosodic information.\nHence, GSLM fails to leverage prosody for better comprehension, and does not\ngenerate expressive speech. In this work, we present a prosody-aware generative\nspoken language model (pGSLM). It is composed of a multi-stream transformer\nlanguage model (MS-TLM) of speech, represented as discovered unit and prosodic\nfeature streams, and an adapted HiFi-GAN model converting MS-TLM outputs to\nwaveforms. We devise a series of metrics for prosody modeling and generation,\nand re-use metrics from GSLM for content modeling. Experimental results show\nthat the pGSLM can utilize prosody to improve both prosody and content\nmodeling, and also generate natural, meaningful, and coherent speech given a\nspoken prompt. Audio samples can be found at https://speechbot.github.io/pgslm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kharitonov_E/0/1/0/all/0/1\">Eugene Kharitonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Polyak_A/0/1/0/all/0/1\">Adam Polyak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adi_Y/0/1/0/all/0/1\">Yossi Adi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Copet_J/0/1/0/all/0/1\">Jade Copet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lakhotia_K/0/1/0/all/0/1\">Kushal Lakhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1\">Tu-Anh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riviere_M/0/1/0/all/0/1\">Morgane Rivi&#xe8;re</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohamed_A/0/1/0/all/0/1\">Abdelrahman Mohamed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dupoux_E/0/1/0/all/0/1\">Emmanuel Dupoux</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-Decoder Conformer for Multilingual Speech Recognition. (arXiv:2109.03277v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03277","description":"<p>Transformer-based models have recently become very popular for\nsequence-to-sequence applications such as machine translation and speech\nrecognition. This work proposes a dual-decoder transformer model for\nlow-resource multilingual speech recognition for Indian languages. Our proposed\nmodel consists of a Conformer [1] encoder, two parallel transformer decoders,\nand a language classifier. We use a phoneme decoder (PHN-DEC) for the phoneme\nrecognition task and a grapheme decoder (GRP-DEC) to predict grapheme sequence\nalong with language information. We consider phoneme recognition and language\nidentification as auxiliary tasks in the multi-task learning framework. We\njointly optimize the network for phoneme recognition, grapheme recognition, and\nlanguage identification tasks with Joint CTC-Attention [2] training. Our\nexperiments show that we can obtain a significant reduction in WER over the\nbaseline approaches. We also show that our dual-decoder approach obtains\nsignificant improvement over the single decoder approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+N_K/0/1/0/all/0/1\">Krishna D N</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models. (arXiv:2109.03300v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03300","description":"<p>All AI models are susceptible to learning biases in data that they are\ntrained on. For generative dialogue models, being trained on real human\nconversations containing unbalanced gender and race/ethnicity references can\nlead to models that display learned biases, which we define here broadly as any\nmeasurable differences in the distributions of words or semantic content of\nconversations based on demographic groups. We measure the strength of such\nbiases by producing artificial conversations between two copies of a dialogue\nmodel, conditioning one conversational partner to state a name commonly\nassociated with a certain gender and/or race/ethnicity. We find that larger\ncapacity models tend to exhibit more gender bias and greater stereotyping of\noccupations by gender. We show that several methods of tuning these dialogue\nmodels, specifically name scrambling, controlled generation, and unlikelihood\ntraining, are effective in reducing bias in conversation, including on a\ndownstream conversational task. Name scrambling is also effective in lowering\ndifferences in token usage across conversations where partners have names\nassociated with different genders or races/ethnicities.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1\">Eric Michael Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Corpus-based Open-Domain Event Type Induction. (arXiv:2109.03322v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03322","description":"<p>Traditional event extraction methods require predefined event types and their\ncorresponding annotations to learn event extractors. These prerequisites are\noften hard to be satisfied in real-world applications. This work presents a\ncorpus-based open-domain event type induction method that automatically\ndiscovers a set of event types from a given corpus. As events of the same type\ncould be expressed in multiple ways, we propose to represent each event type as\na cluster of &lt;predicate sense, object head&gt; pairs. Specifically, our method (1)\nselects salient predicates and object heads, (2) disambiguates predicate senses\nusing only a verb sense dictionary, and (3) obtains event types by jointly\nembedding and clustering &lt;predicate sense, object head&gt; pairs in a latent\nspherical space. Our experiments, on three datasets from different domains,\nshow our method can discover salient and high-quality event types, according to\nboth automatic and human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1\">Jiaming Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yunyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1\">Heng Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiawei Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Challenges of Evaluating Compositional Explanations in Multi-Hop Inference: Relevance, Completeness, and Expert Ratings. (arXiv:2109.03334v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03334","description":"<p>Building compositional explanations requires models to combine two or more\nfacts that, together, describe why the answer to a question is correct.\nTypically, these \"multi-hop\" explanations are evaluated relative to one (or a\nsmall number of) gold explanations. In this work, we show these evaluations\nsubstantially underestimate model performance, both in terms of the relevance\nof included facts, as well as the completeness of model-generated explanations,\nbecause models regularly discover and produce valid explanations that are\ndifferent than gold explanations. To address this, we construct a large corpus\nof 126k domain-expert (science teacher) relevance ratings that augment a corpus\nof explanations to standardized science exam questions, discovering 80k\nadditional relevant facts not rated as gold. We build three strong models based\non different methodologies (generation, ranking, and schemas), and empirically\nshow that while expert-augmented ratings provide better estimates of\nexplanation quality, both original (gold) and expert-augmented automatic\nevaluations still substantially underestimate performance by up to 36% when\ncompared with full manual expert judgements, with different models being\ndisproportionately affected. This poses a significant methodological challenge\nto accurately evaluating explanations produced by compositional reasoning\nmodels.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1\">Peter Jansen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kelly Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_D/0/1/0/all/0/1\">Dan Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ortiz_H/0/1/0/all/0/1\">Huitzilin Ortiz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Contrastive Cross-Modality Representation Learning for Spoken Question Answering. (arXiv:2109.03381v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03381","description":"<p>Spoken question answering (SQA) requires fine-grained understanding of both\nspoken documents and questions for the optimal answer prediction. In this\npaper, we propose novel training schemes for spoken question answering with a\nself-supervised training stage and a contrastive representation learning stage.\nIn the self-supervised stage, we propose three auxiliary self-supervised tasks,\nincluding utterance restoration, utterance insertion, and question\ndiscrimination, and jointly train the model to capture consistency and\ncoherence among speech documents without any additional data or annotations. We\nthen propose to learn noise-invariant utterance representations in a\ncontrastive objective by adopting multiple augmentation strategies, including\nspan deletion and span substitution. Besides, we design a Temporal-Alignment\nattention to semantically align the speech-text clues in the learned common\nspace and benefit the SQA tasks. By this means, the training schemes can more\neffectively guide the generation model to predict more proper answers.\nExperimental results show that our model achieves state-of-the-art results on\nthree SQA benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+You_C/0/1/0/all/0/1\">Chenyu You</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nuo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yuexian Zou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepZensols: Deep Natural Language Processing Framework. (arXiv:2109.03383v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03383","description":"<p>Reproducing results in publications by distributing publicly available source\ncode is becoming ever more popular. Given the difficulty of reproducing machine\nlearning (ML) experiments, there have been significant efforts in reducing the\nvariance of these results. As in any science, the ability to consistently\nreproduce results effectively strengthens the underlying hypothesis of the\nwork, and thus, should be regarded as important as the novel aspect of the\nresearch itself. The contribution of this work is a framework that is able to\nreproduce consistent results and provides a means of easily creating, training,\nand evaluating natural language processing (NLP) deep learning (DL) models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landes_P/0/1/0/all/0/1\">Paul Landes</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eugenio_B/0/1/0/all/0/1\">Barbara Di Eugenio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caragea_C/0/1/0/all/0/1\">Cornelia Caragea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixup Decoding for Diverse Machine Translation. (arXiv:2109.03402v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03402","description":"<p>Diverse machine translation aims at generating various target language\ntranslations for a given source language sentence. Leveraging the linear\nrelationship in the sentence latent space introduced by the mixup training, we\npropose a novel method, MixDiversity, to generate different translations for\nthe input sentence by linearly interpolating it with different sentence pairs\nsampled from the training corpus when decoding. To further improve the\nfaithfulness and diversity of the translations, we propose two simple but\neffective approaches to select diverse sentence pairs in the training corpus\nand adjust the interpolation weight for each pair correspondingly. Moreover, by\ncontrolling the interpolation weight, our method can achieve the trade-off\nbetween faithfulness and diversity without any additional training, which is\nrequired in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14\nen-de, and WMT'17 zh-en are conducted to show that our method substantially\noutperforms all previous diverse machine translation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuanfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Matters When It Should: Sanity Checking Multimodal Machine Translation Models. (arXiv:2109.03415v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03415","description":"<p>Multimodal machine translation (MMT) systems have been shown to outperform\ntheir text-only neural machine translation (NMT) counterparts when visual\ncontext is available. However, recent studies have also shown that the\nperformance of MMT models is only marginally impacted when the associated image\nis replaced with an unrelated image or noise, which suggests that the visual\ncontext might not be exploited by the model at all. We hypothesize that this\nmight be caused by the nature of the commonly used evaluation benchmark, also\nknown as Multi30K, where the translations of image captions were prepared\nwithout actually showing the images to human translators. In this paper, we\npresent a qualitative study that examines the role of datasets in stimulating\nthe leverage of visual modality and we propose methods to highlight the\nimportance of visual signals in the datasets which demonstrate improvements in\nreliance of models on the source images. Our findings suggest the research on\neffective MMT architectures is currently impaired by the lack of suitable\ndatasets and careful consideration must be taken in creation of future MMT\ndatasets, for which we also provide useful insights.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiaoda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"It is AI's Turn to Ask Human a Question: Question and Answer Pair Generation for Children Storybooks in FairytaleQA Dataset. (arXiv:2109.03423v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03423","description":"<p>Existing question answering (QA) datasets are created mainly for the\napplication of having AI to be able to answer questions asked by humans. But in\neducational applications, teachers and parents sometimes may not know what\nquestions they should ask a child that can maximize their language learning\nresults. With a newly released book QA dataset (FairytaleQA), which educational\nexperts labeled on 46 fairytale storybooks for early childhood readers, we\ndeveloped an automated QA generation model architecture for this novel\napplication. Our model (1) extracts candidate answers from a given storybook\npassage through carefully designed heuristics based on a pedagogical framework;\n(2) generates appropriate questions corresponding to each extracted answer\nusing a language model; and, (3) uses another QA model to rank top QA-pairs.\nAutomatic and human evaluations show that our model outperforms baselines. We\nalso demonstrate that our method can help with the scarcity issue of the\nchildren's book QA dataset via data augmentation on 200 unlabeled storybooks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yao_B/0/1/0/all/0/1\">Bingsheng Yao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1\">Dakuo Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1\">Tongshuang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1\">Tran Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1\">Branda Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Toby Jia-Jun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mo Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Ying Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ArchivalQA: A Large-scale Benchmark Dataset for Open Domain Question Answering over Archival News Collections. (arXiv:2109.03438v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03438","description":"<p>In the last few years, open-domain question answering (ODQA) has advanced\nrapidly due to the development of deep learning techniques and the availability\nof large-scale QA datasets. However, the current datasets are essentially\ndesigned for synchronic document collections (e.g., Wikipedia). Temporal news\ncollections such as long-term news archives spanning several decades, are\nrarely used in training the models despite they are quite valuable for our\nsociety. In order to foster the research in the field of ODQA on such\nhistorical collections, we present ArchivalQA, a large question answering\ndataset consisting of 1,067,056 question-answer pairs which is designed for\ntemporal news QA. In addition, we create four subparts of our dataset based on\nthe question difficulty levels and the containment of temporal expressions,\nwhich we believe could be useful for training or testing ODQA systems\ncharacterized by different strengths and abilities. The novel QA\ndataset-constructing framework that we introduce can be also applied to create\ndatasets over other types of collections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jiexin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masatoshi Yoshikawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Referee: Towards reference-free cross-speaker style transfer with low-quality data for expressive speech synthesis. (arXiv:2109.03439v1 [eess.AS])","link":"http://arxiv.org/abs/2109.03439","description":"<p>Cross-speaker style transfer (CSST) in text-to-speech (TTS) synthesis aims at\ntransferring a speaking style to the synthesised speech in a target speaker's\nvoice. Most previous CSST approaches rely on expensive high-quality data\ncarrying desired speaking style during training and require a reference\nutterance to obtain speaking style descriptors as conditioning on the\ngeneration of a new sentence. This work presents Referee, a robust\nreference-free CSST approach for expressive TTS, which fully leverages\nlow-quality data to learn speaking styles from text. Referee is built by\ncascading a text-to-style (T2S) model with a style-to-wave (S2W) model.\nPhonetic PosteriorGram (PPG), phoneme-level pitch and energy contours are\nadopted as fine-grained speaking style descriptors, which are predicted from\ntext using the T2S model. A novel pretrain-refinement method is adopted to\nlearn a robust T2S model by only using readily accessible low-quality data. The\nS2W model is trained with high-quality target data, which is adopted to\neffectively aggregate style descriptors and generate high-fidelity speech in\nthe target speaker's voice. Experimental results are presented, showing that\nReferee outperforms a global-style-token (GST)-based baseline approach in CSST.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1\">Songxiang Liu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yang_S/0/1/0/all/0/1\">Shan Yang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Su_D/0/1/0/all/0/1\">Dan Su</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Yu_D/0/1/0/all/0/1\">Dong Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence Level Contrastive Learning for Text Summarization. (arXiv:2109.03481v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03481","description":"<p>Contrastive learning models have achieved great success in unsupervised\nvisual representation learning, which maximize the similarities between feature\nrepresentations of different views of the same image, while minimize the\nsimilarities between feature representations of views of different images. In\ntext summarization, the output summary is a shorter form of the input document\nand they have similar meanings. In this paper, we propose a contrastive\nlearning model for supervised abstractive text summarization, where we view a\ndocument, its gold summary and its model generated summaries as different views\nof the same mean representation and maximize the similarities between them\nduring training. We improve over a strong sequence-to-sequence text generation\nmodel (i.e., BART) on three different summarization datasets. Human evaluation\nalso shows that our model achieves better faithfulness ratings compared to its\ncounterpart without contrastive objectives.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1\">Shusheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xingxing Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yi Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Social Analysis of Young Basque Speaking Communities in Twitter. (arXiv:2109.03487v1 [cs.CY])","link":"http://arxiv.org/abs/2109.03487","description":"<p>In this paper we take into account both social and linguistic aspects to\nperform demographic analysis by processing a large amount of tweets in Basque\nlanguage. The study of demographic characteristics and social relationships are\napproached by applying machine learning and modern deep-learning Natural\nLanguage Processing (NLP) techniques, combining social sciences with automatic\ntext processing. More specifically, our main objective is to combine\ndemographic inference and social analysis in order to detect young Basque\nTwitter users and to identify the communities that arise from their\nrelationships or shared content. This social and demographic analysis will be\nentirely based on the~automatically collected tweets using NLP to convert\nunstructured textual information into interpretable knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Landa_J/0/1/0/all/0/1\">J. Fernandez de Landa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">R. Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Spelling provides a precise (but sometimes misplaced) phonological target. Orthography and acoustic variability in second language word learning. (arXiv:2109.03490v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03490","description":"<p>L1 French participants learned novel L2 English words over two days of\nlearning sessions, with half of the words presented with their orthographic\nforms (Audio-Ortho) and half without (Audio only). One group heard the words\npronounced by a single talker, while another group heard them pronounced by\nmultiple talkers. On the third day, they completed a variety of tasks to\nevaluate their learning. Our results show a robust influence of orthography,\nwith faster response times in both production (picture naming) and recognition\n(picture mapping) tasks for words learned in the Audio-Ortho condition.\nMoreover, formant analyses of the picture naming responses show that\northographic input pulls pronunciations of English novel words towards a\nnon-native (French) phonological target. Words learned with their orthographic\nforms were pronounced more precisely (with smaller Dispersion Scores), but were\nmisplaced in the vowel space (as reflected by smaller Euclidian distances with\nrespect to French vowels). Finally, we found only limited evidence of an effect\nof talker-based acoustic variability: novel words learned with multiple talkers\nshowed faster responses times in the picture naming task, but only in the\nAudio-only condition, which suggests that orthographic information may have\noverwhelmed any advantage of talker-based acoustic variability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Welby_P/0/1/0/all/0/1\">Pauline Welby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spinelli_E/0/1/0/all/0/1\">Elsa Spinelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Burki_A/0/1/0/all/0/1\">Audrey B&#xfc;rki</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"R2-D2: A Modular Baseline for Open-Domain Question Answering. (arXiv:2109.03502v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03502","description":"<p>This work presents a novel four-stage open-domain QA pipeline R2-D2 (Rank\ntwice, reaD twice). The pipeline is composed of a retriever, passage reranker,\nextractive reader, generative reader and a mechanism that aggregates the final\nprediction from all system's components. We demonstrate its strength across\nthree open-domain QA datasets: NaturalQuestions, TriviaQA and EfficientQA,\nsurpassing state-of-the-art on the first two. Our analysis demonstrates that:\n(i) combining extractive and generative reader yields absolute improvements up\nto 5 exact match and it is at least twice as effective as the posterior\naveraging ensemble of the same models with different parameters, (ii) the\nextractive reader with fewer parameters can match the performance of the\ngenerative reader on extractive QA datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fajcik_M/0/1/0/all/0/1\">Martin Fajcik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Docekal_M/0/1/0/all/0/1\">Martin Docekal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ondrej_K/0/1/0/all/0/1\">Karel Ondrej</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smrz_P/0/1/0/all/0/1\">Pavel Smrz</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RefineCap: Concept-Aware Refinement for Image Captioning. (arXiv:2109.03529v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03529","description":"<p>Automatically translating images to texts involves image scene understanding\nand language modeling. In this paper, we propose a novel model, termed\nRefineCap, that refines the output vocabulary of the language decoder using\ndecoder-guided visual semantics, and implicitly learns the mapping between\nvisual tag words and images. The proposed Visual-Concept Refinement method can\nallow the generator to attend to semantic details in the image, thereby\ngenerating more semantically descriptive captions. Our model achieves superior\nperformance on the MS-COCO dataset in comparison with previous visual-concept\nbased models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chai_Y/0/1/0/all/0/1\">Yekun Chai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1\">Shuo Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1\">Junliang Xing</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On the Transferability of Pre-trained Language Models: A Study from Artificial Datasets. (arXiv:2109.03537v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03537","description":"<p>Pre-training language models (LMs) on large-scale unlabeled text data makes\nthe model much easier to achieve exceptional downstream performance than their\ncounterparts directly trained on the downstream tasks. In this work, we study\nwhat specific traits in the pre-training data, other than the semantics, make a\npre-trained LM superior to their counterparts trained from scratch on\ndownstream tasks. We propose to use artificially constructed datasets as the\npre-training data to exclude the effect of semantics, and further control what\ncharacteristics the pre-training corpora have. By fine-tuning the pre-trained\nmodels on GLUE benchmark, we can learn how beneficial it is to transfer the\nknowledge from the model trained on the dataset possessing that specific trait.\nWe define and discuss three different characteristics in the artificial\ndataset: 1) matching the token's uni-gram or bi-gram distribution between\npre-training and downstream fine-tuning, 2) the presence of the explicit\ndependencies among the tokens in a sequence, 3) the length of the implicit\ndependencies among the tokens in a sequence. Our experiments show that the\nexplicit dependencies in the sequences of the pre-training data are critical to\nthe downstream performance. Our results also reveal that models achieve better\ndownstream performance when pre-trained on a dataset with a longer range of\nimplicit dependencies. Based on our analysis, we find that models pre-trained\nwith artificial datasets are prone to learn spurious correlation in downstream\ntasks. Our work reveals that even if the LMs are not pre-trained on natural\nlanguage, they still gain transferability on certain human language downstream\ntasks once the LMs learn to model the token dependencies in the sequences. This\nresult helps us understand the exceptional transferability of pre-trained LMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chiang_C/0/1/0/all/0/1\">Cheng-Han Chiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1\">Hung-yi Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Time Alignment using Lip Images for Frame-based Electrolaryngeal Voice Conversion. (arXiv:2109.03551v1 [cs.SD])","link":"http://arxiv.org/abs/2109.03551","description":"<p>Voice conversion (VC) is an effective approach to electrolaryngeal (EL)\nspeech enhancement, a task that aims to improve the quality of the artificial\nvoice from an electrolarynx device. In frame-based VC methods, time alignment\nneeds to be performed prior to model training, and the dynamic time warping\n(DTW) algorithm is widely adopted to compute the best time alignment between\neach utterance pair. The validity is based on the assumption that the same\nphonemes of the speakers have similar features and can be mapped by measuring a\npre-defined distance between speech frames of the source and the target.\nHowever, the special characteristics of the EL speech can break the assumption,\nresulting in a sub-optimal DTW alignment. In this work, we propose to use lip\nimages for time alignment, as we assume that the lip movements of laryngectomee\nremain normal compared to healthy people. We investigate two naive lip\nrepresentations and distance metrics, and experimental results demonstrate that\nthe proposed method can significantly outperform the audio-only alignment in\nterms of objective and subjective evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liou_Y/0/1/0/all/0/1\">Yi-Syuan Liou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wen-Chin Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yen_M/0/1/0/all/0/1\">Ming-Chi Yen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsai_S/0/1/0/all/0/1\">Shu-Wei Tsai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1\">Yu-Huai Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toda_T/0/1/0/all/0/1\">Tomoki Toda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1\">Yu Tsao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hsin-Min Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Offensive Language Identification for Low Resource Languages: The Case of Marathi. (arXiv:2109.03552v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03552","description":"<p>The widespread presence of offensive language on social media motivated the\ndevelopment of systems capable of recognizing such content automatically. Apart\nfrom a few notable exceptions, most research on automatic offensive language\nidentification has dealt with English. To address this shortcoming, we\nintroduce MOLD, the Marathi Offensive Language Dataset. MOLD is the first\ndataset of its kind compiled for Marathi, thus opening a new domain for\nresearch in low-resource Indo-Aryan languages. We present results from several\nmachine learning experiments on this dataset, including zero-short and other\ntransfer learning experiments on state-of-the-art cross-lingual transformers\nfrom existing data in Bengali, English, and Hindi.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gaikwad_S/0/1/0/all/0/1\">Saurabh Gaikwad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Homan_C/0/1/0/all/0/1\">Christopher M. Homan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NSP-BERT: A Prompt-based Zero-Shot Learner Through an Original Pre-training Task--Next Sentence Prediction. (arXiv:2109.03564v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03564","description":"<p>Using prompts to utilize language models to perform various downstream tasks,\nalso known as prompt-based learning or prompt-learning, has lately gained\nsignificant success in comparison to the pre-train and fine-tune paradigm.\nNonetheless, virtually all prompt-based methods are token-level, meaning they\nall utilize GPT's left-to-right language model or BERT's masked language model\nto perform cloze-style tasks. In this paper, we attempt to accomplish several\nNLP tasks in the zero-shot scenario using a BERT original pre-training task\nabandoned by RoBERTa and other models--Next Sentence Prediction (NSP). Unlike\ntoken-level techniques, our sentence-level prompt-based method NSP-BERT does\nnot need to fix the length of the prompt or the position to be predicted,\nallowing it to handle tasks such as entity linking with ease. Based on the\ncharacteristics of NSP-BERT, we offer several quick building templates for\nvarious downstream tasks. We suggest a two-stage prompt method for word sense\ndisambiguation tasks in particular. Our strategies for mapping the labels\nsignificantly enhance the model's performance on sentence pair tasks. On the\nFewCLUE benchmark, our NSP-BERT outperforms other zero-shot methods on most of\nthese tasks and comes close to the few-shot methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1\">Yu Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hao_C/0/1/0/all/0/1\">Chao Hao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1\">Hangping Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03570","description":"<p>This work presents biomedical and clinical language models for Spanish by\nexperimenting with different pretraining choices, such as masking at word and\nsubword level, varying the vocabulary size and testing with domain data,\nlooking for better language representations. Interestingly, in the absence of\nenough clinical data to train a model from scratch, we applied mixed-domain\npretraining and cross-domain transfer approaches to generate a performant\nbio-clinical model suitable for real-world clinical data. We evaluated our\nmodels on Named Entity Recognition (NER) tasks for biomedical documents and\nchallenging hospital discharge reports. When compared against the competitive\nmBERT and BETO models, we outperform them in all NER tasks by a significant\nmargin. Finally, we studied the impact of the model's vocabulary on the NER\nperformances by offering an interesting vocabulary-centric analysis. The\nresults confirm that domain-specific pretraining is fundamental to achieving\nhigher performances in downstream NER tasks, even within a mid-resource\nscenario. To the best of our knowledge, we provide the first biomedical and\nclinical transformer-based pretrained language models for Spanish, intending to\nboost native Spanish NLP applications in biomedicine. Our models will be made\nfreely available after publication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TrollsWithOpinion: A Dataset for Predicting Domain-specific Opinion Manipulation in Troll Memes. (arXiv:2109.03571v1 [cs.SI])","link":"http://arxiv.org/abs/2109.03571","description":"<p>Research into the classification of Image with Text (IWT) troll memes has\nrecently become popular. Since the online community utilizes the refuge of\nmemes to express themselves, there is an abundance of data in the form of\nmemes. These memes have the potential to demean, harras, or bully targeted\nindividuals. Moreover, the targeted individual could fall prey to opinion\nmanipulation. To comprehend the use of memes in opinion manipulation, we define\nthree specific domains (product, political or others) which we classify into\ntroll or not-troll, with or without opinion manipulation. To enable this\nanalysis, we enhanced an existing dataset by annotating the data with our\ndefined classes, resulting in a dataset of 8,881 IWT or multimodal memes in the\nEnglish language (TrollsWithOpinion dataset). We perform baseline experiments\non the annotated dataset, and our result shows that existing state-of-the-art\ntechniques could only reach a weighted-average F1-score of 0.37. This shows the\nneed for a development of a specific technique to deal with multimodal troll\nmemes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suryawanshi_S/0/1/0/all/0/1\">Shardul Suryawanshi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arcan_M/0/1/0/all/0/1\">Mihael Arcan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Little_S/0/1/0/all/0/1\">Suzanne Little</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buitelaar_P/0/1/0/all/0/1\">Paul Buitelaar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Dual-Channel Framework for Sarcasm Recognition by Detecting Sentiment Conflict. (arXiv:2109.03587v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03587","description":"<p>Sarcasm employs ambivalence, where one says something positive but actually\nmeans negative, and vice versa. Due to the sophisticated and obscure sentiment,\nsarcasm brings in great challenges to sentiment analysis. In this paper, we\nshow up the essence of sarcastic text is that the literal sentiment (expressed\nby the surface form of the text) is opposite to the deep sentiment (expressed\nby the actual meaning of the text). To this end, we propose a Dual-Channel\nFramework by modeling both literal and deep sentiments to recognize the\nsentiment conflict. Specifically, the proposed framework is capable of\ndetecting the sentiment conflict between the literal and deep meanings of the\ninput text. Experiments on the political debates and the Twitter datasets show\nthat our framework achieves the best performance on sarcasm recognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yiyi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yequan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1\">Aixin Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jiafeng Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xuying Meng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Formal Query Building with Query Structure Prediction for Complex Question Answering over Knowledge Base. (arXiv:2109.03614v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03614","description":"<p>Formal query building is an important part of complex question answering over\nknowledge bases. It aims to build correct executable queries for questions.\nRecent methods try to rank candidate queries generated by a state-transition\nstrategy. However, this candidate generation strategy ignores the structure of\nqueries, resulting in a considerable number of noisy queries. In this paper, we\npropose a new formal query building approach that consists of two stages. In\nthe first stage, we predict the query structure of the question and leverage\nthe structure to constrain the generation of the candidate queries. We propose\na novel graph generation framework to handle the structure prediction task and\ndesign an encoder-decoder model to predict the argument of the predetermined\noperation in each generative step. In the second stage, we follow the previous\nmethods to rank the candidate queries. The experimental results show that our\nformal query building approach outperforms existing methods on complex\nquestions while staying competitive on simple questions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiying Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1\">Yuncheng Hua</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Discrete and Soft Prompting for Multilingual Models. (arXiv:2109.03630v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03630","description":"<p>It has been shown for English that discrete and soft prompting perform\nstrongly in few-shot learning with pretrained language models (PLMs). In this\npaper, we show that discrete and soft prompting perform better than finetuning\nin multilingual cases: Crosslingual transfer and in-language training of\nmultilingual natural language inference. For example, with 48 English training\nexamples, finetuning obtains 33.74% accuracy in crosslingual transfer, barely\nsurpassing the majority baseline (33.33%). In contrast, discrete and soft\nprompting outperform finetuning, achieving 36.43% and 38.79%. We also\ndemonstrate good performance of prompting with training data in multiple\nlanguages other than English.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1\">Mengjie Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Data Augmentation for Low-Resource Neural Machine Translation: A Multi-Task Learning Approach. (arXiv:2109.03645v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03645","description":"<p>In the context of neural machine translation, data augmentation (DA)\ntechniques may be used for generating additional training samples when the\navailable parallel data are scarce. Many DA approaches aim at expanding the\nsupport of the empirical data distribution by generating new sentence pairs\nthat contain infrequent words, thus making it closer to the true data\ndistribution of parallel sentences. In this paper, we propose to follow a\ncompletely different approach and present a multi-task DA approach in which we\ngenerate new sentence pairs with transformations, such as reversing the order\nof the target sentence, which produce unfluent target sentences. During\ntraining, these augmented sentences are used as auxiliary tasks in a multi-task\nframework with the aim of providing new contexts where the target prefix is not\ninformative enough to predict the next word. This strengthens the encoder and\nforces the decoder to pay more attention to the source representations of the\nencoder. Experiments carried out on six low-resource translation tasks show\nconsistent improvements over the baseline and over DA methods aiming at\nextending the support of the empirical data distribution. The systems trained\nwith our approach rely more on the source tokens, are more robust against\ndomain shift and suffer less hallucinations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Cartagena_V/0/1/0/all/0/1\">V&#xed;ctor M. S&#xe1;nchez-Cartagena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espla_Gomis_M/0/1/0/all/0/1\">Miquel Espl&#xe0;-Gomis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perez_Ortiz_J/0/1/0/all/0/1\">Juan Antonio P&#xe9;rez-Ortiz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanchez_Martinez_F/0/1/0/all/0/1\">Felipe S&#xe1;nchez-Mart&#xed;nez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sustainable Modular Debiasing of Language Models. (arXiv:2109.03646v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03646","description":"<p>Unfair stereotypical biases (e.g., gender, racial, or religious biases)\nencoded in modern pretrained language models (PLMs) have negative ethical\nimplications for widespread adoption of state-of-the-art language technology.\nTo remedy for this, a wide range of debiasing techniques have recently been\nintroduced to remove such stereotypical biases from PLMs. Existing debiasing\nmethods, however, directly modify all of the PLMs parameters, which -- besides\nbeing computationally expensive -- comes with the inherent risk of\n(catastrophic) forgetting of useful language knowledge acquired in pretraining.\nIn this work, we propose a more sustainable modular debiasing approach based on\ndedicated debiasing adapters, dubbed ADELE. Concretely, we (1) inject adapter\nmodules into the original PLM layers and (2) update only the adapters (i.e., we\nkeep the original PLM parameters frozen) via language modeling training on a\ncounterfactually augmented corpus. We showcase ADELE, in gender debiasing of\nBERT: our extensive evaluation, encompassing three intrinsic and two extrinsic\nbias measures, renders ADELE, very effective in bias mitigation. We further\nshow that -- due to its modular nature -- ADELE, coupled with task adapters,\nretains fairness even after large-scale downstream training. Finally, by means\nof multilingual BERT, we successfully transfer ADELE, to six target languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lauscher_A/0/1/0/all/0/1\">Anne Lauscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luken_T/0/1/0/all/0/1\">Tobias L&#xfc;ken</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glava&#x161;</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction. (arXiv:2109.03659v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03659","description":"<p>Relation extraction systems require large amounts of labeled examples which\nare costly to annotate. In this work we reformulate relation extraction as an\nentailment task, with simple, hand-made, verbalizations of relations produced\nin less than 15 min per relation. The system relies on a pretrained textual\nentailment engine which is run as-is (no training examples, zero-shot) or\nfurther fine-tuned on labeled examples (few-shot or fully trained). In our\nexperiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per\nrelation (17% points better than the best supervised system on the same\nconditions), and only 4 points short to the state-of-the-art (which uses 20\ntimes more training data). We also show that the performance can be improved\nsignificantly with larger entailment models, up to 12 points in zero-shot,\nallowing to report the best results to date on TACRED when fully trained. The\nanalysis shows that our few-shot systems are specially effective when\ndiscriminating between relations, and that the performance difference in low\ndata regimes comes mainly from identifying no-relation cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sainz_O/0/1/0/all/0/1\">Oscar Sainz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lacalle_O/0/1/0/all/0/1\">Oier Lopez de Lacalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labaka_G/0/1/0/all/0/1\">Gorka Labaka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrena_A/0/1/0/all/0/1\">Ander Barrena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agirre_E/0/1/0/all/0/1\">Eneko Agirre</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Aspect Target Sentiment Classification with Natural Language Prompts. (arXiv:2109.03685v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03685","description":"<p>For many business applications, we often seek to analyze sentiments\nassociated with any arbitrary aspects of commercial products, despite having a\nvery limited amount of labels or even without any labels at all. However,\nexisting aspect target sentiment classification (ATSC) models are not trainable\nif annotated datasets are not available. Even with labeled data, they fall\nshort of reaching satisfactory performance. To address this, we propose simple\napproaches that better solve ATSC with natural language prompts, enabling the\ntask under zero-shot cases and enhancing supervised settings, especially for\nfew-shot cases. Under the few-shot setting for SemEval 2014 Task 4 laptop\ndomain, our method of reformulating ATSC as an NLI task outperforms supervised\nSOTA approaches by up to 24.13 accuracy points and 33.14 macro F1 points.\nMoreover, we demonstrate that our prompts could handle implicitly stated\naspects as well: our models reach about 77% accuracy on detecting sentiments\nfor aspect categories (e.g., food), which do not necessarily appear within the\ntext, even though we trained the models only with explicitly mentioned aspect\nterms (e.g., fajitas) from just 16 reviews - while the accuracy of the\nno-prompt baseline is only around 65%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Seoh_R/0/1/0/all/0/1\">Ronald Seoh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birle_I/0/1/0/all/0/1\">Ian Birle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tak_M/0/1/0/all/0/1\">Mrinal Tak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1\">Haw-Shiuan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pinette_B/0/1/0/all/0/1\">Brian Pinette</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hough_A/0/1/0/all/0/1\">Alfred Hough</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Continuous Entailment Patterns for Lexical Inference in Context. (arXiv:2109.03695v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03695","description":"<p>Combining a pretrained language model (PLM) with textual patterns has been\nshown to help in both zero- and few-shot settings. For zero-shot performance,\nit makes sense to design patterns that closely resemble the text seen during\nself-supervised pretraining because the model has never seen anything else.\nSupervised training allows for more flexibility. If we allow for tokens outside\nthe PLM's vocabulary, patterns can be adapted more flexibly to a PLM's\nidiosyncrasies. Contrasting patterns where a \"token\" can be any continuous\nvector vs. those where a discrete choice between vocabulary elements has to be\nmade, we call our method CONtinuous pAtterNs (CONAN). We evaluate CONAN on two\nestablished benchmarks for lexical inference in context (LIiC) a.k.a. predicate\nentailment, a challenging natural language understanding task with relatively\nsmall training sets. In a direct comparison with discrete patterns, CONAN\nconsistently leads to improved performance, setting a new state of the art. Our\nexperiments give valuable insights into the kind of pattern that enhances a\nPLM's performance on LIiC and raise important questions regarding our\nunderstanding of PLMs using text patterns.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1\">Martin Schmitt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Policy Compliance Detection via Question Answering. (arXiv:2109.03731v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03731","description":"<p>Policy compliance detection is the task of ensuring that a scenario conforms\nto a policy (e.g. a claim is valid according to government rules or a post in\nan online platform conforms to community guidelines). This task has been\npreviously instantiated as a form of textual entailment, which results in poor\naccuracy due to the complexity of the policies. In this paper we propose to\naddress policy compliance detection via decomposing it into question answering,\nwhere questions check whether the conditions stated in the policy apply to the\nscenario, and an expression tree combines the answers to obtain the label.\nDespite the initial upfront annotation cost, we demonstrate that this approach\nresults in better accuracy, especially in the cross-policy setup where the\npolicies during testing are unseen in training. In addition, it allows us to\nuse existing question answering models pre-trained on existing large datasets.\nFinally, it explicitly identifies the information missing from a scenario in\ncase policy compliance cannot be determined. We conduct our experiments using a\nrecent dataset consisting of government policies, which we augment with expert\nannotations and find that the cost of annotating question answering\ndecomposition is largely offset by improved inter-annotator agreement and\nspeed.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1\">Marzieh Saeidi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yazdani_M/0/1/0/all/0/1\">Majid Yazdani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories. (arXiv:2109.03754v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03754","description":"<p>Measuring event salience is essential in the understanding of stories. This\npaper takes a recent unsupervised method for salience detection derived from\nBarthes Cardinal Functions and theories of surprise and applies it to longer\nnarrative forms. We improve the standard transformer language model by\nincorporating an external knowledgebase (derived from Retrieval Augmented\nGeneration) and adding a memory mechanism to enhance performance on longer\nworks. We use a novel approach to derive salience annotation using\nchapter-aligned summaries from the Shmoop corpus for classic literary works.\nOur evaluation against this data demonstrates that our salience detection model\nimproves performance over and above a non-knowledgebase and memory augmented\nlanguage model, both of which are crucial to this improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilmot_D/0/1/0/all/0/1\">David Wilmot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning by Acquiring Contrastive Examples. (arXiv:2109.03764v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03764","description":"<p>Common acquisition functions for active learning use either uncertainty or\ndiversity sampling, aiming to select difficult and diverse data points from the\npool of unlabeled data, respectively. In this work, leveraging the best of both\nworlds, we propose an acquisition function that opts for selecting\n\\textit{contrastive examples}, i.e. data points that are similar in the model\nfeature space and yet the model outputs maximally different predictive\nlikelihoods. We compare our approach, CAL (Contrastive Active Learning), with a\ndiverse set of acquisition functions in four natural language understanding\ntasks and seven datasets. Our experiments show that CAL performs consistently\nbetter or equal than the best performing baseline across all tasks, on both\nin-domain and out-of-domain data. We also conduct an extensive ablation study\nof our method and we further analyze all actively acquired datasets showing\nthat CAL achieves a better trade-off between uncertainty and diversity compared\nto other strategies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Margatina_K/0/1/0/all/0/1\">Katerina Margatina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vernikos_G/0/1/0/all/0/1\">Giorgos Vernikos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barrault_L/0/1/0/all/0/1\">Lo&#xef;c Barrault</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension. (arXiv:2109.03772v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03772","description":"<p>Multi-party dialogue machine reading comprehension (MRC) brings tremendous\nchallenge since it involves multiple speakers at one dialogue, resulting in\nintricate speaker information flows and noisy dialogue contexts. To alleviate\nsuch difficulties, previous models focus on how to incorporate these\ninformation using complex graph-based modules and additional manually labeled\ndata, which is usually rare in real scenarios. In this paper, we design two\nlabour-free self- and pseudo-self-supervised prediction tasks on speaker and\nkey-utterance to implicitly model the speaker information flows, and capture\nsalient clues in a long dialogue. Experimental results on two benchmark\ndatasets have justified the effectiveness of our method over competitive\nbaselines and current state-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Forget me not: A Gentle Reminder to Mind the Simple Multi-Layer Perceptron Baseline for Text Classification. (arXiv:2109.03777v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03777","description":"<p>Graph neural networks have triggered a resurgence of graph-based text\nclassification. We show that already a simple MLP baseline achieves comparable\nperformance on benchmark datasets, questioning the importance of synthetic\ngraph structures. When considering an inductive scenario, i. e., when adding\nnew documents to a corpus, a simple MLP even outperforms most graph-based\nmodels. We further fine-tune DistilBERT for comparison and find that it\noutperforms all state-of-the-art models. We suggest that future studies use at\nleast an MLP baseline to contextualize the results. We provide recommendations\nfor the design and training of such a baseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Highly Parallel Autoregressive Entity Linking with Discriminative Correction. (arXiv:2109.03792v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03792","description":"<p>Generative approaches have been recently shown to be effective for both\nEntity Disambiguation and Entity Linking (i.e., joint mention detection and\ndisambiguation). However, the previously proposed autoregressive formulation\nfor EL suffers from i) high computational cost due to a complex (deep) decoder,\nii) non-parallelizable decoding that scales with the source sequence length,\nand iii) the need for training on a large amount of data. In this work, we\npropose a very efficient approach that parallelizes autoregressive linking\nacross all potential mentions and relies on a shallow and efficient decoder.\nMoreover, we augment the generative objective with an extra discriminative\ncomponent, i.e., a correction term which lets us directly optimize the\ngenerator's ranking. When taken together, these techniques tackle all the above\nissues: our model is &gt;70 times faster and more accurate than the previous\ngenerative method, outperforming state-of-the-art approaches on the standard\nEnglish dataset AIDA-CoNLL. Source code available at\nhttps://github.com/nicola-decao/efficient-autoregressive-EL\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cao_N/0/1/0/all/0/1\">Nicola De Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aziz_W/0/1/0/all/0/1\">Wilker Aziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Titov_I/0/1/0/all/0/1\">Ivan Titov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smelting Gold and Silver for Improved Multilingual AMR-to-Text Generation. (arXiv:2109.03808v1 [cs.CL])","link":"http://arxiv.org/abs/2109.03808","description":"<p>Recent work on multilingual AMR-to-text generation has exclusively focused on\ndata augmentation strategies that utilize silver AMR. However, this assumes a\nhigh quality of generated AMRs, potentially limiting the transferability to the\ntarget task. In this paper, we investigate different techniques for\nautomatically generating AMR annotations, where we aim to study which source of\ninformation yields better multilingual results. Our models trained on gold AMR\nwith silver (machine translated) sentences outperform approaches which leverage\ngenerated silver AMR. We find that combining both complementary sources of\ninformation further improves multilingual AMR-to-text generation. Our models\nsurpass the previous state of the art for German, Italian, Spanish, and Chinese\nby a large margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pfeiffer_J/0/1/0/all/0/1\">Jonas Pfeiffer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Common Semantic Space for Monolingual and Cross-Lingual Meta-Embeddings. (arXiv:2001.06381v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2001.06381","description":"<p>This paper presents a new technique for creating monolingual and\ncross-lingual meta-embeddings. Our method integrates multiple word embeddings\ncreated from complementary techniques, textual sources, knowledge bases and\nlanguages. Existing word vectors are projected to a common semantic space using\nlinear transformations and averaging. With our method the resulting\nmeta-embeddings maintain the dimensionality of the original embeddings without\nlosing information while dealing with the out-of-vocabulary problem. An\nextensive empirical evaluation demonstrates the effectiveness of our technique\nwith respect to previous work on various intrinsic and extrinsic multilingual\nevaluations, obtaining competitive results for Semantic Textual Similarity and\nstate-of-the-art performance for word similarity and POS tagging (English and\nSpanish). The resulting cross-lingual meta-embeddings also exhibit excellent\ncross-lingual transfer learning capabilities. In other words, we can leverage\npre-trained source embeddings from a resource-rich language in order to improve\nthe word representations for under-resourced languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Garcia_Ferrero_I/0/1/0/all/0/1\">Iker Garc&#xed;a-Ferrero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rigau_G/0/1/0/all/0/1\">German Rigau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Chart-based Constituency Parse Extraction from Pre-trained Language Models. (arXiv:2004.13805v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13805","description":"<p>As it has been unveiled that pre-trained language models (PLMs) are to some\nextent capable of recognizing syntactic concepts in natural language, much\neffort has been made to develop a method for extracting complete (binary)\nparses from PLMs without training separate parsers. We improve upon this\nparadigm by proposing a novel chart-based method and an effective top-K\nensemble technique. Moreover, we demonstrate that we can broaden the scope of\napplication of the approach into multilingual settings. Specifically, we show\nthat by applying our method on multilingual PLMs, it becomes possible to induce\nnon-trivial parses for sentences from nine languages in an integrated and\nlanguage-agnostic manner, attaining performance superior or comparable to that\nof unsupervised PCFGs. We also verify that our approach is robust to\ncross-lingual transfer. Finally, we provide analyses on the inner workings of\nour method. For instance, we discover universal attention heads which are\nconsistently sensitive to syntactic information irrespective of the input\nlanguage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taeuk Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bowen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1\">Sang-goo Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13847","description":"<p>Word embeddings are a powerful natural lan-guage processing technique, but\nthey are ex-tremely difficult to interpret. To enable inter-pretable NLP\nmodels, we create vectors whereeach dimension isinherently interpretable.\nByinherently interpretable, we mean a systemwhere each dimension is associated\nwith somehuman-understandablehintthat can describethe meaning of that\ndimension. In order tocreate more interpretable word embeddings,we transform\npretrained dense word embed-dings into sparse embeddings. These new em-beddings\nare inherently interpretable: each oftheir dimensions is created from and\nrepre-sents a natural language word or specific gram-matical concept. We\nconstruct these embed-dings through sparse coding, where each vec-tor in the\nbasis set is itself a word embedding.Therefore, each dimension of our sparse\nvec-tors corresponds to a natural language word.We also show that models\ntrained using thesesparse embeddings can achieve good perfor-mance and are more\ninterpretable in practice,including through human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Templeton_A/0/1/0/all/0/1\">Adly Templeton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2010.09313v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.09313","description":"<p>Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\npaper, we probe BERT specifically to understand and measure the relational\nknowledge it captures. We utilize knowledge base completion tasks to probe\nevery layer of pre-trained as well as fine-tuned BERT (ranking, question\nanswering, NER). Our findings show that knowledge is not just contained in\nBERT's final layers. Intermediate layers contribute a significant amount\n(17-60%) to the total knowledge found. Probing intermediate layers also reveals\nhow different types of knowledge emerge at varying rates. When BERT is\nfine-tuned, relational knowledge is forgotten but the extent of forgetting is\nimpacted by the fine-tuning objective but not the size of the dataset. We found\nthat ranking models forget the least and retain more knowledge in their final\nlayer. We release our code on github to repeat the experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1\">Jonas Wallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Subformer: Exploring Weight Sharing for Parameter Efficiency in Generative Transformers. (arXiv:2101.00234v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00234","description":"<p>Transformers have shown improved performance when compared to previous\narchitectures for sequence processing such as RNNs. Despite their sizeable\nperformance gains, as recently suggested, the model is computationally\nexpensive to train and with a high parameter budget. In light of this, we\nexplore parameter-sharing methods in Transformers with a specific focus on\ngenerative models. We perform an analysis of different parameter\nsharing/reduction methods and develop the Subformer. Our model combines\nsandwich-style parameter sharing, which overcomes naive cross-layer parameter\nsharing in generative models, and self-attentive embedding factorization\n(SAFE). Experiments on machine translation, abstractive summarization and\nlanguage modeling show that the Subformer can outperform the Transformer even\nwhen using significantly fewer parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Reid_M/0/1/0/all/0/1\">Machel Reid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marrese_Taylor_E/0/1/0/all/0/1\">Edison Marrese-Taylor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsuo_Y/0/1/0/all/0/1\">Yutaka Matsuo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Minimum projective linearizations of trees in linear time. (arXiv:2102.03277v4 [cs.DS] UPDATED)","link":"http://arxiv.org/abs/2102.03277","description":"<p>The Minimum Linear Arrangement problem (MLA) consists of finding a mapping\n$\\pi$ from vertices of a graph to distinct integers that minimizes\n$\\sum_{\\{u,v\\}\\in E}|\\pi(u) - \\pi(v)|$. In that setting, vertices are often\nassumed to lie on a horizontal line and edges are drawn as semicircles above\nsaid line. For trees, various algorithms are available to solve the problem in\npolynomial time in $n=|V|$. There exist variants of the MLA in which the\narrangements are constrained. Iordanskii, and later Hochberg and Stallmann\n(HS), put forward $O(n)$-time algorithms that solve the problem when\narrangements are constrained to be planar (also known as one-page book\nembeddings). We also consider linear arrangements of rooted trees that are\nconstrained to be projective (planar embeddings where the root is not covered\nby any edge). Gildea and Temperley (GT) sketched an algorithm for projective\narrangements which they claimed runs in $O(n)$ but did not provide any\njustification of its cost. In contrast, Park and Levy claimed that GT's\nalgorithm runs in $O(n \\log d_{max})$ where $d_{max}$ is the maximum degree but\ndid not provide sufficient detail. Here we correct an error in HS's algorithm\nfor the planar case, show its relationship with the projective case, and derive\nsimple algorithms for the projective and planar cases that run without a doubt\nin $O(n)$ time.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alemany_Puig_L/0/1/0/all/0/1\">Llu&#xed;s Alemany-Puig</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Esteban_J/0/1/0/all/0/1\">Juan Luis Esteban</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferrer_i_Cancho_R/0/1/0/all/0/1\">Ramon Ferrer-i-Cancho</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Structural Adapters in Pretrained Language Models for AMR-to-text Generation. (arXiv:2103.09120v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.09120","description":"<p>Pretrained language models (PLM) have recently advanced graph-to-text\ngeneration, where the input graph is linearized into a sequence and fed into\nthe PLM to obtain its representation. However, efficiently encoding the graph\nstructure in PLMs is challenging because such models were pretrained on natural\nlanguage, and modeling structured data may lead to catastrophic forgetting of\ndistributional knowledge. In this paper, we propose StructAdapt, an adapter\nmethod to encode graph structure into PLMs. Contrary to prior work, StructAdapt\neffectively models interactions among the nodes based on the graph\nconnectivity, only training graph structure-aware adapter parameters. In this\nway, we incorporate task-specific knowledge while maintaining the topological\nstructure of the graph. We empirically show the benefits of explicitly encoding\ngraph structure into PLMs using StructAdapt, outperforming the state of the art\non two AMR-to-text datasets, training only 5.1% of the PLM parameters.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ribeiro_L/0/1/0/all/0/1\">Leonardo F. R. Ribeiro</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SelfExplain: A Self-Explaining Architecture for Neural Text Classifiers. (arXiv:2103.12279v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.12279","description":"<p>We introduce SelfExplain, a novel self-explaining model that explains a text\nclassifier's predictions using phrase-based concepts. SelfExplain augments\nexisting neural classifiers by adding (1) a globally interpretable layer that\nidentifies the most influential concepts in the training set for a given sample\nand (2) a locally interpretable layer that quantifies the contribution of each\nlocal input concept by computing a relevance score relative to the predicted\nlabel. Experiments across five text-classification datasets show that\nSelfExplain facilitates interpretability without sacrificing performance. Most\nimportantly, explanations from SelfExplain show sufficiency for model\npredictions and are perceived as adequate, trustworthy and understandable by\nhuman judges compared to existing widely-used baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rajagopal_D/0/1/0/all/0/1\">Dheeraj Rajagopal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balachandran_V/0/1/0/all/0/1\">Vidhisha Balachandran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tsvetkov_Y/0/1/0/all/0/1\">Yulia Tsvetkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training. (arXiv:2104.01027v2 [cs.SD] UPDATED)","link":"http://arxiv.org/abs/2104.01027","description":"<p>Self-supervised learning of speech representations has been a very active\nresearch area but most work is focused on a single domain such as read audio\nbooks for which there exist large quantities of labeled and unlabeled data. In\nthis paper, we explore more general setups where the domain of the unlabeled\ndata for pre-training data differs from the domain of the labeled data for\nfine-tuning, which in turn may differ from the test data domain. Our\nexperiments show that using target domain data during pre-training leads to\nlarge performance improvements across a variety of setups. On a large-scale\ncompetitive setup, we show that pre-training on unlabeled in-domain data\nreduces the gap between models trained on in-domain and out-of-domain labeled\ndata by 66%-73%. This has obvious practical implications since it is much\neasier to obtain unlabeled target domain data than labeled data. Moreover, we\nfind that pre-training on multiple domains improves generalization performance\non domains not seen during training. Code and models will be made available at\nhttps://github.com/pytorch/fairseq.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1\">Wei-Ning Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sriram_A/0/1/0/all/0/1\">Anuroop Sriram</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baevski_A/0/1/0/all/0/1\">Alexei Baevski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Likhomanenko_T/0/1/0/all/0/1\">Tatiana Likhomanenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1\">Qiantong Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pratap_V/0/1/0/all/0/1\">Vineel Pratap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kahn_J/0/1/0/all/0/1\">Jacob Kahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Ann Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collobert_R/0/1/0/all/0/1\">Ronan Collobert</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Synnaeve_G/0/1/0/all/0/1\">Gabriel Synnaeve</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auli_M/0/1/0/all/0/1\">Michael Auli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adapting Language Models for Zero-shot Learning by Meta-tuning on Dataset and Prompt Collections. (arXiv:2104.04670v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04670","description":"<p>Large pre-trained language models (LMs) such as GPT-3 have acquired a\nsurprising ability to perform zero-shot learning. For example, to classify\nsentiment without any training examples, we can \"prompt\" the LM with the review\nand the label description \"Does the user like this movie?\", and ask whether the\nnext word is \"yes\" or \"no\". However, the next word prediction training\nobjective is still misaligned with the target zero-shot learning objective. To\naddress this weakness, we propose meta-tuning, which directly optimizes the\nzero-shot learning objective by fine-tuning pre-trained language models on a\ncollection of datasets. We focus on classification tasks, and construct the\nmeta-dataset by aggregating 43 existing datasets and annotating 441 label\ndescriptions in a question-answering (QA) format. When evaluated on unseen\ntasks, meta-tuned models outperform a same-sized QA model and the previous SOTA\nzero-shot learning system based on natural language inference. Additionally,\nincreasing parameter count from 220M to 770M improves AUC-ROC scores by 6.3%,\nand we forecast that even larger models would perform better. Therefore,\nmeasuring zero-shot learning performance on language models out-of-the-box\nmight underestimate their true potential, and community-wide efforts on\naggregating datasets and unifying their formats can help build models that\nanswer prompts better.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhong_R/0/1/0/all/0/1\">Ruiqi Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1\">Kristy Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Adaptation of BERT and Performance on Downstream Document Classification: Insights from Social Media. (arXiv:2104.08116v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08116","description":"<p>Language use differs between domains and even within a domain, language use\nchanges over time. For pre-trained language models like BERT, domain adaptation\nthrough continued pre-training has been shown to improve performance on\nin-domain downstream tasks. In this article, we investigate whether temporal\nadaptation can bring additional benefits. For this purpose, we introduce a\ncorpus of social media comments sampled over three years. It contains\nunlabelled data for adaptation and evaluation on an upstream masked language\nmodelling task as well as labelled data for fine-tuning and evaluation on a\ndownstream document classification task. We find that temporality matters for\nboth tasks: temporal adaptation improves upstream and temporal fine-tuning\ndownstream task performance. Time-specific models generally perform better on\npast than on future test sets, which matches evidence on the bursty usage of\ntopical words. However, adapting BERT to time and domain does not improve\nperformance on the downstream task over only adapting to domain. Token-level\nanalysis shows that temporal adaptation captures event-driven changes in\nlanguage use in the downstream task, but not those changes that are actually\nrelevant to task performance. Based on our findings, we discuss when temporal\nadaptation may be more effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rottger_P/0/1/0/all/0/1\">Paul R&#xf6;ttger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pierrehumbert_J/0/1/0/all/0/1\">Janet B. Pierrehumbert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Noisy Labels for Entity-Centric Information Extraction. (arXiv:2104.08656v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08656","description":"<p>Recent information extraction approaches have relied on training deep neural\nmodels. However, such models can easily overfit noisy labels and suffer from\nperformance degradation. While it is very costly to filter noisy labels in\nlarge learning resources, recent studies show that such labels take more\ntraining steps to be memorized and are more frequently forgotten than clean\nlabels, therefore are identifiable in training. Motivated by such properties,\nwe propose a simple co-regularization framework for entity-centric information\nextraction, which consists of several neural models with identical structures\nbut different parameter initialization. These models are jointly optimized with\nthe task-specific losses and are regularized to generate similar predictions\nbased on an agreement loss, which prevents overfitting on noisy labels.\nExtensive experiments on two widely used but noisy benchmarks for information\nextraction, TACRED and CoNLL03, demonstrate the effectiveness of our framework.\nWe release our code to the community for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information Retrieval Models. (arXiv:2104.08663v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2104.08663","description":"<p>Existing neural information retrieval (IR) models have often been studied in\nhomogeneous and narrow settings, which has considerably limited insights into\ntheir out-of-distribution (OOD) generalization capabilities. To address this,\nand to facilitate researchers to broadly evaluate the effectiveness of their\nmodels, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous\nevaluation benchmark for information retrieval. We leverage a careful selection\nof 18 publicly available datasets from diverse text retrieval tasks and domains\nand evaluate 10 state-of-the-art retrieval systems including lexical, sparse,\ndense, late-interaction and re-ranking architectures on the BEIR benchmark. Our\nresults show BM25 is a robust baseline and re-ranking and\nlate-interaction-based models on average achieve the best zero-shot\nperformances, however, at high computational costs. In contrast, dense and\nsparse-retrieval models are computationally more efficient but often\nunderperform other approaches, highlighting the considerable room for\nimprovement in their generalization capabilities. We hope this framework allows\nus to better evaluate and understand existing retrieval systems, and\ncontributes to accelerating progress towards better robust and generalizable\nsystems in the future. BEIR is publicly available at\nhttps://github.com/UKPLab/beir.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Thakur_N/0/1/0/all/0/1\">Nandan Thakur</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reimers_N/0/1/0/all/0/1\">Nils Reimers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ruckle_A/0/1/0/all/0/1\">Andreas R&#xfc;ckl&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1\">Abhishek Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1\">Iryna Gurevych</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Out-of-Distribution Detection for Pretrained Transformers. (arXiv:2104.08812v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08812","description":"<p>Pretrained Transformers achieve remarkable performance when training and test\ndata are from the same distribution. However, in real-world scenarios, the\nmodel often faces out-of-distribution (OOD) instances that can cause severe\nsemantic shift problems at inference time. Therefore, in practice, a reliable\nmodel should identify such instances, and then either reject them during\ninference or pass them over to models that handle another distribution. In this\npaper, we develop an unsupervised OOD detection method, in which only the\nin-distribution (ID) data are used in training. We propose to fine-tune the\nTransformers with a contrastive loss, which improves the compactness of\nrepresentations, such that OOD instances can be better differentiated from ID\nones. These OOD instances can then be accurately detected using the Mahalanobis\ndistance in the model's penultimate layer. We experiment with comprehensive\nsettings and achieve near-perfect OOD detection performance, outperforming\nbaselines drastically. We further investigate the rationales behind the\nimprovement, finding that more compact representations through margin-based\ncontrastive learning bring the improvement. We release our code to the\ncommunity for future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wenxuan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stream-level Latency Evaluation for Simultaneous Machine Translation. (arXiv:2104.08817v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08817","description":"<p>Simultaneous machine translation has recently gained traction thanks to\nsignificant quality improvements and the advent of streaming applications.\nSimultaneous translation systems need to find a trade-off between translation\nquality and response time, and with this purpose multiple latency measures have\nbeen proposed. However, latency evaluations for simultaneous translation are\nestimated at the sentence level, not taking into account the sequential nature\nof a streaming scenario. Indeed, these sentence-level latency measures are not\nwell suited for continuous stream translation resulting in figures that are not\ncoherent with the simultaneous translation policy of the system being assessed.\nThis work proposes a stream-level adaptation of the current latency measures\nbased on a re-segmentation approach applied to the output translation, that is\nsuccessfully evaluated on streaming conditions for a reference IWSLT task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iranzo_Sanchez_J/0/1/0/all/0/1\">Javier Iranzo-S&#xe1;nchez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Civera_J/0/1/0/all/0/1\">Jorge Civera</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Juan_A/0/1/0/all/0/1\">Alfons Juan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BERTnesia: Investigating the capture and forgetting of knowledge in BERT. (arXiv:2106.02902v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.02902","description":"<p>Probing complex language models has recently revealed several insights into\nlinguistic and semantic patterns found in the learned representations. In this\narticle, we probe BERT specifically to understand and measure the relational\nknowledge it captures in its parametric memory. While probing for linguistic\nunderstanding is commonly applied to all layers of BERT as well as fine-tuned\nmodels, this has not been done for factual knowledge. We utilize existing\nknowledge base completion tasks (LAMA) to probe every layer of pre-trained as\nwell as fine-tuned BERT models(ranking, question answering, NER). Our findings\nshow that knowledge is not just contained in BERT's final layers. Intermediate\nlayers contribute a significant amount (17-60%) to the total knowledge found.\nProbing intermediate layers also reveals how different types of knowledge\nemerge at varying rates. When BERT is fine-tuned, relational knowledge is\nforgotten. The extent of forgetting is impacted by the fine-tuning objective\nand the training data. We found that ranking models forget the least and retain\nmore knowledge in their final layer compared to masked language modeling and\nquestion-answering. However, masked language modeling performed the best at\nacquiring new knowledge from the training data. When it comes to learning\nfacts, we found that capacity and fact density are key factors. We hope this\ninitial work will spur further research into understanding the parametric\nmemory of language models and the effect of training objectives on factual\nknowledge. The code to repeat the experiments is publicly available on GitHub.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wallat_J/0/1/0/all/0/1\">Jonas Wallat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1\">Jaspreet Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anand_A/0/1/0/all/0/1\">Avishek Anand</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"JNLP Team: Deep Learning Approaches for Legal Processing Tasks in COLIEE 2021. (arXiv:2106.13405v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.13405","description":"<p>COLIEE is an annual competition in automatic computerized legal text\nprocessing. Automatic legal document processing is an ambitious goal, and the\nstructure and semantics of the law are often far more complex than everyday\nlanguage. In this article, we survey and report our methods and experimental\nresults in using deep learning in legal document processing. The results show\nthe difficulties as well as potentials in this family of approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1\">Phuong Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vuong_T/0/1/0/all/0/1\">Thi-Hai-Yen Vuong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_Q/0/1/0/all/0/1\">Quan Minh Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_C/0/1/0/all/0/1\">Chau Minh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_B/0/1/0/all/0/1\">Binh Tran Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh Le Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Satoh_K/0/1/0/all/0/1\">Ken Satoh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BROS: A Layout-Aware Pre-trained Language Model for Understanding Documents. (arXiv:2108.04539v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.04539","description":"<p>Understanding documents from their visual snapshots is an emerging problem\nthat requires both advanced computer vision and NLP methods. The recent advance\nin OCR enables the accurate recognition of text blocks, yet it is still\nchallenging to extract key information from documents due to the diversity of\ntheir layouts. Although recent studies on pre-trained language models show the\nimportance of incorporating layout information on this task, the conjugation of\ntexts and their layouts still follows the style of BERT optimized for\nunderstanding the 1D text. This implies there is room for further improvement\nconsidering the 2D nature of text layouts. This paper introduces a pre-trained\nlanguage model, BERT Relying On Spatiality (BROS), which effectively utilizes\nthe information included in individual text blocks and their layouts.\nSpecifically, BROS encodes spatial information by utilizing relative positions\nand learns spatial dependencies between OCR blocks with a novel area-masking\nstrategy. These two novel approaches lead to an efficient encoding of spatial\nlayout information highlighted by the robust performance of BROS under\nlow-resource environments. We also introduce a general-purpose parser that can\nbe combined with BROS to extract key information even when there is no order\ninformation between text blocks. BROS shows its superiority on four public\nbenchmarks -- FUNSD, SROIE*, CORD, and SciTSR -- and its robustness in\npractical cases where order information of text blocks is not available.\nFurther experiments with a varying number of training examples demonstrate the\nhigh training efficiency of our approach. Our code will be open to the public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hong_T/0/1/0/all/0/1\">Teakgyu Hong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1\">Donghyun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1\">Mingi Ji</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hwang_W/0/1/0/all/0/1\">Wonseok Hwang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1\">Daehyun Nam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1\">Sungrae Park</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on six public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work has claimed, our auxiliary experiments suggest that\nrelation prediction is contributory to named entity prediction in a\nnon-negligible way. The source code can be found at\nhttps://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12229","description":"<p>The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications since the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current algorithms can tackle such\nproblem reliably in a realistic scenario where zero OOD training data is\navailable. In this study, we propose ProtoInfoMax, a new architecture that\nextends Prototypical Networks to simultaneously process In-Domain (ID) and OOD\nsentences via Mutual Information Maximization (InfoMax) objective. Experimental\nresults show that our proposed method can substantially improve performance up\nto 20% for OOD detection in low resource settings of text classification. We\nalso show that ProtoInfoMax is less prone to typical over-confidence Error of\nNeural Networks, leading to more reliable ID and OOD prediction outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Factual Consistency Evaluation for Text Summarization via Counterfactual Estimation. (arXiv:2108.13134v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13134","description":"<p>Despite significant progress has been achieved in text summarization, factual\ninconsistency in generated summaries still severely limits its practical\napplications. Among the key factors to ensure factual consistency, a reliable\nautomatic evaluation metric is the first and the most crucial one. However,\nexisting metrics either neglect the intrinsic cause of the factual\ninconsistency or rely on auxiliary tasks, leading to an unsatisfied correlation\nwith human judgments or increasing the inconvenience of usage in practice. In\nlight of these challenges, we propose a novel metric to evaluate the factual\nconsistency in text summarization via counterfactual estimation, which\nformulates the causal relationship among the source document, the generated\nsummary, and the language prior. We remove the effect of language prior, which\ncan cause factual inconsistency, from the total causal effect on the generated\nsummary, and provides a simple yet effective way to evaluate consistency\nwithout relying on other auxiliary tasks. We conduct a series of experiments on\nthree public abstractive text summarization datasets, and demonstrate the\nadvantages of the proposed metric in both improving the correlation with human\njudgments and the convenience of usage. The source code is available at\nhttps://github.com/xieyxclack/factual_coco.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1\">Yuexiang Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1\">Fei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yang Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yaliang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1\">Bolin Ding</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Effective Sequence-to-Sequence Dialogue State Tracking. (arXiv:2108.13990v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13990","description":"<p>Sequence-to-sequence models have been applied to a wide variety of NLP tasks,\nbut how to properly use them for dialogue state tracking has not been\nsystematically investigated. In this paper, we study this problem from the\nperspectives of pre-training objectives as well as the formats of context\nrepresentations. We demonstrate that the choice of pre-training objective makes\na significant difference to the state tracking quality. In particular, we find\nthat masked span prediction is more effective than auto-regressive language\nmodeling. We also explore using Pegasus, a span prediction-based pre-training\nobjective for text summarization, for the state tracking model. We found that\npre-training for the seemingly distant summarization task works surprisingly\nwell for dialogue state tracking. In addition, we found that while recurrent\nstate context representation works also reasonably well, the model may have a\nhard time recovering from earlier mistakes. We conducted experiments on the\nMultiWOZ 2.1-2.4, WOZ 2.0, and DSTC2 datasets with consistent observations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jeffrey Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdieh_M/0/1/0/all/0/1\">Mahdis Mahdieh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Ye Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1\">Yuan Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yonghui Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MergeBERT: Program Merge Conflict Resolution via Neural Transformers. (arXiv:2109.00084v2 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2109.00084","description":"<p>Collaborative software development is an integral part of the modern software\ndevelopment life cycle, essential to the success of large-scale software\nprojects. When multiple developers make concurrent changes around the same\nlines of code, a merge conflict may occur. Such conflicts stall pull requests\nand continuous integration pipelines for hours to several days, seriously\nhurting developer productivity.\n</p>\n<p>In this paper, we introduce MergeBERT, a novel neural program merge framework\nbased on the token-level three-way differencing and a transformer encoder\nmodel. Exploiting restricted nature of merge conflict resolutions, we\nreformulate the task of generating the resolution sequence as a classification\ntask over a set of primitive merge patterns extracted from real-world merge\ncommit data.\n</p>\n<p>Our model achieves 64--69% precision of merge resolution synthesis, yielding\nnearly a 2x performance improvement over existing structured and neural program\nmerge tools. Finally, we demonstrate versatility of our model, which is able to\nperform program merge in a multilingual setting with Java, JavaScript,\nTypeScript, and C# programming languages, generalizing zero-shot to unseen\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mytkowicz_T/0/1/0/all/0/1\">Todd Mytkowicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghorbani_N/0/1/0/all/0/1\">Negar Ghorbani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fakhoury_S/0/1/0/all/0/1\">Sarah Fakhoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dinella_E/0/1/0/all/0/1\">Elizabeth Dinella</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bird_C/0/1/0/all/0/1\">Christian Bird</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lahiri_S/0/1/0/all/0/1\">Shuvendu Lahiri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"M^2-MedDialog: A Dataset and Benchmarks for Multi-domain Multi-service Medical Dialogues. (arXiv:2109.00430v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00430","description":"<p>Medical dialogue systems (MDSs) aim to assist doctors and patients with a\nrange of professional medical services, i.e., diagnosis, consultation, and\ntreatment. However, one-stop MDS is still unexplored because: (1) no dataset\nhas so large-scale dialogues contains both multiple medical services and\nfine-grained medical labels (i.e., intents, slots, values); (2) no model has\naddressed a MDS based on multiple-service conversations in a unified framework.\nIn this work, we first build a Multiple-domain Multiple-service medical\ndialogue (M^2-MedDialog)dataset, which contains 1,557 conversations between\ndoctors and patients, covering 276 types of diseases, 2,468 medical entities,\nand 3 specialties of medical services. To the best of our knowledge, it is the\nonly medical dialogue dataset that includes both multiple medical services and\nfine-grained medical labels. Then, we formulate a one-stop MDS as a\nsequence-to-sequence generation problem. We unify a MDS with causal language\nmodeling and conditional causal language modeling, respectively. Specifically,\nwe employ several pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5)\nand their variants to get benchmarks on M^2-MedDialog dataset. We also propose\npseudo labeling and natural perturbation methods to expand M2-MedDialog dataset\nand enhance the state-of-the-art pretrained models. We demonstrate the results\nachieved by the benchmarks so far through extensive experiments on\nM2-MedDialog. We release the dataset, the code, as well as the evaluation\nscripts to facilitate future research in this important research direction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1\">Guojun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1\">Jiahuan Pei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1\">Pengjie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhumin Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1\">Zhaochun Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Huasheng Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PermuteFormer: Efficient Relative Position Encoding for Long Sequences. (arXiv:2109.02377v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02377","description":"<p>A recent variation of Transformer, Performer, scales Transformer to longer\nsequences with a linear attention mechanism. However, it is not compatible with\nrelative position encoding, which has advantages over absolute position\nencoding. In this paper, we discuss possible ways to add relative position\nencoding to Performer. Based on the analysis, we propose PermuteFormer, a\nPerformer-based model with relative position encoding that scales linearly on\nlong sequences. PermuteFormer applies position-dependent transformation on\nqueries and keys to encode positional information into the attention module.\nThis transformation is carefully crafted so that the final output of\nself-attention is not affected by absolute positions of tokens. PermuteFormer\nintroduces negligible computational overhead by design that it runs as fast as\nPerformer. We evaluate PermuteFormer on Long-Range Arena, a dataset for long\nsequences, as well as WikiText-103, a language modeling dataset. The\nexperiments show that PermuteFormer uniformly improves the performance of\nPerformer with almost no computational overhead and outperforms vanilla\nTransformer on most of the tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1\">Peng Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Vision Guided Generative Pre-trained Language Models for Multimodal Abstractive Summarization. (arXiv:2109.02401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02401","description":"<p>Multimodal abstractive summarization (MAS) models that summarize videos\n(vision modality) and their corresponding transcripts (text modality) are able\nto extract the essential information from massive multimodal data on the\nInternet. Recently, large-scale generative pre-trained language models (GPLMs)\nhave been shown to be effective in text generation tasks. However, existing MAS\nmodels cannot leverage GPLMs' powerful generation ability. To fill this\nresearch gap, we aim to study two research questions: 1) how to inject visual\ninformation into GPLMs without hurting their generation ability; and 2) where\nis the optimal place in GPLMs to inject the visual information? In this paper,\nwe present a simple yet effective method to construct vision guided (VG) GPLMs\nfor the MAS task using attention-based add-on layers to incorporate visual\ninformation while maintaining their original text generation ability. Results\nshow that our best model significantly surpasses the prior state-of-the-art\nmodel by 5.7 ROUGE-1, 5.3 ROUGE-2, and 5.1 ROUGE-L scores on the How2 dataset,\nand our visual guidance method contributes 83.6% of the overall improvement.\nFurthermore, we conduct thorough ablation studies to analyze the effectiveness\nof various modality fusion methods and fusion locations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tiezheng Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1\">Wenliang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zihan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_P/0/1/0/all/0/1\">Pascale Fung</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient conformer: Progressive downsampling and grouped attention for automatic speech recognition. (arXiv:2109.01163v2 [eess.AS] CROSS LISTED)","link":"http://arxiv.org/abs/2109.01163","description":"<p>The recently proposed Conformer architecture has shown state-of-the-art\nperformances in Automatic Speech Recognition by combining convolution with\nattention to model both local and global dependencies. In this paper, we study\nhow to reduce the Conformer architecture complexity with a limited computing\nbudget, leading to a more efficient architecture design that we call Efficient\nConformer. We introduce progressive downsampling to the Conformer encoder and\npropose a novel attention mechanism named grouped attention, allowing us to\nreduce attention complexity from $O(n^{2}d)$ to $O(n^{2}d / g)$ for sequence\nlength $n$, hidden dimension $d$ and group size parameter $g$. We also\nexperiment the use of strided multi-head self-attention as a global\ndownsampling operation. Our experiments are performed on the LibriSpeech\ndataset with CTC and RNN-Transducer losses. We show that within the same\ncomputing budget, the proposed architecture achieves better performances with\nfaster training and decoding compared to the Conformer. Our 13M parameters CTC\nmodel achieves competitive WERs of 3.6%/9.0% without using a language model and\n2.7%/6.7% with an external n-gram language model on the test-clean/test-other\nsets while being 29% faster than our CTC Conformer baseline at inference and\n36% faster to train.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Burchi_M/0/1/0/all/0/1\">Maxime Burchi</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Vielzeuf_V/0/1/0/all/0/1\">Valentin Vielzeuf</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-08T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#"}}]}]}