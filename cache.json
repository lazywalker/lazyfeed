{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-29T04:27:39.172019049Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"【求助】Rust 如何在一个 crate 里划分模块，让 no_std 和 std 共存？","link":"https://rustcc.cn/article?id=e8a4a5cc-3316-4d63-a122-ee4a4027e2f6","description":"<p>我希望写一个库，同时支持 <code>no_std</code> 和 <code>std</code>。</p>\n<p>参考别的库试着写了一下，但似乎写得不对。</p>\n<p><code>lib.rs</code>:</p>\n<pre><code>#![cfg_attr(not(feature = \"std\"), no_std)]\npub mod no_std;\n\n#[cfg(feature = \"std\")]\npub mod std;\n</code></pre>\n<p>我想让 mod std 支持 std，而 mod no_std 仅允许使用 core，应如何正确设置呢？</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-29 03:26:58","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"#![windows_subsystem = \"windows\"] 的副作用","link":"https://rustcc.cn/article?id=2e80c186-a06a-40cf-ae32-26a96388fb8e","description":"<p>我发现一个奇怪的问题。\n禁用控制台，会在启动时导致鼠标出现一秒的漏斗状态。\n而如果显示控制台，启动过程非常丝滑，完全感受不到任何异状。</p>\n<p>二者在CPU占用方面并没有显示差异。</p>\n<p>请问是什么原因引发了这个问题？</p>\n<p>我没有在内部使用任何 println!。</p>\n<p>====</p>\n<p>这也会显示在加载外部进程时。\n如果禁用控制台，运行一个外部进程，通常鼠标会出现漏斗。\n显示控制台，则不会出现漏斗。</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-29 02:50:40","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust 日报】2021-09-28 RustConf 2021 还是精选","link":"https://rustcc.cn/article?id=42fcae33-c4cd-4b8d-8932-0ee84749cdd7","description":"<h2>语言团队 leader Niko Matsakis 的演讲</h2>\n<p>此视频是 Niko 关于语言团队的一些思考：</p>\n<ul>\n<li>Rust 进入 Linux 内核就已经“完成使命”了嘛？</li>\n<li>Rust 的现状\n<ul>\n<li>兼具 c/c++ 的性能与 JavaScript/Ruby/Python 的易用性(然而实际上易用性还是差了一点）\n<ul>\n<li>可以在一个两周 sprint 将 JavaScript 编写的日志服务降低 75% 使用与 95%内存使用</li>\n<li>而且不需要太多的额外工程成本——只要 Rust 程序通过编译了，它通常就能工作（If it compiles, it works!)</li>\n</ul>\n</li>\n<li>但其实还有不少人认为 “rust 不能提高生产力”（据2019 survey）</li>\n<li>2015年证明了 Rust 能用，2021年我们还证明了 Rust 还很持久</li>\n</ul>\n</li>\n<li>现在我们可能需要花六个月时间让使用 Rust 变成有生产力的行为，但我们的目标是六周就能感到有生产力</li>\n<li>并且我们希望Rust 是“可靠、高性能、并为使用者赋能”，为此，我们（语言团队）需要专注、富有创造力以及拓宽思考广度</li>\n<li>介绍了一些 2021 版本的特性</li>\n<li>关于 async Rust -- MVP版本后我们应该做什么？ 可以参考一下 async vision doc</li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-28 15:11:13","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"我在看structopt源码时，发现它源码里并没有`#[proc_macro_attribute]`，那它使用属性宏不应该报错嘛？","link":"https://rustcc.cn/article?id=f59dd1fd-394d-44f6-aac3-cfd1f2ac8cf5","description":"<pre><code>#[derive(StructOpt, Debug)]\n#[structopt(name = \"basic\")] // 这里不是属性宏么\nstruct Opt {\n</code></pre>\n<p>难道有其他的定义属性宏的方式？</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-28 04:12:27","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"[成都/杭州/远程] 长桥 Rust 跨平台基础架构师","link":"https://rustcc.cn/article?id=200431f7-9b4a-4198-bd75-679781fcc244","description":"<h2>公司介绍</h2>\n<p><a href=\"https://longbridgeapp.com\" rel=\"noopener noreferrer\">长桥</a> 成立于 2019 年 2 月，是一家致力于提供全球股票经纪的互联网券商，核心成员均来自于阿里巴巴，蚂蚁金服，华为等知名公司。</p>\n<p>我司技术文化开放，实际上目前大部分服务端是采用 Go 语言开发，我们通过一些初步尝试与验证，发现了 Rust 对于跨平台开发有很不错的前景，准备持续投入资源开拓、探索 Rust 生态对于我们公司技术的各种可能性。</p>\n<blockquote>\n<p>我司除此岗位之外，也长期开放很多其他岗位如：Go、Ruby、前端，有兴趣可以邮件联系我。</p>\n</blockquote>\n<h2>岗位职责</h2>\n<p>参与公司跨平台的基础架构设计与开发工作，采用 Rust + FFI / WebAssembly，用来解决多端基础库的支持。并基于这个可能性在基础架构内实现更多有益与多端支持的功能，比如：API 封装、复杂行情资产计算、数据离线/持久化/预加载、网络协议改进（HTTP/3）、数据传输格式改进（JSON to Protobuf 或其他）、传输加密等等。</p>\n<p>同时这个岗位也将会持续探索 Rust 在跨平台方向可行性。</p>\n<p>全职岗位，你可以选择成都、杭州、新加坡、香港、北京等多个办公地入职，或也可以选择远程。</p>\n<h2>岗位要求</h2>\n<p>这个岗位是要招来做跨平台基础架构的，得有一定的设计能力和技术功底。</p>\n<p>满足以下任意一项：</p>\n<ul>\n<li>有 Rust 项目经验</li>\n<li>GitHub Profile 过硬，是个开源社区参与者</li>\n<li>独立 App 开发经历，有较好的作品</li>\n<li>有 Rust FFI / WebAssembly 的相关开发经验</li>\n<li>擅长多种语言，爱折腾</li>\n<li>对 Rust 有兴趣，有一定其他语言（Go、Ruby、Node.js），此项不支持远程</li>\n</ul>\n<h2>待遇</h2>\n<ul>\n<li>入职配 MacBook Pro (Apple M1) + LG 4K 27 寸显示器；</li>\n<li>薪资范围开放，面议</li>\n<li>完整社保、公积金、以及带薪年假；</li>\n</ul>\n<p>简历投递：huacnlee@gmail.com</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-28 02:05:04","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-09-27 20W+行 Rust 构建的游戏居然这么好玩！","link":"https://rustcc.cn/article?id=373312af-8ae5-4f78-af87-37171af41f4e","description":"<h1>20W+行 Rust 构建的游戏居然这么好玩！</h1>\n<p>真的是一款很有趣的游戏。\n视频中还未展示很多游戏内容，欢迎大家来游玩。\n有能力的玩家欢迎提供建模模型，修改源码提供MOD。</p>\n<p><a href=\"https://www.bilibili.com/video/BV1mL411x7rF?p=1&amp;share_medium=android&amp;share_plat=android&amp;share_session_id=1b28a632-f60f-4ee5-9d71-4bcb96a744b1&amp;share_source=WEIXIN&amp;share_tag=s_i&amp;timestamp=1632706296&amp;unique_k=tsDj2D\" rel=\"noopener noreferrer\">B站视频</a></p>\n<h1>rustls 0.20 发布了</h1>\n<p>russtls 是一个 Rust 编写的现代的 TLS库。它使用ring进行加密，使用libwebpki进行证书验证。</p>\n<p>目前已发布 0.20 版本.</p>\n<p><a href=\"https://github.com/rustls/rustls\" rel=\"noopener noreferrer\">github地址</a></p>\n<h1>dune: 一个 Rust 写的 shell</h1>\n<p>如下图所示:</p>\n<p><img src=\"https://github.com/adam-mcdaniel/dune/raw/main/assets/welcome2.png\" alt=\"img\"></p>\n<p><a href=\"https://github.com/adam-mcdaniel/dune\" rel=\"noopener noreferrer\">github 地址</a></p>\n<h1>SeaORM: 异步动态的 ORM</h1>\n<p>SeaORM 是一个关系型 ORM,可以帮助你构建轻量的高并发的 web 服务.</p>\n<p><a href=\"https://www.sea-ql.org/SeaORM/blog/2021-09-20-introducing-sea-orm/\" rel=\"noopener noreferrer\">原文链接</a></p>\n<h1>emoji_pix: 一个可以将图片转换为像素风的工具</h1>\n<p>如下图效果:</p>\n<p><img src=\"https://github.com/multimeric/emoji_pix/raw/master/discord_example.png\" alt=\"img\"></p>\n<p><a href=\"https://github.com/multimeric/emoji_pix\" rel=\"noopener noreferrer\">github地址</a></p>\n<p>--</p>\n<p>From 日报小组 BobQin，FBI小白</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-27 12:27:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust开发postgres扩展","link":"https://rustcc.cn/article?id=ffa8347d-c09c-4636-9346-f3d861cd9dda","description":"<p>文章中介绍了如何基于 cargo-pgx  开发postgres相关扩展，如何开发自定义聚集函数等</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-27 08:13:29","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"有没有兴趣来讨论一下NFT？？","link":"https://rustcc.cn/article?id=1682a83c-4e59-4b36-992d-93dbddaf8cce","description":"<blockquote>\n<p>本文不包含区块链专有名词，不包含 cx 内容，可放心食用。</p>\n</blockquote>\n<p>最近区块链世界里面发生了几件大事，一是 Elon Musk 宣布15亿美元购入 Bitcoin 作为公司储备资产；二是迈阿密市长提议使用比特币作为该市的税收和公务员工资支付手段；三是 Visa 开始接受 Bitcoin 在其支付网络上进行结算。这几件事情说明区块链叙事开始加速走向主流世界，也说明是时候认真考虑这个行业的职业机会了。毕竟美国的科技和政治精英并不是傻子，他们押注这个行业说明它“有点东西”。</p>\n<h2>1. 价值观</h2>\n<p>不得不承认，自中本聪在2009年初通过比特币创世块写下反传统金融体系的宣言开始，指引区块链前进的根本动力就一直是其价值观。如果你也认可以下的价值观，那说明你适合在这个行业生存。</p>\n<p><strong>Can't be evil.</strong></p>\n<p>谷歌的“Don't be evil”不作恶原则是远远不够的，因为是否作恶的决定权仍然在中心化权威手里。不作恶成为是一种施舍，而不是一种约束。而区块链世界则强调开源、自由 fork、去中心。代码一旦完成即自主运行。即使是中本聪也无法私自增发比特币，因为它从协议层就限制了作恶的可能性。</p>\n<p><strong>Permissionless.</strong></p>\n<p>无需许可是区块链世界的最高原则，任何人不需要第三方的授权就可以操作区块链。它保障了任何人都有相同的机会使用区块链上的信息、金融等服务，同时为不同服务之间的相互组合提供了可能性。对比来说，微信、支付宝互相封锁，Tw、Fb等社交平台随意禁言。他们不但违背了互联网的初衷，也扼杀了思想交流和技术创新。</p>\n<p><strong>Don't trust, verify.</strong></p>\n<p>你不应该相信任何人声明的事情、提供的服务、做出的承诺，你需要自己确认。正如这篇文章的所有观点你都不应该选择直接相信，你应该自己去考证。</p>\n<h2>2. 对古典互联网的革命性优势</h2>\n<p>互联网世界发展到现在遇到了哪些无法解决的问题？第一，大公司的数据霸权。南山必胜客刚刚打赢了“好友关系不属于个人隐私”的官司，用户在大公司面前形同裸奔。同时用户没有平台迁移的权利，我不爽，但是我还离不开，因为我的社交关系和历史都沉淀在这个平台上。第二，算法霸权。大公司利用算法优势对用户进行差别对待，精准杀熟。第三，平台割裂。平台越大，平台间的鸿沟越大。不同平台间的协作难度也越大。同一个小程序要在微信、支付宝各做一份，同一个用户要在不同的应用中分别注册。</p>\n<p>区块链世界采用了完全不同的用户-服务范式。所有的服务都是以用户为中心进行整合，这个范式也被称为“用户端集成”。这个范式一举打破了传统互联网产品的壁垒，数据、资产、关系都掌握在用户手中，服务商围绕着用户提供服务。想象一个这样的互联网世界：你的好友关系在各个软件平台中共享，你的数据由自己掌握，即使服务商跑路了，你的资产和数据也不会丢失。这样的一个新的互联网世界对古典互联网世界显然具有维度上的优势。</p>\n<h2>3. 那么问题是什么</h2>\n<p>区块链这么厉害，为什么到现在还只有炒币这一个场景呢。个人认为区块链和主流应用结合主要面临两方面的障碍。</p>\n<p>第一是用户门槛高、体验差。创建一个账户需要理解私钥、地址、交易、手续费等概念，创建过程涉及助记词、长密码、助记词校验等流程，对普通人绝对是劝退模式。做过互联网产品的都知道，一个不合理的按钮位置都会降低用户留存，而区块链的用户体验是灾难级别的。做个上链业务居然要平均半分钟才有成功失败的反馈。</p>\n<p>第二是收费模型极不友好。用户早已习惯互联网产品免费使用、付费升级或者广告收费的方式。而区块链应用则不然，做任何业务都必须用户付费，而且<strong>必须购买区块链代币才能付费</strong>。有这个障碍存在，大众用户绝对进不来，所以这两年区块链世界内卷越来越严重。</p>\n<h2>4. 怎么解决</h2>\n<p>终于可以介绍一下我们是谁，以及在做什么了。我们是原 <a href=\"https://www.nervos.org\" rel=\"noopener noreferrer\">Nervos</a> 应用开发工程师团队独立成立的区块链应用公司 <strong>Nervina Labs</strong>，Nervina 是个合成词，Nervos + China。我们的使命是在中国市场进行商业合作、技术服务和产品开发工作。关于 Nervos 公链多说无益，大家秉持区块链 Don't trust, verify 的精神可以自己翻一翻 <a href=\"https://github.com/nervosnetwork/\" rel=\"noopener noreferrer\">GitHub</a> 和媒体报道。这里重点说说我们的产品和业务内容。我们认为 Nervos 可以很好地解决前面提出的那两个问题，从而让区块链出圈，进入主流互联网世界。</p>\n<p>首先，比特币、以太坊等传统的区块链平台在账户层只支持硬编码的特定密码学算法，用户必须创建并自行管理账户的公私钥对，因此认知和使用门槛非常高。而 Nervos 采用了原生的账户抽象方案，支持任意的密码学算法，包括在互联网世界已经应用很久的 RSA、P256、和 SM2 等算法。大家熟悉的 https 协议、email 协议、iOS/Android 的生物识别模块、甚至护照身份证等都支持相应的密码学算法。所以我们可以把它们拿来在 Nervos 上创建和管理账户，这样用户操作区块链的体验和操作互联网应用基本没有区别。</p>\n<p>其次，收费模型方面尽管 Nervos 默认也是需要原生代币作为手续费，但 Nervos 允许服务商代付，并且在链上凭证转移的时候可以把小额手续费一起转移。从用户角度上看，大家完全不需要理解什么叫手续费，甚至不需要知道业务发生在区块链上。更大的好处是用户不需要购买区块链代币，规避了合规风险。</p>\n<p>我们的今年主推的产品是为互联网企业准备的“+区块链”方案，帮助对区块链有一定了解的头部互联网公司实现部分业务向区块链做迁移。我们将帮助互联网公司改造现有的会员卡、论坛勋章、电子票务、商品预售、粉丝经济等等生态，用区块链为更多的大众用户提供价值。</p>\n<h2>5. 收入</h2>\n<p>不谈收入只谈理想就是耍流氓。好在区块链是最不缺钱的行业，我们会提供和一线大厂相一致的薪资水平，包括五险一金等各方面国家规定的福利待遇一个不少。但对于很多人来说，单纯的线性工资是远远不够在一线城市安身立命的。大厂的股票期权，创业公司的干股才是大家上升到富裕阶层的砝码。在我厂工作可以有两个获得非线性收入的机会，我们分别谈谈。</p>\n<p>首先是期权。我厂会在内部推动项目单独融资进而独立发展，项目参与者会获得相应的期权。在区块链世界中，优秀项目从创立到股份/期权获得实际价值的周期要远远短于传统企业上市的周期。拼夕夕光速发展也走了3年才上市，在区块链世界中几乎只要开始对外运营其期权、股份就立刻有了二级市场，可以完成价值兑现。从项目融资到“上市”中间基本上就是一个开发周期。</p>\n<p>其次是行业信息优势。“康波周期理论”告诉我们人生一般也就一到两次大的机遇，抓住的话就可以完成阶层跨越。上一波机遇大家都知道是买房，而这一波应该就是区块链了。尽管从圈外人看比特币上涨到6万美金一个已经和普通人无缘了，但只有你站在这个圈子里面你才会对整个浪潮获得全面的感知，从而得出自己理性的结论。所谓“链圈一日、人间一年”，区块链世界进化速度极快，你待在这个圈子里面会发现无数的机会，比特币只是其中名声最大的一个而已。</p>\n<h2>6. 职业发展</h2>\n<p>我们一直戏谑 Nervos 为链圈的黄埔军校，因为确实为圈子输送了很多技术和市场人才。时不时会遇到一个圈内的 panel 四个嘉宾三个是我们的老员工。究其原因，Nervos 生态会更关注诸如“区块链究竟会带来什么价值”、“区块链的长期发展必须解决什么核心问题”等这种原则性问题而不是短平快地追逐最新热点。所以在我厂绝对不用担心只能当工具人而学不到真东西。在 Nervos 生态下大家接触的一定是业界最前沿的技术和产品。</p>\n<p>Nervina Labs 对待人才管理有三个关键词：开放、自驱和涌现。开放指的是所有产品所有部门之间绝大多数信息都是互相开放的，并且几乎所有的代码库都是开源的，员工可以使用私人的 github 账号贡献代码。自驱是我们把每个员工当做对自己负责任的成年人看待，相信自己可以管理好自己的时间，不需要用严格的打卡、kpi 等制度进行管理。涌现则是指除了自上而下的任务以外，我们提倡员工自己为生态添砖加瓦。你认为整个区块链生态缺什么就可以提出来方案、预算、招聘需求，我们内部讨论通过后就给你资源让你去实现。这也是人才晋升的重要通道。你甚至可以提出融资需求，拉团队出去创业，我们提供必要的启动资金。这种模式只有在区块链行业才有可能，因为从公链整体的角度看，所有的生态企业都会带来价值，最优的模式就是由社区自发维护整个生态的发展。</p>\n<h2>7. 工作环境与强度</h2>\n<p>我们是<strong>100% 远程工作</strong>。沟通工具主要是 G Suite、GitHub、Notion 和 Telegram。既然是远程，也就没什么打卡、加班，全凭自觉。我们强调交付，产品和技术充分沟通后大家约定交付时间，按照交付时间和质量评估绩效。因此自驱型的人最适应我厂的工作。</p>\n<p>作为团队的传统，我们每年计划有两次封闭开发的“团建”活动。一般会选在像青岛、杭州等风景饮食俱佳的城市，包下一个民宿或者别墅，用一周左右时间大家聚在一起冲刺开发产品。大家远程久了通过一两次这种活动加深了解促进协作效率，事实证明很有效果。</p>\n<h2>8. 投简历 or 交朋友</h2>\n<p>区块链是我们这代人肉眼可见的一次绝佳的机遇，不论你是否考虑我厂，我都建议你认真关注一下这个行业。我是qiao，很乐意交朋友，愿意回答你关于区块链的任何问题。或者你恰巧对我们感兴趣，对以下的职位感兴趣，你可以直接发邮件给我你的简历，我们聊聊看。</p>\n<h2>9.招聘岗位</h2>\n<p>平面设计\n岗位职责：\n1、负责线上线下活动海报、宣传物料等宣传品的设计；\n2、辅助新媒体进行企业外宣的输出；\n3、品牌VI设计、制作及其它图文处理；\n4、协助其他部门人员对设计及美学方面的工作顺利完成；\n职位要求：\n1、大专及以上学历，美术、设计类相关专业；\n2、2年以上平面设计工作经验；\n3、具有良好的美术基础，精通Photoshop、Illustrator、AE等设计软件，热衷于设计工作；会C4D加分，具备较好的手绘能力优先；\n4、强烈的自我驱动力，进取心。</p>\n<h3>附加信息：</h3>\n<ul>\n<li>工作时间：周末双休</li>\n<li>上下班时间：10:00-19:00</li>\n</ul>\n<h3>面试信息：</h3>\n<ul>\n<li>面试方式： 视频面试</li>\n<li>面试轮数： 1-2轮</li>\n<li>时间安排： 分多次完成</li>\n<li>补充标签： 可周末面试 | 可下班后面试</li>\n</ul>\n<p>前端工程师</p>\n<h2>岗位职责</h2>\n<ul>\n<li>根据产品需求高质量完成 web 应用以及 electron 应用;</li>\n<li>对具体产品进行性能优化;</li>\n<li>维护团队的工具链;</li>\n<li>对可复用组件进行抽象并独立维护.</li>\n</ul>\n<h2>职位要求</h2>\n<ul>\n<li>具备良好的前端开发技能, 熟悉 HTML, CSS 和 TypeScript, 了解 Web 标准化(可访问性, 安全性);</li>\n<li>在泛前端范围有开发经验, 比如 Node 应用, Electron 应用;</li>\n<li>熟练使用前端的各种工具, 比如各类脚手架, CSS 处理器, 模板引擎;</li>\n<li>Web 技术栈偏向 React 及 TypeScript.</li>\n</ul>\n<h2>加分项</h2>\n<ul>\n<li>具有开源项目经验;</li>\n<li>提供 GitHub 或技术博客;</li>\n<li>有区块链产品开发经验；\n-本岗位可接受应届生，欢迎投递</li>\n</ul>\n<p>市场运营总监</p>\n<h3>职位描述：</h3>\n<p>1、全面负责平台运营工作，根据公司的OKR，制定运营策略和平台发展规划；\n2、搭建运营推广渠道，进行品牌传播；\n3、建立业务增长模型、用户增长模型，驱动用户增长的持续性和规模化；\n4、负责平台用户（B端/C端）的活动策划，活动流程和规则设计，协同技术团队、其他运营推广团队做好活动组织安排，活动效果评估等相关工作；\n5、参与公司战略及经营目标讨论及制定，向公司高层提供运营及发展建议；\n任职要求：\n1、本科及以上学历；3-5年相关岗位从业经验；\n2、区块链相关背景、熟悉NFT等优先考虑；\n3、本岗位为远程办公模式，每年两次集中封闭，每期5-10天。</p>\n<h3>附加信息：</h3>\n<ul>\n<li>工作时间：周末双休</li>\n<li>上下班时间：10:00-19:00</li>\n</ul>\n<p>rust工程师</p>\n<h3>职位描述：</h3>\n<p>岗位职责：1、负责智能合约的开发及设计；2、负责区块链业务系统分析与设计工作；3、负责智能合约代码测试、运行和维护。任职要求：1、计算机相关专业本科及以上学历，3年以上工作经验；2、熟练掌握 C/C++、Rust 等系统开发语言至少一种，至少有过两年相关开发经验；3、对数据结构和算法，对密码学，安全协议和加密算法有研究者优先；4、优秀的英语文档撰写与阅读能力者优先；5、了解区块链，有合约开发经验更佳。</p>\n<h3>附加信息：</h3>\n<ul>\n<li>工作时间：周末双休</li>\n<li>上下班时间：10:00-19:00</li>\n</ul>\n<h3>面试信息：</h3>\n<ul>\n<li>面试方式： 视频面试</li>\n<li>面试轮数： 1-2轮</li>\n<li>时间安排： 分多次完成</li>\n<li>补充标签： 可周末面试 | 可下班后面试</li>\n</ul>\n<p>联系方式：15005209448（微信同）  wangmeng@nervina.io</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-27 02:53:35","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust 日报】2021-09-26 RustConf 2021 项目精选","link":"https://rustcc.cn/article?id=218ae5ee-1938-4ab4-a893-18e80cca61f6","description":"<h3>RustConf 2021 项目精选</h3>\n<p>以下项目来自 RustConf 2021。</p>\n<p><strong>移动构造函数：有可能吗？</strong></p>\n<p>“自引用” 类型是一种对自身引用的类型；异步 Features 是当今 Rust 中最常见的自引用类型。但是，它们不能在不使引用无效的情况下移动，因此常被固定在堆或栈上。你不能返回它们或将之放入 Collection（当然了不用 Boxing）。C++ 通过移动构造函数大量使用可安全移动的自引用类型，将移动构造函数桥接到 Rust 是 C++ FFI 未解决的重大问题之一。使用对 Pin P 保证的新颖解释，我们将所有 C++ 构造函数（而不仅仅是移动构造函数）移植到 Rust，而不影响 Rust 的「使用后移动」保护（C++ 所缺乏的）。今天，稳定的 Rust 完全支持按移动返回和集合，除了零成本的 C++ FFI，Rust 的 “构造函数” 还可用于在纯 Rust 中表达新颖的数据结构。</p>\n<p>视频：<a href=\"https://www.youtube.com/watch?v=UrDhMWISR3w\" rel=\"noopener noreferrer\">(4) RustConf 2021 - Move Constructors: Is it Possible? by Miguel Young de la Sota - YouTube</a></p>\n<p><strong>在 Rust 中不要过度优化的重要性</strong></p>\n<p>Rust 编程语言具有许多高级功能，可以实现一些出色的优化。对于新的 Rust 开发人员来说，这些既令人兴奋又令人沮丧。因为 Rust 允许我们编写经验丰富的 Rust 开发人员可以理解的高度优化的代码，所以很容易一直这样做。本演讲将展示这些优化通常对于获得优于 Python 等高度动态语言的性能来说是不必要的。对于新的 Rust 开发人员来说，打破过度优化的诱惑可以提高 Rust 的生产力和满意度。</p>\n<p>视频：<a href=\"https://www.youtube.com/watch?v=CV5CjUlcqsw\" rel=\"noopener noreferrer\">(4) RustConf 2021 - The Importance of Not Over-Optimizing in Rust by Lily Mara - YouTube</a></p>\n<p><strong>模糊驱动开发</strong></p>\n<p>有时候你可以想出一种简单的方法验证正确性，但很难找到单元测试的实际示例。你知道「对于所有 x 都成立」但是无法为 x 想出好的可能性。这就是依靠模糊测试可以通过提供一些代码尚未涵盖的真实示例来快速推动开发的地方。本演讲我们将一起完成使用 cargo fuzz 的过程，以构建一个可以压缩 JSON Patch 操作的快速程序，涉及 Rust 社区的一些实用程序/库（模糊测试工具，serde_json 等），以及学习一些关于在开始模糊测试时你可能会有什么样的误解。</p>\n<p>视频：<a href=\"https://www.youtube.com/watch?v=qUu1vJNg8yo\" rel=\"noopener noreferrer\">(4) RustConf 2021 - Fuzz Driven Development by Midas Lambrichts - YouTube</a></p>\n<p><strong>使用 Rust 写最快的 GBDT 库</strong></p>\n<p>本演讲将分享作者优化梯度提升决策树机器学习算法的 Rust 实现的经验。 通过代码片段、堆栈跟踪和基准测试，探索如何使用 rayon、perf、cargo-asm、编译器内在函数和 unsafe rust 来编写一个 GBDT 库，该库的训练速度比用 C/C++ 编写的类似库更快。PS：作者是个美女。</p>\n<p>视频：<a href=\"https://www.youtube.com/watch?v=D1NAREuicNs\" rel=\"noopener noreferrer\">(4) RustConf 2021 - Writing the Fastest GBDT Library in Rust by Isabella Tromba - YouTube</a></p>\n<p><strong>Twitter 工程师使用 Rust 重构的故事</strong></p>\n<p>三名工程师，在不同的方面，各自采用自己的方法将 Rust 添加到 C 代码库中，每个人都越来越雄心勃勃。最初只是想用同样快速的 Rust 实现替换服务器的网络和事件循环。 我们会重用 C 中的许多核心组件，然后从 Rust 中调用它们。肯定不会有那么多代码...... Pelikan 是 Twitter 用于内存缓存的开源和模块化框架，允许我们用单个代码库替换 Memcached 和 Redis 分支并获得更好的性能。在 Twitter，我们运行数百个缓存集群，在内存中存储数百 TB 的小对象。内存缓存至关重要，需要性能、可靠性和效率，本演讲将分享在 Pelikan 工作的冒险经历以及如何用 Rust 重写它。</p>\n<p>视频：<a href=\"https://www.youtube.com/watch?v=m-Qg3OoPIdc\" rel=\"noopener noreferrer\">(4) RustConf 2021 - Whoops! I Rewrote It in Rust by Brian Martin - YouTube</a></p>\n<p><strong>五个鲜为人知的属性增强你的代码</strong></p>\n<p>属性是 Rust 语言中最有用和最方便的特性之一，它使程序员能够自动推导 Trait，在几分钟内设置测试套件，并有条件地为不同平台编译代码。但是在标准库的内部和外部，还有许多其他有用的属性，它们常常被忽视。</p>\n<p>视频：<a href=\"https://www.youtube.com/watch?v=8d7DqeYXq7A\" rel=\"noopener noreferrer\">(4) RustConf 2021 - Supercharging Your Code With Five Little-Known Attributes by Jackson Lewis - YouTube</a></p>\n<p><strong>编译时协调</strong></p>\n<p>你可以写出好的代码，我可以，但是我们可以一起编写正确的代码吗？今天普通开发者面临的最困难的问题不是算法或框架。错误通常在代码间发现。项目包含必须在任何地方遵守但未在任何地方指定的规则。它们是惯例、部落知识和最佳实践。让我们了解 Rust 如何让编写在文件、crates 和人员之间代码的一致变得更容易。这是关于如何停止踩到每个人的脚趾并学会爱上 borrow checker 的故事。</p>\n<p>视频：<a href=\"https://www.youtube.com/watch?v=4_Jg-rLDy-Y\" rel=\"noopener noreferrer\">(4) RustConf 2021 - Compile-Time Social Coordination by Zac Burns - YouTube</a></p>\n<p><strong>Hacking Rustc</strong></p>\n<p>编译器通常被视为一个令人生畏的 “黑匣子”，只有少数人才能理解。实际上，编译器 “只是” 另一种类型的程序。  只要你知道编译器是用什么语言编写的，编译器编译的语言是什么，并且有时间，你也可以处理它们。如果你曾经对修改编译器感兴趣，或者遇到了一个真心希望尽快修复的错误，那么这是卷起袖子自己动手的机会，因为我们将介绍 rustc 上的 hacking 基础知识。</p>\n<p>视频：<a href=\"https://www.youtube.com/watch?v=9H9SO2u6Q20\" rel=\"noopener noreferrer\">(4) RustConf 2021 - Hacking rustc: Contributing to the Compiler by Esteban Kuber - YouTube</a></p>\n<p>完整内容可以查看：<a href=\"https://this-week-in-rust.org/blog/2021/09/22/this-week-in-rust-409/\" rel=\"noopener noreferrer\">This Week in Rust 409 · This Week in Rust</a></p>\n<h3>全栈 Rust：包含示例的 Tutorial</h3>\n<p>本 Tutorial 是一个简单的全栈 Web 应用，包含具有数据库支持的 REST 后端和基于 Wasm 的单页应用程序前端。具体是构建一个简单的宠物主人应用程序，使用户能够添加主人和他们的宠物。程序需要将所有者和他们的宠物列表提供详细视图，能够根据需要删除和添加宠物。</p>\n<p>教程涵盖以下内容：</p>\n<ul>\n<li>创建一个全栈的 Rust APP</li>\n<li>常用功能</li>\n<li>构建 REST 后端</li>\n<li>前端实现</li>\n<li>测试</li>\n</ul>\n<p>教程地址：<a href=\"https://blog.logrocket.com/full-stack-rust-a-complete-tutorial-with-examples/\" rel=\"noopener noreferrer\">Full-stack Rust: A complete tutorial with examples - LogRocket Blog</a></p>\n<h3>Rust For Javaer</h3>\n<p>写给 Java 程序员的 Rust 介绍 Tutorial。包括以下内容：</p>\n<ul>\n<li>简介</li>\n<li>Rust 构建和运行</li>\n<li>Rust 变量</li>\n<li>Rust 默认的不可变性</li>\n<li>Rust 函数</li>\n<li>Rust if/else 和表达式</li>\n<li>Rust 字符串</li>\n<li>Rust 结构体（Java 类）</li>\n<li>Rust 结构体函数（Java 静态函数）</li>\n<li>Rust 结构体方法（Java 方法）</li>\n<li>Rust Trait（Java 接口）</li>\n<li>Rust 数组（Java 数组）</li>\n<li>Rust 元组</li>\n<li>Rust Vec</li>\n<li>Rust 枚举（Java 枚举）</li>\n<li>Rust Match</li>\n<li>Java 17 Switch 表达式（Rust Match）</li>\n<li>Rest Generics</li>\n<li>Rust Option（vs Null/Optional）</li>\n<li>Rust Result（Java Exception）</li>\n</ul>\n<p>非常不错的介绍，值得一看看。</p>\n<p>视频地址：<a href=\"https://www.youtube.com/playlist?list=PL7r-PXl6ZPcD63DS2djSiz4SlXkaTfobc\" rel=\"noopener noreferrer\">(4) Rust for Java Developers - YouTube</a></p>\n<h3>eztd：让 Rust 更易学习</h3>\n<p>项目致力于『可学习性和可控制』，目标包括：</p>\n<ul>\n<li>低语法噪声</li>\n<li>对 Python 开发者熟悉</li>\n<li>允许优化内循环</li>\n<li>与 Rust 生态互操作</li>\n</ul>\n<p>设计指南：<a href=\"https://github.com/epage/eztd/blob/main/CONTRIBUTING.md#design-guidelines\" rel=\"noopener noreferrer\">eztd/CONTRIBUTING.md at main · epage/eztd</a></p>\n<p>GitHub：<a href=\"https://github.com/epage/eztd#about\" rel=\"noopener noreferrer\">epage/eztd: Source code spell checker</a></p>\n<p>网址：<a href=\"https://epage.github.io/blog/2021/09/learning-rust/\" rel=\"noopener noreferrer\">Learnability of Rust</a></p>\n<hr>\n<p>From 日报小组 长琴</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc 论坛：支持 rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust 语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-26 15:35:01","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-09-25 GitHub Advisory Database 现已支持 Rust","link":"https://rustcc.cn/article?id=43ce24e2-fba4-494e-841b-a36f7445958c","description":"<h3>GitHub Advisory Database 现已支持 Rust</h3>\n<p>GitHub Advisory Database（安全咨询数据库）现在已经支持 Rust 了。</p>\n<p>下一步将支持  dependabot ， dependabot是 GitHub 推出的一个提醒依赖更新机器人，当你项目的依赖有更新的时候就会自动推送一个 Pull requests。</p>\n<p>GitHub Advisory Database 官方写道：</p>\n<p>这一覆盖范围确保了Rust社区的任何成员都可以在他们的代码所在的同一个地方检查安全问题：GitHub上。这仅仅是第一步! 请查看我们的公共路线图，我们正在努力实现Rust对依赖关系图和Dependabot警报的支持。</p>\n<p>谢谢你，RustSec和Rust社区!\n在我们努力将Rust生态系统加入咨询数据库的过程中，我们得到了RustSec和Rust社区的大量支持。</p>\n<p>我们非常感谢RustSec，这是一个独立的组织，负责收集、规范和发布与Rust库相关的安全建议。它的免费公共数据库是我们自己的Rust漏洞数据集的起点。</p>\n<p>我们计划继续与RustSec和更广泛的Rust社区合作，使我们自己的GitHub安全咨询数据可用并易于使用，以进一步补充他们的数据。通过合作，我们可以为减少漏洞的可见性问题做更多的工作，而不是单独行动。</p>\n<p>原文<a href=\"https://github.blog/2021-09-23-github-advisory-database-now-supports-rust/\" rel=\"noopener noreferrer\">链接</a>，https://github.blog/2021-09-23-github-advisory-database-now-supports-rust/</p>\n<h3>Klask</h3>\n<p>Klask，从 clap v3 apps 自动创建 GUI 应用，将 egui 用于图形。</p>\n<p>Github <a href=\"https://github.com/MichalGniadek/klask\" rel=\"noopener noreferrer\">链接</a>，https://github.com/MichalGniadek/klask</p>\n<h3>MiniJinja</h3>\n<p>MiniJinja 是一个强大但最小依赖的 Rust 模板引擎，基于 Python 的 <a href=\"https://jinja.palletsprojects.com/en/3.0.x/\" rel=\"noopener noreferrer\">Jinja2</a> 模板引擎的语法和行为。</p>\n<pre><code>use minijinja::Environment;\nuse serde::Serialize;\n\n#[derive(Serialize)]\npub struct Context {\n    name: String,\n}\n\nfn main() {\n    let mut env = Environment::new();\n    env.add_template(\"hello.txt\", \"Hello {{ name }}!\").unwrap();\n    let template = env.get_template(\"hello.txt\").unwrap();\n    println!(\"{}\", template.render(&amp;Context {\n        name: \"World\".into()\n    }).unwrap());\n}\n</code></pre>\n<p>Github <a href=\"https://github.com/mitsuhiko/minijinja\" rel=\"noopener noreferrer\">链接</a>，https://github.com/mitsuhiko/minijinja</p>\n<hr>\n<p>From 日报小组 <a href=\"https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929\" rel=\"noopener noreferrer\">洋芋</a></p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-25 15:43:52","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了","link":"https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b","description":"<p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>\n<ol>\n<li>认识面向基础架构语言</li>\n<li>理解 Rust 所有权</li>\n<li>通过实战理解 Rust 宏</li>\n<li>通过 Datafuse 理解全链路跟踪</li>\n<li>Rust 异步编程入门 Future Part 1</li>\n<li>Rust 异步编程入门 Future Part 2</li>\n</ol>\n<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。\n本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>\n<hr>\n<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>\n<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>\n<p>分享讲师： 董泽润</p>\n<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现\n讲师介绍：\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<hr>\n<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>\n<p>分享时间：  周日晚上2021-09-12 20:00-21:00</p>\n<p>分享讲师： 苏林</p>\n<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>\n<ol>\n<li>利用 Tokio 实现一个 Mini Http server</li>\n<li>基于 Http server提供内容动态的 API 网关</li>\n<li>利用 Redis 实现对 API 网关加速</li>\n<li>学习 Rust RPC 调用，实现微服务调用</li>\n</ol>\n<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<p>Rust 异步编程入门 Future Part 1   | Vol. 5\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<p>Rust 异步编程入门 Future Part 2  | Vol. 6\nhttps://www.bilibili.com/video/bv1oy4y1G7jC</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 02:23:16","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust 学习随笔","link":"https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8","description":"<h1>切换镜像源</h1>\n<p>crm =&gt; https://github.com/wtklbm/crm</p>\n<p>常用命令就是 <code>crm best</code></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 14:35:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"pretree 补全文档发布了,再次谢谢大神的指点终于入门了。","link":"https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827","description":"<h1>Pretree</h1>\n<p>pretree is a package for storing and querying routing rules with prefix tree .</p>\n<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>\n<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>\n<p>Inspired by <a href=\"https://github.com/obity/pretree\" rel=\"noopener noreferrer\">obity/pretree</a> (golang)</p>\n<h1>Doc</h1>\n<p>See this document at <a href=\"https://docs.rs/pretree\" rel=\"noopener noreferrer\">API documentation</a></p>\n<h1>Install</h1>\n<p>Add the following line to your Cargo.toml file:</p>\n<pre><code>pretree = \"1.0.0\"\n</code></pre>\n<h1>Example</h1>\n<pre><code>use pretree::Pretree;\nlet mut p = Pretree::new();\np.store(\"GET\",\"account/{id}/info/:name\");\np.store(\"GET\",\"account/:id/login\");\np.store(\"GET\",\"account/{id}\");\np.store(\"GET\",\"bacteria/count_number_by_month\");\nlet (ok,rule,vars) = p.query(\"GET\",\"account/929239\");\nprintln!(\"ok:{} rule:{} vars:{:#?}\",ok,rule,vars);\n\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 09:37:30","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6","link":"https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a","description":"<h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>\n<p><strong>课程时间:</strong>  2021年9月5日 20:00-21:00</p>\n<p><strong>课程介绍:</strong>  上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识,  Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>\n<h3>课程大纲</h3>\n<p>1、回顾 Rust 异步编程模型.</p>\n<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>\n<p>3、Tokio 介绍.</p>\n<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>\n<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/\nRust 异步编程入门 Future Part 1  回放地址：\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 08:40:15","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"公开课：《 Rust 异步编程入门 Future 》|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>\n<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>\n<p><strong>课程介绍:</strong>  讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是  Rust 异步编程的核心基础。</p>\n<h3>课程大纲</h3>\n<p>1、为什么需要异步.</p>\n<p>2、理解异步编程模型.</p>\n<p>3、Future 编程模型讲解.</p>\n<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>\n<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>异步引擎 C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D 游戏引擎</h3>\n<ul>\n<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>\n<li><strong>延迟着色</strong></li>\n<li><strong>内置保存/加载</strong></li>\n<li><strong>独立场景编辑器</strong></li>\n<li><strong>高级物理模型</strong></li>\n<li><strong>分层模型资源</strong></li>\n<li><strong>几何实例化</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>\n<p><strong>课程时间：</strong>  2021年8月22日 20:30-21:30</p>\n<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>\n<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>\n<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>课程大纲</h3>\n<ol>\n<li>\n<p>什么是分布式追踪系统OpenTracing及应用场景</p>\n</li>\n<li>\n<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>\n</li>\n<li>\n<p>为什么需要tokio-rs/tracing库</p>\n</li>\n<li>\n<p>演示Datafuse项目中tokio-rs/tracing的使用</p>\n</li>\n</ol>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>课程中苏林老师推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"论坛github账户无法登录解决笔记","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>有反映这两天github账户无法登录了。</p>\n<p>报这个错：</p>\n<pre><code>get github user info err\n</code></pre>\n<p>查了几个地方：</p>\n<ol>\n<li>代码是否运行正常：Ok</li>\n<li>https代理是否正常：Ok</li>\n<li>检查了github返回日志，发现是：</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>进入这个地址一看：<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>\n<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>\n<p>特此记录。</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 的 Future 与 Javascript 的 Promise 功能对照参考","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>\n<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>\n<blockquote>\n<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* 什么都不做 */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被  （1）跨线程传递  （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust公开课：《通过实战理解 Rust 宏》| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>\n<p><strong>课程时间：</strong>  2021年8月15日 20:30-21:30</p>\n<p><strong>课程介绍：</strong></p>\n<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>这就是通过宏实现配置的统一行为，代码参考：\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>\n<h3>课程大纲</h3>\n<ul>\n<li>什么是 Rust 宏</li>\n<li>什么是宏运行原理</li>\n<li>如何创建 Rust 宏过程</li>\n<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>\n</ul>\n<p><strong>讲师介绍</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>课程中苏林老师推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-09-29T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Visually Grounded Reasoning across Languages and Cultures. (arXiv:2109.13238v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13238","description":"<p>The design of widespread vision-and-language datasets and pre-trained\nencoders directly adopts, or draws inspiration from, the concepts and images of\nImageNet. While one can hardly overestimate how much this benchmark contributed\nto progress in computer vision, it is mostly derived from lexical databases and\nimage queries in English, resulting in source material with a North American or\nWestern European bias. Therefore, we devise a new protocol to construct an\nImageNet-style hierarchy representative of more languages and cultures. In\nparticular, we let the selection of both concepts and images be entirely driven\nby native speakers, rather than scraping them automatically. Specifically, we\nfocus on a typologically diverse set of languages, namely, Indonesian, Mandarin\nChinese, Swahili, Tamil, and Turkish. On top of the concepts and images\nobtained through this new protocol, we create a multilingual dataset for\n{M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting\nstatements from native speaker annotators about pairs of images. The task\nconsists of discriminating whether each grounded statement is true or false. We\nestablish a series of baselines using state-of-the-art models and find that\ntheir cross-lingual transfer performance lags dramatically behind supervised\nperformance in English. These results invite us to reassess the robustness and\naccuracy of current state-of-the-art models beyond a narrow domain, but also\nopen up new exciting challenges for the development of truly multilingual and\nmulticultural systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1\">Emanuele Bugliarello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ponti_E/0/1/0/all/0/1\">Edoardo Maria Ponti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1\">Siva Reddy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Collier_N/0/1/0/all/0/1\">Nigel Collier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TURINGBENCH: A Benchmark Environment for Turing Test in the Age of Neural Text Generation. (arXiv:2109.13296v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13296","description":"<p>Recent progress in generative language models has enabled machines to\ngenerate astonishingly realistic texts. While there are many legitimate\napplications of such models, there is also a rising need to distinguish\nmachine-generated texts from human-written ones (e.g., fake news detection).\nHowever, to our best knowledge, there is currently no benchmark environment\nwith datasets and tasks to systematically study the so-called \"Turing Test\"\nproblem for neural text generation methods. In this work, we present the\nTuringBench benchmark environment, which is comprised of (1) a dataset with\n200K human- or machine-generated samples across 20 labels {Human, GPT-1,\nGPT-2_small, GPT-2_medium, GPT-2_large, GPT-2_xl, GPT-2_PyTorch, GPT-3,\nGROVER_base, GROVER_large, GROVER_mega, CTRL, XLM, XLNET_base, XLNET_large,\nFAIR_wmt19, FAIR_wmt20, TRANSFORMER_XL, PPLM_distil, PPLM_gpt2}, (2) two\nbenchmark tasks -- i.e., Turing Test (TT) and Authorship Attribution (AA), and\n(3) a website with leaderboards. Our preliminary experimental results using\nTuringBench show that FAIR_wmt20 and GPT-3 are the current winners, among all\nlanguage models tested, in generating the most human-like indistinguishable\ntexts with the lowest F1 score by five state-of-the-art TT detection models.\nThe TuringBench is available at: https://turingbench.ist.psu.edu/\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Uchendu_A/0/1/0/all/0/1\">Adaku Uchendu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zeyu Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1\">Thai Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongwon Lee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Isotropy Calibration of Transformers. (arXiv:2109.13304v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13304","description":"<p>Different studies of the embedding space of transformer models suggest that\nthe distribution of contextual representations is highly anisotropic - the\nembeddings are distributed in a narrow cone. Meanwhile, static word\nrepresentations (e.g., Word2Vec or GloVe) have been shown to benefit from\nisotropic spaces. Therefore, previous work has developed methods to calibrate\nthe embedding space of transformers in order to ensure isotropy. However, a\nrecent study (Cai et al. 2021) shows that the embedding space of transformers\nis locally isotropic, which suggests that these models are already capable of\nexploiting the expressive capacity of their embedding space. In this work, we\nconduct an empirical evaluation of state-of-the-art methods for isotropy\ncalibration on transformers and find that they do not provide consistent\nimprovements across models and tasks. These results support the thesis that,\ngiven the local isotropy, transformers do not benefit from additional isotropy\ncalibration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1\">Yue Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinkus_K/0/1/0/all/0/1\">Karolis Martinkus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pascual_D/0/1/0/all/0/1\">Damian Pascual</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clematide_S/0/1/0/all/0/1\">Simon Clematide</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wattenhofer_R/0/1/0/all/0/1\">Roger Wattenhofer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stochastic Transformer Networks with Linear Competing Units: Application to end-to-end SL Translation. (arXiv:2109.13318v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13318","description":"<p>Automating sign language translation (SLT) is a challenging real world\napplication. Despite its societal importance, though, research progress in the\nfield remains rather poor. Crucially, existing methods that yield viable\nperformance necessitate the availability of laborious to obtain gloss sequence\ngroundtruth. In this paper, we attenuate this need, by introducing an\nend-to-end SLT model that does not entail explicit use of glosses; the model\nonly needs text groundtruth. This is in stark contrast to existing end-to-end\nmodels that use gloss sequence groundtruth, either in the form of a modality\nthat is recognized at an intermediate model stage, or in the form of a parallel\noutput process, jointly trained with the SLT model. Our approach constitutes a\nTransformer network with a novel type of layers that combines: (i) local\nwinner-takes-all (LWTA) layers with stochastic winner sampling, instead of\nconventional ReLU layers, (ii) stochastic weights with posterior distributions\nestimated via variational inference, and (iii) a weight compression technique\nat inference time that exploits estimated posterior variance to perform\nmassive, almost lossless compression. We demonstrate that our approach can\nreach the currently best reported BLEU-4 score on the PHOENIX 2014T benchmark,\nbut without making use of glosses for model training, and with a memory\nfootprint reduced by more than 70%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Voskou_A/0/1/0/all/0/1\">Andreas Voskou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Panousis_K/0/1/0/all/0/1\">Konstantinos P. Panousis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kosmopoulos_D/0/1/0/all/0/1\">Dimitrios Kosmopoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Metaxas_D/0/1/0/all/0/1\">Dimitris N. Metaxas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chatzis_S/0/1/0/all/0/1\">Sotirios Chatzis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Biomedical BERT Models for Vocabulary Alignment at Scale in the UMLS Metathesaurus. (arXiv:2109.13348v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13348","description":"<p>The current UMLS (Unified Medical Language System) Metathesaurus construction\nprocess for integrating over 200 biomedical source vocabularies is expensive\nand error-prone as it relies on the lexical algorithms and human editors for\ndeciding if the two biomedical terms are synonymous. Recent advances in Natural\nLanguage Processing such as Transformer models like BERT and its biomedical\nvariants with contextualized word embeddings have achieved state-of-the-art\n(SOTA) performance on downstream tasks. We aim to validate if these approaches\nusing the BERT models can actually outperform the existing approaches for\npredicting synonymy in the UMLS Metathesaurus. In the existing Siamese Networks\nwith LSTM and BioWordVec embeddings, we replace the BioWordVec embeddings with\nthe biomedical BERT embeddings extracted from each BERT model using different\nways of extraction. In the Transformer architecture, we evaluate the use of the\ndifferent biomedical BERT models that have been pre-trained using different\ndatasets and tasks. Given the SOTA performance of these BERT models for other\ndownstream tasks, our experiments yield surprisingly interesting results: (1)\nin both model architectures, the approaches employing these biomedical\nBERT-based models do not outperform the existing approaches using Siamese\nNetwork with BioWordVec embeddings for the UMLS synonymy prediction task, (2)\nthe original BioBERT large model that has not been pre-trained with the UMLS\noutperforms the SapBERT models that have been pre-trained with the UMLS, and\n(3) using the Siamese Networks yields better performance for synonymy\nprediction when compared to using the biomedical BERT models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bajaj_G/0/1/0/all/0/1\">Goonmeet Bajaj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vinh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wijesiriwardene_T/0/1/0/all/0/1\">Thilini Wijesiriwardene</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yip_H/0/1/0/all/0/1\">Hong Yung Yip</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Javangula_V/0/1/0/all/0/1\">Vishesh Javangula</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parthasarathy_S/0/1/0/all/0/1\">Srinivasan Parthasarathy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1\">Amit Sheth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bodenreider_O/0/1/0/all/0/1\">Olivier Bodenreider</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SYGMA: System for Generalizable Modular Question Answering OverKnowledge Bases. (arXiv:2109.13430v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13430","description":"<p>Knowledge Base Question Answering (KBQA) tasks that in-volve complex\nreasoning are emerging as an important re-search direction. However, most KBQA\nsystems struggle withgeneralizability, particularly on two dimensions: (a)\nacrossmultiple reasoning types where both datasets and systems haveprimarily\nfocused on multi-hop reasoning, and (b) across mul-tiple knowledge bases, where\nKBQA approaches are specif-ically tuned to a single knowledge base. In this\npaper, wepresent SYGMA, a modular approach facilitating general-izability\nacross multiple knowledge bases and multiple rea-soning types. Specifically,\nSYGMA contains three high levelmodules: 1) KB-agnostic question understanding\nmodule thatis common across KBs 2) Rules to support additional reason-ing types\nand 3) KB-specific question mapping and answeringmodule to address the\nKB-specific aspects of the answer ex-traction. We demonstrate effectiveness of\nour system by evalu-ating on datasets belonging to two distinct knowledge\nbases,DBpedia and Wikidata. In addition, to demonstrate extensi-bility to\nadditional reasoning types we evaluate on multi-hopreasoning datasets and a new\nTemporal KBQA benchmarkdataset on Wikidata, namedTempQA-WD1, introduced in\nthispaper. We show that our generalizable approach has bettercompetetive\nperformance on multiple datasets on DBpediaand Wikidata that requires both\nmulti-hop and temporal rea-soning\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Neelam_S/0/1/0/all/0/1\">Sumit Neelam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_U/0/1/0/all/0/1\">Udit Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Karanam_H/0/1/0/all/0/1\">Hima Karanam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ikbal_S/0/1/0/all/0/1\">Shajith Ikbal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kapanipathi_P/0/1/0/all/0/1\">Pavan Kapanipathi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelaziz_I/0/1/0/all/0/1\">Ibrahim Abdelaziz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1\">Young-Suk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Santosh Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pendus_C/0/1/0/all/0/1\">Cezar Pendus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dana_S/0/1/0/all/0/1\">Saswati Dana</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1\">Dinesh Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fokoue_A/0/1/0/all/0/1\">Achille Fokoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhargav_G/0/1/0/all/0/1\">G P Shrivatsa Bhargav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khandelwal_D/0/1/0/all/0/1\">Dinesh Khandelwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_S/0/1/0/all/0/1\">Srinivas Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gurajada_S/0/1/0/all/0/1\">Sairam Gurajada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1\">Maria Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uceda_Sosa_R/0/1/0/all/0/1\">Rosario Uceda-Sosa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roukos_S/0/1/0/all/0/1\">Salim Roukos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gray_A/0/1/0/all/0/1\">Alexander Gray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riegel_G/0/1/0/all/0/1\">Guilherme LimaRyan Riegel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luus_F/0/1/0/all/0/1\">Francois Luus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramaniam_L/0/1/0/all/0/1\">L Venkata Subramaniam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"When in Doubt: Improving Classification Performance with Alternating Normalization. (arXiv:2109.13449v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13449","description":"<p>We introduce Classification with Alternating Normalization (CAN), a\nnon-parametric post-processing step for classification. CAN improves\nclassification accuracy for challenging examples by re-adjusting their\npredicted class probability distribution using the predicted class\ndistributions of high-confidence validation examples. CAN is easily applicable\nto any probabilistic classifier, with minimal computation overhead. We analyze\nthe properties of CAN using simulated experiments, and empirically demonstrate\nits effectiveness across a diverse set of classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1\">Menglin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reiter_A/0/1/0/all/0/1\">Austin Reiter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1\">Ser-Nam Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cardie_C/0/1/0/all/0/1\">Claire Cardie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Teacher-Student Learning Approach for Multi-lingual Speech-to-Intent Classification. (arXiv:2109.13486v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13486","description":"<p>End-to-end speech-to-intent classification has shown its advantage in\nharvesting information from both text and speech. In this paper, we study a\ntechnique to develop such an end-to-end system that supports multiple\nlanguages. To overcome the scarcity of multi-lingual speech corpus, we exploit\nknowledge from a pre-trained multi-lingual natural language processing model.\nMulti-lingual bidirectional encoder representations from transformers (mBERT)\nmodels are trained on multiple languages and hence expected to perform well in\nthe multi-lingual scenario. In this work, we employ a teacher-student learning\napproach to sufficiently extract information from an mBERT model to train a\nmulti-lingual speech model. In particular, we use synthesized speech generated\nfrom an English-Mandarin text corpus for analysis and training of a\nmulti-lingual intent classification model. We also demonstrate that the\nteacher-student learning approach obtains an improved performance (91.02%) over\nthe traditional end-to-end (89.40%) intent classification approach in a\npractical multi-lingual scenario.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sharma_B/0/1/0/all/0/1\">Bidisha Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhavi_M/0/1/0/all/0/1\">Maulik Madhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xuehao Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haizhou Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"How Robust r u?\": Evaluating Task-Oriented Dialogue Systems on Spoken Conversations. (arXiv:2109.13489v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13489","description":"<p>Most prior work in dialogue modeling has been on written conversations mostly\nbecause of existing data sets. However, written dialogues are not sufficient to\nfully capture the nature of spoken conversations as well as the potential\nspeech recognition errors in practical spoken dialogue systems. This work\npresents a new benchmark on spoken task-oriented conversations, which is\nintended to study multi-domain dialogue state tracking and knowledge-grounded\ndialogue modeling. We report that the existing state-of-the-art models trained\non written conversations are not performing well on our spoken data, as\nexpected. Furthermore, we observe improvements in task performances when\nleveraging n-best speech recognition hypotheses such as by combining\npredictions based on individual hypotheses. Our data set enables speech-based\nbenchmarking of task-oriented dialogue systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1\">Di Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Papangelis_A/0/1/0/all/0/1\">Alexandros Papangelis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Instance-Based Neural Dependency Parsing. (arXiv:2109.13497v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13497","description":"<p>Interpretable rationales for model predictions are crucial in practical\napplications. We develop neural models that possess an interpretable inference\nprocess for dependency parsing. Our models adopt instance-based inference,\nwhere dependency edges are extracted and labeled by comparing them to edges in\na training set. The training edges are explicitly used for the predictions;\nthus, it is easy to grasp the contribution of each edge to the predictions. Our\nexperiments show that our instance-based models achieve competitive accuracy\nwith standard neural models and have the reasonable plausibility of\ninstance-based explanations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouchi_H/0/1/0/all/0/1\">Hiroki Ouchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_J/0/1/0/all/0/1\">Jun Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kobayashi_S/0/1/0/all/0/1\">Sosuke Kobayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yokoi_S/0/1/0/all/0/1\">Sho Yokoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kuribayashi_T/0/1/0/all/0/1\">Tatsuki Kuribayashi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshikawa_M/0/1/0/all/0/1\">Masashi Yoshikawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"VoxCeleb Enrichment for Age and Gender Recognition. (arXiv:2109.13510v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13510","description":"<p>VoxCeleb datasets are widely used in speaker recognition studies. Our work\nserves two purposes. First, we provide speaker age labels and (an alternative)\nannotation of speaker gender. Second, we demonstrate the use of this metadata\nby constructing age and gender recognition models with different features and\nclassifiers. We query different celebrity databases and apply consensus rules\nto derive age and gender labels. We also compare the original VoxCeleb gender\nlabels with our labels to identify records that might be mislabeled in the\noriginal VoxCeleb data. On modeling side, we design a comprehensive study of\nmultiple features and models for recognizing gender and age. Our best system,\nusing i-vector features, achieved an F1-score of 0.9829 for gender recognition\ntask using logistic regression, and the lowest mean absolute error (MAE) in age\nregression, 9.443 years, is obtained with ridge regression. This indicates\nchallenge in age estimation from in-the-wild style speech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hechmi_K/0/1/0/all/0/1\">Khaled Hechmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trong_T/0/1/0/all/0/1\">Trung Ngo Trong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hautamaki_V/0/1/0/all/0/1\">Ville Hautamaki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kinnunen_T/0/1/0/all/0/1\">Tomi Kinnunen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Template-free Prompt Tuning for Few-shot NER. (arXiv:2109.13532v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13532","description":"<p>Prompt-based methods have been successfully applied in sentence-level\nfew-shot learning tasks, mostly owing to the sophisticated design of templates\nand label words. However, when applied to token-level labeling tasks such as\nNER, it would be time-consuming to enumerate the template queries over all\npotential entity spans. In this work, we propose a more elegant method to\nreformulate NER tasks as LM problems without any templates. Specifically, we\ndiscard the template construction process while maintaining the word prediction\nparadigm of pre-training models to predict a class-related pivot word (or label\nword) at the entity position. Meanwhile, we also explore principled ways to\nautomatically search for appropriate label words that the pre-trained models\ncan easily adapt to. While avoiding complicated template-based process, the\nproposed LM objective also reduces the gap between different objectives used in\npre-training and fine-tuning, thus it can better benefit the few-shot\nperformance. Experimental results demonstrate the effectiveness of the proposed\nmethod over bert-tagger and template-based method under few-shot setting.\nMoreover, the decoding speed of the proposed method is up to 1930.12 times\nfaster than the template-based method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_R/0/1/0/all/0/1\">Ruotian Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1\">Xin Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1\">Yiding Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Agreeing to Disagree: Annotating Offensive Language Datasets with Annotators' Disagreement. (arXiv:2109.13563v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13563","description":"<p>Since state-of-the-art approaches to offensive language detection rely on\nsupervised learning, it is crucial to quickly adapt them to the continuously\nevolving scenario of social media. While several approaches have been proposed\nto tackle the problem from an algorithmic perspective, so to reduce the need\nfor annotated data, less attention has been paid to the quality of these data.\nFollowing a trend that has emerged recently, we focus on the level of agreement\namong annotators while selecting data to create offensive language datasets, a\ntask involving a high level of subjectivity. Our study comprises the creation\nof three novel datasets of English tweets covering different topics and having\nfive crowd-sourced judgments each. We also present an extensive set of\nexperiments showing that selecting training and test data according to\ndifferent levels of annotators' agreement has a strong effect on classifiers\nperformance and robustness. Our findings are further validated in cross-domain\nexperiments and studied using a popular benchmark dataset. We show that such\nhard cases, where low agreement is present, are not necessarily due to\npoor-quality annotation and we advocate for a higher presence of ambiguous\ncases in future datasets, particularly in test sets, to better account for the\ndifferent points of view expressed online.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Leonardelli_E/0/1/0/all/0/1\">Elisa Leonardelli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menini_S/0/1/0/all/0/1\">Stefano Menini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aprosio_A/0/1/0/all/0/1\">Alessio Palmero Aprosio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tonelli_S/0/1/0/all/0/1\">Sara Tonelli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Generating texts under constraint through discriminator-guided MCTS. (arXiv:2109.13582v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13582","description":"<p>Large pre-trained language models (LM) based on Transformers allow to\ngenerate very plausible long texts. In this paper, we explore how this\ngeneration can be further controlled to satisfy certain constraints (eg. being\nnon-toxic, positive or negative, convey certain emotions, etc.) without\nfine-tuning the LM. Precisely, we formalize constrained generation as a tree\nexploration process guided by a discriminator according to how well the\nassociated sequence respects the constraint. Using a discriminator to guide\nthis generation, rather than fine-tuning the LM, in addition to be easier and\ncheaper to train, allows to apply the constraint more finely and dynamically.\nWe propose several original methods to search this generation tree, notably the\nMonte Carlo Tree Search (MCTS) which provides theoretical guarantees on the\nsearch efficiency, but also simpler methods based on re-ranking a pool of\ndiverse sequences using the discriminator scores. We evaluate these methods on\ntwo types of constraints and languages: review polarity and emotion control in\nFrench and English. We show that MCTS achieves state-of-the-art results in\nconstrained generation, without having to tune the language model, in both\ntasks and languages. We also demonstrate that our other proposed methods based\non re-ranking can be really effective when diversity among the generated\npropositions is encouraged.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chaffin_A/0/1/0/all/0/1\">Antoine Chaffin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Claveau_V/0/1/0/all/0/1\">Vincent Claveau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kijak_E/0/1/0/all/0/1\">Ewa Kijak</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Active Learning for Argument Mining: A Practical Approach. (arXiv:2109.13611v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13611","description":"<p>Despite considerable recent progress, the creation of well-balanced and\ndiverse resources remains a time-consuming and costly challenge in Argument\nMining. Active Learning reduces the amount of data necessary for the training\nof machine learning models by querying the most informative samples for\nannotation and therefore is a promising method for resource creation. In a\nlarge scale comparison of several Active Learning methods, we show that Active\nLearning considerably decreases the effort necessary to get good deep learning\nperformance on the task of Argument Unit Recognition and Classification (AURC).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Solmsdorf_N/0/1/0/all/0/1\">Nikolai Solmsdorf</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trautmann_D/0/1/0/all/0/1\">Dietrich Trautmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Intermediate Fine-tuning improves Dialogue State Tracking. (arXiv:2109.13620v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13620","description":"<p>Recent progress in task-oriented neural dialogue systems is largely focused\non a handful of languages, as annotation of training data is tedious and\nexpensive. Machine translation has been used to make systems multilingual, but\nthis can introduce a pipeline of errors. Another promising solution is using\ncross-lingual transfer learning through pretrained multilingual models.\nExisting methods train multilingual models with additional code-mixed task data\nor refine the cross-lingual representations through parallel ontologies. In\nthis work, we enhance the transfer learning process by intermediate fine-tuning\nof pretrained multilingual models, where the multilingual models are fine-tuned\nwith different but related data and/or tasks. Specifically, we use parallel and\nconversational movie subtitles datasets to design cross-lingual intermediate\ntasks suitable for downstream dialogue tasks. We use only 200K lines of\nparallel data for intermediate fine-tuning which is already available for 1782\nlanguage pairs. We test our approach on the cross-lingual dialogue state\ntracking task for the parallel MultiWoZ (English -&gt; Chinese, Chinese -&gt;\nEnglish) and Multilingual WoZ (English -&gt; German, English -&gt; Italian) datasets.\nWe achieve impressive improvements (&gt; 20% on joint goal accuracy) on the\nparallel MultiWoZ dataset and the Multilingual WoZ dataset over the vanilla\nbaseline with only 10% of the target language task data and zero-shot setup\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moghe_N/0/1/0/all/0/1\">Nikita Moghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steedman_M/0/1/0/all/0/1\">Mark Steedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Birch_A/0/1/0/all/0/1\">Alexandra Birch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DeepPSL: End-to-end perception and reasoning with applications to zero shot learning. (arXiv:2109.13662v1 [eess.SY])","link":"http://arxiv.org/abs/2109.13662","description":"<p>We introduce DeepPSL a variant of Probabilistic Soft Logic (PSL) to produce\nan end-to-end trainable system that integrates reasoning and perception. PSL\nrepresents first-order logic in terms of a convex graphical model -- Hinge Loss\nMarkov random fields (HL-MRFs). PSL stands out among probabilistic logic\nframeworks due to its tractability having been applied to systems of more than\n1 billion ground rules. The key to our approach is to represent predicates in\nfirst-order logic using deep neural networks and then to approximately\nback-propagate through the HL-MRF and thus train every aspect of the\nfirst-order system being represented. We believe that this approach represents\nan interesting direction for the integration of deep learning and reasoning\ntechniques with applications to knowledge base learning, multi-task learning,\nand explainability. We evaluate DeepPSL on a zero shot learning problem in\nimage classification. State of the art results demonstrate the utility and\nflexibility of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Duffy_N/0/1/0/all/0/1\">Nigel Duffy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Puranam_S/0/1/0/all/0/1\">Sai Akhil Puranam</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Dasaratha_S/0/1/0/all/0/1\">Sridhar Dasaratha</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Phogat_K/0/1/0/all/0/1\">Karmvir Singh Phogat</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Tiyyagura_S/0/1/0/all/0/1\">Sunil Reddy Tiyyagura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Counter Narrative Type Classification. (arXiv:2109.13664v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13664","description":"<p>The growing interest in employing counter narratives for hatred intervention\nbrings with it a focus on dataset creation and automation strategies. In this\nscenario, learning to recognize counter narrative types from natural text is\nexpected to be useful for applications such as hate speech countering, where\noperators from non-governmental organizations are supposed to answer to hate\nwith several and diverse arguments that can be mined from online sources. This\npaper presents the first multilingual work on counter narrative type\nclassification, evaluating SoTA pre-trained language models in monolingual,\nmultilingual and cross-lingual settings. When considering a fine-grained\nannotation of counter narrative classes, we report strong baseline\nclassification results for the majority of the counter narrative types,\nespecially if we translate every language to English before cross-lingual\nprediction. This suggests that knowledge about counter narratives can be\nsuccessfully transferred across languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chung_Y/0/1/0/all/0/1\">Yi-Ling Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guerini_M/0/1/0/all/0/1\">Marco Guerini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agerri_R/0/1/0/all/0/1\">Rodrigo Agerri</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Nana-HDR: A Non-attentive Non-autoregressive Hybrid Model for TTS. (arXiv:2109.13673v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13673","description":"<p>This paper presents Nana-HDR, a new non-attentive non-autoregressive model\nwith hybrid Transformer-based Dense-fuse encoder and RNN-based decoder for TTS.\nIt mainly consists of three parts: Firstly, a novel Dense-fuse encoder with\ndense connections between basic Transformer blocks for coarse feature fusion\nand a multi-head attention layer for fine feature fusion. Secondly, a\nsingle-layer non-autoregressive RNN-based decoder. Thirdly, a duration\npredictor instead of an attention model that connects the above hybrid encoder\nand decoder. Experiments indicate that Nana-HDR gives full play to the\nadvantages of each component, such as strong text encoding ability of\nTransformer-based encoder, stateful decoding without being bothered by exposure\nbias and local information preference, and stable alignment provided by\nduration predictor. Due to these advantages, Nana-HDR achieves competitive\nperformance in naturalness and robustness on two Mandarin corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1\">Shilun Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1\">Wenchao Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1\">Li Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1\">Fenglong Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xinhui Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1\">Li Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CIDEr-R: Robust Consensus-based Image Description Evaluation. (arXiv:2109.13701v1 [cs.CV])","link":"http://arxiv.org/abs/2109.13701","description":"<p>This paper shows that CIDEr-D, a traditional evaluation metric for image\ndescription, does not work properly on datasets where the number of words in\nthe sentence is significantly greater than those in the MS COCO Captions\ndataset. We also show that CIDEr-D has performance hampered by the lack of\nmultiple reference sentences and high variance of sentence length. To bypass\nthis problem, we introduce CIDEr-R, which improves CIDEr-D, making it more\nflexible in dealing with datasets with high sentence length variance. We\ndemonstrate that CIDEr-R is more accurate and closer to human judgment than\nCIDEr-D; CIDEr-R is more robust regarding the number of available references.\nOur results reveal that using Self-Critical Sequence Training to optimize\nCIDEr-R generates descriptive captions. In contrast, when CIDEr-D is optimized,\nthe generated captions' length tends to be similar to the reference length.\nHowever, the models also repeat several times the same word to increase the\nsentence length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Santos_G/0/1/0/all/0/1\">Gabriel Oliveira dos Santos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Colombini_E/0/1/0/all/0/1\">Esther Luna Colombini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Avila_S/0/1/0/all/0/1\">Sandra Avila</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One to rule them all: Towards Joint Indic Language Hate Speech Detection. (arXiv:2109.13711v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13711","description":"<p>This paper is a contribution to the Hate Speech and Offensive Content\nIdentification in Indo-European Languages (HASOC) 2021 shared task. Social\nmedia today is a hotbed of toxic and hateful conversations, in various\nlanguages. Recent news reports have shown that current models struggle to\nautomatically identify hate posted in minority languages. Therefore,\nefficiently curbing hate speech is a critical challenge and problem of\ninterest. We present a multilingual architecture using state-of-the-art\ntransformer language models to jointly learn hate and offensive speech\ndetection across three languages namely, English, Hindi, and Marathi. On the\nprovided testing corpora, we achieve Macro F1 scores of 0.7996, 0.7748, 0.8651\nfor sub-task 1A and 0.6268, 0.5603 during the fine-grained classification of\nsub-task 1B. These results show the efficacy of exploiting a multilingual\ntraining scheme.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhatia_M/0/1/0/all/0/1\">Mehar Bhatia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotia_T/0/1/0/all/0/1\">Tenzin Singhay Bhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1\">Akshat Agarwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_P/0/1/0/all/0/1\">Prakash Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1\">Shubham Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shridhar_K/0/1/0/all/0/1\">Kumar Shridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laumann_F/0/1/0/all/0/1\">Felix Laumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dash_A/0/1/0/all/0/1\">Ayushman Dash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Analyzing the Use of Character-Level Translation with Sparse and Noisy Datasets. (arXiv:2109.13723v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13723","description":"<p>This paper provides an analysis of character-level machine translation models\nused in pivot-based translation when applied to sparse and noisy datasets, such\nas crowdsourced movie subtitles. In our experiments, we find that such\ncharacter-level models cut the number of untranslated words by over 40% and are\nespecially competitive (improvements of 2-3 BLEU points) in the case of limited\ntraining data. We explore the impact of character alignment, phrase table\nfiltering, bitext size and the choice of pivot language on translation quality.\nWe further compare cascaded translation models to the use of synthetic training\ndata via multiple pivots, and we find that the latter works significantly\nbetter. Finally, we demonstrate that neither word-nor character-BLEU correlate\nperfectly with human judgments, due to BLEU's sensitivity to length.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1\">J&#xf6;rg Tiedemann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translating from Morphologically Complex Languages: A Paraphrase-Based Approach. (arXiv:2109.13724v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13724","description":"<p>We propose a novel approach to translating from a morphologically complex\nlanguage. Unlike previous research, which has targeted word inflections and\nconcatenations, we focus on the pairwise relationship between morphologically\nrelated words, which we treat as potential paraphrases and handle using\nparaphrasing techniques at the word, phrase, and sentence level. An important\nadvantage of this framework is that it can cope with derivational morphology,\nwhich has so far remained largely beyond the capabilities of statistical\nmachine translation systems. Our experiments translating from Malay, whose\nmorphology is mostly derivational, into English show significant improvements\nover rivaling approaches based on five automatic evaluation measures (for\n320,000 sentence pairs; 9.5 million English word tokens).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_H/0/1/0/all/0/1\">Hwee Tou Ng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentiment Analysis in Twitter for Macedonian. (arXiv:2109.13725v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13725","description":"<p>We present work on sentiment analysis in Twitter for Macedonian. As this is\npioneering work for this combination of language and genre, we created suitable\nresources for training and evaluating a system for sentiment analysis of\nMacedonian tweets. In particular, we developed a corpus of tweets annotated\nwith tweet-level sentiment polarity (positive, negative, and neutral), as well\nas with phrase-level sentiment, which we made freely available for research\npurposes. We further bootstrapped several large-scale sentiment lexicons for\nMacedonian, motivated by previous work for English. The impact of several\ndifferent pre-processing steps as well as of various features is shown in\nexperiments that represent the first attempt to build a system for sentiment\nanalysis in Twitter for the morphologically rich Macedonian language. Overall,\nour experimental results show an F1-score of 92.16, which is very strong and is\non par with the best results for English, which were achieved in recent SemEval\ncompetitions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jovanoski_D/0/1/0/all/0/1\">Dame Jovanoski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pachovski_V/0/1/0/all/0/1\">Veno Pachovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exposing Paid Opinion Manipulation Trolls. (arXiv:2109.13726v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13726","description":"<p>Recently, Web forums have been invaded by opinion manipulation trolls. Some\ntrolls try to influence the other users driven by their own convictions, while\nin other cases they can be organized and paid, e.g., by a political party or a\nPR agency that gives them specific instructions what to write. Finding paid\ntrolls automatically using machine learning is a hard task, as there is no\nenough training data to train a classifier; yet some test data is possible to\nobtain, as these trolls are sometimes caught and widely exposed. In this paper,\nwe solve the training data problem by assuming that a user who is called a\ntroll by several different people is likely to be such, and one who has never\nbeen called a troll is unlikely to be such. We compare the profiles of (i) paid\ntrolls vs. (ii)\"mentioned\" trolls vs. (iii) non-trolls, and we further show\nthat a classifier trained to distinguish (ii) from (iii) does quite well also\nat telling apart (i) from (iii).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mihaylov_T/0/1/0/all/0/1\">Todor Mihaylov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1\">Ivan Koychev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Georgiev_G/0/1/0/all/0/1\">Georgi Georgiev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Task Triplet Loss for Named Entity Recognition using Supplementary Text. (arXiv:2109.13736v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13736","description":"<p>Retail item data contains many different forms of text like the title of an\nitem, the description of an item, item name and reviews. It is of interest to\nidentify the item name in the other forms of text using a named entity tagger.\nHowever, the title of an item and its description are syntactically different\n(but semantically similar) in that the title is not necessarily a well formed\nsentence while the description is made up of well formed sentences. In this\nwork, we use a triplet loss to contrast the embeddings of the item title with\nthe description to establish a proof of concept. We find that using the triplet\nloss in a multi-task NER algorithm improves both the precision and recall by a\nsmall percentage. While the improvement is small, we think it is a step in the\nright direction of using various forms of text in a multi-task algorithm. In\naddition to precision and recall, the multi task triplet loss method is also\nfound to significantly improve the exact match accuracy i.e. the accuracy of\ntagging the entire set of tokens in the text with correct tags.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Siskind_R/0/1/0/all/0/1\">Ryan Siskind</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1\">Shalin Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Homophony and R\\'enyi Entropy. (arXiv:2109.13766v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13766","description":"<p>Homophony's widespread presence in natural languages is a controversial\ntopic. Recent theories of language optimality have tried to justify its\nprevalence, despite its negative effects on cognitive processing time; e.g.,\nPiantadosi et al. (2012) argued homophony enables the reuse of efficient\nwordforms and is thus beneficial for languages. This hypothesis has recently\nbeen challenged by Trott and Bergen (2020), who posit that good wordforms are\nmore often homophonous simply because they are more phonotactically probable.\nIn this paper, we join in on the debate. We first propose a new\ninformation-theoretic quantification of a language's homophony: the sample\nR\\'enyi entropy. Then, we use this quantification to revisit Trott and Bergen's\nclaims. While their point is theoretically sound, a specific methodological\nissue in their experiments raises doubts about their results. After addressing\nthis issue, we find no clear pressure either towards or against homophony -- a\nmuch more nuanced result than either Piantadosi et al.'s or Trott and Bergen's\nfindings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1\">Tiago Pimentel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1\">Clara Meister</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Teufel_S/0/1/0/all/0/1\">Simone Teufel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying and Mitigating Gender Bias in Hyperbolic Word Embeddings. (arXiv:2109.13767v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13767","description":"<p>Euclidean word embedding models such as GloVe and Word2Vec have been shown to\nreflect human-like gender biases. In this paper, we extend the study of gender\nbias to the recently popularized hyperbolic word embeddings. We propose\ngyrocosine bias, a novel measure for quantifying gender bias in hyperbolic word\nrepresentations and observe a significant presence of gender bias. To address\nthis problem, we propose Poincar\\'e Gender Debias (PGD), a novel debiasing\nprocedure for hyperbolic word representations. Experiments on a suit of\nevaluation tests show that PGD effectively reduces bias while adding a minimal\nsemantic offset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bhotia_T/0/1/0/all/0/1\">Tenzin Singhay Bhotia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1\">Vaibhav Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health. (arXiv:2109.13770v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13770","description":"<p>Many statistical models have high accuracy on test benchmarks, but are not\nexplainable, struggle in low-resource scenarios, cannot be reused for multiple\ntasks, and cannot easily integrate domain expertise. These factors limit their\nuse, particularly in settings such as mental health, where it is difficult to\nannotate datasets and model outputs have significant impact. We introduce a\nmicromodel architecture to address these challenges. Our approach allows\nresearchers to build interpretable representations that embed domain knowledge\nand provide explanations throughout the model's decision process. We\ndemonstrate the idea on multiple mental health tasks: depression\nclassification, PTSD classification, and suicidal risk assessment. Our systems\nconsistently produce strong results, even in low-resource scenarios, and are\nmore interpretable than alternative methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lee_A/0/1/0/all/0/1\">Andrew Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kummerfeld_J/0/1/0/all/0/1\">Jonathan K. Kummerfeld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_L/0/1/0/all/0/1\">Lawrence C. An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihalcea_R/0/1/0/all/0/1\">Rada Mihalcea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Chekhov's Gun Recognition. (arXiv:2109.13855v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13855","description":"<p>Chekhov's gun is a dramatic principle stating that every element in a story\nmust be necessary, and irrelevant elements should be removed. This paper\npresents a new natural language processing task - Chekhov's gun recognition or\n(CGR) - recognition of entities that are pivotal for the development of the\nplot. Though similar to classical Named Entity Recognition (NER) it has\nprofound differences and is crucial for the tasks of narrative processing,\nsince Chekhov's guns have a profound impact on the causal relationship in a\nstory. The paper presents a new benchmark dataset for the CGR task that\nincludes 5550 descriptions with one or more Chekhov's Gun in each and validates\nthe task on two more datasets available in the natural language processing\n(NLP) literature.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamshchikov_I/0/1/0/all/0/1\">Ivan P. Yamshchikov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expectation-based Minimalist Grammars. (arXiv:2109.13871v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13871","description":"<p>Expectation-based Minimalist Grammars (e-MGs) are simplified versions of the\n(Conflated) Minimalist Grammars, (C)MGs, formalized by Stabler (Stabler, 2011,\n2013, 1997) and Phase-based Minimalist Grammars, PMGs (Chesi, 2005, 2007;\nStabler, 2011). The crucial simplification consists of driving structure\nbuilding only by relying on lexically encoded categorial top-down expectations.\nThe commitment on a top-down derivation (as in e-MGs and PMGs, as opposed to\n(C)MGs, Chomsky, 1995; Stabler, 2011) allows us to define a core derivation\nthat should be the same in both parsing and generation (Momma &amp; Phillips,\n2018).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chesi_C/0/1/0/all/0/1\">Cristiano Chesi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Single-dataset Experts for Multi-dataset Question Answering. (arXiv:2109.13880v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13880","description":"<p>Many datasets have been created for training reading comprehension models,\nand a natural question is whether we can combine them to build models that (1)\nperform better on all of the training datasets and (2) generalize and transfer\nbetter to new datasets. Prior work has addressed this goal by training one\nnetwork simultaneously on multiple datasets, which works well on average but is\nprone to over- or under-fitting different sub-distributions and might transfer\nworse compared to source models with more overlap with the target dataset. Our\napproach is to model multi-dataset question answering with a collection of\nsingle-dataset experts, by training a collection of lightweight,\ndataset-specific adapter modules (Houlsby et al., 2019) that share an\nunderlying Transformer model. We find that these Multi-Adapter Dataset Experts\n(MADE) outperform all our baselines in terms of in-distribution accuracy, and\nsimple methods based on parameter-averaging lead to better zero-shot\ngeneralization and few-shot transfer performance, offering a strong and\nversatile starting point for building new reading comprehension systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Friedman_D/0/1/0/all/0/1\">Dan Friedman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dodge_B/0/1/0/all/0/1\">Ben Dodge</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Different Text-preprocessing Techniques Using The BERT Model Affect The Gender Profiling of Authors. (arXiv:2109.13890v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13890","description":"<p>Forensic author profiling plays an important role in indicating possible\nprofiles for suspects. Among the many automated solutions recently proposed for\nauthor profiling, transfer learning outperforms many other state-of-the-art\ntechniques in natural language processing. Nevertheless, the sophisticated\ntechnique has yet to be fully exploited for author profiling. At the same time,\nwhereas current methods of author profiling, all largely based on features\nengineering, have spawned significant variation in each model used, transfer\nlearning usually requires a preprocessed text to be fed into the model. We\nreviewed multiple references in the literature and determined the most common\npreprocessing techniques associated with authors' genders profiling.\nConsidering the variations in potential preprocessing techniques, we conducted\nan experimental study that involved applying five such techniques to measure\neach technique's effect while using the BERT model, chosen for being one of the\nmost-used stock pretrained models. We used the Hugging face transformer library\nto implement the code for each preprocessing case. In our five experiments, we\nfound that BERT achieves the best accuracy in predicting the gender of the\nauthor when no preprocessing technique is applied. Our best case achieved\n86.67% accuracy in predicting the gender of authors.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alzahrani_E/0/1/0/all/0/1\">Esam Alzahrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jololian_L/0/1/0/all/0/1\">Leon Jololian</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Temporal Information and Event Markup Language: TIE-ML Markup Process and Schema Version 1.0. (arXiv:2109.13892v1 [cs.CL])","link":"http://arxiv.org/abs/2109.13892","description":"<p>Temporal Information and Event Markup Language (TIE-ML) is a markup strategy\nand annotation schema to improve the productivity and accuracy of temporal and\nevent related annotation of corpora to facilitate machine learning based model\ntraining. For the annotation of events, temporal sequencing, and durations, it\nis significantly simpler by providing an extremely reduced tag set for just\ntemporal relations and event enumeration. In comparison to other standards, as\nfor example the Time Markup Language (TimeML), it is much easier to use by\ndropping sophisticated formalisms, theoretical concepts, and annotation\napproaches. Annotations of corpora using TimeML can be mapped to TIE-ML with a\nloss, and TIE-ML annotations can be fully mapped to TimeML with certain\nunder-specification.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cavar_D/0/1/0/all/0/1\">Damir Cavar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dickson_B/0/1/0/all/0/1\">Billy Dickson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aljubailan_A/0/1/0/all/0/1\">Ali Aljubailan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Soyoung Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsolved Problems in ML Safety. (arXiv:2109.13916v1 [cs.LG])","link":"http://arxiv.org/abs/2109.13916","description":"<p>Machine learning (ML) systems are rapidly increasing in size, are acquiring\nnew capabilities, and are increasingly deployed in high-stakes settings. As\nwith other powerful technologies, safety for ML should be a leading research\npriority. In response to emerging safety challenges in ML, such as those\nintroduced by recent large-scale models, we provide a new roadmap for ML Safety\nand refine the technical problems that the field needs to address. We present\nfour problems ready for research, namely withstanding hazards (\"Robustness\"),\nidentifying hazards (\"Monitoring\"), steering ML systems (\"Alignment\"), and\nreducing risks to how ML systems are handled (\"External Safety\"). Throughout,\nwe clarify each problem's motivation and provide concrete research directions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1\">Dan Hendrycks</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1\">Nicholas Carlini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schulman_J/0/1/0/all/0/1\">John Schulman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1\">Jacob Steinhardt</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Word Equations: Inherently Interpretable Sparse Word Embeddingsthrough Sparse Coding. (arXiv:2004.13847v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13847","description":"<p>Word embeddings are a powerful natural language processing technique, but\nthey are extremely difficult to interpret. To enable interpretable NLP models,\nwe create vectors where each dimension is inherently interpretable. By\ninherently interpretable, we mean a system where each dimension is associated\nwith some human understandable hint that can describe the meaning of that\ndimension. In order to create more interpretable word embeddings, we transform\npretrained dense word embeddings into sparse embeddings. These new embeddings\nare inherently interpretable: each of their dimensions is created from and\nrepresents a natural language word or specific grammatical concept. We\nconstruct these embeddings through sparse coding, where each vector in the\nbasis set is itself a word embedding. Therefore, each dimension of our sparse\nvectors corresponds to a natural language word. We also show that models\ntrained using these sparse embeddings can achieve good performance and are more\ninterpretable in practice, including through human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Templeton_A/0/1/0/all/0/1\">Adly Templeton</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Open Knowledge Graphs Canonicalization using Variational Autoencoders. (arXiv:2012.04780v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04780","description":"<p>Noun phrases and Relation phrases in open knowledge graphs are not\ncanonicalized, leading to an explosion of redundant and ambiguous\nsubject-relation-object triples. Existing approaches to solve this problem take\na two-step approach. First, they generate embedding representations for both\nnoun and relation phrases, then a clustering algorithm is used to group them\nusing the embeddings as features. In this work, we propose Canonicalizing Using\nVariational Autoencoders (CUVA), a joint model to learn both embeddings and\ncluster assignments in an end-to-end approach, which leads to a better vector\nrepresentation for the noun and relation phrases. Our evaluation over multiple\nbenchmarks shows that CUVA outperforms the existing state-of-the-art\napproaches. Moreover, we introduce CanonicNell, a novel dataset to evaluate\nentity canonicalization systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1\">Sarthak Dash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mihindukulasooriya_N/0/1/0/all/0/1\">Nandana Mihindukulasooriya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1\">Sugato Bagchi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models. (arXiv:2102.07988v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.07988","description":"<p>Model parallelism has become a necessity for training modern large-scale deep\nlanguage models. In this work, we identify a new and orthogonal dimension from\nexisting model parallel approaches: it is possible to perform pipeline\nparallelism within a single training sequence for Transformer-based language\nmodels thanks to its autoregressive property. This enables a more fine-grained\npipeline compared with previous work. With this key idea, we design TeraPipe, a\nhigh-performance token-level pipeline parallel algorithm for synchronous\nmodel-parallel training of Transformer-based language models. We develop a\nnovel dynamic programming-based algorithm to calculate the optimal pipelining\nexecution scheme given a specific model and cluster configuration. We show that\nTeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175\nbillion parameters on an AWS cluster with 48 p3.16xlarge instances compared\nwith state-of-the-art model-parallel methods. The code for reproduction can be\nfound at https://github.com/zhuohan123/terapipe\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuohan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1\">Siyuan Zhuang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1\">Shiyuan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhuo_D/0/1/0/all/0/1\">Danyang Zhuo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1\">Dawn Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1\">Ion Stoica</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving and Simplifying Pattern Exploiting Training. (arXiv:2103.11955v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11955","description":"<p>Recently, pre-trained language models (LMs) have achieved strong performance\nwhen fine-tuned on difficult benchmarks like SuperGLUE. However, performance\ncan suffer when there are very few labeled examples available for fine-tuning.\nPattern Exploiting Training (PET) is a recent approach that leverages patterns\nfor few-shot learning. However, PET uses task-specific unlabeled data. In this\npaper, we focus on few-shot learning without any unlabeled data and introduce\nADAPET, which modifies PET's objective to provide denser supervision during\nfine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any\ntask-specific unlabeled data. Our code can be found at\nhttps://github.com/rrmenon10/ADAPET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1\">Derek Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1\">Rakesh R Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Intent Recognition and Unsupervised Slot Identification for Low Resourced Spoken Dialog Systems. (arXiv:2104.01287v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01287","description":"<p>Intent Recognition and Slot Identification are crucial components in spoken\nlanguage understanding (SLU) systems. In this paper, we present a novel\napproach towards both these tasks in the context of low resourced and unwritten\nlanguages. We present an acoustic based SLU system that converts speech to its\nphonetic transcription using a universal phone recognition system. We build a\nword-free natural language understanding module that does intent recognition\nand slot identification from these phonetic transcription. Our proposed SLU\nsystem performs competitively for resource rich scenarios and significantly\noutperforms existing approaches as the amount of available data reduces. We\nobserve more than 10% improvement for intent classification in Tamil and more\nthan 5% improvement for intent classification in Sinhala. We also present a\nnovel approach towards unsupervised slot identification using normalized\nattention scores. This approach can be used for unsupervised slot labelling,\ndata augmentation and to generate data for a new slot in a one-shot way with\nonly one speech recording\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1\">Akshat Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_O/0/1/0/all/0/1\">Olivia Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kushwaha_A/0/1/0/all/0/1\">Akruti Kushwaha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1\">Saloni Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1\">William Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rallabandi_S/0/1/0/all/0/1\">Sai Krishna Rallabandi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Black_A/0/1/0/all/0/1\">Alan W Black</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hidden Backdoors in Human-Centric Language Models. (arXiv:2105.00164v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00164","description":"<p>Natural language processing (NLP) systems have been proven to be vulnerable\nto backdoor attacks, whereby hidden features (backdoors) are trained into a\nlanguage model and may only be activated by specific inputs (called triggers),\nto trick the model into producing unexpected behaviors. In this paper, we\ncreate covert and natural triggers for textual backdoor attacks, \\textit{hidden\nbackdoors}, where triggers can fool both modern language models and human\ninspection. We deploy our hidden backdoors through two state-of-the-art trigger\nembedding methods. The first approach via homograph replacement, embeds the\ntrigger into deep neural networks through the visual spoofing of lookalike\ncharacter replacement. The second approach uses subtle differences between text\ngenerated by language models and real natural text to produce trigger sentences\nwith correct grammar and high fluency. We demonstrate that the proposed hidden\nbackdoors can be effective across three downstream security-critical NLP tasks,\nrepresentative of modern human-centric NLP systems, including toxic comment\ndetection, neural machine translation (NMT), and question answering (QA). Our\ntwo hidden backdoor attacks can achieve an Attack Success Rate (ASR) of at\nleast $97\\%$ with an injection rate of only $3\\%$ in toxic comment detection,\n$95.1\\%$ ASR in NMT with less than $0.5\\%$ injected data, and finally $91.12\\%$\nASR against QA updated with only 27 poisoning data samples on a model\npreviously trained with 92,024 samples (0.029\\%). We are able to demonstrate\nthe adversary's high success rate of attacks, while maintaining functionality\nfor regular users, with triggers inconspicuous by the human administrators.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shaofeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hui Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_T/0/1/0/all/0/1\">Tian Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1\">Benjamin Zi Hao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1\">Minhui Xue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Haojin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1\">Jialiang Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Low-resource Reading Comprehension via Cross-lingual Transposition Rethinking. (arXiv:2107.05002v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05002","description":"<p>Extractive Reading Comprehension (ERC) has made tremendous advances enabled\nby the availability of large-scale high-quality ERC training data. Despite of\nsuch rapid progress and widespread application, the datasets in languages other\nthan high-resource languages such as English remain scarce. To address this\nissue, we propose a Cross-Lingual Transposition ReThinking (XLTT) model by\nmodelling existing high-quality extractive reading comprehension datasets in a\nmultilingual environment. To be specific, we present multilingual adaptive\nattention (MAA) to combine intra-attention and inter-attention to learn more\ngeneral generalizable semantic and lexical knowledge from each pair of language\nfamilies. Furthermore, to make full use of existing datasets, we adopt a new\ntraining framework to train our model by calculating task-level similarities\nbetween each existing dataset and target dataset. The experimental results show\nthat our XLTT model surpasses six baselines on two multilingual ERC benchmarks,\nespecially more effective for low-resource languages with 3.9 and 4.1 average\nimprovement in F1 and EM, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1\">Gaochen Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1\">Bin Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yuxin Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1\">Fei Kong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1\">Bangchang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hongwen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1\">Dejie Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Arabic aspect based sentiment analysis using BERT. (arXiv:2107.13290v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.13290","description":"<p>Aspect-based sentiment analysis(ABSA) is a textual analysis methodology that\ndefines the polarity of opinions on certain aspects related to specific\ntargets. The majority of research on ABSA is in English, with a small amount of\nwork available in Arabic. Most previous Arabic research has relied on deep\nlearning models that depend primarily on context-independent word embeddings\n(e.g.word2vec), where each word has a fixed representation independent of its\ncontext. This article explores the modeling capabilities of contextual\nembeddings from pre-trained language models, such as BERT, and making use of\nsentence pair input on Arabic ABSA tasks. In particular, we are building a\nsimple but effective BERT-based neural baseline to handle this task. Our BERT\narchitecture with a simple linear classification layer surpassed the\nstate-of-the-art works, according to the experimental results on the\nbenchmarked Arabic hotel reviews dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdelgwad_M/0/1/0/all/0/1\">Mohammed M.Abdelgwad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SeqScore: Addressing Barriers to Reproducible Named Entity Recognition Evaluation. (arXiv:2107.14154v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.14154","description":"<p>To address a looming crisis of unreproducible evaluation for named entity\nrecognition, we propose guidelines and introduce SeqScore, a software package\nto improve reproducibility. The guidelines we propose are extremely simple and\ncenter around transparency regarding how chunks are encoded and scored. We\ndemonstrate that despite the apparent simplicity of NER evaluation, unreported\ndifferences in the scoring procedure can result in changes to scores that are\nboth of noticeable magnitude and statistically significant. We describe\nSeqScore, which addresses many of the issues that cause replication failures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Palen_Michel_C/0/1/0/all/0/1\">Chester Palen-Michel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Holley_N/0/1/0/all/0/1\">Nolan Holley</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lignos_C/0/1/0/all/0/1\">Constantine Lignos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HeadlineCause: A Dataset of News Headlines for Detecting Causalities. (arXiv:2108.12626v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12626","description":"<p>Detecting implicit causal relations in texts is a task that requires both\ncommon sense and world knowledge. Existing datasets are focused either on\ncommonsense causal reasoning or explicit causal relations. In this work, we\npresent HeadlineCause, a dataset for detecting implicit causal relations\nbetween pairs of news headlines. The dataset includes over 5000 headline pairs\nfrom English news and over 9000 headline pairs from Russian news labeled\nthrough crowdsourcing. The pairs vary from totally unrelated or belonging to\nthe same general topic to the ones including causation and refutation\nrelations. We also present a set of models and experiments that demonstrates\nthe dataset validity, including a multilingual XLM-RoBERTa based model for\ncausality detection and a GPT-2 based model for possible effects prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gusev_I/0/1/0/all/0/1\">Ilya Gusev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05473","description":"<p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by\nlearning with merely a handful of annotated instances. Meta-learning has been\nwidely adopted for such a task, which trains on randomly generated few-shot\ntasks to learn generic data representations. Despite impressive results\nachieved, existing models still perform suboptimally when handling hard FSRE\ntasks, where the relations are fine-grained and similar to each other. We argue\nthis is largely because existing models do not distinguish hard tasks from easy\nones in the learning process. In this paper, we introduce a novel approach\nbased on contrastive learning that learns better representations by exploiting\nrelation label information. We further design a method that allows the model to\nadaptively learn how to focus on hard tasks. Experiments on two standard\ndatasets demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiale Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Something Old, Something New: Grammar-based CCG Parsing with Transformer Models. (arXiv:2109.10044v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10044","description":"<p>This report describes the parsing problem for Combinatory Categorial Grammar\n(CCG), showing how a combination of Transformer-based neural models and a\nsymbolic CCG grammar can lead to substantial gains over existing approaches.\nThe report also documents a 20-year research program, showing how NLP methods\nhave evolved over this time. The staggering accuracy improvements provided by\nneural models for CCG parsing can be seen as a reflection of the improvements\nseen in NLP more generally. The report provides a minimal introduction to CCG\nand CCG parsing, with many pointers to the relevant literature. It then\ndescribes the CCG supertagging problem, and some recent work from Tian et al.\n(2020) which applies Transformer-based models to supertagging with great\neffect. I use this existing model to develop a CCG multitagger, which can serve\nas a front-end to an existing CCG parser. Simply using this new multitagger\nprovides substantial gains in parsing accuracy. I then show how a\nTransformer-based model from the parsing literature can be combined with the\ngrammar-based CCG parser, setting a new state-of-the-art for the CCGbank\nparsing task of almost 93% F-score for labelled dependencies, with complete\nsentence accuracies of over 50%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clark_S/0/1/0/all/0/1\">Stephen Clark</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Recursively Summarizing Books with Human Feedback. (arXiv:2109.10862v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.10862","description":"<p>A major challenge for scaling machine learning is training models to perform\ntasks that are very difficult or time-consuming for humans to evaluate. We\npresent progress on this problem on the task of abstractive summarization of\nentire fiction novels. Our method combines learning from human feedback with\nrecursive task decomposition: we use models trained on smaller parts of the\ntask to assist humans in giving feedback on the broader task. We collect a\nlarge volume of demonstrations and comparisons from human labelers, and\nfine-tune GPT-3 using behavioral cloning and reward modeling to do\nsummarization recursively. At inference time, the model first summarizes small\nsections of the book and then recursively summarizes these summaries to produce\na summary of the entire book. Our human labelers are able to supervise and\nevaluate the models quickly, despite not having read the entire books\nthemselves. Our resulting model generates sensible summaries of entire books,\neven matching the quality of human-written summaries in a few cases ($\\sim5\\%$\nof books). We achieve state-of-the-art results on the recent BookSum dataset\nfor book-length summarization. A zero-shot question-answering model using these\nsummaries achieves state-of-the-art results on the challenging NarrativeQA\nbenchmark for answering questions about books and movie scripts. We release\ndatasets of samples from our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1\">Jeff Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_L/0/1/0/all/0/1\">Long Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1\">Daniel M. Ziegler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stiennon_N/0/1/0/all/0/1\">Nisan Stiennon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lowe_R/0/1/0/all/0/1\">Ryan Lowe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leike_J/0/1/0/all/0/1\">Jan Leike</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1\">Paul Christiano</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AES Systems Are Both Overstable And Oversensitive: Explaining Why And Proposing Defenses. (arXiv:2109.11728v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.11728","description":"<p>Deep-learning based Automatic Essay Scoring (AES) systems are being actively\nused by states and language testing agencies alike to evaluate millions of\ncandidates for life-changing decisions ranging from college applications to\nvisa approvals. However, little research has been put to understand and\ninterpret the black-box nature of deep-learning based scoring algorithms.\nPrevious studies indicate that scoring models can be easily fooled. In this\npaper, we explore the reason behind their surprising adversarial brittleness.\nWe utilize recent advances in interpretability to find the extent to which\nfeatures such as coherence, content, vocabulary, and relevance are important\nfor automated scoring mechanisms. We use this to investigate the\noversensitivity i.e., large change in output score with a little change in\ninput essay content) and overstability i.e., little change in output scores\nwith large changes in input essay content) of AES. Our results indicate that\nautoscoring models, despite getting trained as \"end-to-end\" models with rich\ncontextual embeddings such as BERT, behave like bag-of-words models. A few\nwords determine the essay score without the requirement of any context making\nthe model largely overstable. This is in stark contrast to recent probing\nstudies on pre-trained representation learning models, which show that rich\nlinguistic features such as parts-of-speech and morphology are encoded by them.\nFurther, we also find that the models have learnt dataset biases, making them\noversensitive. To deal with these issues, we propose detection-based protection\nmodels that can detect oversensitivity and overstability causing samples with\nhigh accuracies. We find that our proposed models are able to detect unusual\nattribution patterns and flag adversarial samples successfully.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kumar_Y/0/1/0/all/0/1\">Yaman Singla Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Parekh_S/0/1/0/all/0/1\">Swapnil Parekh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Somesh Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Junyi Jessy Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OpenViDial 2.0: A Larger-Scale, Open-Domain Dialogue Generation Dataset with Visual Contexts. (arXiv:2109.12761v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.12761","description":"<p>In order to better simulate the real human conversation process, models need\nto generate dialogue utterances based on not only preceding textual contexts\nbut also visual contexts. However, with the development of multi-modal dialogue\nlearning, the dataset scale gradually becomes a bottleneck. In this report, we\nrelease OpenViDial 2.0, a larger-scale open-domain multi-modal dialogue dataset\ncompared to the previous version OpenViDial 1.0. OpenViDial 2.0 contains a\ntotal number of 5.6 million dialogue turns extracted from either movies or TV\nseries from different resources, and each dialogue turn is paired with its\ncorresponding visual context. We hope this large-scale dataset can help\nfacilitate future researches on open-domain multi-modal dialog generation,\ne.g., multi-modal pretraining for dialogue generation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuhe Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaoya Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_R/0/1/0/all/0/1\">Rongbin Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Trans-Encoder: Unsupervised sentence-pair modelling through self- and mutual-distillations. (arXiv:2109.13059v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13059","description":"<p>In NLP, a large volume of tasks involve pairwise comparison between two\nsequences (e.g. sentence similarity and paraphrase identification).\nPredominantly, two formulations are used for sentence-pair tasks: bi-encoders\nand cross-encoders. Bi-encoders produce fixed-dimensional sentence\nrepresentations and are computationally efficient, however, they usually\nunderperform cross-encoders. Cross-encoders can leverage their attention heads\nto exploit inter-sentence interactions for better performance but they require\ntask fine-tuning and are computationally more expensive. In this paper, we\npresent a completely unsupervised sentence representation model termed as\nTrans-Encoder that combines the two learning paradigms into an iterative joint\nframework to simultaneously learn enhanced bi- and cross-encoders.\nSpecifically, on top of a pre-trained Language Model (PLM), we start with\nconverting it to an unsupervised bi-encoder, and then alternate between the bi-\nand cross-encoder task formulations. In each alternation, one task formulation\nwill produce pseudo-labels which are used as learning signals for the other\ntask formulation. We then propose an extension to conduct such\nself-distillation approach on multiple PLMs in parallel and use the average of\ntheir pseudo-labels for mutual-distillation. Trans-Encoder creates, to the best\nof our knowledge, the first completely unsupervised cross-encoder and also a\nstate-of-the-art unsupervised bi-encoder for sentence similarity. Both the\nbi-encoder and cross-encoder formulations of Trans-Encoder outperform recently\nproposed state-of-the-art unsupervised sentence encoders such as Mirror-BERT\nand SimCSE by up to 5% on the sentence similarity benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fangyu Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1\">Yunlong Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Massiah_J/0/1/0/all/0/1\">Jordan Massiah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Emine Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Havrylov_S/0/1/0/all/0/1\">Serhii Havrylov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prefix-to-SQL: Text-to-SQL Generation from Incomplete User Questions. (arXiv:2109.13066v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13066","description":"<p>Existing text-to-SQL research only considers complete questions as the input,\nbut lay-users might strive to formulate a complete question. To build a smarter\nnatural language interface to database systems (NLIDB) that also processes\nincomplete questions, we propose a new task, prefix-to-SQL which takes question\nprefix from users as the input and predicts the intended SQL. We construct a\nnew benchmark called PAGSAS that contains 124K user question prefixes and the\nintended SQL for 5 sub-tasks Advising, GeoQuery, Scholar, ATIS, and Spider.\nAdditionally, we propose a new metric SAVE to measure how much effort can be\nsaved by users. Experimental results show that PAGSAS is challenging even for\nstrong baseline models such as T5. As we observe the difficulty of\nprefix-to-SQL is related to the number of omitted tokens, we incorporate\ncurriculum learning of feeding examples with an increasing number of omitted\ntokens. This improves scores on various sub-tasks by as much as 9% recall\nscores on sub-task GeoQuery in PAGSAS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_N/0/1/0/all/0/1\">Naihao Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1\">Shuaichen Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_P/0/1/0/all/0/1\">Peng Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1\">Tao Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Rui Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automatic Generation of Word Problems for Academic Education via Natural Language Processing (NLP). (arXiv:2109.13123v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.13123","description":"<p>Digital learning platforms enable students to learn on a flexible and\nindividual schedule as well as providing instant feedback mechanisms. The field\nof STEM education requires students to solve numerous training exercises to\ngrasp underlying concepts. It is apparent that there are restrictions in\ncurrent online education in terms of exercise diversity and individuality. Many\nexercises show little variance in structure and content, hindering the adoption\nof abstraction capabilities by students. This thesis proposes an approach to\ngenerate diverse, context rich word problems. In addition to requiring the\ngenerated language to be grammatically correct, the nature of word problems\nimplies additional constraints on the validity of contents. The proposed\napproach is proven to be effective in generating valid word problems for\nmathematical statistics. The experimental results present a tradeoff between\ngeneration time and exercise validity. The system can easily be parametrized to\nhandle this tradeoff according to the requirements of specific use cases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Keller_S/0/1/0/all/0/1\">Stanley Uros Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-28T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/","dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}