{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-14T04:21:09.755367936Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ã€æ‹›è˜ï½œåŒ—äº¬/ä¸Šæµ·/æˆéƒ½/æ·±åœ³/ç¾å›½åŠ å·ã€‘Rustå¼€å‘å·¥ç¨‹å¸ˆï¼ˆ30k-60k+ï¼‰","link":"https://rustcc.cn/article?id=ab239e8d-3a99-4b85-930f-a3e57d5fb63f","description":"<p>ç°é˜¶æ®µï¼Œéå‡¸ç§‘æŠ€æ­£åœ¨å¯»æ‰¾è¡Œä¸šå†…ä¼˜ç§€çš„Rustå¼€å‘å·¥ç¨‹å¸ˆï¼Œè–ªèµ„ç¦åˆ©è¶…çº§ä¼˜åšã€‚å…³é”®æ˜¯å›¢é˜Ÿæœ‰å¾ˆå¥½çš„Rustå¼€å‘æ°›å›´ï¼ŒRustå¤§ç¥æ‰‹æŠŠæ‰‹è¾…å¯¼ï¼ŒåŠ©ä½ ä»Rustæ–°äººä¸æ–­å‡çº§ã€‚æ¬¢è¿åŠ å…¥æˆ‘ä»¬ã€‚</p>\n<p>å²—ä½èŒè´£ï¼š\n1.è®¾è®¡å¹¶å¼€å‘åŸºäºRUSTçš„é«˜æ€§èƒ½ï¼Œä½æ—¶å»¶ç®—æ³•äº¤æ˜“ç³»ç»Ÿï¼›\n2.è®¾è®¡å¹¶å¼€å‘æ•°æ®å¤„ç†å¹³å°ï¼Œç›‘æ§è¿ç»´å¹³å°ï¼›\n3.è®¾è®¡å¹¶å¼€å‘é¢å‘å®¢æˆ·çš„é«˜å¯ç”¨äº¤æ˜“å·¥å…·ç­‰ï¼›\n4.è®¾è®¡å¹¶å¼€å‘ç­–ç•¥ç›¸å…³çš„å›æµ‹å¹³å°ã€‚</p>\n<p>å²—ä½è¦æ±‚ï¼š\n1.æœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼ˆ985ä¼˜å…ˆï¼‰ã€‚ç¼–ç¨‹åŸºç¡€æ‰å®ï¼Œå…·æœ‰è‰¯å¥½çš„è®¡ç®—æœºç†è®ºåŸºç¡€ï¼›\n2.ç†Ÿç»ƒæŒæ¡Linuxæ“ä½œï¼Œæ€§èƒ½åˆ†æï¼Œå…·å¤‡Rust/C++/Java/Goä¸°å¯Œå¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰å¸¸ç”¨çš„è®¾è®¡æ¨¡å¼ï¼Œæœ‰åˆ†å¸ƒå¼ç›¸å…³ç»éªŒåŠ åˆ†ï¼›\n3.æœ‰ç ”å‘é«˜æ€§èƒ½ï¼Œä½æ—¶å»¶ç³»ç»Ÿç»éªŒåŠ åˆ†ï¼›\n4.å¯¹æŠ€æœ¯å……æ»¡çƒ­æƒ…ï¼Œæ€è€ƒæ·±å…¥ã€‚è‡ªæˆ‘é©±åŠ¨ï¼Œèƒ½å¿«é€Ÿå­¦ä¹ æ–°é²œäº‹ç‰©ã€‚</p>\n<p>å…¬å¸ç¦åˆ©\n1.æä¾›ç§Ÿæˆ¿è¡¥è´´ï¼›\n2.æ—¥å¸¸ä¸é—´æ–­ç½‘çº¢é›¶é£Ÿã€é¥®æ–™ã€èŒ¶æ°´ä¾›ç»™ï¼›\n3.ä¸å®šæœŸç»„ç»‡å„éƒ¨é—¨æŠ€æœ¯äº¤æµå­¦ä¹ ç ”è®¨ä¼šï¼Œåˆ†äº«å¿ƒå¾—ï¼Œäº’ç›¸æˆé•¿ï¼›\n4.å›¢å»ºæ´»åŠ¨ä¸°å¯Œå¤šå½©ï¼Œæ”¾æ¾å¿ƒæƒ…ç¼“è§£ç–²åŠ³ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 14:39:59","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-09-13 Rust åœ¨ linux å†…æ ¸ä¸­çš„æœ€æ–°è¿›å±•","link":"https://rustcc.cn/article?id=1c26513e-c4c2-4d52-becf-8c39379474e9","description":"<h1>Rust åœ¨ linux å†…æ ¸ä¸­çš„æœ€æ–°è¿›å±•</h1>\n<p>è™½ç„¶Rustç¼–ç¨‹è¯­è¨€åœ¨å†…æ ¸ä¸­ä½¿ç”¨çš„æ”¯æŒè¿˜æ²¡æœ‰ç™»é™†åˆ°æœ¬å‘¨æœ«ç»“æŸçš„ <code>Linux 5.15</code> åˆå¹¶çª—å£ï¼Œä½†è¿™é¡¹å·¥ä½œä»åœ¨è¿›è¡Œä¸­ã€‚æœ¬å‘¨ï¼Œå…³äºRuståœ¨Linuxå†…æ ¸ä¸­çš„ä½¿ç”¨çš„æœ€æ–°è¿›å±•è¢«åˆ†äº«äº†å‡ºæ¥ã€‚</p>\n<p>ä½œä¸ºRust for Linuxé¡¹ç›®çš„ä¸»è¦å¼€å‘äººå‘˜ä¹‹ä¸€ï¼ŒMiguel Ojedaåœ¨æœ¬å‘¨çš„Linaro Connectè™šæ‹Ÿä¼šè®®ä¸Šä»‹ç»äº†è¯¥é¡¹ç›®ï¼Œä»–ç›®å‰æ­£åœ¨ä¸ºè°·æ­Œçš„åˆåŒå·¥ä½œã€‚</p>\n<p>å¯¹å‘¨äº”çš„æ¼”è®²æ„Ÿå…´è¶£çš„äººå¯ä»¥æŸ¥çœ‹ä¸‹é¢çš„ Presentationã€‚</p>\n<p><a href=\"https://bigthinkbuzz.com/the-latest-progress-on-rust-for-the-linux-kernel/\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<p><a href=\"https://static.linaro.org/connect/lvc21f/presentations/LVC21F-317.pdf\" rel=\"noopener noreferrer\">Presentationåœ°å€</a></p>\n<h1>Matchbox: Rust wasm ä¸­çš„ p2p ç½‘ç»œè§£å†³æ–¹æ¡ˆ</h1>\n<p>Matchbox çš„è¯ç”Ÿæ˜¯å› ä¸ºä½œè€…åœ¨<code>rust</code> ä¸­åˆ¶ä½œäº†ä¸€æ¬¾å¤šäººç½‘é¡µæ¸¸æˆï¼Œé‡åˆ°äº†ä»¥ä¸‹é—®é¢˜:</p>\n<p>å¦‚ä½•ä½¿ç”¨ä¸å¯é çš„ã€æ— åºçš„ p2p connection è¿æ¥ N ä¸ªwebæµè§ˆå™¨?</p>\n<p><a href=\"https://johanhelsing.studio/posts/introducing-matchbox\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<h1>Learn Wgpu æ›´æ–°äº†</h1>\n<p><code>wgrpu</code> æ˜¯ <code>WebGPU API spec</code> çš„ Rust å®ç°, ç›®å‰è¿™ä¸ªæ•™ç¨‹å·²ç»æ›´æ–°åˆ°äº† 0.10 ç‰ˆæœ¬, æœ‰å¤§é‡çš„åŸç†å’Œä»£ç ç¤ºä¾‹è®²è§£.</p>\n<p><a href=\"https://sotrh.github.io/learn-wgpu/beginner/tutorial2-surface/\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<h1>Sycamore: v0.6.0 ç‰ˆæœ¬å‘å¸ƒäº†</h1>\n<p>Sycamoreæ˜¯ä¸€ä¸ªç”¨ Rust å’Œ WebAssembly æ„å»ºåŒæ„webåº”ç”¨ç¨‹åºçš„åº“. ç›®å‰å‘å¸ƒäº† 0.6.0 ç‰ˆæœ¬äº†.</p>\n<ul>\n<li>é™æ€ç”Ÿæˆ</li>\n<li>æœåŠ¡ç«¯æ¸²æŸ“</li>\n<li>é‡éªŒè¯</li>\n<li>å¢é‡æ„å»º</li>\n<li>å¼€æ”¾æ„å»ºçŸ©é˜µ</li>\n<li>CLIåˆ©ç”¨ï¼Œè®©æ‚¨è½»æ¾å’Œè‡ªä¿¡åœ°æ„å»ºåº”ç”¨ç¨‹åº</li>\n<li>å……åˆ†åˆ©ç”¨ Fluent å¼€ç®±å³ç”¨çš„ i18n æ”¯æŒ</li>\n</ul>\n<p><a href=\"https://sycamore-rs.netlify.app/news/announcing-v0.6.0\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<p>--</p>\n<p>From æ—¥æŠ¥å°ç»„ BobQinï¼ŒFBIå°ç™½</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 13:10:04","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€æˆéƒ½ã€‘æ‹›è˜Rustå¼€å‘å·¥ç¨‹å¸ˆ","link":"https://rustcc.cn/article?id=ed86a028-a5b8-41e6-9849-e43a55ff7faf","description":"<h2>Rustå¼€å‘å·¥ç¨‹å¸ˆæ‹›è˜</h2>\n<h3>å²—ä½èŒè´£ï¼š</h3>\n<ul>\n<li>1ã€è´Ÿè´£ç”µå•†äº§å“åç«¯åŠŸèƒ½æ¥å£çš„å¼€å‘ï¼›</li>\n<li>2ã€è´Ÿè´£ç”µå•†äº§å“ä¸šåŠ¡åŠŸèƒ½å¼€å‘ã€è¿­ä»£å’Œç»´æŠ¤ï¼Œå¯¹ä¸šåŠ¡æ•°æ®è¿›è¡Œå¤„ç†å’Œåˆ†æï¼›</li>\n<li>3ã€é…åˆå‰ç«¯å¼€å‘å®ŒæˆåŠŸèƒ½çš„å‰åå°åŠŸèƒ½è”è°ƒï¼›</li>\n<li>4ã€é…åˆå®Œæˆäº§å“æµ‹è¯•ï¼ŒBUGä¿®æ”¹ã€‚</li>\n</ul>\n<h3>ä»»èŒè¦æ±‚ï¼š</h3>\n<ul>\n<li>1ã€åç«¯å¼€å‘è¯­è¨€åŸºç¡€æ‰å®ï¼Œæœ‰ç”µå•†äº§å“åç«¯å¼€å‘ç»éªŒï¼›</li>\n<li>2ã€ç†Ÿç»ƒä½¿ç”¨ä½¿ç”¨Mysqlå…³ç³»å‹æ•°æ®åº“ï¼›</li>\n<li>3ã€è‡³å°‘äº†è§£å¹¶ä½¿ç”¨è¿‡RocketMQã€RabbitMQã€Kafkaä¸­çš„ä¸€ç§ï¼›</li>\n<li>4ã€æœ‰Rustè¯­è¨€çš„åŸºç¡€ï¼Œæˆ–è€…æ„¿æ„è½¬Rustå¼€å‘ï¼›</li>\n<li>5ã€ä¸‰å¹´ä»¥ä¸Šçš„äº’è”ç½‘å¼€å‘å·¥ä½œç»éªŒï¼›</li>\n<li>6ã€ç†Ÿä¹ å¾®æœåŠ¡æˆ–ServicesMeshæ¶æ„è€…ä¼˜å…ˆï¼›</li>\n</ul>\n<p>å·¥ä½œåœ°ç‚¹å››å·æˆéƒ½ç¯çƒæ—¶ä»£ä¸­å¿ƒ\næœ‰æ„è€…è¯·å‘é‚®ä»¶è‡³ï¼šshaipe@sina.com æˆ–ç›´æ¥æ·»åŠ å¾®ä¿¡å·ï¼šshaipe</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 10:59:47","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"read_dir è¿”å›çš„ io::Result<DirEntry> ä¼šåœ¨ä»€ä¹ˆæƒ…å†µä¸‹è¿”å› Error å‘¢ï¼Ÿ","link":"https://rustcc.cn/article?id=c1f3f464-9146-4b43-8467-7eacc8f53bf8","description":"<p><code>read_dir</code> è¿­ä»£çš„æ—¶å€™ç»™åˆ°çš„æ˜¯ä¸€ä¸ª <code>io::Result&lt;DirEntry&gt;</code>ï¼Œæ–‡æ¡£é‡Œé¢åªæ˜¯ç®€å•è¯´äº† <strong>New errors may be encountered after an iterator is initially constructed.</strong></p>\n<p>ä½†æ˜¯å…·ä½“è¿™ä¸ª New errors åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ</p>\n<p>æˆ‘è¯•è¿‡äº†åœ¨è¿­ä»£çš„æ—¶å€™å¯¹æ–‡ä»¶å¤¹æˆ–è€…é‡Œé¢çš„æ–‡ä»¶ä½œåˆ é™¤ã€é‡å‘½åã€æ”¹å˜æƒé™ï¼Œéƒ½æ²¡æœ‰è¿”å› errorï¼›\nï¼ˆæµ‹è¯•å¹³å°åŒ…æ‹¬ Mac å’Œ Linuxï¼Œæ²¡æœ‰ windows æš‚æ—¶æ²¡æµ‹ï¼‰ã€‚</p>\n<p><img src=\"https://i.loli.net/2021/09/13/AbE9KdTL1sSxWJg.png\" alt=\"screenshot-20210913-174925.png\"></p>\n<pre><code>/// Iterator over the entries in a directory.\n///\n/// This iterator is returned from the [`read_dir`] function of this module and\n/// will yield instances of [`io::Result`]`&lt;`[`DirEntry`]`&gt;`. Through a [`DirEntry`]\n/// information like the entry's path and possibly other metadata can be\n/// learned.\n///\n/// The order in which this iterator returns entries is platform and filesystem\n/// dependent.\n///\n/// # Errors\n///\n/// This [`io::Result`] will be an [`Err`] if there's some sort of intermittent\n/// IO error during iteration.\n#[stable(feature = \"rust1\", since = \"1.0.0\")]\n#[derive(Debug)]\npub struct ReadDir(fs_imp::ReadDir);\n</code></pre>\n<p>This [<code>io::Result</code>] will be an [<code>Err</code>] if there's some sort of intermittent IO error during iteration.</p>\n<p>çœ‹èµ·æ¥ä¸€å®šè¦æ˜¯æ¯”è¾ƒç½•è§çš„ IO é”™è¯¯ï¼Ÿ</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 09:45:42","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…³äºæˆ‘å‰äº›å¤©ï¼Œåœ¨ GitHub ä¸Š Rust çš„ repo çš„é‚£äº›äº‹","link":"https://rustcc.cn/article?id=111d596f-f53f-44dc-9a59-69ccf2ff7563","description":"<p>ä¸Šå‘¨æˆ‘åœ¨ GitHub ä¸Šæ•´ç†çš„ <a href=\"https://github.com/0voice/Understanding_in_Rust\" rel=\"noopener noreferrer\">ã€Š Rust å·¥ç¨‹å¸ˆæ•è¾¹èµ„æ–™ã€‹</a> ,æ¶‰åŠäº†ä¾µæƒè¡Œä¸ºã€‚åœ¨è¿™é‡Œå‘å¤§å®¶èµ”ç¤¼é“æ­‰ã€‚å¹¶ä¸”åœ¨ç¬¬ä¸€æ—¶é—´ï¼Œå¤„ç†äº†ç›¸å…³å†…å®¹ã€‚\næˆ‘åœ¨æ•´ç†çš„ä¹‹å‰çš„åˆè¡·åªæ˜¯å•çº¯ä¸ºäº†ç»™å¤§å®¶æä¾›æ›´å¥½ã€æ›´å¤šã€æ›´å…¨ã€æ›´ä¸“ä¸šåœ°çš„ Rust å­¦ä¹ èµ„æ–™ã€‚å¹¶æ²¡æœ‰ä¸æ¯«çš„å•†ä¸šåŒ–æ‰‹æ®µã€‚\næˆ‘æ”¶é›†çš„å†…å®¹å…¨éƒ¨æ¥æºäºäº’è”ç½‘ï¼Œç”±äºæˆ‘çš„ç–å¿½æ²¡æœ‰æ³¨æ˜æ–‡ç« å‡ºå¤„é“¾æ¥ï¼Œç¡®å®æ˜¯ä¸åº”è¯¥çš„ã€‚</p>\n<p>å†ä¸€æ¬¡ï¼Œç»™ä½œå“çš„ä½œè€…é“æ­‰ã€‚</p>\n<p>æˆ‘å°†åœ¨ä»¥å repo é‡Œå°†ä¸ä¼šå‡ºç°ç±»ä¼¼çš„é”™è¯¯äº‹ä»¶ï¼ŒåŒæ—¶ä¹Ÿå¸Œæœ›å¹¿å¤§å¼€å‘è€…ä»¬ç›‘ç£ã€‚å¦‚æœæœ‰ä»»ä½•é—®é¢˜ï¼Œå¯ä»¥é‚®ç®±è‡³ï¼šwchao_isvip@163.com ï¼Œæˆ‘ä¼šåœ¨ç¬¬ä¸€æ—¶é—´å¤„ç†çš„ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 08:11:56","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"filecoiné¡¹ç›®RUSTå¤§ç‰›æ‹›è˜","link":"https://rustcc.cn/article?id=3a173b77-c75e-4cbb-a8c9-8c0496b17619","description":"<p>è¯¥å²—ä½è–ªèµ„é¢è°ˆ\nå²—ä½èŒè´£ï¼š\n1.å‚ä¸åŒºå—é“¾é¡¹ç›®å¼€å‘ï¼Œä»¥C++/rustä¸ºä¸»ï¼›\n2.ç†è§£ä¸šåŠ¡é€»è¾‘ä¸å¯¹åç«¯æœåŠ¡çš„éœ€æ±‚ï¼Œèƒ½å¤Ÿåˆ†æéœ€æ±‚å¹¶äº§ç”Ÿåˆç†æŠ€æœ¯æ–¹æ¡ˆï¼›\n3.è´Ÿè´£å¹³å°å¯¹å¤–æ¥å£ï¼Œç›¸å…³æ•°æ®æœåŠ¡çš„è®¾è®¡ä¸å®ç°ï¼›\n4.æ ¹æ®æŠ€æœ¯éœ€æ±‚éƒ¨ç½²Filecoinç¯å¢ƒï¼Œç¼–å†™è„šæœ¬ï¼Œå¯¹ç¯å¢ƒè¿›è¡Œæµ‹è¯•éƒ¨ç½²ï¼›\n5.å‚ä¸å…¬å¸é¡¹ç›®ä¸“åˆ©çš„ç¼–å†™ï¼›\nå²—ä½è¦æ±‚ï¼š\n1.å¤§ä¸“ä»¥ä¸Šå­¦å†ï¼Œè®¡ç®—æœºæˆ–è€…ç›¸å…³ä¸“ä¸šï¼Œç²¾é€šrustè¯­è¨€ï¼›\n2.è‡³å°‘ç†Ÿæ‚‰ä¸¤ç§å…¶ä»–å¼€å‘è¯­è¨€ï¼Œå¦‚C++ã€goã€Pythonç­‰ï¼›\n3.2å¹´ä»¥ä¸Šåç«¯å¼€å‘å·¥ä½œç»éªŒï¼Œåšè¿‡åŒºå—é“¾é¡¹ç›®å¼€å‘ç»éªŒçš„å¯ä¼˜å…ˆè€ƒè™‘ï¼›\n4.ç†Ÿæ‚‰Ethereumã€EOSã€Bitcoinã€Filecoinä¸­è‡³å°‘ä¸¤ä¸ªé¡¹ç›®çš„åŸºæœ¬åŸç†å’Œè®¾è®¡ï¼›\n5.ç†Ÿæ‚‰åŒºå—é“¾é¡¹ç›®ä¸­å¸¸è§çš„å…±è¯†æœºåˆ¶ã€åŠ å¯†ç®—æ³•ã€P2Pç½‘ç»œç­‰ï¼›\n6.æ€è·¯æ¸…æ™°ï¼Œå…·å¤‡è‰¯å¥½çš„æ²Ÿé€šèƒ½åŠ›ã€å›¢é˜Ÿåˆä½œæ„è¯†ï¼Œèƒ½æŠ—å‹ï¼Œèƒ½ä¸»åŠ¨æ‰¿æ‹…ï¼Œä¹äºåˆ†äº«ã€‚</p>\n<p>è¯¦æƒ…å¯è”ç³»yhcaozyyz@qq.com or 18109055866</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 06:45:02","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€æ‹›è˜  æ­å·ï¼Œä¸Šæµ·ã€‘Rustå¼€å‘å·¥ç¨‹å¸ˆï¼ˆ30K-50Kï¼‰","link":"https://rustcc.cn/article?id=4f8da484-d0fc-4f56-88ca-c19e7ea32b5a","description":"<p>ã€å²—ä½èŒè´£ã€‘</p>\n<ol>\n<li>è´Ÿè´£åˆ†å¸ƒå¼è®¡ç®—åŠå­˜å‚¨ç³»ç»Ÿçš„é«˜å¯æ‰©å±•åç«¯ç³»ç»Ÿï¼ŒæœåŠ¡å’ŒAPIï¼›</li>\n<li>è®¾è®¡é«˜æ€§èƒ½ã€é«˜å¯é æ€§çš„æœåŠ¡ï¼Œå»ºç«‹å¿«é€Ÿã€ç¨³å®šã€å®‰å…¨çš„åç«¯ä»£ç &nbsp;ï¼›</li>\n<li>ä¸ºå…¶ä»–å¼€å‘äººå‘˜æä¾›æŒ‡å¯¼ï¼Œå‚ä¸ç®—æ³•è®¾è®¡å’Œå®ç°ã€‚</li>\n<li>è´Ÿè´£è®¾è®¡å’Œä¼˜åŒ–åè®®ã€å¼±ç½‘é€šä¿¡ã€å­˜å‚¨ã€ç½‘ç»œå¹¶å‘ã€å¹¶è¡Œè®¡ç®—ã€åŠ å¯†ä»¥åŠå®‰å…¨ç­‰ï¼›</li>\n<li>ä¿è¯å·¥ç¨‹è´¨é‡å’Œå¼€å‘æ•ˆç‡ã€‚</li>\n<li>è®¾è®¡å’Œç»´æŠ¤æ€§èƒ½æµ‹è¯•ç”¨ä¾‹ï¼›</li>\n</ol>\n<p>ã€å²—ä½è¦æ±‚ã€‘</p>\n<ol>\n<li>è®¡ç®—æœºæˆ–è€…ç›¸å…³ä¸“ä¸šæœ¬ç§‘ä»¥ä¸Šå­¦å†ï¼Œä¸¤å¹´ä»¥ä¸Šç›¸å…³å·¥ä½œç»éªŒ</li>\n<li>æŠ€æœ¯æ‰å®ï¼Œç†Ÿæ‚‰Rustè¯­è¨€ç¼–ç¨‹</li>\n<li>ç†è§£ownership, trait, asyncç­‰è¯­è¨€æœºåˆ¶ã€‚</li>\n<li>ç†Ÿç»ƒä½¿ç”¨tokioã€‚ç†Ÿç»ƒä½¿ç”¨rustå¸¸ç”¨åº“</li>\n<li>æœ‰ä¸°å¯Œçš„å¤šçº¿ç¨‹åº”ç”¨å’Œå¹³å°æ„å»ºç»éªŒï¼Œå¯ç†Ÿç»ƒæ„å»ºç¨³å®šã€é«˜æ•ˆç‡å’Œå®‰å…¨çš„ä»£ç &nbsp;ï¼›</li>\n<li>æœ‰å¼ºçƒˆçš„ä¸Šè¿›å¿ƒå’Œæ±‚çŸ¥æ¬²ï¼Œå–„äºå­¦ä¹ å’Œè¿ç”¨æ–°çŸ¥è¯†ï¼Œå–„äºæ²Ÿé€šå’Œé€»è¾‘è¡¨è¾¾ï¼Œæœ‰å¼ºçƒˆçš„å›¢é˜Ÿæ„è¯†å’Œæ‰§è¡ŒåŠ›ã€‚</li>\n<li>ç†Ÿæ‚‰Linuxä¸‹å¤šçº¿ç¨‹/å¤šè¿›ç¨‹ç¼–ç¨‹æ¨¡å‹ï¼Œè¿›ç¨‹é—´é€šè®¯ï¼Œæ¶ˆæ¯äº‹ä»¶é€šçŸ¥ï¼ŒåŒæ­¥/å¼‚æ­¥ã€‚</li>\n<li>ç†Ÿæ‚‰Linuxä¸‹å†…å­˜ç®¡ç†æœºåˆ¶ï¼Œä½å»¶è¿Ÿã€é«˜å¹¶å‘æ— é”åŒ–ç¼–ç¨‹ã€‚</li>\n</ol>\n<p>ã€ç‰¹åˆ«å¤‡æ³¨ã€‘</p>\n<ol>\n<li>äº†è§£å®‰å…¨åŠ å¯†ç›¸å…³ç®—æ³•è€…ä¼˜å…ˆ&nbsp;ï¼›</li>\n<li>æœ‰ä¸°å¯Œçš„c++ã€pythonç¼–ç¨‹ç»éªŒè€…ä¼˜å…ˆ</li>\n<li>å‚ä¸å¤§å‹ç³»ç»Ÿçš„å¼€å‘ï¼Œå¹¶æˆåŠŸéƒ¨ç½²ã€å¹¿æ³›åº”ç”¨è€…ä¼˜å…ˆï¼›</li>\n<li>ç†Ÿæ‚‰å¤§æ•°æ®ã€æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå¦‚:sparkï¼Œflink, tensorflowè€…ä¼˜å…ˆã€‚</li>\n</ol>\n<p>ã€å·¥ä½œåœ°ç‚¹ã€‘\nbase1: æ­å·å¸‚è¥¿æ¹–åŒºä¸­ç”µä¸‡è°·å›­åŒº\nbase2: ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºå‰æ»©ä¸œæ–¹å¹¿åœºä¸€æœŸ\næ­å·ä¸Šæµ·å‡æœ‰å²—ä½ã€‚</p>\n<p>è”ç³»æ–¹å¼ï¼šæœæ­Œ13732914991ï¼ˆå¾®ä¿¡åŒå·ï¼‰ é‚®ç®±ï¼šzhaoge@fudata.cn</p>\n<p>ã€å…¬å¸ä»‹ç»ã€‘\nä¸Šæµ·å¯Œæ•°ç§‘æŠ€æœ‰é™å…¬å¸ ç®€ç§°â€œå¯Œæ•°ç§‘æŠ€â€ï¼Œæ˜¯å›½å†…é¢†å…ˆçš„é‡‘èAIå’Œå®‰å…¨è®¡ç®—æŠ€æœ¯é¢†è·‘è€…ï¼Œæ ¸å¿ƒå›¢é˜Ÿæ¥è‡ªCapitalOneï¼ŒAlibabaå’ŒIBMï¼Œå…¬å¸è‡ª2016å¹´æˆç«‹ä»¥æ¥å—å›½å†…é¡¶çº§é£æŠ•é’çï¼Œå·²å®ŒæˆCè½®èèµ„ã€‚å¯Œæ•°ç§‘æŠ€åšæŒä»¥â€œä»¥æ•°æ®å®‰å…¨é©±åŠ¨äººå·¥æ™ºèƒ½â€ï¼Œä¾æ‰˜äºå®‰å…¨è®¡ç®—å’Œæœºå™¨å­¦ä¹ AIæŠ€æœ¯ï¼ŒåŠ©åŠ›é‡‘èå’Œå„è¡Œä¸šæœºæ„ç»„ç»‡æé«˜æ™ºèƒ½é£æ§ã€è¥é”€å’Œè¿è¥çš„æ•ˆç‡ï¼Œå®ç°æ•°æ®åˆè§„å®‰å…¨åœ°èåˆè®¡ç®—å’Œä»·å€¼æµé€šã€‚</p>\n<p>å¯Œæ•°ç§‘æŠ€æ˜¯ä¸­å›½é€šä¿¡æ ‡å‡†åŒ–åä¼šä¼šå‘˜ã€å·¥ä¿¡éƒ¨ä¿¡é€šé™¢å¤§æ•°æ®å®‰å…¨åŠæµé€šæ ‡å‡†ç»„æˆå‘˜ã€å®‰å…¨å¤šæ–¹è®¡ç®—æ ‡å‡†å‚ä¸æ–¹ï¼Œä¸ºè¡Œä¸šè§„èŒƒæ ‡å‡†åˆ¶å®šè´¡çŒ®åˆ›æ–°æŠ€æœ¯æˆæœã€‚å¯Œæ•°ç§‘æŠ€ç»“åˆæœ€æ–°å¯†ç å­¦å’ŒåŒºå—é“¾æŠ€æœ¯ç ”å‘åˆ›æ–°ï¼Œå…¶å®‰å…¨è®¡ç®—å’Œè”é‚¦å­¦ä¹ å¼€åˆ›æ€§åœ°é‡‡ç”¨â€œæ¾å¼›è¿­ä»£æ³•â€ï¼Œåœ¨æ™ºèƒ½åˆçº¦ã€MLç®—æ³•ä¼˜åŒ–ã€ä»£ç ç¼–è¯‘å’Œè®¡ç®—ç¡¬ä»¶èŠ¯ç‰‡èåˆæ–¹é¢æ”¹å–„æ€§èƒ½ï¼Œåœ¨åŒç­‰æ¡ä»¶ä¸‹å®ç°äº†æ”¶æ•›é€Ÿåº¦çš„å¤§å¹…æå‡ï¼Œç²¾åº¦å’Œå‡†ç¡®åº¦æŸå¤±ä½äº1%ï¼Œé€Ÿåº¦è¾ƒè¡Œä¸šæ°´å¹³æé«˜äº†3å€ã€‚</p>\n<p>å¯Œæ•°ç§‘æŠ€è‡´åŠ›äºé©±åŠ¨å®‰å…¨å¯ä¿¡çš„äººå·¥æ™ºèƒ½ç§‘æŠ€ä¸å„è¡Œä¸šåœºæ™¯çš„æ·±åº¦èåˆèµ‹èƒ½ï¼Œåœ¨å…¼é¡¾éšç§ä¿æŠ¤ä¸‹å‘æŒ¥å¤§æ•°æ®çš„å•†ä¸šä»·å€¼ã€‚å¯Œæ•°ç§‘æŠ€è‡ª2017å¹´æŠ•å…¥æ•°æ®å®‰å…¨è®¡ç®—é¢†åŸŸç ”å‘åˆ›æ–°ï¼Œæ‹¥æœ‰å¤šé¡¹ä¸“åˆ©å‘æ˜å’Œè½¯è‘—ï¼Œå¹¶ä¸å›½å†…å¤–é‡‘èæœºæ„å’Œç§‘ç ”æœºæ„ï¼ˆä¸Šæµ·äº¤å¤§ç­‰ï¼‰è”åˆç ”å‘å’Œæ¨åŠ¨å·¥ç¨‹åŒ–å•†ä¸šåŒ–è½åœ°ã€‚å¯Œæ•°ç§‘æŠ€å®‰å…¨è®¡ç®—è§£å†³æ–¹æ¡ˆå·²ç»è½åœ°åœ¨æ™ºèƒ½é£æ§ã€æ™ºèƒ½è¥é”€ã€ç›‘ç®¡å’Œç§‘ç ”ç»Ÿè®¡åˆ†æã€å¼‚ä¸šæˆ–åŒä¸šæ•°æ®å®‰å…¨èåˆè®¡ç®—ç­‰åœºæ™¯ï¼Œç›®å‰å·²åœ¨é“¶è¡Œã€æŒç‰Œæ¶ˆé‡‘ã€æ”¿åŠ¡ã€åŒ»ç–—ã€è¿è¥å•†ç­‰é¢†åŸŸç§¯ç´¯ä¸Šç™¾æ¡ˆä¾‹ï¼Œåœ¨å®‰å…¨çš„æœºå™¨å­¦ä¹ é¢†åŸŸå…·æœ‰çªå‡ºçš„é¢†å…ˆä¼˜åŠ¿ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-12 15:57:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rust æ—¥æŠ¥ã€‘2021-09-12 Rust çš„ Logging æ¨è","link":"https://rustcc.cn/article?id=91b91a60-cbe9-4f8a-a5ec-8825da34457b","description":"<h3>Rust çš„ Logging æ¨è</h3>\n<p>å†…å®¹æ•´ç†è‡ª Reddit çš„è®¨è®ºï¼š<a href=\"https://www.reddit.com/r/rust/comments/pmdh6a/what_is_the_current_recommendation_for_logging_in/\" rel=\"noopener noreferrer\">What is the current recommendation for logging in Rust? : rust</a>ã€‚</p>\n<p>é—®é¢˜ç®€è¿°ï¼šé™¤äº†æ ‡å‡†çš„ <code>log</code>ï¼Œè¿˜æœ‰ä¸å°‘é€‰æ‹©ï¼š<code>env_logger</code>ï¼Œ<code>tracing</code>ï¼Œ<code>slog</code>ï¼Œ<code>simplelog</code> ç­‰ç­‰ï¼Œæœ€ä½³å®è·µæ˜¯ä»€ä¹ˆï¼Ÿ</p>\n<p>æ¥è‡ª <a href=\"https://www.reddit.com/user/Koxiaet/\" rel=\"noopener noreferrer\">Koxiaet</a> çš„ç­”å¤ï¼šé€šå¸¸æœ‰ä¸¤ç±»ä¸æ—¥å¿—ç›¸å…³çš„ crateï¼šæ—¥å¿—æ¥å£å’Œæ—¥å¿—æ¶ˆè´¹è€…ã€‚æ¥å£æä¾›äº†æƒ³è¦è®°å½•æŸäº›ä¸œè¥¿æ—¶è°ƒç”¨çš„å‡½æ•°ï¼Œæ¶ˆè´¹è€…å¤„ç†å°†ç»“æ„åŒ–æ—¥å¿—æ•°æ®æ ¼å¼åŒ–åˆ°æŸä¸ªåœ°æ–¹ï¼ˆstderr æˆ–æ–‡ä»¶ï¼‰ã€‚ä¸¤ä¸ªä¸»è¦çš„æ¥å£æ˜¯ <code>log</code> å’Œ <code>tracing</code>ï¼Œåè€…åŠŸèƒ½æ›´å¼ºå¤§å› ä¸ºå®ƒæ”¯æŒç»“æ„åŒ–æ—¥å¿—è®°å½•ï¼Œä½†å‰è€…æ›´æ™®éã€‚è¿˜æœ‰å¦ä¸€ä¸ªç»“æ„åŒ–æ—¥å¿—æ¥å£ slogï¼Œæ¯” <code>tracing</code> æ›´å¤è€ä½†ç”¨çš„è¾ƒå°‘ã€‚æ¯ä¸ªæ—¥å¿—æ¥å£éƒ½æœ‰è‡ªå·±ç”Ÿæ€ç³»ç»Ÿï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦é€‰æ‹©ã€‚å¦‚æœåœ¨å†™ä¸€ä¸ªåº“ï¼Œ<code>log</code> æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œå› ä¸ºæ‰€æœ‰çš„æ—¥å¿—è®°å½•æ¥å£éƒ½ä¸å®ƒå…¼å®¹ã€‚ä½†å¦‚æœä½ ç¡®å®éœ€è¦ç»“æ„åŒ–æ—¥å¿—è®°å½•ï¼Œåˆ™å¯ä»¥æ”¹ç”¨ <code>tracing</code>ï¼Œè¿™å–å†³äºä½ çš„éœ€æ±‚ï¼Œæ¯”å¦‚ä½ æ˜¯éœ€è¦å†™åˆ°æ–‡ä»¶è¿˜æ˜¯åªæ˜¯ç»ˆç«¯ã€‚</p>\n<p>å…¶ä»–ç½‘å‹çš„æ¨èï¼š</p>\n<ul>\n<li>File Loggingï¼š<a href=\"https://github.com/emabee/flexi_logger\" rel=\"noopener noreferrer\">emabee/flexi_logger: A flexible logger for rust programs that can write to stderr or to log files</a>ã€‚ï¼ˆæ¥è‡ª cfsamsonï¼‰</li>\n<li><code>tracing</code> çš„æ¥å£ï¼š<a href=\"https://docs.rs/tracing-log/0.1.2/tracing_log/\" rel=\"noopener noreferrer\">tracing_log - Rust</a>ï¼Œæœ‰å¤šä¸ªåŒæ—¶æ“ä½œäº¤é”™æ—¥å¿—æ¶ˆæ¯æ—¶ç‰¹åˆ«æ–¹ä¾¿ï¼Œå¯ä»¥æŒ‰æŸäº›å±æ€§å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç»„å¹¶å•ç‹¬æŸ¥çœ‹å®ƒä»¬ã€‚ï¼ˆæ¥è‡ª class_two_perversionï¼‰</li>\n<li><a href=\"https://github.com/estk/log4rs\" rel=\"noopener noreferrer\">estk/log4rs: A highly configurable logging framework for Rust</a>ï¼Œlog4rs æ˜¯ä¸€ä¸ªé«˜åº¦å¯é…ç½®çš„æ—¥å¿—æ¡†æ¶ï¼Œä»¥ Java çš„ Logback å’Œ log4j åº“ä¸ºæ¨¡å‹ã€‚é€šè¿‡ Yaml é…ç½®ï¼Œåˆ° sdout å’Œæ–‡ä»¶ï¼Œå¸¦æœ‰æ–‡ä»¶å¤§å°é™åˆ¶é€‰é¡¹ï¼Œè¿˜å¯ä»¥é…ç½®ä¸åŒçº§åˆ«çš„æ—¥å¿—ã€‚ï¼ˆæ¥è‡ª tms102ï¼‰</li>\n<li><a href=\"https://crates.io/crates/tracing-appender\" rel=\"noopener noreferrer\">tracing-appender - crates.io: Rust Package Registry</a>ï¼Œæ¨èè€…æ‰€çŸ¥é“çš„å”¯ä¸€çº¿ç¨‹å¤–æ—¥å¿—è®°å½•è§£å†³æ–¹æ¡ˆï¼Œä¸ä»…é€‚ç”¨äºå¼‚æ­¥åº”ç”¨ç¨‹åºã€‚ï¼ˆæ¥è‡ª Pand9ï¼‰</li>\n<li><a href=\"https://github.com/daboross/fern\" rel=\"noopener noreferrer\">daboross/fern: Simple, efficient logging for Rust</a>ï¼Œåƒ Python çš„ <code>logging</code> å’Œ JS çš„ <code>Winston</code>ã€‚ï¼ˆæ¥è‡ª RapBeauticianï¼‰</li>\n</ul>\n<h3>Rust å…¨æ ˆ</h3>\n<p>æœ¬æ–‡æ˜¯ä¸€ç¯‡åšå®¢ç¿»è¯‘ï¼Œæ¥è‡ªï¼š<a href=\"https://www.justinm.one/blog/2021/09/11/fullstackrust/\" rel=\"noopener noreferrer\">Full Stack Rust - Blog</a>ã€‚</p>\n<p>ä¸€å¹´å‰ï¼Œæˆ‘çš„é¦–é€‰è¯­è¨€å¦‚ä¸‹ï¼š</p>\n<ul>\n<li>Python ç”¨äºé«˜çº§ä»£ç å¿«é€ŸåŸå‹è®¾è®¡ï¼Œæˆ–ç”¨äºéœ€è¦ç¬¬ä¸‰æ–¹åŠŸèƒ½çš„ä»£ç </li>\n<li>C/C++ ç”¨äºé•¿æœŸçš„ low-level é¡¹ç›®</li>\n</ul>\n<p>å½“æ—¶åªå¬è¿‡ Rust å¹¶ç®€å•ä½¿ç”¨è¿‡ï¼Œæˆ‘çš„ç»éªŒæ¥è‡ªç”¨ Rust å†™äº†ä¸€ä¸ªå¤„ç†å¤§æ–‡ä»¶ï¼ˆ&gt;4GBï¼‰çš„äº‹åŠ¡å¹¶ä»ä¸­æŒ–æ˜ä¸€äº›ç»Ÿè®¡ä¿¡æ¯çš„å°å·¥å…·ã€‚æˆ‘ç”¨äº†ä¸€ä¸ªåº“å°†æ–‡ä»¶æ˜ å°„åˆ°å†…å­˜ï¼Œç¼¤ç‘æŒ‰ç…§é¡ºåºå¯¹å…¶è¿›è¡Œåˆ†æã€‚æœ‰ä¸€äº›å¾ˆé…·çš„æ¦‚å¿µï¼Œæ¯”å¦‚ç¼–è¯‘å™¨é™æ€åœ°å¼ºåˆ¶å†…å­˜æ˜ å°„åœ¨å®ƒè¢«å–æ¶ˆæ˜ å°„åæ— æ³•è®¿é—®â€”â€”å¦‚æœä½ ä¸å°å¿ƒï¼ŒC++ ä¸­å¯èƒ½å°±ä¼šå‘ç”Ÿè¿™ç§é”™è¯¯ã€‚</p>\n<p>ä¸è¿‡å½“æ—¶å¹¶æ²¡æœ‰çœŸæ­£å¸å¼•æˆ‘ï¼Œå› ä¸ºé‚£åªæ˜¯ä¸€ä¸ªå°æ–°å¥‡ã€‚å½“æˆ‘å‘ <a href=\"https://github.com/DrChat/pdblister\" rel=\"noopener noreferrer\">pdblister</a> æ·»åŠ æ–°åŠŸèƒ½ä»¥å¹¶è¡Œè·å–æ•°åƒä¸ª PDB æ–‡ä»¶æ—¶è¯€çªæ¥äº†ã€‚ç”±äº GILï¼Œåœ¨ CPython ä¸­å‡ ä¹ä¸å¯èƒ½ï¼Œè€Œåœ¨ C/C++ ä¸­åšåˆ°ä¸é¢ä¸´å¹¶è¡Œé”™è¯¯æ˜¯æå…¶å›°éš¾çš„ã€‚ç„¶è€Œ Rust è®©è¿™å˜å¾—å®¹æ˜“ã€‚æˆ‘æ·»åŠ äº† tokio é©±åŠ¨çš„å¼‚æ­¥ï¼Œä½¿ç”¨ <code>tokio::spawn</code> ç”Ÿæˆæ–°ä»»åŠ¡æ¥ä¸‹è½½ PDBï¼Œå¹¶ä¿®å¤äº†ç¼–è¯‘å™¨æŠ¥çš„é”™è¯¯ï¼Œå®ƒå¯ä»¥æ­£å¸¸å·¥ä½œäº†ã€‚Rust ç¼–è¯‘å™¨è¾“å‡ºä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå®ƒå¯ä»¥åœ¨ä»»ä½•åœ°æ–¹è¿è¡Œï¼Œæ²¡æœ‰è¿è¡Œæ—¶ä¾èµ–ã€‚</p>\n<p><strong>å–ä»£ Python</strong></p>\n<p>è¿™æ˜¯ç¬¬ä¸€ç‚¹ï¼ŒRust æ˜¯ Python ä½œä¸ºä¸­é•¿æœŸå·¥å…·è¯­è¨€çš„ç»ä½³æ›¿ä»£å“ã€‚Python çš„å¥½å¤„æ˜¯åºå¤§çš„åº“å’Œç”Ÿæ€ç³»ç»Ÿï¼Œé€šè¿‡ pip å¯ä»¥ç›´æ¥æ‹¿åˆ°ï¼Œæƒ³è¦å¿«é€Ÿåˆ¶ä½œä¸ API äº¤äº’çš„åŸå‹ï¼Œå¯ä»¥ä½¿ç”¨ <code>requests</code>ï¼Œåªè¦ <code>import requests</code> å°±å¯ä»¥ä½¿ç”¨äº†ã€‚Rust çš„ <code>reqwest\t</code> ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œåªè¦è¾“å…¥ <code>cargo add reqwest</code> å°±å¯ä»¥åœ¨ä»£ç ä¸­ä½¿ç”¨å®ƒã€‚</p>\n<p>ç„¶è€Œå½“è¿›å…¥æ›´é•¿æœŸçš„ç”Ÿå‘½å‘¨æœŸæ—¶ï¼ŒPython å°±æ˜¾ç¤ºå‡ºåŠ£åŠ¿ï¼Œ<code>requests</code> æ˜¯ç¨‹åºçš„ä¾èµ–ï¼Œç”¨æˆ·éœ€è¦åå»åæ‰èƒ½ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œç”±äºå¼±ç±»å‹å’Œé”™è¯¯å¤„ç†èƒ½åŠ›ï¼ˆä¸ Rust æ¯”ï¼‰ï¼ŒPython å˜å¾—æ›´åŠ åŠ£åŠ¿ã€‚è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘å¯ä»¥ä½¿ç”¨ Rust æ¯”ä½¿ç”¨ Python æ›´å¿«åœ°ç¼–å†™åŸå‹å·¥å…·ï¼Œå¹¶ä¸”æˆ‘å¯ä»¥è‡ªä¿¡åœ°çŸ¥é“æˆ‘çš„å·¥å…·æ¯”ç­‰æ•ˆçš„ Python æ›´æ˜“äºç»´æŠ¤ä¸”å¯¿å‘½æ›´é•¿ã€‚ä½†æ˜¯ï¼Œå¯¹äºçŸ­æœŸå·¥å…·ï¼ŒPython å¯èƒ½ä»ç„¶æ›´å¥½ï¼Œå› ä¸ºå®ƒä¸éœ€è¦å¯åŠ¨é¡¹ç›®å³å¯åœ¨ VSCode ä¸­è·å¾—æ™ºèƒ½æ„ŸçŸ¥æ”¯æŒã€‚ Rust çš„ cargo-script æ¥è¿‘å°† Rust æ¨å…¥è„šæœ¬è¯­è¨€çš„é¢†åŸŸï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘è¿˜æ²¡æœ‰åœ¨ VSCode ä¸­æ‰¾åˆ°ä¸ä¹‹é›†æˆçš„æ’ä»¶ã€‚</p>\n<p><strong>å–ä»£ C</strong></p>\n<p>Rust ä¹Ÿæ˜¯ C çš„ç›´æ¥æ›¿ä»£å“ï¼Œå®ƒåœ¨å„æ–¹é¢éƒ½æ›´å¥½ï¼Œå¹¶ä¸”å¯ä»¥ä¸é—ç•™ C ä»£ç åŸç”Ÿäº’æ“ä½œä»¥è¿›è¡Œå¢é‡æ›¿æ¢ã€‚Rust æœ€å¤§çš„æ”¹è¿›æ˜¯ç”Ÿæ€ç³»ç»Ÿï¼šå¦‚ä¸Šæ‰€è¿°ï¼Œåˆ©ç”¨ Rust ç”Ÿæ€ä¸­å·²æœ‰çš„åº“æ˜¯å¾ˆå®¹æ˜“çš„ã€‚å¦‚æœä½ ä»æœªä½¿ç”¨è¿‡ Cï¼Œé‚£å¾ˆå¹¸è¿ï¼Œå®é™…ä¸Š C ä¸­ä½¿ç”¨é«˜çº§åŠŸèƒ½çš„æœ€ä½³æ–¹æ³•æ˜¯è‡ªå·±å†™ã€‚</p>\n<p>C ç”Ÿæ€ç³»ç»Ÿæ˜¯æ”¯ç¦»ç ´ç¢çš„ï¼Œè€Œä¸”å¾ˆè„†å¼±ã€‚ABI æˆ–æ„å»ºç³»ç»Ÿæ²¡æœ‰ä¸€è‡´çš„æ ‡å‡†ï¼š</p>\n<ul>\n<li>ç”±äºç¼ºä¹ ABI ä¸€è‡´æ€§ï¼Œä½ ä¸èƒ½è·¨å¹³å°æˆ–æ“ä½œç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„äºŒè¿›åˆ¶æ–‡ä»¶ã€‚  æ‰€ä»¥ä½ å¿…é¡»ä»æºä»£ç æ„å»ºã€‚</li>\n<li>ç”±äºç¼ºä¹ä¸€è‡´çš„æ„å»ºç³»ç»Ÿï¼Œä½ ä¸èƒ½ç®€å•åœ°å’Œåº”ç”¨ç¨‹åºä¸€èµ·æ„å»º C åº“ï¼Œå¿…é¡»ä¿®è¡¥æˆ–é‡å†™è¦ä½¿å…¶ä¸ä½ çš„åº“å…¼å®¹çš„åº“çš„æ„å»ºç³»ç»Ÿã€‚</li>\n<li>C åº“å¾ˆå°‘è·¨å¹³å°å…¼å®¹ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹å¯ä»¥ä¾èµ–çš„å…±äº«æŠ½è±¡ã€‚</li>\n</ul>\n<p>ç„¶åè¿˜æœ‰ Rust æœ€ç‰¹è‰²çš„å®‰å…¨æ”¹è¿›â€”â€”æˆ‘å°±ä¸å±•å¼€äº†ã€‚ä½†æ ¹æ®æˆ‘çš„ç»éªŒ - å®‰å…¨æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ä¸€ç§å·¥å…·ï¼Œå¯ä»¥è®©ç¬¬ä¸‰æ–¹åº“å¼€å‘äººå‘˜æ›´å®¹æ˜“å¼ºè¿«æˆ‘æ­£ç¡®ä½¿ç”¨ä»–ä»¬çš„åº“ï¼Œè¿™æ˜¯ C åº“ä¸èƒ½åšçš„äº‹æƒ…ã€‚</p>\n<p><strong>å…¨æ ˆ Rust</strong></p>\n<p>æ€»è€Œè¨€ä¹‹ï¼Œåœ¨è¿‡å»çš„ä¸€å¹´ä¸­ï¼Œæˆ‘ä¸€ç›´åœ¨å †æ ˆçš„æ‰€æœ‰éƒ¨åˆ†ä½¿ç”¨ Rustï¼Œè€Œæˆ‘ä¹‹å‰ä½¿ç”¨è¿‡å…¶ä»–è¯­è¨€ã€‚æˆ‘å·²ç»ä½¿ç”¨ Rust æ¥å®ç°å¼•å¯¼åŠ è½½ç¨‹åºï¼š<a href=\"https://github.com/xenia-project/xell-rs\" rel=\"noopener noreferrer\">xenia-project/xell-rs: Xell Bootloader, rewritten in Rust because Â¯_(ãƒ„)_/Â¯ï¼Œ</a>æˆ‘å·²ç»ä½¿ç”¨å®ƒé€šè¿‡ <a href=\"https://github.com/DrChat/pdblister\" rel=\"noopener noreferrer\">pdblister</a> å’Œ <a href=\"https://github.com/panamax-rs/panamax\" rel=\"noopener noreferrer\">panamax</a> ä¸­çš„é«˜çº§ HTTP/HTTPS å’Œå…¶ä»–æŠ€æœ¯æ¥é•œåƒæ–‡ä»¶ã€‚æˆ‘åˆ©ç”¨å¹¶è´¡çŒ®äº†ä¼˜ç§€çš„ <a href=\"https://github.com/DrChat/gdbstub\" rel=\"noopener noreferrer\">gdbstub</a> åº“ï¼Œç”¨äºæ§åˆ¶ç”±è‡ªå®šä¹‰ VMM è¿è¡Œçš„ VMã€‚è¿™äº›é¡¹ç›®éƒ½æ˜¯åœ¨å †æ ˆçš„ä¸åŒçº§åˆ«å®Œæˆçš„ï¼Œè€Œ Rust éå¸¸é€‚åˆæ‰€æœ‰çº§åˆ«ã€‚  æˆ‘å·²ç»å¼€å§‹åœ¨æˆ‘çš„ä¸ªäººé¡¹ç›®ä¸­ä¸“é—¨ä½¿ç”¨ Rustï¼Œå¹¶åœ¨é€‚åˆçš„æ—¶å€™æ¨åŠ¨å®ƒåœ¨æˆ‘çš„å·¥ä½œä¸­ä½¿ç”¨ã€‚</p>\n<h3>tagged_cellï¼šå¿«é€Ÿã€å¯åˆå§‹åŒ–å’Œçº¿ç¨‹å®‰å…¨çš„é™æ€å˜é‡</h3>\n<p>é€šè¿‡ <code>TaggedCell</code> å’Œ <code>Tag</code> ç±»å‹å®ç°ï¼Œä¸ºäº†å®‰å…¨æ“ä½œï¼Œ<code>TaggedCell</code> çš„æ¯ä¸ªå®ä¾‹éƒ½å¿…é¡»æ˜¯å”¯ä¸€çš„ã€‚ç„¶åå¿…é¡»é€šè¿‡ <code>TaggedCell::init ()</code> åˆå§‹åŒ– <code>TaggedCell</code>ï¼Œå®ƒä½¿ç”¨ç”¨æˆ·æä¾›çš„å‡½æ•°æˆ–é—­åŒ…åˆå§‹åŒ–åº•å±‚æ•°æ®ï¼Œç„¶åè¿”å›ä¸€ä¸ªç‰¹æ®Šçš„é›¶å¤§å°çš„ <code>Init&lt;Tag&gt;</code> ç”¨äºè®¿é—® Cell çš„æ•°æ®ã€‚ä¸ºäº†ç¡®ä¿æ¯ä¸ªå•å…ƒæ ¼ä½¿ç”¨å”¯ä¸€çš„æ ‡ç­¾ç±»å‹ï¼Œ<code>tagged_cell!</code> æä¾›å®ã€‚è¯¥å®æ ¹æ®å˜é‡çš„åç§°åˆ›å»ºä¸€ä¸ªæ–°çš„æ ‡è®°ç±»å‹ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°å£°æ˜ä¸­ã€‚</p>\n<pre><code>use tagged_cell::tagged_cell;\ntagged_cell!{\n   static BAR: TaggedCell&lt;Vec&lt;usize&gt;, _&gt; = TaggedCell::new();\n}\n\nlet tag = BAR.init(|| vec![0, 10, 20]);\nlet vec = BAR.get(tag);\n\nassert_eq!(vec[2], 20);\n</code></pre>\n<p>ä¸ºäº†å…è®¸è·¨çº¿ç¨‹ä½¿ç”¨ï¼Œåªæœ‰ç¬¬ä¸€æ¬¡è°ƒç”¨ <code>TaggedCell::init</code> æ‰ä¼šåˆå§‹åŒ– Cell çš„æ•°æ®ã€‚æ‰€æœ‰æœªæ¥çš„ <code>TaggedCell::init</code> è°ƒç”¨éƒ½å°†è¿”å›ä¸€ä¸ªæ–°æ ‡ç­¾ã€‚æœªç¡®å®šå“ªä¸ªçº¿ç¨‹å°†åˆå§‹åŒ– Cell çš„æ•°æ®ã€‚</p>\n<pre><code>use std::thread;\nuse tagged_cell::tagged_cell;\n\ntagged_cell!{\n    static TABLE: TaggedCell&lt;Vec&lt;usize&gt;, _&gt; = TaggedCell::new();\n}\n\nthread::spawn(move || {\n    let tag = TABLE.init(|| vec![0, 10, 20]);\n    let table = TABLE.get(tag);\n    assert_eq!(table[2], 20);\n});\n\nthread::spawn(move || {\n    let tag = TABLE.init(|| vec![0, 10, 20]);\n    let table = TABLE.get(tag);\n    assert_eq!(table[1], 10);\n});\n</code></pre>\n<p>GitHubï¼š<a href=\"https://github.com/Dasch0/tagged_cell\" rel=\"noopener noreferrer\">Dasch0/tagged_cell: Fast, initializable, and thread safe static variables</a></p>\n<h3>ukanren-rsï¼šÂµKanren çš„ Rust å®ç°</h3>\n<p>ÂµKanren æ˜¯ä¸€ç§è½»é‡çº§å…³ç³»ç¼–ç¨‹è¯­è¨€</p>\n<ul>\n<li>åŸå§‹çš„ Schema å®ç°åœ¨è¿™é‡Œï¼š<a href=\"https://github.com/jasonhemann/microKanren\" rel=\"noopener noreferrer\">jasonhemann/microKanren: The implementation of microKanren, a featherweight relational programming language</a></li>\n<li>ç›¸å…³å‚è€ƒï¼š<a href=\"http://minikanren.org/\" rel=\"noopener noreferrer\">miniKanren.org</a></li>\n</ul>\n<pre><code>use ukanren::*;\n\nfn appendo(first: Value, second: Value, out: Value) -&gt; BoxedGoal&lt;impl Iterator&lt;Item = State&gt;&gt; {\n    eq(&amp;first, &amp;())\n        .and(eq(&amp;second, &amp;out))\n        .or(fresh(move |a: Value, d: Value, res: Value| {\n            eq(&amp;(a.clone(), d.clone()), &amp;first)\n                .and(eq(&amp;(a.clone(), res.clone()), &amp;out))\n                .and(appendo(d.clone(), second.clone(), res))\n        }))\n        .boxed()\n}\n\nlet goal = fresh(|x, y| appendo(x, y, [1, 2, 3, 4, 5].to_value()));\nassert_eq!(\n    goal.run(2).collect::&lt;Vec&lt;_&gt;&gt;(),\n    vec![\n        state![(), [1, 2, 3, 4, 5]],\n        state![[1], [2, 3, 4, 5]],\n        state![[1, 2], [3, 4, 5]],\n        state![[1, 2, 3], [4, 5]],\n        state![[1, 2, 3, 4], [5]],\n        state![[1, 2, 3, 4, 5], ()],\n    ],\n);\n</code></pre>\n<p>GitHubï¼š<a href=\"https://github.com/ekzhang/ukanren-rs\" rel=\"noopener noreferrer\">ekzhang/ukanren-rs: Rust implementation of ÂµKanren, a featherweight relational programming language.</a></p>\n<h3>rust-counter-stringsï¼šå¿«é€Ÿå®šä½å­—ç¬¦ä¸²ä½ç½®</h3>\n<p>å­—ç¬¦ä¸²ä¸­çš„æ¯ä¸ªæ˜Ÿå·éƒ½å‡ºç°åœ¨ç”±ç´§æ¥å‰é¢çš„æ•°å­—æŒ‡å®šçš„ä½ç½®ã€‚å› æ­¤ï¼Œ29 åé¢çš„æ˜Ÿå·æ˜¯è¯¥å­—ç¬¦ä¸²ä¸­çš„ç¬¬ 29 ä¸ªå­—ç¬¦ã€‚å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹ç æ‰å­—ç¬¦ä¸²çš„æœ«å°¾ï¼Œå¹¶ä¸”ç¡®åˆ‡åœ°çŸ¥é“å®ƒåœ¨å“ªé‡Œè¢«å‰ªæ‰äº†ã€‚æ¯”å¦‚ä¸ç”¨æ•°å°±çŸ¥é“å­—ç¬¦ä¸² <code>2*4*6*8*11*14*17*2</code> æ­£å¥½æœ‰ 18 ä¸ªå­—ç¬¦ã€‚å½“å¤„ç† 50 ä¸‡ä¸ªå­—ç¬¦æ—¶ä¼šæ¯”è¾ƒçœäº‹ã€‚</p>\n<pre><code>$ ./rust-counter-strings 50\n# 2*4*6*8*11*14*17*20*23*26*29*32*35*38*41*44*47*50*\n</code></pre>\n<p>è¿™å°±æ˜¯ä¸ªå°å·¥å…·ï¼Œä»£ç ä¹Ÿåªæœ‰å‡ åè¡Œã€‚</p>\n<p>GitHubï¼š<a href=\"https://github.com/thomaschaplin/rust-counter-strings\" rel=\"noopener noreferrer\">thomaschaplin/rust-counter-strings: ğŸ§µ Generate self-describing strings of a given length to help aid software testing</a></p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ é•¿ç´</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc è®ºå›ï¼šæ”¯æŒ rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRust è¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-12 14:30:38","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-09-11 Tabled å‘å¸ƒv0.3, bma-benchmark, ferros, Velorenå‘å¸ƒv0.11","link":"https://rustcc.cn/article?id=db077b1a-5af6-4fb2-b065-ff8be974fd62","description":"<h3>Tabled å‘å¸ƒv0.3</h3>\n<p>Tabled æ˜¯ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„åº“ï¼Œç”¨äºç¾åŒ– Rust ç»“æ„å’Œæšä¸¾çš„è¾“å‡ºã€‚</p>\n<p>Github<a href=\"https://github.com/zhiburt/tabled\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼Œhttps://github.com/zhiburt/tabled</p>\n<h3>bma-benchmark ä¸€ä¸ªå‹å¥½çš„åŸºå‡†æµ‹è¯•å·¥å…·</h3>\n<p>ä½¿ç”¨ <code>bma_benchmark</code></p>\n<pre><code>#[macro_use]\nextern crate bma_benchmark;\n\nuse std::sync::Mutex;\n\nlet n = 100_000_000;\nlet mutex = Mutex::new(0);\nbenchmark_start!();\nfor _ in 0..n {\n    let _a = mutex.lock().unwrap();\n}\nbenchmark_print!(n);\n</code></pre>\n<p>ä½¿ç”¨å® <code>benchmark!</code></p>\n<pre><code>#[macro_use]\nextern crate bma_benchmark;\n\nuse std::sync::Mutex;\n\nlet mutex = Mutex::new(0);\nbenchmark!(100_000_000, {\n    let _a = mutex.lock().unwrap();\n    });\n</code></pre>\n<p><img src=\"https://raw.githubusercontent.com/alttch/bma-benchmark/main/simple.png\" alt=\"ç»“æœ\"></p>\n<p>Crate <a href=\"https://crates.io/crates/bma-benchmark\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼Œhttps://crates.io/crates/bma-benchmark</p>\n<h3>ferros</h3>\n<p>seL4 æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæ“ä½œç³»ç»Ÿå’ŒåµŒå…¥å¼ç¨‹åºçš„å·¥å…·åŒ…ï¼Œè¿™ä¸ªå¼€æºé¡¹ç›®æ˜¯ä½¿ Rust ä¸­çš„ seL4 ç¼–ç¨‹å˜å¾—æ›´å¥½ã€‚</p>\n<p>ä»¥ä¸‹ä»£ç æ¼”ç»ƒå‡å®šä½¿ç”¨ç¤ºä¾‹ sel4_start åº“æ‰§è¡Œ selfeï¼Œå¹¶ä»‹ç»äº† ferros çš„æŸäº›æ–¹é¢ã€‚</p>\n<pre><code>use selfe_sys;\nuse ferros::alloc::{self, micro_alloc, smart_alloc};\nuse ferros::userland::{root_cnode, BootInfo};\n\n// The raw boot info is provided by the sel4_start library\nlet raw_boot_info: &amp;'static selfe_sys::seL4_BootInfo = unsafe { &amp;*sel4_start::BOOTINFO };\n\n\n// Utility for finding and claiming `Untyped` instances supplied by the boot info.\nlet mut allocator = micro_alloc::Allocator::bootstrap(&amp;raw_boot_info)?;\nlet initial_untyped = allocator\n    .get_untyped::&lt;U20&gt;() // The size of the Untyped instance, as bits\n    .expect(\"Couldn't find an untyped instance of the desired size\");\n\n// Create the top-level CNode wrapper with type-level-tracked remaining slot capacity\nlet (root_cnode, local_slots) = root_cnode(&amp;raw_boot_info);\n\n// Once we have an initial Untyped instance, memory distribution from it\n// can be tracked with compile-time checks. The smart_alloc macro synthesizes\n// the allocation code, and the capacity bounds are statically verified by\n// the type checker. The effect is that you can write 'slots' in the macro body \n// anywhere you need some slots, and you'll get the right number allocated\n// with type inference. A reference to 'ut' does the same for untyped memory. \nsmart_alloc!(|slots from local_slots, ut from uts| {\n\n    // Create a page table seL4 kernel object and return a capability pointer to it.\n    // Here we use a variable binding type annotation and Rust's type system can figure out\n    // if it can allocate a large enough Untyped instance and enough cnode slots\n    // to represent this particular kernel object.\n    let example_page_table: LocalCap&lt;UnmappedPageTable&gt; = retype(ut, slots)?;\n\n    // Create a resource-tracking wrapper around the raw boot info to assist in\n    // virtual memory related operations.\n    let boot_info  = BootInfo::wrap(raw_boot_info, ut, slots);\n    let (root_page_table, boot_info) = boot_info.map_page_table(root_page_table)?;\n});\n\n</code></pre>\n<p>Github<a href=\"https://github.com/auxoncorp/ferros\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼Œhttps://github.com/auxoncorp/ferros</p>\n<h3>Velorenå‘å¸ƒv0.11</h3>\n<p>ä»Šå¤©ï¼ŒVeloren å‘å¸ƒäº† 0.11ã€‚ è¿™ä¸ªç‰ˆæœ¬å·²ç»åˆ¶ä½œäº† 3 ä¸ªæœˆï¼Œå…¶ä¸€å¤§é‡ç‚¹æ˜¯è®©ä¸–ç•Œå„åœ°çš„æˆ˜æ–—æ›´å…·æ´»åŠ›ã€‚è¿™æ˜¯ä»¥æ–°çš„åœ°ç‚¹ç³»ç»Ÿçš„å½¢å¼å‡ºç°ï¼Œä»¥åŠ NPC å’Œç”Ÿç‰©å¦‚ä½•ä¸ä¸–ç•Œäº’åŠ¨ã€‚</p>\n<p>è¦äº†è§£è¿˜æœ‰å“ªäº›æ–°åŠŸèƒ½ï¼è¯·ç»§ç»­é˜…è¯» V0.11 å˜æ›´æ—¥å¿—<a href=\"https://veloren.net/release-0-11/\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼Œhttps://veloren.net/release-0-11/</p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ <a href=\"https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929\" rel=\"noopener noreferrer\">æ´‹èŠ‹</a></p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 15:33:08","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"cargo fix å¦‚ä½•è‡ªåŠ¨ä¿®å¤warningï¼Ÿ","link":"https://rustcc.cn/article?id=0d747849-2064-4ba2-a725-2fe64f6ab556","description":"<p>ä»£ç ä¸­æœ‰äº›å¤–é¢copyè¿‡æ¥çš„enumï¼Œå¯¼è‡´å¾ˆå¤šçš„â€œshould have an upper camel case nameâ€warningã€‚å°±æ˜¯ç¼–ç é£æ ¼çš„é—®é¢˜ï¼Œå¯æ˜¯<code>cargo fix</code>æ²¡æœ‰åŠæ³•æŒ‰ç…§rustç»™çš„å»ºè®®è‡ªåŠ¨å¸®æˆ‘æ”¹æ‰ã€‚ã€‚ã€‚ã€‚</p>\n<p>æœ‰åŠæ³•å—ï¼Ÿ</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 14:31:22","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust åŸ¹å…»æé«˜è®¡åˆ’ Vol. 7 - 8 | Rust é¡¹ç›®å·¥ç¨‹æ¥äº†","link":"https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b","description":"<p>æˆ‘ä»¬çš„ Rust å…¬å¼€è¯¾è¿›è¡Œäº† 6 æœŸäº†ï¼Œå¸¦å¤§å®¶äº†è§£äº† ï¼š</p>\n<ol>\n<li>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€</li>\n<li>ç†è§£ Rust æ‰€æœ‰æƒ</li>\n<li>é€šè¿‡å®æˆ˜ç†è§£ Rust å®</li>\n<li>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª</li>\n<li>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1</li>\n<li>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 2</li>\n</ol>\n<p>ç›®å‰è§†é¢‘å›æ”¾ä¼ åˆ° B ç«™æ”¶è·è®¸å¤šå¥½è¯„ï¼Œèµï¼Œä¹Ÿç»™æˆ‘ä»¬å¾ˆå¤§çš„é¼“åŠ±ã€‚å¸Œæœ›æˆ‘ä»¬çš„ Rust åŸ¹å…»æé«˜è®¡åˆ’ | Datafuse å¯ä»¥å¸®åŠ©æ›´å¤šçš„æœ‹å‹å¿«é€Ÿçš„ä½¿ç”¨ä¸Š Rust ã€‚\næœ¬å‘¨ç»™å¤§å®¶æ’ä¸¤ä¸ªå…¬å¼€è¯¾ï¼šå‘¨å››æ™šä¸Šï¼Œå‘¨æ—¥æ™šä¸Šã€‚æˆ‘ä»¬ Rust åŸ¹å…»æé«˜è®¡åˆ’é‚€è¯·åˆ°ç¬¬äºŒä½åˆ†äº«å˜‰å®¾ è‘£æ³½æ¶¦è€å¸ˆï¼Œ å¦å¤– Rust åŸ¹å…»æé«˜è®¡åˆ’ çš„å†…å®¹ä¸Šä¹Ÿåšäº†ä¸€äº›è°ƒæ•´ã€‚</p>\n<hr>\n<p>åˆ†äº«ä¸»é¢˜ï¼šã€Šæ·±å…¥äº†è§£rust é—­åŒ…ã€‹ | Vol. 7</p>\n<p>åˆ†äº«æ—¶é—´ï¼š å‘¨å››æ™šä¸Š2021-09-09 20:00-21:00</p>\n<p>åˆ†äº«è®²å¸ˆï¼š è‘£æ³½æ¶¦</p>\n<p>å†…å®¹ä»‹ç»ï¼š æ·±å…¥æµ…å‡ºäº†è§£ rust é—­åŒ…å·¥ä½œåŸç†ï¼Œè®©å¤§å®¶äº†è§£åº•å±‚å®ç°\nè®²å¸ˆä»‹ç»ï¼š\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<hr>\n<p>åˆ†äº«ä¸»é¢˜ï¼šã€Šåˆ©ç”¨ Tokio å®ç°ä¸€ä¸ªé«˜æ€§èƒ½ Mini Http serverã€‹ | Vol. 8</p>\n<p>åˆ†äº«æ—¶é—´ï¼š  å‘¨æ—¥æ™šä¸Š2021-09-12 20:00-21:00</p>\n<p>åˆ†äº«è®²å¸ˆï¼š è‹æ—</p>\n<p>é¦–å…ˆæ„Ÿè°¢è‹æ—è€å¸ˆçš„åšæŒä»˜å‡ºï¼Œ å¸¦æˆ‘ä»¬å­¦ä¹  Rust çš„é‡ç‚¹çŸ¥è¯†ã€‚ ç»è¿‡å’Œè‹ç³è€å¸ˆæ²Ÿé€šï¼Œæˆ‘ä»¬åç»­çš„è¯¾ç¨‹ï¼Œä¼šæ›´åŠ å¾€å®æˆ˜æ–¹å‘è½¬å˜ã€‚æ¥ä¸‹æ˜¯ä¸€ä¸ªç³»åˆ—çš„å†…å®¹ï¼š</p>\n<ol>\n<li>åˆ©ç”¨ Tokio å®ç°ä¸€ä¸ª Mini Http server</li>\n<li>åŸºäº Http serveræä¾›å†…å®¹åŠ¨æ€çš„ API ç½‘å…³</li>\n<li>åˆ©ç”¨ Redis å®ç°å¯¹ API ç½‘å…³åŠ é€Ÿ</li>\n<li>å­¦ä¹  Rust RPC è°ƒç”¨ï¼Œå®ç°å¾®æœåŠ¡è°ƒç”¨</li>\n</ol>\n<p>è¿™ä¸ªå†…å®¹å¯èƒ½éœ€è¦4æ¬¡å·¦å³çš„å…¬å¼€è¯¾ï¼Œç›®çš„æ˜¯å¸¦ç€å¤§å®¶åšä¸€äº›å°é¡¹ç›®ï¼Œå¸¦å¤§å®¶ç†Ÿæ‚‰ä¸€ä¸‹ Rust å·¥ç¨‹ï¼Œè®©å¤§å®¶å¯ä»¥å¿«é€ŸæŠŠ Rust ç”¨åˆ°åç«¯å¼€å‘ä¸­ã€‚</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<p>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1   | Vol. 5\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<p>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 2  | Vol. 6\nhttps://www.bilibili.com/video/bv1oy4y1G7jC</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust å¼‚æ­¥ç¼–ç¨‹æ•™æï¼šhttps://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 02:23:16","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust å­¦ä¹ éšç¬”","link":"https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8","description":"<h1>åˆ‡æ¢é•œåƒæº</h1>\n<p>crm =&gt; https://github.com/wtklbm/crm</p>\n<p>å¸¸ç”¨å‘½ä»¤å°±æ˜¯ <code>crm best</code></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 14:35:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"pretree è¡¥å…¨æ–‡æ¡£å‘å¸ƒäº†,å†æ¬¡è°¢è°¢å¤§ç¥çš„æŒ‡ç‚¹ç»ˆäºå…¥é—¨äº†ã€‚","link":"https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827","description":"<h1>Pretree</h1>\n<p>pretree is a package for storing and querying routing rules with prefix tree .</p>\n<p>pretree æ˜¯ä¸€ä¸ªç”¨äºå­˜å‚¨å’ŒæŸ¥è¯¢è·¯ç”±è§„åˆ™çš„åŒ…ã€‚å®ƒç”¨å‰ç¼€æ ‘å­˜å‚¨è·¯ç”±è§„åˆ™ï¼Œæ”¯æŒåŒ…å«å˜é‡çš„è·¯ç”±ã€‚</p>\n<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>\n<p>Inspired by <a href=\"https://github.com/obity/pretree\" rel=\"noopener noreferrer\">obity/pretree</a> (golang)</p>\n<h1>Doc</h1>\n<p>See this document at <a href=\"https://docs.rs/pretree\" rel=\"noopener noreferrer\">API documentation</a></p>\n<h1>Install</h1>\n<p>Add the following line to your Cargo.toml file:</p>\n<pre><code>pretree = \"1.0.0\"\n</code></pre>\n<h1>Example</h1>\n<pre><code>use pretree::Pretree;\nlet mut p = Pretree::new();\np.store(\"GET\",\"account/{id}/info/:name\");\np.store(\"GET\",\"account/:id/login\");\np.store(\"GET\",\"account/{id}\");\np.store(\"GET\",\"bacteria/count_number_by_month\");\nlet (ok,rule,vars) = p.query(\"GET\",\"account/929239\");\nprintln!(\"ok:{} rule:{} vars:{:#?}\",ok,rule,vars);\n\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 09:37:30","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust å¼‚æ­¥ç¼–ç¨‹äºŒ: Tokio å…¥é—¨è¿è¡Œæ—¶ä»‹ç» | Rust åŸ¹å…»æé«˜è®¡åˆ’ Vol. 6","link":"https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a","description":"<h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹äºŒ: Tokio å…¥é—¨è¿è¡Œæ—¶ä»‹ç»ã€‹|Vol. 6</h3>\n<p><strong>è¯¾ç¨‹æ—¶é—´:</strong>  2021å¹´9æœˆ5æ—¥ 20:00-21:00</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»:</strong>  ä¸Šå‘¨å…¬å¼€è¯¾æˆ‘ä»¬è®²è§£äº† Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹ï¼ˆ å±äºä¸€ä¸ªéå¸¸ç»å…¸çš„å†…å®¹ï¼Œå»ºè®®è§‚çœ‹ ï¼‰, å¤§å®¶å¯¹ Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹æœ‰äº†ä¸€ä¸ªåˆæ­¥è®¤è¯†,  Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹é‡Œéœ€è¦ Executorã€Reactorã€Future ç­‰, æœ¬å‘¨å…¬å¼€è¯¾å°†ä»¥ Tokio æ¡†æ¶ä¸ºåŸºç¡€, å’Œå¤§å®¶ä¸€èµ·èŠèŠ Tokio é‡Œçš„ Executorã€Reactorã€Future æ˜¯ä»€ä¹ˆ?</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<p>1ã€å›é¡¾ Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>\n<p>2ã€è°ˆè°ˆå¯¹ Rust å¼‚æ­¥æ¡†æ¶çš„è®¤è¯† ( futures-rsã€async-stdã€tokio ) .</p>\n<p>3ã€Tokio ä»‹ç».</p>\n<p>4ã€Tokio é‡Œçš„ Executorã€Reactorã€Future å¦‚ä½•ä½¿ç”¨.</p>\n<p>5ã€ä½¿ç”¨ Tokio å®ç°ä¸€ä¸ªç®€å•çš„æœåŠ¡ç«¯ä¸å®¢æˆ·ç«¯ç¨‹åº.</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/\nRust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1  å›æ”¾åœ°å€ï¼š\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust å¼‚æ­¥ç¼–ç¨‹æ•™æï¼šhttps://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 08:40:15","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</h3>\n<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´8æœˆ29æ—¥ 20:00-21:00</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»:</strong>  è®²åˆ° Rust ä½¿ç”¨ Future å¼‚æ­¥ç¼–ç¨‹ï¼Œå°±ä¸å¾—ä¸è¯´ futures å’Œ tokio è¿™ä¸¤ä¸ª crateï¼Œå…¶å®æ ‡å‡†åº“ä¸­çš„ futureï¼Œä»¥åŠ async/await å°±æ˜¯ä» futures åº“ä¸­æ•´åˆè¿›æ ‡å‡†åº“çš„, Tokio æ‹¥æœ‰æå¿«çš„æ€§èƒ½ï¼Œæ˜¯å¤§éƒ¨åˆ†ç³»ç»Ÿå¼‚æ­¥å¤„ç†çš„é€‰æ‹©ï¼Œå…¶æ„å»ºäº future ä¹‹ä¸Šã€‚Future æ˜¯  Rust å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒåŸºç¡€ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<p>1ã€ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥.</p>\n<p>2ã€ç†è§£å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>\n<p>3ã€Future ç¼–ç¨‹æ¨¡å‹è®²è§£.</p>\n<p>4ã€å¸¦é¢†å¤§å®¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ future , å†æ¬¡å¸®å¿™å¤§å®¶ç†è§£</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-19 -- Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</h3>\n<p>å·²ç»åœ¨ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªcrate ä¸Šæµ‹è¯•äº†ç‰ˆæœ¬è¿ç§»,å¹¶ä¸”å°†æµ‹è¯•æ‰€æœ‰å…¬å…±çš„ crateã€‚</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>å¼‚æ­¥å¼•æ“ C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D æ¸¸æˆå¼•æ“</h3>\n<ul>\n<li><strong>PCï¼ˆWindowsã€Linuxã€macOSï¼‰å’Œ Web (WebAssembly)</strong> æ”¯æŒã€‚</li>\n<li><strong>å»¶è¿Ÿç€è‰²</strong></li>\n<li><strong>å†…ç½®ä¿å­˜/åŠ è½½</strong></li>\n<li><strong>ç‹¬ç«‹åœºæ™¯ç¼–è¾‘å™¨</strong></li>\n<li><strong>é«˜çº§ç‰©ç†æ¨¡å‹</strong></li>\n<li><strong>åˆ†å±‚æ¨¡å‹èµ„æº</strong></li>\n<li><strong>å‡ ä½•å®ä¾‹åŒ–</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ å†°å±±ä¸Šçš„ mook &amp;&amp; æŒºè‚¥</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾: é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Šé€šè¿‡Datafuseç†è§£å…¨é“¾è·¯è·Ÿè¸ªã€‹| Vol. 4</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ22æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong> æ•°æ®åº“ç³»ç»Ÿä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚ï¼Œåºå¤§çš„ç³»ç»Ÿã€‚ç‰¹åˆ«æ˜¯åœ¨è°ƒè¯•å’Œè§‚å¯ŸSQLæ‰§è¡Œï¼Œå¤šçº¿ç¨‹ä»»åŠ¡åˆ‡æ¢ï¼Œå› ä¸ºæ²¡æœ‰å†…å­˜è°ƒç”¨æˆ–å †æ ˆè·Ÿè¸ªï¼Œè¿™ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¿½è¸ªçš„ç”±æ¥ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°å¤šè¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ªä¸ºæè¿°å’Œåˆ†æè·¨è¿›ç¨‹äº‹åŠ¡æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚Google Dapper(Dapper: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿé“¾è·¯è¿½è¸ªåŸºç¡€è®¾æ–½)è®ºæ–‡(å„tracerçš„åŸºç¡€)ä¸­æè¿°äº†åˆ†å¸ƒå¼è¿½è¸ªçš„ä¸€äº›ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­ç¨³æ€é—®é¢˜ã€åˆ†å¸ƒå¼åˆ†æã€èµ„æºå±æ€§å’Œå¾®æœåŠ¡çš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ã€‚</p>\n<p>æœ¬æ¬¡å…¬å¼€è¯¾é€š Google çš„ OpenTraceing ä»‹ç»ï¼Œç»“åˆRustçš„ tokio-rs/tracing ä½¿ç”¨ï¼Œæœ€ç»ˆç»“åˆ Datafuse é¡¹ç›®ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹å¤§å‹åº”ç”¨çš„å…¨é“¾è·¯è·Ÿè¸ªåˆ†æè¿‡ç¨‹ã€‚</p>\n<p>å…³äºDatafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ol>\n<li>\n<p>ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»ŸOpenTracingåŠåº”ç”¨åœºæ™¯</p>\n</li>\n<li>\n<p>ä»‹ç» tokio-rs/tracing åŠåœ¨ç¨‹åºå¼€å‘ä¸­çš„ä½œç”¨</p>\n</li>\n<li>\n<p>ä¸ºä»€ä¹ˆéœ€è¦tokio-rs/tracingåº“</p>\n</li>\n<li>\n<p>æ¼”ç¤ºDatafuseé¡¹ç›®ä¸­tokio-rs/tracingçš„ä½¿ç”¨</p>\n</li>\n</ol>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"è®ºå›githubè´¦æˆ·æ— æ³•ç™»å½•è§£å†³ç¬”è®°","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>æœ‰åæ˜ è¿™ä¸¤å¤©githubè´¦æˆ·æ— æ³•ç™»å½•äº†ã€‚</p>\n<p>æŠ¥è¿™ä¸ªé”™ï¼š</p>\n<pre><code>get github user info err\n</code></pre>\n<p>æŸ¥äº†å‡ ä¸ªåœ°æ–¹ï¼š</p>\n<ol>\n<li>ä»£ç æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼šOk</li>\n<li>httpsä»£ç†æ˜¯å¦æ­£å¸¸ï¼šOk</li>\n<li>æ£€æŸ¥äº†githubè¿”å›æ—¥å¿—ï¼Œå‘ç°æ˜¯ï¼š</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>è¿›å…¥è¿™ä¸ªåœ°å€ä¸€çœ‹ï¼š<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>åŸæ¥2020å¹´2æœˆå°±å·²ç»è¯´äº†ï¼Œè¦æ”¹è¦æ”¹ã€‚ä¸è¿‡æˆ‘ç¡®å®æ²¡ç•™æ„åˆ°è¿™ä¸ªä¿¡æ¯ã€‚ï¼šï¼ˆ</p>\n<p>æ„æ€å°±æ˜¯è¯´access_tokenä¸è¦æ”¾åœ¨queryå‚æ•°ä¸­ï¼Œè€Œæ˜¯è¦æ”¾åœ¨headeré‡Œé¢ã€‚ç…§å®ƒè¯´çš„ï¼Œæ”¹äº†åå°±å¥½äº†ã€‚</p>\n<p>ç‰¹æ­¤è®°å½•ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust çš„ Future ä¸ Javascript çš„ Promise åŠŸèƒ½å¯¹ç…§å‚è€ƒ","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>çš„<code>Future</code>ä¸<code>Javascript</code>çš„<code>Promise</code>åŠŸèƒ½å¯¹ç…§å‚è€ƒ</h1>\n<p>å­¦ä¹ æ–°é²œæŠ€æœ¯æ—¶ï¼Œæˆ‘æ€»æ˜¯ä¼šä¹ æƒ¯æ€§å‘æ›¾ç»ç†Ÿæ‚‰çš„å†…å®¹ä¸Šé ï¼Œç”šè‡³å¥—ç”¨ç°æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¿™æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œå¯¹ç…§<code>Javascript - Promise/A+ API</code>æ¥è®°å¿†ä¸€éƒ¨åˆ†<code>Rust Future</code>å¸¸ç”¨<code>API</code>ã€‚</p>\n<blockquote>\n<p>æ³¨æ„ï¼šæ‰€æœ‰çš„<code>Rust - Future</code>æ“ä½œéƒ½æ˜¯ä»¥<code>.await</code>ç»“å°¾çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œä¸åŒäº<code>Javascript - Promise/A+</code>ï¼Œ<code>Rust - Future</code>æ˜¯æƒ°æ€§çš„ã€‚åªæœ‰è¢«<code>.await</code>æŒ‡ä»¤æ¿€æ´»åï¼Œåœ¨<code>Rust - Future</code>å†…å°è£…çš„æ“ä½œæ‰ä¼šè¢«çœŸæ­£åœ°æ‰§è¡Œã€‚</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">æè¿°</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* ä»€ä¹ˆéƒ½ä¸åš */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll ä¸èƒ½è¢«ç”¨æ¥æ„é€ åŒ…å«äº†å¼‚æ­¥æ“ä½œçš„ Future å®ä¾‹ï¼Œå› ä¸ºã€å›è°ƒé—­åŒ…ã€‘å†…çš„ã€å¯ä¿®æ”¹å¼•ç”¨ã€‘&amp;mut Context&lt;'_&gt; ä¸èƒ½è¢«  ï¼ˆ1ï¼‰è·¨çº¿ç¨‹ä¼ é€’  ï¼ˆ2ï¼‰ä¼ é€’å‡ºé—­åŒ…ä½œç”¨åŸŸ2. task::spawn_blocking() ã€å›è°ƒé—­åŒ…ã€‘è¾“å…¥å‚æ•°å†…çš„ thread::sleep() ä¸æ˜¯é˜»å¡è¿è¡Œ task::spawn_blocking() çš„ä¸»çº¿ç¨‹ï¼Œè€Œæ˜¯é˜»å¡ä»ã€é˜»å¡ä»»åŠ¡çº¿ç¨‹æ± ã€‘ä¸­åˆ†é…æ¥è¿è¡Œé˜»å¡ä»»åŠ¡çš„ã€å·¥ä½œçº¿ç¨‹ã€‘ã€‚</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. æœ‰ä¸€ä¸ª promise/future å¤±è´¥å°±æ•´ä½“æ€§åœ°å¤±è´¥ã€‚2. try_join æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future çš„æˆåŠŸä¸å¤±è´¥ç»“æœéƒ½æ”¶é›†2. è¿”å›ç»“æœï¼š(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. ä»…åªæ”¶é›†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ promise/future2. try_race æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. æ”¶é›†ç¬¬ä¸€ä¸ªç»“æŸçš„ promise/futureï¼Œæ— è®ºå®ƒæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯å¤±è´¥æ”¶åœºã€‚2. è¿”å›ç»“æœï¼šT</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼š</strong>ã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹</p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ15æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong></p>\n<p>å¦‚æœæƒ³ç”¨ Rust å¼€å‘å¤§å‹ç›®ï¼Œæˆ–è€…å­¦ä¹ å¤§å‹é¡¹ç›®ä»£ç ï¼Œç‰¹åˆ«æ˜¯æ¡†æ¶çº§åˆ«çš„é¡¹ç›®ï¼Œé‚£ä¹ˆ Rust çš„å®æœºåˆ¶è‚¯å®šæ˜¯ä¸€ä¸ªå¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚ ä¾‹å¦‚ datafuse ä¸­çš„ä¸€äº›é…ç½®ç®¡ç†ï¼š\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>è¿™å°±æ˜¯é€šè¿‡å®å®ç°é…ç½®çš„ç»Ÿä¸€è¡Œä¸ºï¼Œä»£ç å‚è€ƒï¼š\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust è¯­è¨€å¼ºå¤§çš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¯ä»¥åˆ›å»ºå’Œåˆ©ç”¨å®ï¼Œä¸è¿‡åˆ›å»ºå®çœ‹èµ·æ¥æŒºå¤æ‚ï¼Œå¸¸å¸¸ä»¤åˆšæ¥è§¦ Rust çš„å¼€å‘è€…ç”Ÿç•æƒ§ã€‚ åœ¨æœ¬æ¬¡å…¬å¼€è¯¾ä¸­å¸®åŠ©ä½ ç†è§£ Rust Macro çš„åŸºæœ¬åŸç†ï¼Œå­¦ä¹ å¦‚ä½•åˆ›è‡ªå·²çš„ Rust å®ï¼Œä»¥åŠæŸ¥çœ‹æºç å­¦ä¹ å®çš„å®ç°ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ul>\n<li>ä»€ä¹ˆæ˜¯ Rust å®</li>\n<li>ä»€ä¹ˆæ˜¯å®è¿è¡ŒåŸç†</li>\n<li>å¦‚ä½•åˆ›å»º Rust å®è¿‡ç¨‹</li>\n<li>é˜…è¯» datafuse é¡¹ç›®æºç ï¼Œ å­¦ä¹ é¡¹ç›®ä¸­å®çš„å®ç°</li>\n</ul>\n<p><strong>è®²å¸ˆä»‹ç»</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-09-14T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Reference-Centric Models for Grounded Collaborative Dialogue. (arXiv:2109.05042v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05042","description":"<p>We present a grounded neural dialogue model that successfully collaborates\nwith people in a partially-observable reference game. We focus on a setting\nwhere two agents each observe an overlapping part of a world context and need\nto identify and agree on some object they share. Therefore, the agents should\npool their information and communicate pragmatically to solve the task. Our\ndialogue agent accurately grounds referents from the partner's utterances using\na structured reference resolver, conditions on these referents using a\nrecurrent memory, and uses a pragmatic generation procedure to ensure the\npartner can resolve the references the agent produces. We evaluate on the\nOneCommon spatial grounding dialogue task (Udagawa and Aizawa 2019), involving\na number of dots arranged on a board with continuously varying positions,\nsizes, and shades. Our agent substantially outperforms the previous state of\nthe art for the task, obtaining a 20% relative improvement in successful task\ncompletion in self-play evaluations and a 50% relative improvement in success\nin human evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1\">Daniel Fried</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiu_J/0/1/0/all/0/1\">Justin T. Chiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Klein_D/0/1/0/all/0/1\">Dan Klein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Entity-Based Knowledge Conflicts in Question Answering. (arXiv:2109.05052v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05052","description":"<p>Knowledge-dependent tasks typically use two sources of knowledge: parametric,\nlearned at training time, and contextual, given as a passage at inference time.\nTo understand how models use these sources together, we formalize the problem\nof knowledge conflicts, where the contextual information contradicts the\nlearned information. Analyzing the behaviour of popular models, we measure\ntheir over-reliance on memorized information (the cause of hallucinations), and\nuncover important factors that exacerbate this behaviour. Lastly, we propose a\nsimple method to mitigate over-reliance on parametric knowledge, which\nminimizes hallucination, and improves out-of-distribution generalization by\n4%-7%. Our findings demonstrate the importance for practitioners to evaluate\nmodel tendency to hallucinate rather than read, and show that our mitigation\nstrategy encourages generalization to evolving information (i.e.,\ntime-dependent queries). To encourage these practices, we have released our\nframework for generating knowledge conflicts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1\">Shayne Longpre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Perisetla_K/0/1/0/all/0/1\">Kartik Perisetla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1\">Anthony Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramesh_N/0/1/0/all/0/1\">Nikhil Ramesh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+DuBois_C/0/1/0/all/0/1\">Chris DuBois</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Sameer Singh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker Turn Modeling for Dialogue Act Classification. (arXiv:2109.05056v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05056","description":"<p>Dialogue Act (DA) classification is the task of classifying utterances with\nrespect to the function they serve in a dialogue. Existing approaches to DA\nclassification model utterances without incorporating the turn changes among\nspeakers throughout the dialogue, therefore treating it no different than\nnon-interactive written text. In this paper, we propose to integrate the turn\nchanges in conversations among speakers when modeling DAs. Specifically, we\nlearn conversation-invariant speaker turn embeddings to represent the speaker\nturns in a conversation; the learned speaker turn embeddings are then merged\nwith the utterance embeddings for the downstream task of DA classification.\nWith this simple yet effective mechanism, our model is able to capture the\nsemantics from the dialogue content while accounting for different speaker\nturns in a conversation. Validation on three benchmark public datasets\ndemonstrates superior performance of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavabi_L/0/1/0/all/0/1\">Leili Tavabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1\">Mohammad Soleymani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FBERT: A Neural Transformer for Identifying Offensive Content. (arXiv:2109.05074v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05074","description":"<p>Transformer-based models such as BERT, XLNET, and XLM-R have achieved\nstate-of-the-art performance across various NLP tasks including the\nidentification of offensive language and hate speech, an important problem in\nsocial media. In this paper, we present fBERT, a BERT model retrained on SOLID,\nthe largest English offensive language identification corpus available with\nover $1.4$ million offensive instances. We evaluate fBERT's performance on\nidentifying offensive content on multiple English datasets and we test several\nthresholds for selecting instances from SOLID. The fBERT model will be made\nfreely available to the community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sarkar_D/0/1/0/all/0/1\">Diptanu Sarkar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ranasinghe_T/0/1/0/all/0/1\">Tharindu Ranasinghe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ororbia_A/0/1/0/all/0/1\">Alexander Ororbia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Self-Disclosure In Neural Dialog Models By Candidate Re-ranking. (arXiv:2109.05090v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05090","description":"<p>Neural language modelling has progressed the state-of-the-art in different\ndownstream Natural Language Processing (NLP) tasks. One such area is of\nopen-domain dialog modelling, neural dialog models based on GPT-2 such as\nDialoGPT have shown promising performance in single-turn conversation. However,\nsuch (neural) dialog models have been criticized for generating responses which\nalthough may have relevance to the previous human response, tend to quickly\ndissipate human interest and descend into trivial conversation. One reason for\nsuch performance is the lack of explicit conversation strategy being employed\nin human-machine conversation. Humans employ a range of conversation strategies\nwhile engaging in a conversation, one such key social strategies is\nSelf-disclosure(SD). A phenomenon of revealing information about one-self to\nothers. Social penetration theory (SPT) proposes that communication between two\npeople moves from shallow to deeper levels as the relationship progresses\nprimarily through self-disclosure. Disclosure helps in creating rapport among\nthe participants engaged in a conversation. In this paper, Self-disclosure\nenhancement architecture (SDEA) is introduced utilizing Self-disclosure Topic\nModel (SDTM) during inference stage of a neural dialog model to re-rank\nresponse candidates to enhance self-disclosure in single-turn responses from\nfrom the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Soni_M/0/1/0/all/0/1\">Mayank Soni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cowan_B/0/1/0/all/0/1\">Benjamin Cowan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wade_V/0/1/0/all/0/1\">Vincent Wade</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PICARD: Parsing Incrementally for Constrained Auto-Regressive Decoding from Language Models. (arXiv:2109.05093v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05093","description":"<p>Large pre-trained language models for textual data have an unconstrained\noutput space; at each decoding step, they can produce any of 10,000s of\nsub-word tokens. When fine-tuned to target constrained formal languages like\nSQL, these models often generate invalid code, rendering it unusable. We\npropose PICARD (code and trained models available at\nhttps://github.com/ElementAI/picard), a method for constraining auto-regressive\ndecoders of language models through incremental parsing. PICARD helps to find\nvalid output sequences by rejecting inadmissible tokens at each decoding step.\nOn the challenging Spider and CoSQL text-to-SQL translation tasks, we show that\nPICARD transforms fine-tuned T5 models with passable performance into\nstate-of-the-art solutions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scholak_T/0/1/0/all/0/1\">Torsten Scholak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schucher_N/0/1/0/all/0/1\">Nathan Schucher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HypoGen: Hyperbole Generation with Commonsense and Counterfactual Knowledge. (arXiv:2109.05097v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05097","description":"<p>A hyperbole is an intentional and creative exaggeration not to be taken\nliterally. Despite its ubiquity in daily life, the computational explorations\nof hyperboles are scarce. In this paper, we tackle the under-explored and\nchallenging task: sentence-level hyperbole generation. We start with a\nrepresentative syntactic pattern for intensification and systematically study\nthe semantic (commonsense and counterfactual) relationships between each\ncomponent in such hyperboles. Next, we leverage the COMeT and reverse COMeT\nmodels to do commonsense and counterfactual inference. We then generate\nmultiple hyperbole candidates based on our findings from the pattern, and train\nneural classifiers to rank and select high-quality hyperboles. Automatic and\nhuman evaluations show that our generation method is able to generate\nhyperboles creatively with high success rate and intensity scores.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1\">Yufei Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1\">Arvind krishna Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Zero-shot Commonsense Reasoning with Self-supervised Refinement of Language Models. (arXiv:2109.05105v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05105","description":"<p>Can we get existing language models and refine them for zero-shot commonsense\nreasoning? This paper presents an initial study exploring the feasibility of\nzero-shot commonsense reasoning for the Winograd Schema Challenge by\nformulating the task as self-supervised refinement of a pre-trained language\nmodel. In contrast to previous studies that rely on fine-tuning annotated\ndatasets, we seek to boost conceptualization via loss landscape refinement. To\nthis end, we propose a novel self-supervised learning approach that refines the\nlanguage model utilizing a set of linguistic perturbations of similar concept\nrelationships. Empirical analysis of our conceptually simple framework\ndemonstrates the viability of zero-shot commonsense reasoning on multiple\nbenchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1\">Tassilo Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Attention-based Contrastive Learning for Winograd Schemas. (arXiv:2109.05108v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05108","description":"<p>Self-supervised learning has recently attracted considerable attention in the\nNLP community for its ability to learn discriminative features using a\ncontrastive objective. This paper investigates whether contrastive learning can\nbe extended to Transfomer attention to tackling the Winograd Schema Challenge.\nTo this end, we propose a novel self-supervised framework, leveraging a\ncontrastive loss directly at the level of self-attention. Experimental analysis\nof our attention-based models on multiple datasets demonstrates superior\ncommonsense reasoning capabilities. The proposed approach outperforms all\ncomparable unsupervised approaches while occasionally surpassing supervised\nones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1\">Tassilo Klein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nabi_M/0/1/0/all/0/1\">Moin Nabi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improved Latent Tree Induction with Distant Supervision via Span Constraints. (arXiv:2109.05112v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05112","description":"<p>For over thirty years, researchers have developed and analyzed methods for\nlatent tree induction as an approach for unsupervised syntactic parsing.\nNonetheless, modern systems still do not perform well enough compared to their\nsupervised counterparts to have any practical use as structural annotation of\ntext. In this work, we present a technique that uses distant supervision in the\nform of span constraints (i.e. phrase bracketing) to improve performance in\nunsupervised constituency parsing. Using a relatively small number of span\nconstraints we can substantially improve the output from DIORA, an already\ncompetitive unsupervised parsing system. Compared with full parse tree\nannotation, span constraints can be acquired with minimal effort, such as with\na lexicon derived from Wikipedia, to find exact text matches. Our experiments\nshow span constraints based on entities improves constituency parsing on\nEnglish WSJ Penn Treebank by more than 5 F1. Furthermore, our method extends to\nany domain where span constraints are easily attainable, and as a case study we\ndemonstrate its effectiveness by parsing biomedical text from the CRAFT\ndataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1\">Zhiyang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drozdov_A/0/1/0/all/0/1\">Andrew Drozdov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jay Yoon Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OGorman_T/0/1/0/all/0/1\">Tim O&#x27;Gorman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rongali_S/0/1/0/all/0/1\">Subendhu Rongali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Finkbeiner_D/0/1/0/all/0/1\">Dylan Finkbeiner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suresh_S/0/1/0/all/0/1\">Shilpa Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McCallum_A/0/1/0/all/0/1\">Andrew McCallum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Partially-supervised novel object captioning leveraging context from paired data. (arXiv:2109.05115v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05115","description":"<p>In this paper, we propose an approach to improve image captioning solutions\nfor images with novel objects that do not have caption labels in the training\ndataset. Our approach is agnostic to model architecture, and primarily focuses\non training technique that uses existing fully paired image-caption data and\nthe images with only the novel object detection labels (partially paired data).\nWe create synthetic paired captioning data for these novel objects by\nleveraging context from existing image-caption pairs. We further re-use these\npartially paired images with novel objects to create pseudo-label captions that\nare used to fine-tune the captioning model. Using a popular captioning model\n(Up-Down) as baseline, our approach achieves state-of-the-art results on\nheld-out MS COCO out-of-domain test split, and improves F1 metric and CIDEr for\nnovel object images by 75.8 and 26.6 points respectively, compared to baseline\nmodel that does not use partially paired images during training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bujimalla_S/0/1/0/all/0/1\">Shashank Bujimalla</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subedar_M/0/1/0/all/0/1\">Mahesh Subedar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tickoo_O/0/1/0/all/0/1\">Omesh Tickoo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MURAL: Multimodal, Multitask Retrieval Across Languages. (arXiv:2109.05125v1 [cs.IR])","link":"http://arxiv.org/abs/2109.05125","description":"<p>Both image-caption pairs and translation pairs provide the means to learn\ndeep representations of and connections between languages. We use both types of\npairs in MURAL (MUltimodal, MUltitask Representations Across Languages), a dual\nencoder that solves two tasks: 1) image-text matching and 2) translation pair\nmatching. By incorporating billions of translation pairs, MURAL extends ALIGN\n(Jia et al. PMLR'21)--a state-of-the-art dual encoder learned from 1.8 billion\nnoisy image-text pairs. When using the same encoders, MURAL's performance\nmatches or exceeds ALIGN's cross-modal retrieval performance on well-resourced\nlanguages across several datasets. More importantly, it considerably improves\nperformance on under-resourced languages, showing that text-text learning can\novercome a paucity of image-caption examples for these languages. On the\nWikipedia Image-Text dataset, for example, MURAL-base improves zero-shot mean\nrecall by 8.1% on average for eight under-resourced languages and by 6.8% on\naverage when fine-tuning. We additionally show that MURAL's text\nrepresentations cluster not only with respect to genealogical connections but\nalso based on areal linguistics, such as the Balkan Sprachbund.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1\">Aashi Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1\">Mandy Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srinivasan_K/0/1/0/all/0/1\">Krishna Srinivasan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1\">Ting Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kudugunta_S/0/1/0/all/0/1\">Sneha Kudugunta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1\">Chao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yinfei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldridge_J/0/1/0/all/0/1\">Jason Baldridge</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"D-REX: Dialogue Relation Extraction with Explanations. (arXiv:2109.05126v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05126","description":"<p>Existing research studies on cross-sentence relation extraction in long-form\nmulti-party conversations aim to improve relation extraction without\nconsidering the explainability of such methods. This work addresses that gap by\nfocusing on extracting explanations that indicate that a relation exists while\nusing only partially labeled data. We propose our model-agnostic framework,\nD-REX, a policy-guided semi-supervised algorithm that explains and ranks\nrelations. We frame relation extraction as a re-ranking task and include\nrelation- and entity-specific explanations as an intermediate step of the\ninference process. We find that about 90% of the time, human annotators prefer\nD-REX's explanations over a strong BERT-based joint relation extraction and\nexplanation model. Finally, our evaluations on a dialogue relation extraction\ndataset show that our method is simple yet effective and achieves a\nstate-of-the-art F1 score on relation extraction, improving upon existing\nmethods by 13.5%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Embar_V/0/1/0/all/0/1\">Varun Embar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuan_Y/0/1/0/all/0/1\">Yi-Lin Tuan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1\">Lise Getoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Refocusing on Relevance: Personalization in NLG. (arXiv:2109.05140v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05140","description":"<p>Many NLG tasks such as summarization, dialogue response, or open domain\nquestion answering focus primarily on a source text in order to generate a\ntarget response. This standard approach falls short, however, when a user's\nintent or context of work is not easily recoverable based solely on that source\ntext -- a scenario that we argue is more of the rule than the exception. In\nthis work, we argue that NLG systems in general should place a much higher\nlevel of emphasis on making use of additional context, and suggest that\nrelevance (as used in Information Retrieval) be thought of as a crucial tool\nfor designing user-oriented text-generating tasks. We further discuss possible\nharms and hazards around such personalization, and argue that value-sensitive\ndesign represents a crucial path forward through these challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dudy_S/0/1/0/all/0/1\">Shiran Dudy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bedrick_S/0/1/0/all/0/1\">Steven Bedrick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webber_B/0/1/0/all/0/1\">Bonnie Webber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extract, Integrate, Compete: Towards Verification Style Reading Comprehension. (arXiv:2109.05149v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05149","description":"<p>In this paper, we present a new verification style reading comprehension\ndataset named VGaokao from Chinese Language tests of Gaokao. Different from\nexisting efforts, the new dataset is originally designed for native speakers'\nevaluation, thus requiring more advanced language understanding skills. To\naddress the challenges in VGaokao, we propose a novel Extract-Integrate-Compete\napproach, which iteratively selects complementary evidence with a novel query\nupdating mechanism and adaptively distills supportive evidence, followed by a\npairwise competition to push models to learn the subtle difference among\nsimilar text pieces. Experiments show that our methods outperform various\nbaselines on VGaokao with retrieved complementary evidence, while having the\nmerits of efficiency and explainability. Our dataset and code are released for\nfurther research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1\">Yuxuan Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yansong Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1\">Dongyan Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Natural SQL: Making SQL Easier to Infer from Natural Language Specifications. (arXiv:2109.05153v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05153","description":"<p>Addressing the mismatch between natural language descriptions and the\ncorresponding SQL queries is a key challenge for text-to-SQL translation. To\nbridge this gap, we propose an SQL intermediate representation (IR) called\nNatural SQL (NatSQL). Specifically, NatSQL preserves the core functionalities\nof SQL, while it simplifies the queries as follows: (1) dispensing with\noperators and keywords such as GROUP BY, HAVING, FROM, JOIN ON, which are\nusually hard to find counterparts for in the text descriptions; (2) removing\nthe need for nested subqueries and set operators; and (3) making schema linking\neasier by reducing the required number of schema items. On Spider, a\nchallenging text-to-SQL benchmark that contains complex and nested SQL queries,\nwe demonstrate that NatSQL outperforms other IRs, and significantly improves\nthe performance of several previous SOTA models. Furthermore, for existing\nmodels that do not support executable SQL generation, NatSQL easily enables\nthem to generate executable SQL queries, and achieves the new state-of-the-art\nexecution accuracy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yujian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jinxia Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodward_J/0/1/0/all/0/1\">John R. Woodward</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drake_J/0/1/0/all/0/1\">John Drake</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiaofu Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Underexplored Limitations of Cross-Domain Text-to-SQL Generalization. (arXiv:2109.05157v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05157","description":"<p>Recently, there has been significant progress in studying neural networks for\ntranslating text descriptions into SQL queries under the zero-shot cross-domain\nsetting. Despite achieving good performance on some public benchmarks, we\nobserve that existing text-to-SQL models do not generalize when facing domain\nknowledge that does not frequently appear in the training data, which may\nrender the worse prediction performance for unseen domains. In this work, we\ninvestigate the robustness of text-to-SQL models when the questions require\nrarely observed domain knowledge. In particular, we define five types of domain\nknowledge and introduce Spider-DK (DK is the abbreviation of domain knowledge),\na human-curated dataset based on the Spider benchmark for text-to-SQL\ntranslation. NL questions in Spider-DK are selected from Spider, and we modify\nsome samples by adding domain knowledge that reflects real-world question\nparaphrases. We demonstrate that the prediction accuracy dramatically drops on\nsamples that require such domain knowledge, even if the domain knowledge\nappears in the training set, and the model provides the correct predictions for\nrelated training samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1\">Yujian Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xinyun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Purver_M/0/1/0/all/0/1\">Matthew Purver</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"StreamHover: Livestream Transcript Summarization and Annotation. (arXiv:2109.05160v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05160","description":"<p>With the explosive growth of livestream broadcasting, there is an urgent need\nfor new summarization technology that enables us to create a preview of\nstreamed content and tap into this wealth of knowledge. However, the problem is\nnontrivial due to the informal nature of spoken language. Further, there has\nbeen a shortage of annotated datasets that are necessary for transcript\nsummarization. In this paper, we present StreamHover, a framework for\nannotating and summarizing livestream transcripts. With a total of over 500\nhours of videos annotated with both extractive and abstractive summaries, our\nbenchmark dataset is significantly larger than currently existing annotated\ncorpora. We explore a neural extractive summarization model that leverages\nvector-quantized variational autoencoder to learn latent vector representations\nof spoken utterances and identify salient utterances from the transcripts to\nform summaries. We show that our model generalizes better and improves\nperformance over strong baselines. The results of this study provide an avenue\nfor future research to improve summarization solutions for efficient browsing\nof livestreams.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1\">Sangwoo Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1\">Franck Dernoncourt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganter_T/0/1/0/all/0/1\">Tim Ganter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1\">Nedim Lipka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Walter Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1\">Hailin Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1\">Jonathan Brandt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Foroosh_H/0/1/0/all/0/1\">Hassan Foroosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1\">Fei Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Categorization of Social Knowledge for Commonsense Question Answering. (arXiv:2109.05168v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05168","description":"<p>Large pre-trained language models (PLMs) have led to great success on various\ncommonsense question answering (QA) tasks in an end-to-end fashion. However,\nlittle attention has been paid to what commonsense knowledge is needed to\ndeeply characterize these QA tasks. In this work, we proposed to categorize the\nsemantics needed for these tasks using the SocialIQA as an example. Building\nupon our labeled social knowledge categories dataset on top of SocialIQA, we\nfurther train neural QA models to incorporate such social knowledge categories\nand relation information from a knowledge base. Unlike previous work, we\nobserve our models with semantic categorizations of social knowledge can\nachieve comparable performance with a relatively simple model and smaller size\ncompared to other complex approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gengyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1\">Xiaochen Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1\">Jing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"College Student Retention Risk Analysis From Educational Database using Multi-Task Multi-Modal Neural Fusion. (arXiv:2109.05178v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05178","description":"<p>We develop a Multimodal Spatiotemporal Neural Fusion network for Multi-Task\nLearning (MSNF-MTCL) to predict 5 important students' retention risks: future\ndropout, next semester dropout, type of dropout, duration of dropout and cause\nof dropout. First, we develop a general purpose multi-modal neural fusion\nnetwork model MSNF for learning students' academic information representation\nby fusing spatial and temporal unstructured advising notes with spatiotemporal\nstructured data. MSNF combines a Bidirectional Encoder Representations from\nTransformers (BERT)-based document embedding framework to represent each\nadvising note, Long-Short Term Memory (LSTM) network to model temporal advising\nnote embeddings, LSTM network to model students' temporal performance variables\nand students' static demographics altogether. The final fused representation\nfrom MSNF has been utilized on a Multi-Task Cascade Learning (MTCL) model\ntowards building MSNF-MTCL for predicting 5 student retention risks. We\nevaluate MSNFMTCL on a large educational database consists of 36,445 college\nstudents over 18 years period of time that provides promising performances\ncomparing with the nearest state-of-art models. Additionally, we test the\nfairness of such model given the existence of biases.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1\">Mohammad Arif Ul Alam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05179","description":"<p>Generating high quality question-answer pairs is a hard but meaningful task.\nAlthough previous works have achieved great results on answer-aware question\ngeneration, it is difficult to apply them into practical application in the\neducation field. This paper for the first time addresses the question-answer\npair generation task on the real-world examination data, and proposes a new\nunified framework on RACE. To capture the important information of the input\npassage we first automatically generate(rather than extracting) keyphrases,\nthus this task is reduced to keyphrase-question-answer triplet joint\ngeneration. Accordingly, we propose a multi-agent communication model to\ngenerate and optimize the question and keyphrases iteratively, and then apply\nthe generated question and keyphrases to guide the generation of answers. To\nestablish a solid benchmark, we build our model on the strong generative\npre-training model. Experimental results show that our model makes great\nbreakthroughs in the question-answer pair generation task. Moreover, we make a\ncomprehensive analysis on our model, suggesting new directions for this\nchallenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_F/0/1/0/all/0/1\">Fanyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker-Oriented Latent Structures for Dialogue-Based Relation Extraction. (arXiv:2109.05182v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05182","description":"<p>Dialogue-based relation extraction (DiaRE) aims to detect the structural\ninformation from unstructured utterances in dialogues. Existing relation\nextraction models may be unsatisfactory under such a conversational setting,\ndue to the entangled logic and information sparsity issues in utterances\ninvolving multiple speakers. To this end, we introduce SOLS, a novel model\nwhich can explicitly induce speaker-oriented latent structures for better\nDiaRE. Specifically, we learn latent structures to capture the relationships\namong tokens beyond the utterance boundaries, alleviating the entangled logic\nissue. During the learning process, our speaker-specific regularization method\nprogressively highlights speaker-related key clues and erases the irrelevant\nones, alleviating the information sparsity issue. Experiments on three public\ndatasets demonstrate the effectiveness of our proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Guoshun Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1\">Guoqing Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leng_S/0/1/0/all/0/1\">Sicong Leng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1\">Yao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MOMENTA: A Multimodal Framework for Detecting Harmful Memes and Their Targets. (arXiv:2109.05184v1 [cs.MM])","link":"http://arxiv.org/abs/2109.05184","description":"<p>Internet memes have become powerful means to transmit political,\npsychological, and socio-cultural ideas. Although memes are typically humorous,\nrecent days have witnessed an escalation of harmful memes used for trolling,\ncyberbullying, and abusing social entities. Detecting such harmful memes is\nchallenging as they can be highly satirical and cryptic. Moreover, while\nprevious work has focused on specific aspects of memes such as hate speech and\npropaganda, there has been little work on harm in general, and only one\nspecialized dataset for it. Here, we focus on bridging this gap. In particular,\nwe aim to solve two novel tasks: detecting harmful memes and identifying the\nsocial entities they target. We further extend the recently released HarMeme\ndataset to generalize on two prevalent topics - COVID-19 and US politics and\nname the two datasets as Harm-C and Harm-P, respectively. We then propose\nMOMENTA (MultimOdal framework for detecting harmful MemEs aNd Their tArgets), a\nnovel multimodal (text + image) deep neural model, which uses global and local\nperspectives to detect harmful memes. MOMENTA identifies the object proposals\nand attributes and uses a multimodal model to perceive the comprehensive\ncontext in which the objects and the entities are portrayed in a given meme.\nMOMENTA is interpretable and generalizable, and it outperforms numerous\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanick_S/0/1/0/all/0/1\">Shraman Pramanick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Shivam Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1\">Dimitar Dimitrov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akhtar_M/0/1/0/all/0/1\">Md Shad Akhtar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1\">Tanmoy Chakraborty</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Total Recall: a Customized Continual Learning Method for Neural Semantic Parsers. (arXiv:2109.05186v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05186","description":"<p>This paper investigates continual learning for semantic parsing. In this\nsetting, a neural semantic parser learns tasks sequentially without accessing\nfull training data from previous tasks. Direct application of the SOTA\ncontinual learning algorithms to this problem fails to achieve comparable\nperformance with re-training models with all seen tasks because they have not\nconsidered the special properties of structured outputs yielded by semantic\nparsers. Therefore, we propose TotalRecall, a continual learning method\ndesigned for neural semantic parsers from two aspects: i) a sampling method for\nmemory replay that diversifies logical form templates and balances\ndistributions of parse actions in a memory; ii) a two-stage training method\nthat significantly improves generalization capability of the parsers across\ntasks. We conduct extensive experiments to study the research problems involved\nin continual semantic parsing and demonstrate that a neural semantic parser\ntrained with TotalRecall achieves superior performance than the one trained\ndirectly with the SOTA continual learning algorithms and achieve a 3-6 times\nspeedup compared to re-training from scratch. Code and datasets are available\nat: https://github.com/zhuang-li/cl_nsp.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zhuang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1\">Lizhen Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haffari_G/0/1/0/all/0/1\">Gholamreza Haffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TopicRefine: Joint Topic Prediction and Dialogue Response Generation for Multi-turn End-to-End Dialogue System. (arXiv:2109.05187v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05187","description":"<p>A multi-turn dialogue always follows a specific topic thread, and topic shift\nat the discourse level occurs naturally as the conversation progresses,\nnecessitating the model's ability to capture different topics and generate\ntopic-aware responses. Previous research has either predicted the topic first\nand then generated the relevant response, or simply applied the attention\nmechanism to all topics, ignoring the joint distribution of the topic\nprediction and response generation models and resulting in uncontrollable and\nunrelated responses. In this paper, we propose a joint framework with a topic\nrefinement mechanism to learn these two tasks simultaneously. Specifically, we\ndesign a three-pass iteration mechanism to generate coarse response first, then\npredict corresponding topics, and finally generate refined response conditioned\non predicted topics. Moreover, we utilize GPT2DoubleHeads and BERT for the\ntopic prediction task respectively, aiming to investigate the effects of joint\nlearning and the understanding ability of GPT model. Experimental results\ndemonstrate that our proposed framework achieves new state-of-the-art\nperformance at response generation task and the great potential understanding\ncapability of GPT model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1\">Mingyu Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1\">Zimo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Gabriel Pui Cheong Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Eliciting Knowledge from Language Models for Event Extraction. (arXiv:2109.05190v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05190","description":"<p>Eliciting knowledge contained in language models via prompt-based learning\nhas shown great potential in many natural language processing tasks, such as\ntext classification and generation. Whereas, the applications for more complex\ntasks such as event extraction are less studied, since the design of prompt is\nnot straightforward due to the complicated types and arguments. In this paper,\nwe explore to elicit the knowledge from pre-trained language models for event\ntrigger detection and argument extraction. Specifically, we present various\njoint trigger/argument prompt methods, which can elicit more complementary\nknowledge by modeling the interactions between different triggers or arguments.\nThe experimental results on the benchmark dataset, namely ACE2005, show the\ngreat advantages of our proposed approach. In particular, our approach is\nsuperior to the recent advanced methods in the few-shot scenario where only a\nfew samples are used for training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1\">Jiaju Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jian_J/0/1/0/all/0/1\">Jin Jian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1\">Qin Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Survey on Multi-modal Summarization. (arXiv:2109.05199v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05199","description":"<p>The new era of technology has brought us to the point where it is convenient\nfor people to share their opinions over an abundance of platforms. These\nplatforms have a provision for the users to express themselves in multiple\nforms of representations, including text, images, videos, and audio. This,\nhowever, makes it difficult for users to obtain all the key information about a\ntopic, making the task of automatic multi-modal summarization (MMS) essential.\nIn this paper, we present a comprehensive survey of the existing research in\nthe area of MMS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jangra_A/0/1/0/all/0/1\">Anubhav Jangra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1\">Sriparna Saha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasanuzzaman_M/0/1/0/all/0/1\">Mohammad Hasanuzzaman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering Main Causalities for Long-tailed Information Extraction. (arXiv:2109.05213v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05213","description":"<p>Information Extraction (IE) aims to extract structural information from\nunstructured texts. In practice, long-tailed distributions caused by the\nselection bias of a dataset, may lead to incorrect correlations, also known as\nspurious correlations, between entities and labels in the conventional\nlikelihood models. This motivates us to propose counterfactual IE (CFIE), a\nnovel framework that aims to uncover the main causalities behind data in the\nview of causal inference. Specifically, 1) we first introduce a unified\nstructural causal model (SCM) for various IE tasks, describing the\nrelationships among variables; 2) with our SCM, we then generate\ncounterfactuals based on an explicit language structure to better calculate the\ndirect causal effect during the inference stage; 3) we further propose a novel\ndebiasing approach to yield more robust predictions. Experiments on three IE\ntasks across five public datasets show the effectiveness of our CFIE model in\nmitigating the spurious correlation issues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1\">Guoshun Nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1\">Jiaqi Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_R/0/1/0/all/0/1\">Rui Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zhijiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Empirical Analysis of Training Strategies of Transformer-based Japanese Chit-chat Systems. (arXiv:2109.05217v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05217","description":"<p>In recent years, several high-performance conversational systems have been\nproposed based on the Transformer encoder-decoder model. Although previous\nstudies analyzed the effects of the model parameters and the decoding method on\nsubjective dialogue evaluations with overall metrics, they did not analyze how\nthe differences of fine-tuning datasets affect on user's detailed impression.\nIn addition, the Transformer-based approach has only been verified for English,\nnot for such languages with large inter-language distances as Japanese. In this\nstudy, we develop large-scale Transformer-based Japanese dialogue models and\nJapanese chit-chat datasets to examine the effectiveness of the\nTransformer-based approach for building chit-chat dialogue systems. We\nevaluated and analyzed the impressions of human dialogues in different\nfine-tuning datasets, model parameters, and the use of additional information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sugiyama_H/0/1/0/all/0/1\">Hiroaki Sugiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mizukami_M/0/1/0/all/0/1\">Masahiro Mizukami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arimoto_T/0/1/0/all/0/1\">Tsunehiro Arimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narimatsu_H/0/1/0/all/0/1\">Hiromi Narimatsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chiba_Y/0/1/0/all/0/1\">Yuya Chiba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakajima_H/0/1/0/all/0/1\">Hideharu Nakajima</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meguro_T/0/1/0/all/0/1\">Toyomi Meguro</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AdaK-NER: An Adaptive Top-K Approach for Named Entity Recognition with Incomplete Annotations. (arXiv:2109.05233v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05233","description":"<p>State-of-the-art Named Entity Recognition(NER) models rely heavily on large\namountsof fully annotated training data. However, ac-cessible data are often\nincompletely annotatedsince the annotators usually lack comprehen-sive\nknowledge in the target domain. Normallythe unannotated tokens are regarded as\nnon-entities by default, while we underline thatthese tokens could either be\nnon-entities orpart of any entity. Here, we study NER mod-eling with incomplete\nannotated data whereonly a fraction of the named entities are la-beled, and the\nunlabeled tokens are equiva-lently multi-labeled by every possible label.Taking\nmulti-labeled tokens into account, thenumerous possible paths can distract the\ntrain-ing model from the gold path (ground truthlabel sequence), and thus\nhinders the learn-ing ability. In this paper, we propose AdaK-NER, named the\nadaptive top-Kapproach, tohelp the model focus on a smaller feasible re-gion\nwhere the gold path is more likely to belocated. We demonstrate the superiority\nofour approach through extensive experimentson both English and Chinese\ndatasets, aver-agely improving 2% in F-score on the CoNLL-2003 and over 10% on\ntwo Chinese datasetscompared with the prior state-of-the-art works.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1\">Hongtao Ruan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1\">Liying Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_P/0/1/0/all/0/1\">Peixian Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liang Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jing Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Prior Omission of Dissimilar Source Domain(s) for Cost-Effective Few-Shot Learning. (arXiv:2109.05234v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05234","description":"<p>Few-shot slot tagging is an emerging research topic in the field of Natural\nLanguage Understanding (NLU). With sufficient annotated data from source\ndomains, the key challenge is how to train and adapt the model to another\ntarget domain which only has few labels. Conventional few-shot approaches use\nall the data from the source domains without considering inter-domain relations\nand implicitly assume each sample in the domain contributes equally. However,\nour experiments show that the data distribution bias among different domains\nwill significantly affect the adaption performance. Moreover, transferring\nknowledge from dissimilar domains will even introduce some extra noises so that\naffect the performance of models. To tackle this problem, we propose an\neffective similarity-based method to select data from the source domains. In\naddition, we propose a Shared-Private Network (SP-Net) for the few-shot slot\ntagging task. The words from the same class would have some shared features. We\nextract those shared features from the limited annotated data on the target\ndomain and merge them together as the label embedding to help us predict other\nunlabelled data on the target domain. The experiment shows that our method\noutperforms the state-of-the-art approaches with fewer source data. The result\nalso proves that some training data from dissimilar sources are redundant and\neven negative for the adaption.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zezhong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Hongru Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_K/0/1/0/all/0/1\">Kwan Wai Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jia Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fung_G/0/1/0/all/0/1\">Gabriel Pui Cheong Fung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05238","description":"<p>Simultaneous machine translation (SiMT) generates translation before reading\nthe entire source sentence and hence it has to trade off between translation\nquality and latency. To fulfill the requirements of different translation\nquality and latency in practical applications, the previous methods usually\nneed to train multiple SiMT models for different latency levels, resulting in\nlarge computational costs. In this paper, we propose a universal SiMT model\nwith Mixture-of-Experts Wait-k Policy to achieve the best translation quality\nunder arbitrary latency with only one trained model. Specifically, our method\nemploys multi-head attention to accomplish the mixture of experts where each\nhead is treated as a wait-k expert with its own waiting words number, and given\na test latency and source inputs, the weights of the experts are accordingly\nadjusted to produce the best translation. Experiments on three datasets show\nthat our method outperforms all the strong baselines under different latency,\nincluding the state-of-the-art adaptive policy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. (arXiv:2109.05244v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05244","description":"<p>Cross-attention is an important component of neural machine translation\n(NMT), which is always realized by dot-product attention in previous methods.\nHowever, dot-product attention only considers the pair-wise correlation between\nwords, resulting in dispersion when dealing with long sentences and neglect of\nsource neighboring relationships. Inspired by linguistics, the above issues are\ncaused by ignoring a type of cross-attention, called concentrated attention,\nwhich focuses on several central words and then spreads around them. In this\nwork, we apply Gaussian Mixture Model (GMM) to model the concentrated attention\nin cross-attention. Experiments and analyses we conducted on three datasets\nshow that the proposed method outperforms the baseline and has significant\nimprovement on alignment quality, N-gram accuracy, and long sentence\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Qualitative and Quantitative Analysis of Diversity in Cross-document Coreference Resolution Datasets. (arXiv:2109.05250v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05250","description":"<p>Cross-document coreference resolution (CDCR) datasets, such as ECB+, contain\nmanually annotated event-centric mentions of events and entities that form\ncoreference chains with identity relations. ECB+ is a state-of-the-art CDCR\ndataset that focuses on the resolution of events and their descriptive\nattributes, i.e., actors, location, and date-time. NewsWCL50 is a dataset that\nannotates coreference chains of both events and entities with a strong variance\nof word choice and more loosely-related coreference anaphora, e.g., bridging or\nnear-identity relations. In this paper, we qualitatively and quantitatively\ncompare annotation schemes of ECB+ and NewsWCL50 with multiple criteria. We\npropose a phrasing diversity metric (PD) that compares lexical diversity within\ncoreference chains on a more detailed level than previously proposed metric,\ne.g., a number of unique lemmas. We discuss the different tasks that both CDCR\ndatasets create, i.e., lexical disambiguation and lexical diversity challenges,\nand propose a direction for further CDCR evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"XCoref: Cross-document Coreference Resolution in the Wild. (arXiv:2109.05252v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05252","description":"<p>Datasets and methods for cross-document coreference resolution (CDCR) focus\non events or entities with strict coreference relations. They lack, however,\nannotating and resolving coreference mentions with more abstract or loose\nrelations that may occur when news articles report about controversial and\npolarized events. Bridging and loose coreference relations trigger associations\nthat may lead to exposing news readers to bias by word choice and labeling. For\nexample, coreferential mentions of \"direct talks between U.S. President Donald\nTrump and Kim\" such as \"an extraordinary meeting following months of heated\nrhetoric\" or \"great chance to solve a world problem\" form a more positive\nperception of this event. A step towards bringing awareness of bias by word\nchoice and labeling is the reliable resolution of coreferences with high\nlexical diversity. We propose an unsupervised method named XCoref, which is a\nCDCR method that capably resolves not only previously prevalent entities, such\nas persons, e.g., \"Donald Trump,\" but also abstractly defined concepts, such as\ngroups of persons, \"caravan of immigrants,\" events and actions, e.g., \"marching\nto the U.S. border.\" In an extensive evaluation, we compare the proposed XCoref\nto a state-of-the-art CDCR method and a previous method TCA that resolves such\ncomplex coreference relations and find that XCoref outperforms these methods.\nOutperforming an established CDCR model shows that the new CDCR models need to\nbe evaluated on semantically complex mentions with more loose coreference\nrelations to indicate their applicability of models to resolve mentions in the\n\"wild\" of political news articles.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhukova_A/0/1/0/all/0/1\">Anastasia Zhukova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hamborg_F/0/1/0/all/0/1\">Felix Hamborg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Donnay_K/0/1/0/all/0/1\">Karsten Donnay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gipp_B/0/1/0/all/0/1\">Bela Gipp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilingual Translation via Grafting Pre-trained Language Models. (arXiv:2109.05256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05256","description":"<p>Can pre-trained BERT for one language and GPT for another be glued together\nto translate texts? Self-supervised training using only monolingual data has\nled to the success of pre-trained (masked) language models in many NLP tasks.\nHowever, directly connecting BERT as an encoder and GPT as a decoder can be\nchallenging in machine translation, for GPT-like models lack a cross-attention\ncomponent that is needed in seq2seq decoders. In this paper, we propose\nGraformer to graft separately pre-trained (masked) language models for machine\ntranslation. With monolingual data for pre-training and parallel data for\ngrafting training, we maximally take advantage of the usage of both types of\ndata. Experiments on 60 directions show that our method achieves average\nimprovements of 5.8 BLEU in x2en and 2.9 BLEU in en2x directions comparing with\nthe multilingual Transformer of the same size.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1\">Zewei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COSMic: A Coherence-Aware Generation Metric for Image Descriptions. (arXiv:2109.05281v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05281","description":"<p>Developers of text generation models rely on automated evaluation metrics as\na stand-in for slow and expensive manual evaluations. However, image captioning\nmetrics have struggled to give accurate learned estimates of the semantic and\npragmatic success of output text. We address this weakness by introducing the\nfirst discourse-aware learned generation metric for evaluating image\ndescriptions. Our approach is inspired by computational theories of discourse\nfor capturing information goals using coherence. We present a dataset of\nimage$\\unicode{x2013}$description pairs annotated with coherence relations. We\nthen train a coherence-aware metric on a subset of the Conceptual Captions\ndataset and measure its effectiveness$\\unicode{x2014}$its ability to predict\nhuman ratings of output captions$\\unicode{x2014}$on a test set composed of\nout-of-domain images. We demonstrate a higher Kendall Correlation Coefficient\nfor our proposed metric with the human judgments for the results of a number of\nstate-of-the-art coherence-aware caption generation models when compared to\nseveral other metrics including recently proposed learned metrics such as\nBLEURT and BERTScore.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inan_M/0/1/0/all/0/1\">Mert &#x130;nan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khalid_B/0/1/0/all/0/1\">Baber Khalid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stone_M/0/1/0/all/0/1\">Matthew Stone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alikhani_M/0/1/0/all/0/1\">Malihe Alikhani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What's in a Name? Answer Equivalence For Open-Domain Question Answering. (arXiv:2109.05289v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05289","description":"<p>A flaw in QA evaluation is that annotations often only provide one gold\nanswer. Thus, model predictions semantically equivalent to the answer but\nsuperficially different are considered incorrect. This work explores mining\nalias entities from knowledge bases and using them as additional gold answers\n(i.e., equivalent answers). We incorporate answers for two settings: evaluation\nwith additional answers and model training with equivalent answers. We analyse\nthree QA benchmarks: Natural Questions, TriviaQA, and SQuAD. Answer expansion\nincreases the exact match score on all datasets for evaluation, while\nincorporating it helps model training over real-world datasets. We ensure the\nadditional answers are valid through a human post hoc evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_C/0/1/0/all/0/1\">Chenglei Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chen Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Looking for Confirmations: An Effective and Human-Like Visual Dialogue Strategy. (arXiv:2109.05312v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05312","description":"<p>Generating goal-oriented questions in Visual Dialogue tasks is a challenging\nand long-standing problem. State-Of-The-Art systems are shown to generate\nquestions that, although grammatically correct, often lack an effective\nstrategy and sound unnatural to humans. Inspired by the cognitive literature on\ninformation search and cross-situational word learning, we design Confirm-it, a\nmodel based on a beam search re-ranking algorithm that guides an effective\ngoal-oriented strategy by asking questions that confirm the model's conjecture\nabout the referent. We take the GuessWhat?! game as a case-study. We show that\ndialogues generated by Confirm-it are more natural and effective than beam\nsearch decoding without re-ranking.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Testoni_A/0/1/0/all/0/1\">Alberto Testoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bernardi_R/0/1/0/all/0/1\">Raffaella Bernardi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Bayesian Topic Regression for Causal Inference. (arXiv:2109.05317v1 [stat.ML])","link":"http://arxiv.org/abs/2109.05317","description":"<p>Causal inference using observational text data is becoming increasingly\npopular in many research areas. This paper presents the Bayesian Topic\nRegression (BTR) model that uses both text and numerical information to model\nan outcome variable. It allows estimation of both discrete and continuous\ntreatment effects. Furthermore, it allows for the inclusion of additional\nnumerical confounding factors next to text data. To this end, we combine a\nsupervised Bayesian topic model with a Bayesian regression framework and\nperform supervised representation learning for the text features jointly with\nthe regression parameter training, respecting the Frisch-Waugh-Lovell theorem.\nOur paper makes two main contributions. First, we provide a regression\nframework that allows causal inference in settings when both text and numerical\nconfounders are of relevance. We show with synthetic and semi-synthetic\ndatasets that our joint approach recovers ground truth with lower bias than any\nbenchmark model, when text and numerical features are correlated. Second,\nexperiments on two real-world datasets demonstrate that a joint and supervised\nlearning strategy also yields superior prediction results compared to\nstrategies that estimate regression weights for text and non-text features\nseparately, being even competitive with more complex deep neural networks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/stat/1/au:+Ahrens_M/0/1/0/all/0/1\">Maximilian Ahrens</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Ashwin_J/0/1/0/all/0/1\">Julian Ashwin</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Calliess_J/0/1/0/all/0/1\">Jan-Peter Calliess</a>, <a href=\"http://arxiv.org/find/stat/1/au:+Nguyen_V/0/1/0/all/0/1\">Vu Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Hatred: A Benchmark for Understanding Implicit Hate Speech. (arXiv:2109.05322v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05322","description":"<p>Hate speech has grown significantly on social media, causing serious\nconsequences for victims of all demographics. Despite much attention being paid\nto characterize and detect discriminatory speech, most work has focused on\nexplicit or overt hate speech, failing to address a more pervasive form based\non coded or indirect language. To fill this gap, this work introduces a\ntheoretically-justified taxonomy of implicit hate speech and a benchmark corpus\nwith fine-grained labels for each message and its implication. We present\nsystematic analyses of our dataset using contemporary baselines to detect and\nexplain implicit hate speech, and we discuss key features that challenge\nexisting models. This dataset will continue to serve as a useful benchmark for\nunderstanding this multifaceted issue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+ElSherief_M/0/1/0/all/0/1\">Mai ElSherief</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muchlinski_D/0/1/0/all/0/1\">David Muchlinski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anupindi_V/0/1/0/all/0/1\">Vaishnavi Anupindi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Seybolt_J/0/1/0/all/0/1\">Jordyn Seybolt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1\">Munmun De Choudhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Protect and To Serve? Analyzing Entity-Centric Framing of Police Violence. (arXiv:2109.05325v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05325","description":"<p>Framing has significant but subtle effects on public opinion and policy. We\npropose an NLP framework to measure entity-centric frames. We use it to\nunderstand media coverage on police violence in the United States in a new\nPolice Violence Frames Corpus of 82k news articles spanning 7k police killings.\nOur work uncovers more than a dozen framing devices and reveals significant\ndifferences in the way liberal and conservative news sources frame both the\nissue of police violence and the entities involved. Conservative sources\nemphasize when the victim is armed or attacking an officer and are more likely\nto mention the victim's criminal record. Liberal sources focus more on the\nunderlying systemic injustice, highlighting the victim's race and that they\nwere unarmed. We discover temporary spikes in these injustice frames near\nhigh-profile shooting events, and finally, we show protest volume correlates\nwith and precedes media framing decisions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ziems_C/0/1/0/all/0/1\">Caleb Ziems</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Objective Metric for Explainable AI: How and Why to Estimate the Degree of Explainability. (arXiv:2109.05327v1 [cs.AI])","link":"http://arxiv.org/abs/2109.05327","description":"<p>Numerous government initiatives (e.g. the EU with GDPR) are coming to the\nconclusion that the increasing complexity of modern software systems must be\ncontrasted with some Rights to Explanation and metrics for the Impact\nAssessment of these tools, that allow humans to understand and oversee the\noutput of Automated Decision Making systems. Explainable AI was born as a\npathway to allow humans to explore and understand the inner working of complex\nsystems. But establishing what is an explanation and objectively evaluating\nexplainability, are not trivial tasks. With this paper, we present a new\nmodel-agnostic metric to measure the Degree of eXplainability of correct\ninformation in an objective way, exploiting a specific model from Ordinary\nLanguage Philosophy called the Achinstein's Theory of Explanations. In order to\nunderstand whether this metric is actually behaving as explainability is\nexpected to, we designed a few experiments and a user-study on two realistic\nAI-based systems for healthcare and finance, involving famous AI technology\nincluding Artificial Neural Networks and TreeSHAP. The results we obtained are\nvery encouraging, suggesting that our proposed metric for measuring the Degree\nof eXplainability is robust on several scenarios and it can be eventually\nexploited for a lawful Impact Assessment of an Automated Decision Making\nsystem.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sovrano_F/0/1/0/all/0/1\">Francesco Sovrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vitali_F/0/1/0/all/0/1\">Fabio Vitali</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling and Acceleration of Three-dimensional Structure Determination for Single-Particle Imaging Experiments with SpiniFEL. (arXiv:2109.05339v1 [physics.comp-ph])","link":"http://arxiv.org/abs/2109.05339","description":"<p>The Linac Coherent Light Source (LCLS) is an X- ray free electron laser\n(XFEL) facility enabling the study of the structure and dynamics of single\nmacromolecules. A major upgrade will bring the repetition rate of the X-ray\nsource from 120 to 1 million pulses per second. Exascale high performance\ncomputing (HPC) capabilities will be required to process the corresponding data\nrates. We present SpiniFEL, an application used for structure determination of\nproteins from single-particle imaging (SPI) experiments. An emerging technique\nfor imaging individual proteins and other large molecular complexes by\noutrunning radiation damage, SPI breaks free from the need for crystallization\n(which is difficult for some proteins) and allows for imaging molecular\ndynamics at near ambient conditions. SpiniFEL is being developed to run on\nsupercomputers in near real-time while an experiment is taking place, so that\nthe feedback about the data can guide the data collection strategy. We describe\nhere how we reformulated the mathematical framework for parallelizable\nimplementation and accelerated the most compute intensive parts of the\napplication. We also describe the use of Pygion, a Python interface for the\nLegion task-based programming model and compare to our existing MPI+GPU\nimplementation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/physics/1/au:+Chang_H/0/1/0/all/0/1\">Hsing-Yin Chang</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Slaughter_E/0/1/0/all/0/1\">Elliott Slaughter</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Mirchandaney_S/0/1/0/all/0/1\">Seema Mirchandaney</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Donatelli_J/0/1/0/all/0/1\">Jeffrey Donatelli</a>, <a href=\"http://arxiv.org/find/physics/1/au:+Yoon_C/0/1/0/all/0/1\">Chun Hong Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"HYDRA -- Hyper Dependency Representation Attentions. (arXiv:2109.05349v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05349","description":"<p>Attention is all we need as long as we have enough data. Even so, it is\nsometimes not easy to determine how much data is enough while the models are\nbecoming larger and larger. In this paper, we propose HYDRA heads, lightweight\npretrained linguistic self-attention heads to inject knowledge into transformer\nmodels without pretraining them again. Our approach is a balanced paradigm\nbetween leaving the models to learn unsupervised and forcing them to conform to\nlinguistic knowledge rigidly as suggested in previous studies. Our experiment\nproves that the approach is not only the boost performance of the model but\nalso lightweight and architecture friendly. We empirically verify our framework\non benchmark datasets to show the contribution of linguistic knowledge to a\ntransformer model. This is a promising result for a new approach to\ntransferring knowledge from linguistic resources into transformer-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Ha-Thanh Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1\">Vu Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dang_T/0/1/0/all/0/1\">Tran-Binh Dang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_M/0/1/0/all/0/1\">Minh-Quan Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1\">Minh-Phuong Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Le-Minh Nguyen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning from Language Description: Low-shot Named Entity Recognition via Decomposed Framework. (arXiv:2109.05357v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05357","description":"<p>In this work, we study the problem of named entity recognition (NER) in a low\nresource scenario, focusing on few-shot and zero-shot settings. Built upon\nlarge-scale pre-trained language models, we propose a novel NER framework,\nnamely SpanNER, which learns from natural language supervision and enables the\nidentification of never-seen entity classes without using in-domain labeled\ndata. We perform extensive experiments on 5 benchmark datasets and evaluate the\nproposed method in the few-shot learning, domain transfer and zero-shot\nlearning settings. The experimental results show that the proposed method can\nbring 10%, 23% and 26% improvements in average over the best baselines in\nfew-shot learning, domain transfer and zero-shot learning settings\nrespectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yaqing Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1\">Haoda Chu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jing Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Implicit Premise Generation with Discourse-aware Commonsense Knowledge Models. (arXiv:2109.05358v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05358","description":"<p>Enthymemes are defined as arguments where a premise or conclusion is left\nimplicit. We tackle the task of generating the implicit premise in an\nenthymeme, which requires not only an understanding of the stated conclusion\nand premise but also additional inferences that could depend on commonsense\nknowledge. The largest available dataset for enthymemes (Habernal et al., 2018)\nconsists of 1.7k samples, which is not large enough to train a neural text\ngeneration model. To address this issue, we take advantage of a similar task\nand dataset: Abductive reasoning in narrative text (Bhagavatula et al., 2020).\nHowever, we show that simply using a state-of-the-art seq2seq model fine-tuned\non this data might not generate meaningful implicit premises associated with\nthe given enthymemes. We demonstrate that encoding discourse-aware commonsense\nduring fine-tuning improves the quality of the generated implicit premises and\noutperforms all other baselines both in automatic and human evaluations on\nthree different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1\">Tuhin Chakrabarty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1\">Aadit Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1\">Smaranda Muresan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"COMBO: State-of-the-Art Morphosyntactic Analysis. (arXiv:2109.05361v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05361","description":"<p>We introduce COMBO - a fully neural NLP system for accurate part-of-speech\ntagging, morphological analysis, lemmatisation, and (enhanced) dependency\nparsing. It predicts categorical morphosyntactic features whilst also exposes\ntheir vector representations, extracted from hidden layers. COMBO is an easy to\ninstall Python package with automatically downloadable pre-trained models for\nover 40 languages. It maintains a balance between efficiency and quality. As it\nis an end-to-end system and its modules are jointly trained, its training is\ncompetitively fast. As its models are optimised for accuracy, they achieve\noften better prediction quality than SOTA. The COMBO library is available at:\nhttps://gitlab.clarin-pl.eu/syntactic-tools/combo.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Klimaszewski_M/0/1/0/all/0/1\">Mateusz Klimaszewski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wroblewska_A/0/1/0/all/0/1\">Alina Wr&#xf3;blewska</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modular Self-Supervision for Document-Level Relation Extraction. (arXiv:2109.05362v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05362","description":"<p>Extracting relations across large text spans has been relatively\nunderexplored in NLP, but it is particularly important for high-value domains\nsuch as biomedicine, where obtaining high recall of the latest findings is\ncrucial for practical applications. Compared to conventional information\nextraction confined to short text spans, document-level relation extraction\nfaces additional challenges in both inference and learning. Given longer text\nspans, state-of-the-art neural architectures are less effective and\ntask-specific self-supervision such as distant supervision becomes very noisy.\nIn this paper, we propose decomposing document-level relation extraction into\nrelation detection and argument resolution, taking inspiration from Davidsonian\nsemantics. This enables us to incorporate explicit discourse modeling and\nleverage modular self-supervision for each sub-problem, which is less\nnoise-prone and can be further refined end-to-end via variational EM. We\nconduct a thorough evaluation in biomedical machine reading for precision\noncology, where cross-paragraph relation mentions are prevalent. Our method\noutperforms prior state of the art, such as multi-scale learning and graph\nneural networks, by over 20 absolute F1 points. The gain is particularly\npronounced among the most challenging relation instances whose arguments never\nco-occur in a paragraph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Sheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1\">Sarthak Jain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Impact of Positional Encodings on Multilingual Compression. (arXiv:2109.05388v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05388","description":"<p>In order to preserve word-order information in a non-autoregressive setting,\ntransformer architectures tend to include positional knowledge, by (for\ninstance) adding positional encodings to token embeddings. Several\nmodifications have been proposed over the sinusoidal positional encodings used\nin the original transformer architecture; these include, for instance,\nseparating position encodings and token embeddings, or directly modifying\nattention weights based on the distance between word pairs. We first show that\nsurprisingly, while these modifications tend to improve monolingual language\nmodels, none of them result in better multilingual language models. We then\nanswer why that is: Sinusoidal encodings were explicitly designed to facilitate\ncompositionality by allowing linear projections over arbitrary time steps.\nHigher variances in multilingual training distributions requires higher\ncompression, in which case, compositionality becomes indispensable. Learned\nabsolute positional encodings (e.g., in mBERT) tend to approximate sinusoidal\nembeddings in multilingual settings, but more complex positional encoding\narchitectures lack the inductive bias to effectively learn compositionality and\ncross-lingual alignment. In other words, while sinusoidal positional encodings\nwere originally designed for monolingual applications, they are particularly\nuseful in multilingual language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ravishankar_V/0/1/0/all/0/1\">Vinit Ravishankar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging Table Content for Zero-shot Text-to-SQL with Meta-Learning. (arXiv:2109.05395v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05395","description":"<p>Single-table text-to-SQL aims to transform a natural language question into a\nSQL query according to one single table. Recent work has made promising\nprogress on this task by pre-trained language models and a multi-submodule\nframework. However, zero-shot table, that is, the invisible table in the\ntraining set, is currently the most critical bottleneck restricting the\napplication of existing approaches to real-world scenarios. Although some work\nhas utilized auxiliary tasks to help handle zero-shot tables, expensive extra\nmanual annotation limits their practicality. In this paper, we propose a new\napproach for the zero-shot text-to-SQL task which does not rely on any\nadditional manual annotations. Our approach consists of two parts. First, we\npropose a new model that leverages the abundant information of table content to\nhelp establish the mapping between questions and zero-shot tables. Further, we\npropose a simple but efficient meta-learning strategy to train our model. The\nstrategy utilizes the two-step gradient update to force the model to learn a\ngeneralization ability towards zero-shot tables. We conduct extensive\nexperiments on a public open-domain text-to-SQL dataset WikiSQL and a\ndomain-specific dataset ESQL. Compared to existing approaches using the same\npre-trained model, our approach achieves significant improvements on both\ndatasets. Compared to the larger pre-trained model and the tabular-specific\npre-trained model, our approach is still competitive. More importantly, on the\nzero-shot subsets of both the datasets, our approach further increases the\nimprovements.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yongrui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1\">Xinnan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chaojie Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1\">Jian Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1\">Guilin Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Meng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Huiying Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora. (arXiv:2109.05406v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05406","description":"<p>Human conversations consist of reasonable and natural topic flows, which are\nobserved as the shifts of the mentioned concepts across utterances. Previous\nchatbots that incorporate the external commonsense knowledge graph prove that\nmodeling the concept shifts can effectively alleviate the dull and\nuninformative response dilemma. However, there still exists a gap between the\nconcept relations in the natural conversation and those in the external\ncommonsense knowledge graph, which is an issue to solve. Specifically, the\nconcept relations in the external commonsense knowledge graph are not\nintuitively built from the conversational scenario but the world knowledge,\nwhich makes them insufficient for the chatbot construction. To bridge the above\ngap, we propose the method to supply more concept relations extracted from the\nconversational corpora and reconstruct an enhanced concept graph for the\nchatbot construction. In addition, we present a novel, powerful, and fast graph\nencoding architecture named the Edge-Transformer to replace the traditional GNN\narchitecture. Experimental results on the Reddit conversation dataset indicate\nour proposed method significantly outperforms strong baseline systems and\nachieves new SOTA results. Further analysis individually proves the\neffectiveness of the enhanced concept graph and the Edge-Transformer\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_P/0/1/0/all/0/1\">Pengda Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pairwise Supervised Contrastive Learning of Sentence Representations. (arXiv:2109.05424v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05424","description":"<p>Many recent successes in sentence representation learning have been achieved\nby simply fine-tuning on the Natural Language Inference (NLI) datasets with\ntriplet loss or siamese loss. Nevertheless, they share a common weakness:\nsentences in a contradiction pair are not necessarily from different semantic\ncategories. Therefore, optimizing the semantic entailment and contradiction\nreasoning objective alone is inadequate to capture the high-level semantic\nstructure. The drawback is compounded by the fact that the vanilla siamese or\ntriplet losses only learn from individual sentence pairs or triplets, which\noften suffer from bad local optima. In this paper, we propose PairSupCon, an\ninstance discrimination based approach aiming to bridge semantic entailment and\ncontradiction understanding with high-level categorical concept encoding. We\nevaluate PairSupCon on various downstream tasks that involve understanding\nsentence semantics at different granularities. We outperform the previous\nstate-of-the-art method with $10\\%$--$13\\%$ averaged improvement on eight\nclustering tasks, and $5\\%$--$6\\%$ averaged improvement on seven semantic\ntextual similarity (STS) tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dejiao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shang-Wen Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_W/0/1/0/all/0/1\">Wei Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Henghui Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nallapati_R/0/1/0/all/0/1\">Ramesh Nallapati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arnold_A/0/1/0/all/0/1\">Andrew O. Arnold</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1\">Bing Xiang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Not All Negatives are Equal: Label-Aware Contrastive Loss for Fine-grained Text Classification. (arXiv:2109.05427v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05427","description":"<p>Fine-grained classification involves dealing with datasets with larger number\nof classes with subtle differences between them. Guiding the model to focus on\ndifferentiating dimensions between these commonly confusable classes is key to\nimproving performance on fine-grained tasks. In this work, we analyse the\ncontrastive fine-tuning of pre-trained language models on two fine-grained text\nclassification tasks, emotion classification and sentiment analysis. We\nadaptively embed class relationships into a contrastive objective function to\nhelp differently weigh the positives and negatives, and in particular,\nweighting closely confusable negatives more than less similar negative\nexamples. We find that Label-aware Contrastive Loss outperforms previous\ncontrastive methods, in the presence of larger number and/or more confusable\nclasses, and helps models to produce output distributions that are more\ndifferentiated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Suresh_V/0/1/0/all/0/1\">Varsha Suresh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ong_D/0/1/0/all/0/1\">Desmond C. Ong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are Gender-Neutral Queries Really Gender-Neutral? Mitigating Gender Bias in Image Search. (arXiv:2109.05433v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05433","description":"<p>Internet search affects people's cognition of the world, so mitigating biases\nin search results and learning fair models is imperative for social good. We\nstudy a unique gender bias in image search in this work: the search images are\noften gender-imbalanced for gender-neutral natural language queries. We\ndiagnose two typical image search models, the specialized model trained on\nin-domain datasets and the generalized representation model pre-trained on\nmassive image and text data across the internet. Both models suffer from severe\ngender bias. Therefore, we introduce two novel debiasing approaches: an\nin-processing fair sampling method to address the gender imbalance issue for\ntraining models, and a post-processing feature clipping method base on mutual\ninformation to debias multimodal representations of pre-trained models.\nExtensive experiments on MS-COCO and Flickr30K benchmarks show that our methods\nsignificantly reduce the gender bias in image search models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1\">Jialu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Eric Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"\"Let Your Characters Tell Their Story\": A Dataset for Character-Centric Narrative Understanding. (arXiv:2109.05438v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05438","description":"<p>When reading a literary piece, readers often make inferences about various\ncharacters' roles, personalities, relationships, intents, actions, etc. While\nhumans can readily draw upon their past experiences to build such a\ncharacter-centric view of the narrative, understanding characters in narratives\ncan be a challenging task for machines. To encourage research in this field of\ncharacter-centric narrative understanding, we present LiSCU -- a new dataset of\nliterary pieces and their summaries paired with descriptions of characters that\nappear in them. We also introduce two new tasks on LiSCU: Character\nIdentification and Character Description Generation. Our experiments with\nseveral pre-trained language models adapted for these tasks demonstrate that\nthere is a need for better models of narrative comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Meng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tafjord_O/0/1/0/all/0/1\">Oyvind Tafjord</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chao Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1\">Mrinmaya Sachan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Conversational Search for Online Shopping with Utterance Transfer. (arXiv:2109.05460v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05460","description":"<p>Successful conversational search systems can present natural, adaptive and\ninteractive shopping experience for online shopping customers. However,\nbuilding such systems from scratch faces real word challenges from both\nimperfect product schema/knowledge and lack of training dialog data.In this\nwork we first propose ConvSearch, an end-to-end conversational search system\nthat deeply combines the dialog system with search. It leverages the text\nprofile to retrieve products, which is more robust against imperfect product\nschema/knowledge compared with using product attributes alone. We then address\nthe lack of data challenges by proposing an utterance transfer approach that\ngenerates dialogue utterances by using existing dialog from other domains, and\nleveraging the search behavior data from e-commerce retailer. With utterance\ntransfer, we introduce a new conversational search dataset for online shopping.\nExperiments show that our utterance transfer method can significantly improve\nthe availability of training dialogue data without crowd-sourcing, and the\nconversational search system significantly outperformed the best tested\nbaseline.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1\">Liqiang Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma2_J/0/1/0/all/0/1\">Jun Ma2</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1\">Xin Luna Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martinez_Gomez_P/0/1/0/all/0/1\">Pascual Martinez-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zalmout_N/0/1/0/all/0/1\">Nasser Zalmout</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1\">Tong Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">Hao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1\">Yaohui Jin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Logic Traps in Evaluating Post-hoc Interpretations. (arXiv:2109.05463v1 [cs.LG])","link":"http://arxiv.org/abs/2109.05463","description":"<p>Post-hoc interpretation aims to explain a trained model and reveal how the\nmodel arrives at a decision. Though research on post-hoc interpretations has\ndeveloped rapidly, one growing pain in this field is the difficulty in\nevaluating interpretations. There are some crucial logic traps behind existing\nevaluation methods, which are ignored by most works. In this opinion piece, we\nsummarize four kinds evaluation methods and point out the corresponding logic\ntraps behind them. We argue that we should be clear about these traps rather\nthan ignore them and draw conclusions assertively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ju_Y/0/1/0/all/0/1\">Yiming Ju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yuanzhe Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhao Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhongtao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1\">Kang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jun Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Task Difficulty for Few-Shot Relation Extraction. (arXiv:2109.05473v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05473","description":"<p>Few-shot relation extraction (FSRE) focuses on recognizing novel relations by\nlearning with merely a handful of annotated instances. Meta-learning has been\nwidely adopted for such a task, which trains on randomly generated few-shot\ntasks to learn generic data representations. Despite impressive results\nachieved, existing models still perform suboptimally when handling hard FSRE\ntasks, where the relations are fine-grained and similar to each other. We argue\nthis is largely because existing models do not distinguish hard tasks from easy\nones in the learning process. In this paper, we introduce a novel approach\nbased on contrastive learning that learns better representations by exploiting\nrelation label information. We further design a method that allows the model to\nadaptively learn how to focus on hard tasks. Experiments on two standard\ndatasets demonstrate the effectiveness of our method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1\">Jiale Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1\">Bo Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Stylistic Retrieval-based Dialogue System with Unparallel Training Data. (arXiv:2109.05477v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05477","description":"<p>The ability of a dialog system to express consistent language style during\nconversations has a direct, positive impact on its usability and on user\nsatisfaction. Although previous studies have demonstrated that style transfer\nis feasible with a large amount of parallel data, it is often impossible to\ncollect such data for different styles. In this paper, instead of manually\nconstructing conversation data with a certain style, we propose a flexible\nframework that adapts a generic retrieval-based dialogue system to mimic the\nlanguage style of a specified persona without any parallel data. Our approach\nis based on automatic generation of stylized data by learning the usage of\njargon, and then rewriting the generic conversations to a stylized one by\nincorporating the jargon. In experiments we implemented dialogue systems with\nfive distinct language styles, and the result shows our framework significantly\noutperforms baselines in terms of the average score of responses' relevance and\nstyle degree, and content diversity. A/B testing on a commercial chatbot shows\nthat users are more satisfied with our system. This study demonstrates the\nfeasibility of building stylistic dialogue systems by simple data augmentation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1\">Hao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1\">Ruihua Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1\">Tianran Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1\">Jianyun Nie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Enhanced Fine-Tuning for Better Handling Unseen Entities in Dialogue Generation. (arXiv:2109.05487v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05487","description":"<p>Although pre-training models have achieved great success in dialogue\ngeneration, their performance drops dramatically when the input contains an\nentity that does not appear in pre-training and fine-tuning datasets (unseen\nentity). To address this issue, existing methods leverage an external knowledge\nbase to generate appropriate responses. In real-world scenario, the entity may\nnot be included by the knowledge base or suffer from the precision of knowledge\nretrieval. To deal with this problem, instead of introducing knowledge base as\nthe input, we force the model to learn a better semantic representation by\npredicting the information in the knowledge base, only based on the input\ncontext. Specifically, with the help of a knowledge base, we introduce two\nauxiliary training objectives: 1) Interpret Masked Word, which conjectures the\nmeaning of the masked entity given the context; 2) Hypernym Generation, which\npredicts the hypernym of the entity based on the context. Experiment results on\ntwo dialogue corpus verify the effectiveness of our methods under both\nknowledge available and unavailable settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Leyang Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yue Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Domain Adaptation Schemes for Building ASR in Low-resource Languages. (arXiv:2109.05494v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05494","description":"<p>Building an automatic speech recognition (ASR) system from scratch requires a\nlarge amount of annotated speech data, which is difficult to collect in many\nlanguages. However, there are cases where the low-resource language shares a\ncommon acoustic space with a high-resource language having enough annotated\ndata to build an ASR. In such cases, we show that the domain-independent\nacoustic models learned from the high-resource language through unsupervised\ndomain adaptation (UDA) schemes can enhance the performance of the ASR in the\nlow-resource language. We use the specific example of Hindi in the source\ndomain and Sanskrit in the target domain. We explore two architectures: i)\ndomain adversarial training using gradient reversal layer (GRL) and ii) domain\nseparation networks (DSN). The GRL and DSN architectures give absolute\nimprovements of 6.71% and 7.32%, respectively, in word error rate over the\nbaseline deep neural network model when trained on just 5.5 hours of data in\nthe target domain. We also show that choosing a proper language (Telugu) in the\nsource domain can bring further improvement. The results suggest that UDA\nschemes can be helpful in the development of ASR systems for low-resource\nlanguages, mitigating the hassle of collecting large amounts of annotated\nspeech data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1\">Anoop C S</a>, <a href=\"http://arxiv.org/find/cs/1/au:+P_P/0/1/0/all/0/1\">Prathosh A P</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1\">A G Ramakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TEASEL: A Transformer-Based Speech-Prefixed Language Model. (arXiv:2109.05522v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05522","description":"<p>Multimodal language analysis is a burgeoning field of NLP that aims to\nsimultaneously model a speaker's words, acoustical annotations, and facial\nexpressions. In this area, lexicon features usually outperform other modalities\nbecause they are pre-trained on large corpora via Transformer-based models.\nDespite their strong performance, training a new self-supervised learning (SSL)\nTransformer on any modality is not usually attainable due to insufficient data,\nwhich is the case in multimodal language learning. This work proposes a\nTransformer-Based Speech-Prefixed Language Model called TEASEL to approach the\nmentioned constraints without training a complete Transformer model. TEASEL\nmodel includes speech modality as a dynamic prefix besides the textual modality\ncompared to a conventional language model. This method exploits a conventional\npre-trained language model as a cross-modal Transformer model. We evaluated\nTEASEL for the multimodal sentiment analysis task defined by CMU-MOSI dataset.\nExtensive experiments show that our model outperforms unimodal baseline\nlanguage models by 4% and outperforms the current multimodal state-of-the-art\n(SoTA) model by 1% in F1-score. Additionally, our proposed method is 72%\nsmaller than the SoTA model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arjmand_M/0/1/0/all/0/1\">Mehdi Arjmand</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dousti_M/0/1/0/all/0/1\">Mohammad Javad Dousti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moradi_H/0/1/0/all/0/1\">Hadi Moradi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Constructing Phrase-level Semantic Labels to Form Multi-Grained Supervision for Image-Text Retrieval. (arXiv:2109.05523v1 [cs.CV])","link":"http://arxiv.org/abs/2109.05523","description":"<p>Existing research for image text retrieval mainly relies on sentence-level\nsupervision to distinguish matched and mismatched sentences for a query image.\nHowever, semantic mismatch between an image and sentences usually happens in\nfiner grain, i.e., phrase level. In this paper, we explore to introduce\nadditional phrase-level supervision for the better identification of mismatched\nunits in the text. In practice, multi-grained semantic labels are automatically\nconstructed for a query image in both sentence-level and phrase-level. We\nconstruct text scene graphs for the matched sentences and extract entities and\ntriples as the phrase-level labels. In order to integrate both supervision of\nsentence-level and phrase-level, we propose Semantic Structure Aware Multimodal\nTransformer (SSAMT) for multi-modal representation learning. Inside the SSAMT,\nwe utilize different kinds of attention mechanisms to enforce interactions of\nmulti-grain semantic units in both sides of vision and language. For the\ntraining, we propose multi-scale matching losses from both global and local\nperspectives, and penalize mismatched phrases. Experimental results on MS-COCO\nand Flickr30K show the effectiveness of our approach compared to some\nstate-of-the-art models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1\">Zhihao Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zejun Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1\">Haijun Shan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1\">Jianqing Fan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Extracting Event Temporal Relations via Hyperbolic Geometry. (arXiv:2109.05527v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05527","description":"<p>Detecting events and their evolution through time is a crucial task in\nnatural language understanding. Recent neural approaches to event temporal\nrelation extraction typically map events to embeddings in the Euclidean space\nand train a classifier to detect temporal relations between event pairs.\nHowever, embeddings in the Euclidean space cannot capture richer asymmetric\nrelations such as event temporal relations. We thus propose to embed events\ninto hyperbolic spaces, which are intrinsically oriented at modeling\nhierarchical structures. We introduce two approaches to encode events and their\ntemporal relations in hyperbolic spaces. One approach leverages hyperbolic\nembeddings to directly infer event relations through simple geometrical\noperations. In the second one, we devise an end-to-end architecture composed of\nhyperbolic neural units tailored for the temporal relation extraction task.\nThorough experimental assessments on widely used datasets have shown the\nbenefits of revisiting the tasks on a different geometrical space, resulting in\nstate-of-the-art performance on several standard metrics. Finally, the ablation\nstudy and several qualitative analyses highlighted the rich event semantics\nimplicitly encoded into hyperbolic spaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1\">Xingwei Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pergola_G/0/1/0/all/0/1\">Gabriele Pergola</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1\">Yulan He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Good-Enough Example Extrapolation. (arXiv:2109.05602v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05602","description":"<p>This paper asks whether extrapolating the hidden space distribution of text\nexamples from one class onto another is a valid inductive bias for data\naugmentation. To operationalize this question, I propose a simple data\naugmentation protocol called \"good-enough example extrapolation\" (GE3). GE3 is\nlightweight and has no hyperparameters. Applied to three text classification\ndatasets for various data imbalance scenarios, GE3 improves performance more\nthan upsampling and other hidden-space data augmentation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1\">Jason Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Levenshtein Training for Word-level Quality Estimation. (arXiv:2109.05611v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05611","description":"<p>We propose a novel scheme to use the Levenshtein Transformer to perform the\ntask of word-level quality estimation. A Levenshtein Transformer is a natural\nfit for this task: trained to perform decoding in an iterative manner, a\nLevenshtein Transformer can learn to post-edit without explicit supervision. To\nfurther minimize the mismatch between the translation task and the word-level\nQE task, we propose a two-stage transfer learning procedure on both augmented\ndata and human post-editing data. We also propose heuristics to construct\nreference labels that are compatible with subword-level finetuning and\ninference. Results on WMT 2020 QE shared task dataset show that our proposed\nmethod has superior data efficiency under the data-constrained setting and\ncompetitive performance under the unconstrained setting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1\">Shuoyang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Post_M/0/1/0/all/0/1\">Matt Post</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1\">Philipp Koehn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RockNER: A Simple Method to Create Adversarial Examples for Evaluating the Robustness of Named Entity Recognition Models. (arXiv:2109.05620v1 [cs.CL])","link":"http://arxiv.org/abs/2109.05620","description":"<p>To audit the robustness of named entity recognition (NER) models, we propose\nRockNER, a simple yet effective method to create natural adversarial examples.\nSpecifically, at the entity level, we replace target entities with other\nentities of the same semantic class in Wikidata; at the context level, we use\npre-trained language models (e.g., BERT) to generate word substitutions.\nTogether, the two levels of attack produce natural adversarial examples that\nresult in a shifted distribution from the training data on which our target\nmodels have been trained. We apply the proposed method to the OntoNotes dataset\nand create a new benchmark named OntoRock for evaluating the robustness of\nexisting NER models via a systematic evaluation protocol. Our experiments and\nanalysis reveal that even the best model has a significant performance drop,\nand these models seem to memorize in-domain entity patterns instead of\nreasoning from the context. Our work also studies the effects of a few simple\ndata augmentation methods to improve the robustness of NER models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1\">Wenyang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1\">Jun Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_R/0/1/0/all/0/1\">Ryan Moreno</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DialBERT: A Hierarchical Pre-Trained Model for Conversation Disentanglement. (arXiv:2004.03760v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.03760","description":"<p>Disentanglement is a problem in which multiple conversations occur in the\nsame channel simultaneously, and the listener should decide which utterance is\npart of the conversation he will respond to. We propose a new model, named\nDialogue BERT (DialBERT), which integrates local and global semantics in a\nsingle stream of messages to disentangle the conversations that mixed together.\nWe employ BERT to capture the matching information in each utterance pair at\nthe utterance-level, and use a BiLSTM to aggregate and incorporate the\ncontext-level information. With only a 3% increase in parameters, a 12%\nimprovement has been attained in comparison to BERT, based on the F1-Score. The\nmodel achieves a state-of-the-art result on the a new dataset proposed by IBM\nand surpasses previous work by a substantial margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tianda Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1\">Jia-Chen Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1\">Zhen-Hua Ling</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_Z/0/1/0/all/0/1\">Zhiming Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1\">Si Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00033","description":"<p>With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic has been\ndeclared one of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Ad-dressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nTo address this gap, we release a large dataset of 16K manually annotated\ntweets for fine-grained disinformation analysis that (i) focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society, and (iii) covers Arabic,\nBulgarian, Dutch, and English. Finally, we show strong evaluation results using\npretrained Transformers, thus con-firming the practical utility of the dataset\nin monolingual vs. multilingual, and single task vs. multitask settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1\">Alex Nikolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MedLatinEpi and MedLatinLit: Two Datasets for the Computational Authorship Analysis of Medieval Latin Texts. (arXiv:2006.12289v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2006.12289","description":"<p>We present and make available MedLatinEpi and MedLatinLit, two datasets of\nmedieval Latin texts to be used in research on computational authorship\nanalysis. MedLatinEpi and MedLatinLit consist of 294 and 30 curated texts,\nrespectively, labelled by author; MedLatinEpi texts are of epistolary nature,\nwhile MedLatinLit texts consist of literary comments and treatises about\nvarious subjects. As such, these two datasets lend themselves to supporting\nresearch in authorship analysis tasks, such as authorship attribution,\nauthorship verification, or same-author verification. Along with the datasets\nwe provide experimental results, obtained on these datasets, for the authorship\nverification task, i.e., the task of predicting whether a text of unknown\nauthorship was written by a candidate author or not. We also make available the\nsource code of the authorship verification system we have used, thus allowing\nour experiments to be reproduced, and to be used as baselines, by other\nresearchers. We also describe the application of the above authorship\nverification system, using these datasets as training data, for investigating\nthe authorship of two medieval epistles whose authorship has been disputed by\nscholars.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Corbara_S/0/1/0/all/0/1\">Silvia Corbara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreo_A/0/1/0/all/0/1\">Alejandro Moreo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sebastiani_F/0/1/0/all/0/1\">Fabrizio Sebastiani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tavoni_M/0/1/0/all/0/1\">Mirko Tavoni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"OSCaR: Orthogonal Subspace Correction and Rectification of Biases in Word Embeddings. (arXiv:2007.00049v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.00049","description":"<p>Language representations are known to carry stereotypical biases and, as a\nresult, lead to biased predictions in downstream tasks. While existing methods\nare effective at mitigating biases by linear projection, such methods are too\naggressive: they not only remove bias, but also erase valuable information from\nword embeddings. We develop new measures for evaluating specific information\nretention that demonstrate the tradeoff between bias removal and information\nretention. To address this challenge, we propose OSCaR (Orthogonal Subspace\nCorrection and Rectification), a bias-mitigating method that focuses on\ndisentangling biased associations between concepts instead of removing concepts\nwholesale. Our experiments on gender biases show that OSCaR is a well-balanced\napproach that ensures that semantic information is retained in the embeddings\nand bias is also effectively mitigated.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1\">Tao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1\">Vivek Srikumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Robustness and Bias Analysis of BERT-based Relation Extraction. (arXiv:2009.06206v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.06206","description":"<p>Fine-tuning pre-trained models have achieved impressive performance on\nstandard natural language processing benchmarks. However, the resultant model\ngeneralizability remains poorly understood. We do not know, for example, how\nexcellent performance can lead to the perfection of generalization models. In\nthis study, we analyze a fine-tuned BERT model from different perspectives\nusing relation extraction. We also characterize the differences in\ngeneralization techniques according to our proposed improvements. From\nempirical experimentation, we find that BERT suffers a bottleneck in terms of\nrobustness by way of randomizations, adversarial and counterfactual tests, and\nbiases (i.e., selection and semantic). These findings highlight opportunities\nfor future improvements. Our open-sourced testbed DiagnoseRE is available in\n\\url{https://github.com/zjunlp/DiagnoseRE}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Luoqiu Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Hongbin Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1\">Zhen Bi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GraphCodeBERT: Pre-training Code Representations with Data Flow. (arXiv:2009.08366v4 [cs.SE] UPDATED)","link":"http://arxiv.org/abs/2009.08366","description":"<p>Pre-trained models for programming language have achieved dramatic empirical\nimprovements on a variety of code-related tasks such as code search, code\ncompletion, code summarization, etc. However, existing pre-trained models\nregard a code snippet as a sequence of tokens, while ignoring the inherent\nstructure of code, which provides crucial code semantics and would enhance the\ncode understanding process. We present GraphCodeBERT, a pre-trained model for\nprogramming language that considers the inherent structure of code. Instead of\ntaking syntactic-level structure of code like abstract syntax tree (AST), we\nuse data flow in the pre-training stage, which is a semantic-level structure of\ncode that encodes the relation of \"where-the-value-comes-from\" between\nvariables. Such a semantic-level structure is neat and does not bring an\nunnecessarily deep hierarchy of AST, the property of which makes the model more\nefficient. We develop GraphCodeBERT based on Transformer. In addition to using\nthe task of masked language modeling, we introduce two structure-aware\npre-training tasks. One is to predict code structure edges, and the other is to\nalign representations between source code and code structure. We implement the\nmodel in an efficient way with a graph-guided masked attention function to\nincorporate the code structure. We evaluate our model on four tasks, including\ncode search, clone detection, code translation, and code refinement. Results\nshow that code structure and newly introduced pre-training tasks can improve\nGraphCodeBERT and achieves state-of-the-art performance on the four downstream\ntasks. We further show that the model prefers structure-level attentions over\ntoken-level attentions in the task of code search.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1\">Daya Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuo Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1\">Shuai Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1\">Zhangyin Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1\">Duyu Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Long Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Svyatkovskiy_A/0/1/0/all/0/1\">Alexey Svyatkovskiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_S/0/1/0/all/0/1\">Shengyu Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tufano_M/0/1/0/all/0/1\">Michele Tufano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shao Kun Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clement_C/0/1/0/all/0/1\">Colin Clement</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Drain_D/0/1/0/all/0/1\">Dawn Drain</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sundaresan_N/0/1/0/all/0/1\">Neel Sundaresan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1\">Jian Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1\">Ming Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Program Enhanced Fact Verification with Verbalization and Graph Attention Network. (arXiv:2010.03084v6 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2010.03084","description":"<p>Performing fact verification based on structured data is important for many\nreal-life applications and is a challenging research problem, particularly when\nit involves both symbolic operations and informal inference based on language\nunderstanding. In this paper, we present a Program-enhanced Verbalization and\nGraph Attention Network (ProgVGAT) to integrate programs and execution into\ntextual inference models. Specifically, a verbalization with program execution\nmodel is proposed to accumulate evidences that are embedded in operations over\nthe tables. Built on that, we construct the graph attention verification\nnetworks, which are designed to fuse different sources of evidences from\nverbalized program execution, program structures, and the original statements\nand tables, to make the final verification decision. To support the above\nframework, we propose a program selection module optimized with a new training\nstrategy based on margin loss, to produce more accurate programs, which is\nshown to be effective in enhancing the final verification results. Experimental\nresults show that the proposed framework achieves the new state-of-the-art\nperformance, a 74.4% accuracy, on the benchmark dataset TABFACT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xiaoyu Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_F/0/1/0/all/0/1\">Feng Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yufei Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Quan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhigang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DuoRAT: Towards Simpler Text-to-SQL Models. (arXiv:2010.11119v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.11119","description":"<p>Recent neural text-to-SQL models can effectively translate natural language\nquestions to corresponding SQL queries on unseen databases. Working mostly on\nthe Spider dataset, researchers have proposed increasingly sophisticated\nsolutions to the problem. Contrary to this trend, in this paper we focus on\nsimplifications. We begin by building DuoRAT, a re-implementation of the\nstate-of-the-art RAT-SQL model that unlike RAT-SQL is using only relation-aware\nor vanilla transformers as the building blocks. We perform several ablation\nexperiments using DuoRAT as the baseline model. Our experiments confirm the\nusefulness of some techniques and point out the redundancy of others, including\nstructural SQL features and features that link the question with the schema.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Scholak_T/0/1/0/all/0/1\">Torsten Scholak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Raymond Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bahdanau_D/0/1/0/all/0/1\">Dzmitry Bahdanau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vries_H/0/1/0/all/0/1\">Harm de Vries</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pal_C/0/1/0/all/0/1\">Chris Pal</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Unsupervised Paraphrasing with Pretrained Language Models. (arXiv:2010.12885v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2010.12885","description":"<p>Paraphrase generation has benefited extensively from recent progress in the\ndesigning of training objectives and model architectures. However, previous\nexplorations have largely focused on supervised methods, which require a large\namount of labeled data that is costly to collect. To address this drawback, we\nadopt a transfer learning approach and propose a training pipeline that enables\npre-trained language models to generate high-quality paraphrases in an\nunsupervised setting. Our recipe consists of task-adaptation, self-supervision,\nand a novel decoding algorithm named Dynamic Blocking (DB). To enforce a\nsurface form dissimilar from the input, whenever the language model emits a\ntoken contained in the source sequence, DB prevents the model from outputting\nthe subsequent source token for the next generation step. We show with\nautomatic and human evaluations that our approach achieves state-of-the-art\nperformance on both the Quora Question Pair (QQP) and the ParaNMT datasets and\nis robust to domain shift between the two datasets of distinct distributions.\nWe also demonstrate that our model transfers to paraphrasing in other languages\nwithout any additional finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1\">Tong Niu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keskar_N/0/1/0/all/0/1\">Nitish Shirish Keskar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Huan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-task Language Modeling for Improving Speech Recognition of Rare Words. (arXiv:2011.11715v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.11715","description":"<p>End-to-end automatic speech recognition (ASR) systems are increasingly\npopular due to their relative architectural simplicity and competitive\nperformance. However, even though the average accuracy of these systems may be\nhigh, the performance on rare content words often lags behind hybrid ASR\nsystems. To address this problem, second-pass rescoring is often applied\nleveraging upon language modeling. In this paper, we propose a second-pass\nsystem with multi-task learning, utilizing semantic targets (such as intent and\nslot prediction) to improve speech recognition performance. We show that our\nrescoring model trained with these additional tasks outperforms the baseline\nrescoring model, trained with only the language modeling task, by 1.4% on a\ngeneral test and by 2.6% on a rare word test set in terms of word-error-rate\nrelative (WERR). Our best ASR system with multi-task LM shows 4.6% WERR\ndeduction compared with RNN Transducer only ASR baseline for rare words\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1\">Chao-Han Huck Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Linda Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_A/0/1/0/all/0/1\">Ankur Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yile Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raju_A/0/1/0/all/0/1\">Anirudh Raju</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filimonov_D/0/1/0/all/0/1\">Denis Filimonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bulyko_I/0/1/0/all/0/1\">Ivan Bulyko</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Guided Image Captioning Performance across Domains. (arXiv:2012.02339v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2012.02339","description":"<p>Image captioning models generally lack the capability to take into account\nuser interest, and usually default to global descriptions that try to balance\nreadability, informativeness, and information overload. On the other hand, VQA\nmodels generally lack the ability to provide long descriptive answers, while\nexpecting the textual question to be quite precise. We present a method to\ncontrol the concepts that an image caption should focus on, using an additional\ninput called the guiding text that refers to either groundable or ungroundable\nconcepts in the image. Our model consists of a Transformer-based multimodal\nencoder that uses the guiding text together with global and object-level image\nfeatures to derive early-fusion representations used to generate the guided\ncaption. While models trained on Visual Genome data have an in-domain advantage\nof fitting well when guided with automatic object labels, we find that guided\ncaptioning models trained on Conceptual Captions generalize better on\nout-of-domain images and guiding texts. Our human-evaluation results indicate\nthat attempting in-the-wild guided image captioning requires access to large,\nunrestricted-domain training datasets, and that increased style diversity (even\nwithout increasing the number of unique tokens) is a key factor for improved\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1\">Bo Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1\">Piyush Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soricut_R/0/1/0/all/0/1\">Radu Soricut</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Sense Language Modelling. (arXiv:2012.05776v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.05776","description":"<p>The effectiveness of a language model is influenced by its token\nrepresentations, which must encode contextual information and handle the same\nword form having a plurality of meanings (polysemy). Currently, none of the\ncommon language modelling architectures explicitly model polysemy. We propose a\nlanguage model which not only predicts the next word, but also its sense in\ncontext. We argue that this higher prediction granularity may be useful for end\ntasks such as assistive writing, and allow for more a precise linking of\nlanguage models with knowledge bases. We find that multi-sense language\nmodelling requires architectures that go beyond standard language models, and\nhere propose a structured prediction framework that decomposes the task into a\nword followed by a sense prediction task. To aid sense prediction, we utilise a\nGraph Attention Network, which encodes definitions and example uses of word\nsenses. Overall, we find that multi-sense language modelling is a highly\nchallenging task, and suggest that future work focus on the creation of more\nannotated training datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lekkas_A/0/1/0/all/0/1\">Andrea Lekkas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schneider_Kamp_P/0/1/0/all/0/1\">Peter Schneider-Kamp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YASO: A Targeted Sentiment Analysis Evaluation Dataset for Open-Domain Reviews. (arXiv:2012.14541v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.14541","description":"<p>Current TSA evaluation in a cross-domain setup is restricted to the small set\nof review domains available in existing datasets. Such an evaluation is\nlimited, and may not reflect true performance on sites like Amazon or Yelp that\nhost diverse reviews from many domains. To address this gap, we present YASO -\na new TSA evaluation dataset of open-domain user reviews. YASO contains 2,215\nEnglish sentences from dozens of review domains, annotated with target terms\nand their sentiment. Our analysis verifies the reliability of these\nannotations, and explores the characteristics of the collected data. Benchmark\nresults using five contemporary TSA systems show there is ample room for\nimprovement on this challenging new dataset. YASO is available at\nhttps://github.com/IBM/yaso-tsa.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Orbach_M/0/1/0/all/0/1\">Matan Orbach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toledo_Ronen_O/0/1/0/all/0/1\">Orith Toledo-Ronen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Spector_A/0/1/0/all/0/1\">Artem Spector</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aharonov_R/0/1/0/all/0/1\">Ranit Aharonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Katz_Y/0/1/0/all/0/1\">Yoav Katz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Slonim_N/0/1/0/all/0/1\">Noam Slonim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15283","description":"<p>While pre-trained language models (PTLMs) have achieved noticeable success on\nmany NLP tasks, they still struggle for tasks that require event temporal\nreasoning, which is essential for event-centric applications. We present a\ncontinual pre-training approach that equips PTLMs with targeted knowledge about\nevent temporal relations. We design self-supervised learning objectives to\nrecover masked-out event and temporal indicators and to discriminate sentences\nfrom their corrupted counterparts (where event or temporal indicators got\nreplaced). By further pre-training a PTLM with these objectives jointly, we\nreinforce its attention to event and temporal information, yielding enhanced\ncapability on event temporal reasoning. This effective continual pre-training\nframework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning\nperformances across five relation extraction and question answering tasks and\nachieves new or on-par state-of-the-art performances in most of our downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning From How Human Correct For Data-Centric Deep Learning. (arXiv:2102.00225v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.00225","description":"<p>In industry NLP application, our manually labeled data has a certain number\nof noisy data. We present a simple method to find the noisy data and relabel\nthem manually, meanwhile we collect the correction information. Then we present\nnovel method to incorporate the human correction information into deep learning\nmodel. Human know how to correct noisy data. So the correction information can\nbe inject into deep learning model. We do the experiment on our own text\nclassification dataset, which is manually labeled, because we relabel the noisy\ndata in our dataset for our industry application. The experiment result shows\nthat our method improve the classification accuracy from 91.7% to 92.5%. The\n91.7% baseline is based on BERT training on the corrected dataset, which is\nhard to surpass.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1\">Tong Guo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Do Transformer Modifications Transfer Across Implementations and Applications?. (arXiv:2102.11972v2 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2102.11972","description":"<p>The research community has proposed copious modifications to the Transformer\narchitecture since it was introduced over three years ago, relatively few of\nwhich have seen widespread adoption. In this paper, we comprehensively evaluate\nmany of these modifications in a shared experimental setting that covers most\nof the common uses of the Transformer in natural language processing.\nSurprisingly, we find that most modifications do not meaningfully improve\nperformance. Furthermore, most of the Transformer variants we found beneficial\nwere either developed in the same codebase that we used or are relatively minor\nchanges. We conjecture that performance improvements may strongly depend on\nimplementation details and correspondingly make some recommendations for\nimproving the generality of experimental results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Narang_S/0/1/0/all/0/1\">Sharan Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1\">Hyung Won Chung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1\">Yi Tay</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1\">William Fedus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fevry_T/0/1/0/all/0/1\">Thibault Fevry</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matena_M/0/1/0/all/0/1\">Michael Matena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malkan_K/0/1/0/all/0/1\">Karishma Malkan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1\">Noah Fiedel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1\">Zhenzhong Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yanqi Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1\">Wei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1\">Nan Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marcus_J/0/1/0/all/0/1\">Jake Marcus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_A/0/1/0/all/0/1\">Adam Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages. (arXiv:2103.14583v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14583","description":"<p>Pre-trained speech representations like wav2vec 2.0 are a powerful tool for\nautomatic speech recognition (ASR). Yet many endangered languages lack\nsufficient data for pre-training such models, or are predominantly oral\nvernaculars without a standardised writing system, precluding fine-tuning.\nQuery-by-example spoken term detection (QbE-STD) offers an alternative for\niteratively indexing untranscribed speech corpora by locating spoken query\nterms. Using data from 7 Australian Aboriginal languages and a regional variety\nof Dutch, all of which are endangered or vulnerable, we show that QbE-STD can\nbe improved by leveraging representations developed for ASR (wav2vec 2.0: the\nEnglish monolingual model and XLSR53 multilingual model). Surprisingly, the\nEnglish model outperformed the multilingual model on 4 Australian language\ndatasets, raising questions around how to optimally leverage self-supervised\nspeech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0\nrepresentations (either English or XLSR53) offer large improvements (56-86%\nrelative) over state-of-the-art approaches on our endangered language datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Browne_M/0/1/0/all/0/1\">Mitchell Browne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_L/0/1/0/all/0/1\">Lily Clifford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibson_F/0/1/0/all/0/1\">Fiona Gibson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_J/0/1/0/all/0/1\">John Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_D/0/1/0/all/0/1\">David Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">Jane Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turpin_M/0/1/0/all/0/1\">Myfany Turpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollmer_M/0/1/0/all/0/1\">Maria Vollmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilmoth_S/0/1/0/all/0/1\">Sasha Wilmoth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Convex Aggregation for Opinion Summarization. (arXiv:2104.01371v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01371","description":"<p>Recent advances in text autoencoders have significantly improved the quality\nof the latent space, which enables models to generate grammatical and\nconsistent text from aggregated latent vectors. As a successful application of\nthis property, unsupervised opinion summarization models generate a summary by\ndecoding the aggregated latent vectors of inputs. More specifically, they\nperform the aggregation via simple average. However, little is known about how\nthe vector aggregation step affects the generation quality. In this study, we\nrevisit the commonly used simple average approach by examining the latent space\nand generated summaries. We found that text autoencoders tend to generate\noverly generic summaries from simply averaged latent vectors due to an\nunexpected $L_2$-norm shrinkage in the aggregated latent vectors, which we\nrefer to as summary vector degeneration. To overcome this issue, we develop a\nframework Coop, which searches input combinations for the latent vector\naggregation using input-output word overlap. Experimental results show that\nCoop successfully alleviates the summary vector degeneration issue and\nestablishes new state-of-the-art performance on two opinion summarization\nbenchmarks. Code is available at \\url{https://github.com/megagonlabs/coop}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1\">Hayate Iso</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xiaolan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suhara_Y/0/1/0/all/0/1\">Yoshihiko Suhara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Angelidis_S/0/1/0/all/0/1\">Stefanos Angelidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1\">Wang-Chiew Tan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Role of BERT Token Representations to Explain Sentence Probing Results. (arXiv:2104.01477v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.01477","description":"<p>Several studies have been carried out on revealing linguistic features\ncaptured by BERT. This is usually achieved by training a diagnostic classifier\non the representations obtained from different layers of BERT. The subsequent\nclassification accuracy is then interpreted as the ability of the model in\nencoding the corresponding linguistic property. Despite providing insights,\nthese studies have left out the potential role of token representations. In\nthis paper, we provide a more in-depth analysis on the representation space of\nBERT in search for distinct and meaningful subspaces that can explain the\nreasons behind these probing results. Based on a set of probing tasks and with\nthe help of attribution methods we show that BERT tends to encode meaningful\nknowledge in specific token representations (which are often ignored in\nstandard classification setups), allowing the model to detect syntactic and\nsemantic abnormalities, and to distinctively separate grammatical number and\ntense subspaces.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohebbi_H/0/1/0/all/0/1\">Hosein Mohebbi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Modarressi_A/0/1/0/all/0/1\">Ali Modarressi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pushing the Limits of Non-Autoregressive Speech Recognition. (arXiv:2104.03416v4 [eess.AS] UPDATED)","link":"http://arxiv.org/abs/2104.03416","description":"<p>We combine recent advancements in end-to-end speech recognition to\nnon-autoregressive automatic speech recognition. We push the limits of\nnon-autoregressive state-of-the-art results for multiple datasets: LibriSpeech,\nFisher+Switchboard and Wall Street Journal. Key to our recipe, we leverage CTC\non giant Conformer neural network architectures with SpecAugment and wav2vec2\npre-training. We achieve 1.8%/3.6% WER on LibriSpeech test/test-other sets,\n5.1%/9.8% WER on Switchboard, and 3.4% on the Wall Street Journal, all without\na language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Ng_E/0/1/0/all/0/1\">Edwin G. Ng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chiu_C/0/1/0/all/0/1\">Chung-Cheng Chiu</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chan_W/0/1/0/all/0/1\">William Chan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Reducing Discontinuous to Continuous Parsing with Pointer Network Reordering. (arXiv:2104.06239v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06239","description":"<p>Discontinuous constituent parsers have always lagged behind continuous\napproaches in terms of accuracy and speed, as the presence of constituents with\ndiscontinuous yield introduces extra complexity to the task. However, a\ndiscontinuous tree can be converted into a continuous variant by reordering\ntokens. Based on that, we propose to reduce discontinuous parsing to a\ncontinuous problem, which can then be directly solved by any off-the-shelf\ncontinuous parser. To that end, we develop a Pointer Network capable of\naccurately generating the continuous token arrangement for a given input\nsentence and define a bijective function to recover the original order.\nExperiments on the main benchmarks with two continuous parsers prove that our\napproach is on par in accuracy with purely discontinuous state-of-the-art\nalgorithms, but considerably faster.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fernandez_Gonzalez_D/0/1/0/all/0/1\">Daniel Fern&#xe1;ndez-Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gomez_Rodriguez_C/0/1/0/all/0/1\">Carlos G&#xf3;mez-Rodr&#xed;guez</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Enhancing Interpretable Clauses Semantically using Pretrained Word Representation. (arXiv:2104.06901v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06901","description":"<p>Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based\non propositional logic, which has demonstrated competitive performance in many\nNatural Language Processing (NLP) tasks, including sentiment analysis, text\nclassification, and Word Sense Disambiguation. To obtain human-level\ninterpretability, legacy TM employs Boolean input features such as bag-of-words\n(BOW). However, the BOW representation makes it difficult to use any\npre-trained information, for instance, word2vec and GloVe word representations.\nThis restriction has constrained the performance of TM compared to deep neural\nnetworks (DNNs) in NLP. To reduce the performance gap, in this paper, we\npropose a novel way of using pre-trained word representations for TM. The\napproach significantly enhances the performance and interpretability of TM. We\nachieve this by extracting semantically related words from pre-trained word\nrepresentations as input features to the TM. Our experiments show that the\naccuracy of the proposed approach is significantly higher than the previous\nBOW-based TM, reaching the level of DNN-based models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yadav_R/0/1/0/all/0/1\">Rohan Kumar Yadav</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1\">Lei Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Granmo_O/0/1/0/all/0/1\">Ole-Christoffer Granmo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1\">Morten Goodwin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Disentangling Representations of Text by Masking Transformers. (arXiv:2104.07155v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07155","description":"<p>Representations from large pretrained models such as BERT encode a range of\nfeatures into monolithic vectors, affording strong predictive accuracy across a\nmultitude of downstream tasks. In this paper we explore whether it is possible\nto learn disentangled representations by identifying existing subnetworks\nwithin pretrained models that encode distinct, complementary aspect\nrepresentations. Concretely, we learn binary masks over transformer weights or\nhidden units to uncover subsets of features that correlate with a specific\nfactor of variation; this eliminates the need to train a disentangled model\nfrom scratch for a particular task. We evaluate this method with respect to its\nability to disentangle representations of sentiment from genre in movie\nreviews, \"toxicity\" from dialect in Tweets, and syntax from semantics.\n</p>\n<p>By combining masking with magnitude pruning we find that we can identify\nsparse subnetworks within BERT that strongly encode particular aspects (e.g.,\ntoxicity) while only weakly encoding others (e.g., race). Moreover, despite\nonly learning masks, we find that disentanglement-via-masking performs as well\nas -- and often better than -- previously proposed methods based on variational\nautoencoders and adversarial training.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xiongyi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meent_J/0/1/0/all/0/1\">Jan-Willem van de Meent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1\">Byron C. Wallace</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Domain Label-Adaptive Stance Detection. (arXiv:2104.07467v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07467","description":"<p>Stance detection concerns the classification of a writer's viewpoint towards\na target. There are different task variants, e.g., stance of a tweet vs. a full\narticle, or stance with respect to a claim vs. an (implicit) topic. Moreover,\ntask definitions vary, which includes the label inventory, the data collection,\nand the annotation protocol. All these aspects hinder cross-domain studies, as\nthey require changes to standard domain adaptation approaches. In this paper,\nwe perform an in-depth analysis of 16 stance detection datasets, and we explore\nthe possibility for cross-domain learning from them. Moreover, we propose an\nend-to-end unsupervised framework for out-of-domain prediction of unseen,\nuser-defined labels. In particular, we combine domain adaptation techniques\nsuch as mixture of experts and domain-adversarial training with label\nembeddings, and we demonstrate sizable performance gains over strong baselines,\nboth (i) in-domain, i.e., for seen targets, and (ii) out-of-domain, i.e., for\nunseen targets. Finally, we perform an exhaustive analysis of the cross-domain\nresults, and we highlight the important factors influencing the model\nperformance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1\">Momchil Hardalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1\">Arnav Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1\">Isabelle Augenstein</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Toward Deconfounding the Influence of Entity Demographics for Question Answering Accuracy. (arXiv:2104.07571v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07571","description":"<p>The goal of question answering (QA) is to answer any question. However, major\nQA datasets have skewed distributions over gender, profession, and nationality.\nDespite that skew, model accuracy analysis reveals little evidence that\naccuracy is lower for people based on gender or nationality; instead, there is\nmore variation on professions (question topic). But QA's lack of representation\ncould itself hide evidence of bias, necessitating QA datasets that better\nrepresent global diversity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gor_M/0/1/0/all/0/1\">Maharshi Gor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Webster_K/0/1/0/all/0/1\">Kellie Webster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1\">Jordan Boyd-Graber</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Polarized Topics Using Partisanship-aware Contextualized Topic Embeddings. (arXiv:2104.07814v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07814","description":"<p>Growing polarization of the news media has been blamed for fanning\ndisagreement, controversy and even violence. Early identification of polarized\ntopics is thus an urgent matter that can help mitigate conflict. However,\naccurate measurement of topic-wise polarization is still an open research\nchallenge. To address this gap, we propose Partisanship-aware Contextualized\nTopic Embeddings (PaCTE), a method to automatically detect polarized topics\nfrom partisan news sources. Specifically, utilizing a language model that has\nbeen finetuned on recognizing partisanship of the news articles, we represent\nthe ideology of a news corpus on a topic by corpus-contextualized topic\nembedding and measure the polarization using cosine distance. We apply our\nmethod to a dataset of news articles about the COVID-19 pandemic. Extensive\nexperiments on different news sources and topics demonstrate the efficacy of\nour method to capture topical polarization, as indicated by its effectiveness\nof retrieving the most polarized topics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zihao He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mokhberian_N/0/1/0/all/0/1\">Negar Mokhberian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Camara_A/0/1/0/all/0/1\">Antonio Camara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abeliuk_A/0/1/0/all/0/1\">Andres Abeliuk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1\">Kristina Lerman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Matching-oriented Product Quantization For Ad-hoc Retrieval. (arXiv:2104.07858v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07858","description":"<p>Product quantization (PQ) is a widely used technique for ad-hoc retrieval.\nRecent studies propose supervised PQ, where the embedding and quantization\nmodels can be jointly trained with supervised learning. However, there is a\nlack of appropriate formulation of the joint training objective; thus, the\nimprovements over previous non-supervised baselines are limited in reality. In\nthis work, we propose the Matching-oriented Product Quantization (MoPQ), where\na novel objective Multinoulli Contrastive Loss (MCL) is formulated. With the\nminimization of MCL, we are able to maximize the matching probability of query\nand ground-truth key, which contributes to the optimal retrieval accuracy.\nGiven that the exact computation of MCL is intractable due to the demand of\nvast contrastive samples, we further propose the Differentiable Cross-device\nSampling (DCS), which significantly augments the contrastive samples for\nprecise approximation of MCL. We conduct extensive experimental studies on four\nreal-world datasets, whose results verify the effectiveness of MoPQ. The code\nis available at https://github.com/microsoft/MoPQ.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1\">Shitao Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zheng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yingxia Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lian_D/0/1/0/all/0/1\">Defu Lian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Counter-Interference Adapter for Multilingual Machine Translation. (arXiv:2104.08154v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08154","description":"<p>Developing a unified multilingual model has long been a pursuit for machine\ntranslation. However, existing approaches suffer from performance degradation\n-- a single multilingual model is inferior to separately trained bilingual ones\non rich-resource languages. We conjecture that such a phenomenon is due to\ninterference caused by joint training with multiple languages. To accommodate\nthe issue, we propose CIAT, an adapted Transformer model with a small parameter\noverhead for multilingual machine translation. We evaluate CIAT on multiple\nbenchmark datasets, including IWSLT, OPUS-100, and WMT. Experiments show that\nCIAT consistently outperforms strong multilingual baselines on 64 of total 66\nlanguage directions, 42 of which see above 0.5 BLEU improvement. Our code is\navailable at \\url{https://github.com/Yaoming95/CIAT}~.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yaoming Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jiangtao Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ESTER: A Machine Reading Comprehension Dataset for Event Semantic Relation Reasoning. (arXiv:2104.08350v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08350","description":"<p>Understanding how events are semantically related to each other is the\nessence of reading comprehension. Recent event-centric reading comprehension\ndatasets focus mostly on event arguments or temporal relations. While these\ntasks partially evaluate machines' ability of narrative understanding,\nhuman-like reading comprehension requires the capability to process event-based\ninformation beyond arguments and temporal reasoning. For example, to understand\ncausality between events, we need to infer motivation or purpose; to establish\nevent hierarchy, we need to understand the composition of events. To facilitate\nthese tasks, we introduce ESTER, a comprehensive machine reading comprehension\n(MRC) dataset for Event Semantic Relation Reasoning. The dataset leverages\nnatural language queries to reason about the five most common event semantic\nrelations, provides more than 6K questions and captures 10.1K event relation\npairs. Experimental results show that the current SOTA systems achieve 22.1%,\n63.3%, and 83.5% for token-based exact-match, F1, and event-based HIT@1 scores,\nwhich are all significantly below human performances (36.0%, 79.6%, 100%\nrespectively), highlighting our dataset as a challenging benchmark.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1\">Jiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baylon_J/0/1/0/all/0/1\">Julia Baylon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ning_Q/0/1/0/all/0/1\">Qiang Ning</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MT6: Multilingual Pretrained Text-to-Text Transformer with Translation Pairs. (arXiv:2104.08692v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08692","description":"<p>Multilingual T5 (mT5) pretrains a sequence-to-sequence model on massive\nmonolingual texts, which has shown promising results on many cross-lingual\ntasks. In this paper, we improve multilingual text-to-text transfer Transformer\nwith translation pairs (mT6). Specifically, we explore three cross-lingual\ntext-to-text pre-training tasks, namely, machine translation, translation pair\nspan corruption, and translation span corruption. In addition, we propose a\npartially non-autoregressive objective for text-to-text pre-training. We\nevaluate the methods on eight multilingual benchmark datasets, including\nsentence classification, named entity recognition, question answering, and\nabstractive summarization. Experimental results show that the proposed mT6\nimproves cross-lingual transferability over mT5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1\">Shuming Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1\">Shaohan Huang Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GooAQ: Open Question Answering with Diverse Answer Types. (arXiv:2104.08727v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08727","description":"<p>While day-to-day questions come with a variety of answer types, the current\nquestion-answering (QA) literature has failed to adequately address the answer\ndiversity of questions. To this end, we present GooAQ, a large-scale dataset\nwith a variety of answer types. This dataset contains over 5 million questions\nand 3 million answers collected from Google. GooAQ questions are collected\nsemi-automatically from the Google search engine using its autocomplete\nfeature. This results in naturalistic questions of practical interest that are\nnonetheless short and expressed using simple language. GooAQ answers are mined\nfrom Google's responses to our collected questions, specifically from the\nanswer boxes in the search results. This yields a rich space of answer types,\ncontaining both textual answers (short and long) as well as more structured\nones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)\nin line with recent work, LM's strong performance on GooAQ's short-answer\nquestions heavily benefit from annotated data; however, (b) their quality in\ngenerating coherent and accurate responses for questions requiring long\nresponses (such as 'how' and 'why' questions) is less reliant on observing\nannotated data and mainly supported by their pre-training. We release GooAQ to\nfacilitate further research on improving QA with diverse response types.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1\">Daniel Khashabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ng_A/0/1/0/all/0/1\">Amos Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Khot_T/0/1/0/all/0/1\">Tushar Khot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabharwal_A/0/1/0/all/0/1\">Ashish Sabharwal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1\">Hannaneh Hajishirzi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Callison_Burch_C/0/1/0/all/0/1\">Chris Callison-Burch</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can NLI Models Verify QA Systems' Predictions?. (arXiv:2104.08731v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08731","description":"<p>To build robust question answering systems, we need the ability to verify\nwhether answers to questions are truly correct, not just \"good enough\" in the\ncontext of imperfect QA datasets. We explore the use of natural language\ninference (NLI) as a way to achieve this goal, as NLI inherently requires the\npremise (document context) to contain all necessary information to support the\nhypothesis (proposed answer to the question). We leverage large pre-trained\nmodels and recent prior datasets to construct powerful question converter and\ndecontextualization modules, which can reformulate QA instances as\npremise-hypothesis pairs with very high reliability. Then, by combining\nstandard NLI datasets with NLI examples automatically derived from QA training\ndata, we can train NLI models to judge the correctness of QA models' proposed\nanswers. We show that our NLI approach can generally improve the confidence\nestimation of a QA model across different domains, evaluated in a selective QA\nsetting. Careful manual analysis over the predictions of our NLI model shows\nthat it can further identify cases where the QA model produces the right answer\nfor the wrong reason, or where the answer cannot be verified as addressing all\naspects of the question.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jifan Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scaling End-to-End Models for Large-Scale Multilingual ASR. (arXiv:2104.14830v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.14830","description":"<p>Building ASR models across many languages is a challenging multi-task\nlearning problem due to large variations and heavily unbalanced data. Existing\nwork has shown positive transfer from high resource to low resource languages.\nHowever, degradations on high resource languages are commonly observed due to\ninterference from the heterogeneous multilingual data and reduction in\nper-language capacity. We conduct a capacity study on a 15-language task, with\nthe amount of data per language varying from 7.6K to 53.5K hours. We adopt\nGShard [1] to efficiently scale up to 10B parameters. Empirically, we find that\n(1) scaling the number of model parameters is an effective way to solve the\ncapacity bottleneck - our 500M-param model already outperforms monolingual\nbaselines and scaling it to 1B and 10B brought further quality gains; (2)\nlarger models are not only more data efficient, but also more efficient in\nterms of training cost as measured in TPU days - the 1B-param model reaches the\nsame accuracy at 34% of training time as the 500M-param model; (3) given a\nfixed capacity budget, adding depth works better than width and large encoders\ndo better than large decoders; (4) with continuous training, they can be\nadapted to new languages and domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bo Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1\">Ruoming Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sainath_T/0/1/0/all/0/1\">Tara N. Sainath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gulati_A/0/1/0/all/0/1\">Anmol Gulati</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1\">James Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Haghani_P/0/1/0/all/0/1\">Parisa Haghani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">W. Ronny Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Min Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1\">Junwen Bai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"On Classifying Continuous Constraint Satisfaction problems. (arXiv:2106.02397v2 [cs.CC] UPDATED)","link":"http://arxiv.org/abs/2106.02397","description":"<p>A continuous constraint satisfaction problem (CCSP) is a constraint\nsatisfaction problem (CSP) with a domain $U \\subset \\mathbb{R}$. We engage in a\nsystematic study to classify CCSPs that are complete of the Existential Theory\nof the Reals, i.e., ER-complete. To define this class, we first consider the\nproblem ETR, which also stands for Existential Theory of the Reals. In an\ninstance of this problem we are given some sentence of the form $\\exists x_1,\n\\ldots, x_n \\in \\mathbb{R} : \\Phi(x_1, \\ldots, x_n)$, where $\\Phi$ is a\nwell-formed quantifier-free formula consisting of the symbols $\\{0, 1, +,\n\\cdot, \\geq, &gt;, \\wedge, \\vee, \\neg\\}$, the goal is to check whether this\nsentence is true. Now the class ER is the family of all problems that admit a\npolynomial-time reduction to ETR. It is known that NP $\\subseteq$ ER\n$\\subseteq$ PSPACE.\n</p>\n<p>We restrict our attention on CCSPs with addition constraints ($x + y = z$)\nand some other mild technical condition. Previously, it was shown that\nmultiplication constraints ($x \\cdot y = z$), squaring constraints ($x^2 = y$),\nor inversion constraints ($x\\cdot y = 1$) are sufficient to establish\nER-completeness. We extend this in the strongest possible sense for equality\nconstraints as follows. We show that CCSPs (with addition constraints and some\nother mild technical condition) that have any one well-behaved curved equality\nconstraint ($f(x,y) = 0$) are ER-complete. We further extend our results to\ninequality constraints. We show that any well-behaved convexly curved and any\nwell-behaved concavely curved inequality constraint ($f(x,y) \\geq 0$ and\n$g(x,y) \\geq 0$) imply ER-completeness on the class of such CCSPs.\n</p>\n<p>We apply our findings to geometric packing and answer an open question by\nAbrahamsen et al. [FOCS 2020]. Namely, we establish ER-completeness of packing\nconvex pieces into a square container under rotations and translations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Miltzow_T/0/1/0/all/0/1\">Tillmann Miltzow</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schmiermann_R/0/1/0/all/0/1\">Reinier F. Schmiermann</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Pretrained Cross-Lingual Language Models via Self-Labeled Word Alignment. (arXiv:2106.06381v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.06381","description":"<p>The cross-lingual language models are typically pretrained with masked\nlanguage modeling on multilingual text or parallel sentences. In this paper, we\nintroduce denoising word alignment as a new cross-lingual pre-training task.\nSpecifically, the model first self-labels word alignments for parallel\nsentences. Then we randomly mask tokens in a bitext pair. Given a masked token,\nthe model uses a pointer network to predict the aligned token in the other\nlanguage. We alternately perform the above two steps in an\nexpectation-maximization manner. Experimental results show that our method\nimproves cross-lingual transferability on various datasets, especially on the\ntoken-level tasks, such as question answering, and structured prediction.\nMoreover, the model can serve as a pretrained word aligner, which achieves\nreasonably low error rates on the alignment benchmarks. The code and pretrained\nparameters are available at https://github.com/CZWin32768/XLM-Align.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chi_Z/0/1/0/all/0/1\">Zewen Chi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1\">Li Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1\">Bo Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shaohan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1\">Xian-Ling Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1\">Heyan Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WeChat Neural Machine Translation Systems for WMT21. (arXiv:2108.02401v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.02401","description":"<p>This paper introduces WeChat AI's participation in WMT 2021 shared news\ntranslation task on English-&gt;Chinese, English-&gt;Japanese, Japanese-&gt;English and\nEnglish-&gt;German. Our systems are based on the Transformer (Vaswani et al.,\n2017) with several novel and effective variants. In our experiments, we employ\ndata filtering, large-scale synthetic data generation (i.e., back-translation,\nknowledge distillation, forward-translation, iterative in-domain knowledge\ntransfer), advanced finetuning approaches, and boosted Self-BLEU based model\nensemble. Our constrained systems achieve 36.9, 46.9, 27.8 and 31.3\ncase-sensitive BLEU scores on English-&gt;Chinese, English-&gt;Japanese,\nJapanese-&gt;English and English-&gt;German, respectively. The BLEU scores of\nEnglish-&gt;Chinese, English-&gt;Japanese and Japanese-&gt;English are the highest among\nall submissions, and that of English-&gt;German is the highest among all\nconstrained submissions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xianfeng Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yijin Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_E/0/1/0/all/0/1\">Ernan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ran_Q/0/1/0/all/0/1\">Qiu Ran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Grained Element Identification in Complaint Text of Internet Fraud. (arXiv:2108.08676v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08676","description":"<p>Existing system dealing with online complaint provides a final decision\nwithout explanations. We propose to analyse the complaint text of internet\nfraud in a fine-grained manner. Considering the complaint text includes\nmultiple clauses with various functions, we propose to identify the role of\neach clause and classify them into different types of fraud element. We\nconstruct a large labeled dataset originated from a real finance service\nplatform. We build an element identification model on top of BERT and propose\nadditional two modules to utilize the context of complaint text for better\nelement label classification, namely, global context encoder and label refiner.\nExperimental results show the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Tong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Siyuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jingchao Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1\">Lei Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaqi Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1\">Heng Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Liaosa Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1\">Weiqiang Wan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1\">Xuanjing Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just Say No: Analyzing the Stance of Neural Dialogue Generation in Offensive Contexts. (arXiv:2108.11830v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11830","description":"<p>Dialogue models trained on human conversations inadvertently learn to\ngenerate toxic responses. In addition to producing explicitly offensive\nutterances, these models can also implicitly insult a group or individual by\naligning themselves with an offensive statement. To better understand the\ndynamics of contextually offensive language, we investigate the stance of\ndialogue model responses in offensive Reddit conversations. Specifically, we\ncreate ToxiChat, a crowd-annotated dataset of 2,000 Reddit threads and model\nresponses labeled with offensive language and stance. Our analysis reveals that\n42% of human responses agree with toxic comments, whereas only 13% agree with\nsafe comments. This undesirable behavior is learned by neural dialogue models,\nsuch as DialoGPT, which we show are two times more likely to agree with\noffensive comments. To enable automatic detection of offensive language, we\nfine-tuned transformer-based classifiers on ToxiChat that achieve 0.71 F1 for\noffensive labels and 0.53 Macro-F1 for stance labels. Finally, we quantify the\neffectiveness of controllable text generation (CTG) methods to mitigate the\ntendency of neural dialogue models to agree with offensive comments. Compared\nto the baseline, our best CTG model achieves a 19% reduction in agreement with\noffensive comments and produces 29% fewer offensive replies. Our work\nhighlights the need for further efforts to characterize and analyze\ninappropriate behavior in dialogue models, in order to help make them safer.\nOur code and corpus are available at https://github.com/abaheti95/ToxiChat .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Baheti_A/0/1/0/all/0/1\">Ashutosh Baheti</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1\">Maarten Sap</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1\">Alan Ritter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1\">Mark Riedl</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12084","description":"<p>Gender is widely discussed in the context of language tasks and when\nexamining the stereotypes propagated by language models. However, current\ndiscussions primarily treat gender as binary, which can perpetuate harms such\nas the cyclical erasure of non-binary gender identities. These harms are driven\nby model and dataset biases, which are consequences of the non-recognition and\nlack of understanding of non-binary genders in society. In this paper, we\nexplain the complexity of gender and language around it, and survey non-binary\npersons to understand harms associated with the treatment of gender as binary\nin English language technologies. We also detail how current language\nrepresentations (e.g., GloVe, BERT) capture and perpetuate these harms and\nrelated challenges that need to be acknowledged and addressed for\nrepresentations to equitably encode gender information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1\">Arjun Subramonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v8 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on six public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work has claimed, our auxiliary experiments suggest that\nrelation prediction is contributory to named entity prediction in a\nnon-negligible way. The source code can be found at\nhttps://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sentence Bottleneck Autoencoders from Transformer Language Models. (arXiv:2109.00055v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00055","description":"<p>Representation learning for text via pretraining a language model on a large\ncorpus has become a standard starting point for building NLP systems. This\napproach stands in contrast to autoencoders, also trained on raw text, but with\nthe objective of learning to encode each input as a vector that allows full\nreconstruction. Autoencoders are attractive because of their latent space\nstructure and generative properties. We therefore explore the construction of a\nsentence-level autoencoder from a pretrained, frozen transformer language\nmodel. We adapt the masked language modeling objective as a generative,\ndenoising one, while only training a sentence bottleneck and a single-layer\nmodified transformer decoder. We demonstrate that the sentence representations\ndiscovered by our model achieve better quality than previous methods that\nextract representations from pretrained transformers on text similarity tasks,\nstyle transfer (an example of controlled generation), and single-sentence\nclassification tasks in the GLUE benchmark, while using fewer parameters than\nlarge pretrained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Montero_I/0/1/0/all/0/1\">Ivan Montero</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pappas_N/0/1/0/all/0/1\">Nikolaos Pappas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00544","description":"<p>Adversarial training, a method for learning robust deep neural networks,\nconstructs adversarial examples during training. However, recent methods for\ngenerating NLP adversarial examples involve combinatorial search and expensive\nsentence encoders for constraining the generated instances. As a result, it\nremains challenging to use vanilla adversarial training to improve NLP models'\nperformance, and the benefits are mainly uninvestigated. This paper proposes a\nsimple and improved vanilla adversarial training process for NLP models, which\nwe name Attacking to Training (A2T). The core part of A2T is a new and cheaper\nword substitution attack optimized for vanilla adversarial training. We use A2T\nto train BERT and RoBERTa models on IMDB, Rotten Tomatoes, Yelp, and SNLI\ndatasets. Our results empirically show that it is possible to train robust NLP\nmodels using a much cheaper adversary. We demonstrate that vanilla adversarial\ntraining with A2T can improve an NLP model's robustness to the attack it was\noriginally trained with and also defend the model against other types of word\nsubstitution attacks. Furthermore, we show that A2T can improve NLP models'\nstandard accuracy, cross-domain generalization, and interpretability. Code is\navailable at https://github.com/QData/Textattack-A2T .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jin Yong Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"FewshotQA: A simple framework for few-shot learning of question answering tasks using pre-trained text-to-text models. (arXiv:2109.01951v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.01951","description":"<p>The task of learning from only a few examples (called a few-shot setting) is\nof key importance and relevance to a real-world setting. For question answering\n(QA), the current state-of-the-art pre-trained models typically need\nfine-tuning on tens of thousands of examples to obtain good results. Their\nperformance degrades significantly in a few-shot setting (&lt; 100 examples). To\naddress this, we propose a simple fine-tuning framework that leverages\npre-trained text-to-text models and is directly aligned with their pre-training\nframework. Specifically, we construct the input as a concatenation of the\nquestion, a mask token representing the answer span and a context. Given this\ninput, the model is fine-tuned using the same objective as that of its\npre-training objective. Through experimental studies on various few-shot\nconfigurations, we show that this formulation leads to significant gains on\nmultiple QA benchmarks (an absolute gain of 34.2 F1 points on average when\nthere are only 16 training examples). The gains extend further when used with\nlarger models (Eg:- 72.3 F1 on SQuAD using BART-large with only 32 examples)\nand translate well to a multilingual setting . On the multilingual TydiQA\nbenchmark, our model outperforms the XLM-Roberta-large by an absolute margin of\nupto 40 F1 points and an average of 33 F1 points in a few-shot setting (&lt;= 64\ntraining examples). We conduct detailed ablation studies to analyze factors\ncontributing to these gains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chada_R/0/1/0/all/0/1\">Rakesh Chada</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Pradeep Natarajan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Teaching Autoregressive Language Models Complex Tasks By Demonstration. (arXiv:2109.02102v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.02102","description":"<p>This paper demonstrates that by fine-tuning an autoregressive language model\n(GPT-Neo) on appropriately structured step-by-step demonstrations, it is\npossible to teach it to execute a mathematical task that has previously proved\ndifficult for Transformers - longhand modulo operations - with a relatively\nsmall number of examples. Specifically, we fine-tune GPT-Neo to solve the\nnumbers__div_remainder task from the DeepMind Mathematics Dataset; Saxton et\nal. (<a href=\"/abs/1904.01557\">arXiv:1904.01557</a>) reported below 40% accuracy on this task with 2 million\ntraining examples. We show that after fine-tuning on 200 appropriately\nstructured demonstrations of solving long division problems and reporting the\nremainders, the smallest available GPT-Neo model achieves over 80% accuracy.\nThis is achieved by constructing an appropriate dataset for fine-tuning, with\nno changes to the learning algorithm. These results suggest that fine-tuning\nautoregressive language models on small sets of well-crafted demonstrations may\nbe a useful paradigm for enabling individuals without training in machine\nlearning to coax such models to perform some kinds of complex multi-step tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Recchia_G/0/1/0/all/0/1\">Gabriel Recchia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Low-Resource Dialogue Summarization with Domain-Agnostic Multi-Source Pretraining. (arXiv:2109.04080v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04080","description":"<p>With the rapid increase in the volume of dialogue data from daily life, there\nis a growing demand for dialogue summarization. Unfortunately, training a large\nsummarization model is generally infeasible due to the inadequacy of dialogue\ndata with annotated summaries. Most existing works for low-resource dialogue\nsummarization directly pretrain models in other domains, e.g., the news domain,\nbut they generally neglect the huge difference between dialogues and\nconventional articles. To bridge the gap between out-of-domain pretraining and\nin-domain fine-tuning, in this work, we propose a multi-source pretraining\nparadigm to better leverage the external summary data. Specifically, we exploit\nlarge-scale in-domain non-summary data to separately pretrain the dialogue\nencoder and the summary decoder. The combined encoder-decoder model is then\npretrained on the out-of-domain summary data using adversarial critics, aiming\nto facilitate domain-agnostic summarization. The experimental results on two\npublic datasets show that with only limited training data, our approach\nachieves competitive performance and generalizes well in different dialogue\nscenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1\">Yicheng Zou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1\">Bolin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xingwu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1\">Tao Gui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lexico-semantic and affective modelling of Spanish poetry: A semi-supervised learning approach. (arXiv:2109.04152v2 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2109.04152","description":"<p>Text classification tasks have improved substantially during the last years\nby the usage of transformers. However, the majority of researches focus on\nprose texts, with poetry receiving less attention, specially for Spanish\nlanguage. In this paper, we propose a semi-supervised learning approach for\ninferring 21 psychological categories evoked by a corpus of 4572 sonnets, along\nwith 10 affective and lexico-semantic multiclass ones. The subset of poems used\nfor training an evaluation includes 270 sonnets. With our approach, we achieve\nan AUC beyond 0.7 for 76% of the psychological categories, and an AUC over 0.65\nfor 60% on the multiclass ones. The sonnets are modelled using transformers,\nthrough sentence embeddings, along with lexico-semantic and affective features,\nobtained by using external lexicons. Consequently, we see that this approach\nprovides an AUC increase of up to 0.12, as opposed to using transformers alone.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Barbado_A/0/1/0/all/0/1\">Alberto Barbado</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_M/0/1/0/all/0/1\">Mar&#xed;a Dolores Gonz&#xe1;lez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carrera_D/0/1/0/all/0/1\">D&#xe9;bora Carrera</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning with Different Amounts of Annotation: From Zero to Many Labels. (arXiv:2109.04408v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04408","description":"<p>Training NLP systems typically assumes access to annotated data that has a\nsingle human label per example. Given imperfect labeling from annotators and\ninherent ambiguity of language, we hypothesize that single label is not\nsufficient to learn the spectrum of language interpretation. We explore new\nannotation distribution schemes, assigning multiple labels per example for a\nsmall subset of training examples. Introducing such multi label examples at the\ncost of annotating fewer examples brings clear gains on natural language\ninference task and entity typing task, even when we simply first train with a\nsingle label data and then fine tune with multi label examples. Extending a\nMixUp data augmentation framework, we propose a learning algorithm that can\nlearn from training examples with different amount of annotation (with zero,\none, or multiple labels). This algorithm efficiently combines signals from\nuneven training data and brings additional gains in low annotation budget and\ncross domain settings. Together, our method achieves consistent gains in two\ntasks, suggesting distributing labels unevenly among training examples can be\nbeneficial for many NLP tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shujian Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1\">Chengyue Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1\">Eunsol Choi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-13T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"admin":"http://webns.net/mvcb/","dc":"http://purl.org/dc/elements/1.1/","content":"http://purl.org/rss/1.0/modules/content/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}