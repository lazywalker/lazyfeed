{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-15T04:21:13.586220901Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-09-06 Why Rust for offensive security","link":"https://rustcc.cn/article?id=7750136c-b31b-4e85-ab94-ae1459b4150e","description":"<h2>Why Rust for offensive security</h2>\n<blockquote>\n<p>æƒ³è±¡ä¸€ä¸‹ï¼šä½ çš„å¦å…‹éƒ½æ˜¯çº¸æ¿åšçš„ã€‚ç„¶åä½ çš„é£æœºä¹Ÿéƒ½æ˜¯ç”¨çº¸åšçš„ï¼Œä½ çš„æµ·å†›ä¹Ÿå…¨éƒ½æ˜¯çº¸èˆ¹ï¼Œé‚£ä¹Ÿå¤ªæƒ¨äº†å§ï¼Ÿ</p>\n<p>è™½ç„¶å¾ˆè’å”ï¼Œä½†æ˜¯è¿™å°±æ˜¯ç°åœ¨çš„é»‘å®¢æŠ€æœ¯çš„çŠ¶æ€ã€‚</p>\n<p>Imagine: all the tanks of your army are made of cardboard. Now imagine that not only your tanks but also all your airforce is composed of paper planes and your navy of paper vessels. It would be a pretty bad situation, donâ€™t you think?</p>\n<p>While it sounds absurd, this is the sad state of hacking today.</p>\n</blockquote>\n<h3>TL;DR</h3>\n<p>æ–‡ç« æŒ‡å‡ºï¼Œè¿‡å»çš„ç¼–ç¨‹è¯­è¨€ï¼ˆc, Javaï¼Œ pythonï¼‰ç­‰éƒ½åªèƒ½å±€é™åœ¨ä¸€ä¸ªé¢†åŸŸåº”ç”¨ï¼Œç„¶è€Œç°åœ¨æˆ‘ä»¬ç­‰æ¥äº† Rust æ•‘åœºâ€”â€”ä¸å†æœ‰å¥‡æ€ªçš„åŒ…ç®¡ç†å™¨ã€äºŒçº§åˆ¶æ‰“åŒ…å·¥å…·æˆ–è€…è„†å¼±çš„ç½‘ç»œä»£ç ï¼Œè¿™äº›æ–¹é¢çš„å¯é æ€§ä¸€æ—¦è¢«é»‘å®¢ä»¬æ„è¯†åˆ°ï¼Œå°±å¯èƒ½å¸¦æ¥å®‰å…¨æ”»é˜²çš„å˜é©ã€‚</p>\n<p>ä¸ºäº†å®‰åˆ©å¯é çš„ Rustï¼Œä½œè€…è¿˜å†™äº†æœ¬ä¹¦ <a href=\"https://academy.kerkour.com/black-hat-rust?coupon=BLOG\" rel=\"noopener noreferrer\">Black Hat Rust</a>, æ¥æ€»ç»“è‡ªå·±é€šè¿‡ Rust åœ¨é»‘å®¢æŠ€æœ¯ä¸­çš„å®è·µï¼Œä»¥å…¶è®©è¯»è€…å°‘è¸©å‘ï¼Œæ›´å¥½åœ°ç†è§£ Rust çš„å¯é ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-14 14:09:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-09-13 Rust åœ¨ linux å†…æ ¸ä¸­çš„æœ€æ–°è¿›å±•","link":"https://rustcc.cn/article?id=1c26513e-c4c2-4d52-becf-8c39379474e9","description":"<h1>Rust åœ¨ linux å†…æ ¸ä¸­çš„æœ€æ–°è¿›å±•</h1>\n<p>è™½ç„¶Rustç¼–ç¨‹è¯­è¨€åœ¨å†…æ ¸ä¸­ä½¿ç”¨çš„æ”¯æŒè¿˜æ²¡æœ‰ç™»é™†åˆ°æœ¬å‘¨æœ«ç»“æŸçš„ <code>Linux 5.15</code> åˆå¹¶çª—å£ï¼Œä½†è¿™é¡¹å·¥ä½œä»åœ¨è¿›è¡Œä¸­ã€‚æœ¬å‘¨ï¼Œå…³äºRuståœ¨Linuxå†…æ ¸ä¸­çš„ä½¿ç”¨çš„æœ€æ–°è¿›å±•è¢«åˆ†äº«äº†å‡ºæ¥ã€‚</p>\n<p>ä½œä¸ºRust for Linuxé¡¹ç›®çš„ä¸»è¦å¼€å‘äººå‘˜ä¹‹ä¸€ï¼ŒMiguel Ojedaåœ¨æœ¬å‘¨çš„Linaro Connectè™šæ‹Ÿä¼šè®®ä¸Šä»‹ç»äº†è¯¥é¡¹ç›®ï¼Œä»–ç›®å‰æ­£åœ¨ä¸ºè°·æ­Œçš„åˆåŒå·¥ä½œã€‚</p>\n<p>å¯¹å‘¨äº”çš„æ¼”è®²æ„Ÿå…´è¶£çš„äººå¯ä»¥æŸ¥çœ‹ä¸‹é¢çš„ Presentationã€‚</p>\n<p><a href=\"https://bigthinkbuzz.com/the-latest-progress-on-rust-for-the-linux-kernel/\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<p><a href=\"https://static.linaro.org/connect/lvc21f/presentations/LVC21F-317.pdf\" rel=\"noopener noreferrer\">Presentationåœ°å€</a></p>\n<h1>Matchbox: Rust wasm ä¸­çš„ p2p ç½‘ç»œè§£å†³æ–¹æ¡ˆ</h1>\n<p>Matchbox çš„è¯ç”Ÿæ˜¯å› ä¸ºä½œè€…åœ¨<code>rust</code> ä¸­åˆ¶ä½œäº†ä¸€æ¬¾å¤šäººç½‘é¡µæ¸¸æˆï¼Œé‡åˆ°äº†ä»¥ä¸‹é—®é¢˜:</p>\n<p>å¦‚ä½•ä½¿ç”¨ä¸å¯é çš„ã€æ— åºçš„ p2p connection è¿æ¥ N ä¸ªwebæµè§ˆå™¨?</p>\n<p><a href=\"https://johanhelsing.studio/posts/introducing-matchbox\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<h1>Learn Wgpu æ›´æ–°äº†</h1>\n<p><code>wgrpu</code> æ˜¯ <code>WebGPU API spec</code> çš„ Rust å®ç°, ç›®å‰è¿™ä¸ªæ•™ç¨‹å·²ç»æ›´æ–°åˆ°äº† 0.10 ç‰ˆæœ¬, æœ‰å¤§é‡çš„åŸç†å’Œä»£ç ç¤ºä¾‹è®²è§£.</p>\n<p><a href=\"https://sotrh.github.io/learn-wgpu/beginner/tutorial2-surface/\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<h1>Sycamore: v0.6.0 ç‰ˆæœ¬å‘å¸ƒäº†</h1>\n<p>Sycamoreæ˜¯ä¸€ä¸ªç”¨ Rust å’Œ WebAssembly æ„å»ºåŒæ„webåº”ç”¨ç¨‹åºçš„åº“. ç›®å‰å‘å¸ƒäº† 0.6.0 ç‰ˆæœ¬äº†.</p>\n<ul>\n<li>é™æ€ç”Ÿæˆ</li>\n<li>æœåŠ¡ç«¯æ¸²æŸ“</li>\n<li>é‡éªŒè¯</li>\n<li>å¢é‡æ„å»º</li>\n<li>å¼€æ”¾æ„å»ºçŸ©é˜µ</li>\n<li>CLIåˆ©ç”¨ï¼Œè®©æ‚¨è½»æ¾å’Œè‡ªä¿¡åœ°æ„å»ºåº”ç”¨ç¨‹åº</li>\n<li>å……åˆ†åˆ©ç”¨ Fluent å¼€ç®±å³ç”¨çš„ i18n æ”¯æŒ</li>\n</ul>\n<p><a href=\"https://sycamore-rs.netlify.app/news/announcing-v0.6.0\" rel=\"noopener noreferrer\">åŸæ–‡é“¾æ¥</a></p>\n<p>--</p>\n<p>From æ—¥æŠ¥å°ç»„ BobQinï¼ŒFBIå°ç™½</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 13:10:04","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€æˆéƒ½ã€‘æ‹›è˜Rustå¼€å‘å·¥ç¨‹å¸ˆ","link":"https://rustcc.cn/article?id=ed86a028-a5b8-41e6-9849-e43a55ff7faf","description":"<h2>Rustå¼€å‘å·¥ç¨‹å¸ˆæ‹›è˜</h2>\n<h3>å²—ä½èŒè´£ï¼š</h3>\n<ul>\n<li>1ã€è´Ÿè´£ç”µå•†äº§å“åç«¯åŠŸèƒ½æ¥å£çš„å¼€å‘ï¼›</li>\n<li>2ã€è´Ÿè´£ç”µå•†äº§å“ä¸šåŠ¡åŠŸèƒ½å¼€å‘ã€è¿­ä»£å’Œç»´æŠ¤ï¼Œå¯¹ä¸šåŠ¡æ•°æ®è¿›è¡Œå¤„ç†å’Œåˆ†æï¼›</li>\n<li>3ã€é…åˆå‰ç«¯å¼€å‘å®ŒæˆåŠŸèƒ½çš„å‰åå°åŠŸèƒ½è”è°ƒï¼›</li>\n<li>4ã€é…åˆå®Œæˆäº§å“æµ‹è¯•ï¼ŒBUGä¿®æ”¹ã€‚</li>\n</ul>\n<h3>ä»»èŒè¦æ±‚ï¼š</h3>\n<ul>\n<li>1ã€åç«¯å¼€å‘è¯­è¨€åŸºç¡€æ‰å®ï¼Œæœ‰ç”µå•†äº§å“åç«¯å¼€å‘ç»éªŒï¼›</li>\n<li>2ã€ç†Ÿç»ƒä½¿ç”¨ä½¿ç”¨Mysqlå…³ç³»å‹æ•°æ®åº“ï¼›</li>\n<li>3ã€è‡³å°‘äº†è§£å¹¶ä½¿ç”¨è¿‡RocketMQã€RabbitMQã€Kafkaä¸­çš„ä¸€ç§ï¼›</li>\n<li>4ã€æœ‰Rustè¯­è¨€çš„åŸºç¡€ï¼Œæˆ–è€…æ„¿æ„è½¬Rustå¼€å‘ï¼›</li>\n<li>5ã€ä¸‰å¹´ä»¥ä¸Šçš„äº’è”ç½‘å¼€å‘å·¥ä½œç»éªŒï¼›</li>\n<li>6ã€ç†Ÿä¹ å¾®æœåŠ¡æˆ–ServicesMeshæ¶æ„è€…ä¼˜å…ˆï¼›</li>\n</ul>\n<p>å·¥ä½œåœ°ç‚¹å››å·æˆéƒ½ç¯çƒæ—¶ä»£ä¸­å¿ƒ\næœ‰æ„è€…è¯·å‘é‚®ä»¶è‡³ï¼šshaipe@sina.com æˆ–ç›´æ¥æ·»åŠ å¾®ä¿¡å·ï¼šshaipe</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 10:59:47","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"read_dir è¿”å›çš„ io::Result<DirEntry> ä¼šåœ¨ä»€ä¹ˆæƒ…å†µä¸‹è¿”å› Error å‘¢ï¼Ÿ","link":"https://rustcc.cn/article?id=c1f3f464-9146-4b43-8467-7eacc8f53bf8","description":"<p><code>read_dir</code> è¿­ä»£çš„æ—¶å€™ç»™åˆ°çš„æ˜¯ä¸€ä¸ª <code>io::Result&lt;DirEntry&gt;</code>ï¼Œæ–‡æ¡£é‡Œé¢åªæ˜¯ç®€å•è¯´äº† <strong>New errors may be encountered after an iterator is initially constructed.</strong></p>\n<p>ä½†æ˜¯å…·ä½“è¿™ä¸ª New errors åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ</p>\n<p>æˆ‘è¯•è¿‡äº†åœ¨è¿­ä»£çš„æ—¶å€™å¯¹æ–‡ä»¶å¤¹æˆ–è€…é‡Œé¢çš„æ–‡ä»¶ä½œåˆ é™¤ã€é‡å‘½åã€æ”¹å˜æƒé™ï¼Œéƒ½æ²¡æœ‰è¿”å› errorï¼›\nï¼ˆæµ‹è¯•å¹³å°åŒ…æ‹¬ Mac å’Œ Linuxï¼Œæ²¡æœ‰ windows æš‚æ—¶æ²¡æµ‹ï¼‰ã€‚</p>\n<p><img src=\"https://i.loli.net/2021/09/13/AbE9KdTL1sSxWJg.png\" alt=\"screenshot-20210913-174925.png\"></p>\n<pre><code>/// Iterator over the entries in a directory.\n///\n/// This iterator is returned from the [`read_dir`] function of this module and\n/// will yield instances of [`io::Result`]`&lt;`[`DirEntry`]`&gt;`. Through a [`DirEntry`]\n/// information like the entry's path and possibly other metadata can be\n/// learned.\n///\n/// The order in which this iterator returns entries is platform and filesystem\n/// dependent.\n///\n/// # Errors\n///\n/// This [`io::Result`] will be an [`Err`] if there's some sort of intermittent\n/// IO error during iteration.\n#[stable(feature = \"rust1\", since = \"1.0.0\")]\n#[derive(Debug)]\npub struct ReadDir(fs_imp::ReadDir);\n</code></pre>\n<p>This [<code>io::Result</code>] will be an [<code>Err</code>] if there's some sort of intermittent IO error during iteration.</p>\n<p>çœ‹èµ·æ¥ä¸€å®šè¦æ˜¯æ¯”è¾ƒç½•è§çš„ IO é”™è¯¯ï¼Ÿ</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 09:45:42","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…³äºæˆ‘å‰äº›å¤©ï¼Œåœ¨ GitHub ä¸Š Rust çš„ repo çš„é‚£äº›äº‹","link":"https://rustcc.cn/article?id=111d596f-f53f-44dc-9a59-69ccf2ff7563","description":"<p>ä¸Šå‘¨æˆ‘åœ¨ GitHub ä¸Šæ•´ç†çš„ <a href=\"https://github.com/0voice/Understanding_in_Rust\" rel=\"noopener noreferrer\">ã€Š Rust å·¥ç¨‹å¸ˆæ•è¾¹èµ„æ–™ã€‹</a> ,æ¶‰åŠäº†ä¾µæƒè¡Œä¸ºã€‚åœ¨è¿™é‡Œå‘å¤§å®¶èµ”ç¤¼é“æ­‰ã€‚å¹¶ä¸”åœ¨ç¬¬ä¸€æ—¶é—´ï¼Œå¤„ç†äº†ç›¸å…³å†…å®¹ã€‚\næˆ‘åœ¨æ•´ç†çš„ä¹‹å‰çš„åˆè¡·åªæ˜¯å•çº¯ä¸ºäº†ç»™å¤§å®¶æä¾›æ›´å¥½ã€æ›´å¤šã€æ›´å…¨ã€æ›´ä¸“ä¸šåœ°çš„ Rust å­¦ä¹ èµ„æ–™ã€‚å¹¶æ²¡æœ‰ä¸æ¯«çš„å•†ä¸šåŒ–æ‰‹æ®µã€‚\næˆ‘æ”¶é›†çš„å†…å®¹å…¨éƒ¨æ¥æºäºäº’è”ç½‘ï¼Œç”±äºæˆ‘çš„ç–å¿½æ²¡æœ‰æ³¨æ˜æ–‡ç« å‡ºå¤„é“¾æ¥ï¼Œç¡®å®æ˜¯ä¸åº”è¯¥çš„ã€‚</p>\n<p>å†ä¸€æ¬¡ï¼Œç»™ä½œå“çš„ä½œè€…é“æ­‰ã€‚</p>\n<p>æˆ‘å°†åœ¨ä»¥å repo é‡Œå°†ä¸ä¼šå‡ºç°ç±»ä¼¼çš„é”™è¯¯äº‹ä»¶ï¼ŒåŒæ—¶ä¹Ÿå¸Œæœ›å¹¿å¤§å¼€å‘è€…ä»¬ç›‘ç£ã€‚å¦‚æœæœ‰ä»»ä½•é—®é¢˜ï¼Œå¯ä»¥é‚®ç®±è‡³ï¼šwchao_isvip@163.com ï¼Œæˆ‘ä¼šåœ¨ç¬¬ä¸€æ—¶é—´å¤„ç†çš„ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 08:11:56","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"filecoiné¡¹ç›®RUSTå¤§ç‰›æ‹›è˜","link":"https://rustcc.cn/article?id=3a173b77-c75e-4cbb-a8c9-8c0496b17619","description":"<p>è¯¥å²—ä½è–ªèµ„é¢è°ˆ\nå²—ä½èŒè´£ï¼š\n1.å‚ä¸åŒºå—é“¾é¡¹ç›®å¼€å‘ï¼Œä»¥C++/rustä¸ºä¸»ï¼›\n2.ç†è§£ä¸šåŠ¡é€»è¾‘ä¸å¯¹åç«¯æœåŠ¡çš„éœ€æ±‚ï¼Œèƒ½å¤Ÿåˆ†æéœ€æ±‚å¹¶äº§ç”Ÿåˆç†æŠ€æœ¯æ–¹æ¡ˆï¼›\n3.è´Ÿè´£å¹³å°å¯¹å¤–æ¥å£ï¼Œç›¸å…³æ•°æ®æœåŠ¡çš„è®¾è®¡ä¸å®ç°ï¼›\n4.æ ¹æ®æŠ€æœ¯éœ€æ±‚éƒ¨ç½²Filecoinç¯å¢ƒï¼Œç¼–å†™è„šæœ¬ï¼Œå¯¹ç¯å¢ƒè¿›è¡Œæµ‹è¯•éƒ¨ç½²ï¼›\n5.å‚ä¸å…¬å¸é¡¹ç›®ä¸“åˆ©çš„ç¼–å†™ï¼›\nå²—ä½è¦æ±‚ï¼š\n1.å¤§ä¸“ä»¥ä¸Šå­¦å†ï¼Œè®¡ç®—æœºæˆ–è€…ç›¸å…³ä¸“ä¸šï¼Œç²¾é€šrustè¯­è¨€ï¼›\n2.è‡³å°‘ç†Ÿæ‚‰ä¸¤ç§å…¶ä»–å¼€å‘è¯­è¨€ï¼Œå¦‚C++ã€goã€Pythonç­‰ï¼›\n3.2å¹´ä»¥ä¸Šåç«¯å¼€å‘å·¥ä½œç»éªŒï¼Œåšè¿‡åŒºå—é“¾é¡¹ç›®å¼€å‘ç»éªŒçš„å¯ä¼˜å…ˆè€ƒè™‘ï¼›\n4.ç†Ÿæ‚‰Ethereumã€EOSã€Bitcoinã€Filecoinä¸­è‡³å°‘ä¸¤ä¸ªé¡¹ç›®çš„åŸºæœ¬åŸç†å’Œè®¾è®¡ï¼›\n5.ç†Ÿæ‚‰åŒºå—é“¾é¡¹ç›®ä¸­å¸¸è§çš„å…±è¯†æœºåˆ¶ã€åŠ å¯†ç®—æ³•ã€P2Pç½‘ç»œç­‰ï¼›\n6.æ€è·¯æ¸…æ™°ï¼Œå…·å¤‡è‰¯å¥½çš„æ²Ÿé€šèƒ½åŠ›ã€å›¢é˜Ÿåˆä½œæ„è¯†ï¼Œèƒ½æŠ—å‹ï¼Œèƒ½ä¸»åŠ¨æ‰¿æ‹…ï¼Œä¹äºåˆ†äº«ã€‚</p>\n<p>è¯¦æƒ…å¯è”ç³»yhcaozyyz@qq.com or 18109055866</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-13 06:45:02","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€æ‹›è˜  æ­å·ï¼Œä¸Šæµ·ã€‘Rustå¼€å‘å·¥ç¨‹å¸ˆï¼ˆ30K-50Kï¼‰","link":"https://rustcc.cn/article?id=4f8da484-d0fc-4f56-88ca-c19e7ea32b5a","description":"<p>ã€å²—ä½èŒè´£ã€‘</p>\n<ol>\n<li>è´Ÿè´£åˆ†å¸ƒå¼è®¡ç®—åŠå­˜å‚¨ç³»ç»Ÿçš„é«˜å¯æ‰©å±•åç«¯ç³»ç»Ÿï¼ŒæœåŠ¡å’ŒAPIï¼›</li>\n<li>è®¾è®¡é«˜æ€§èƒ½ã€é«˜å¯é æ€§çš„æœåŠ¡ï¼Œå»ºç«‹å¿«é€Ÿã€ç¨³å®šã€å®‰å…¨çš„åç«¯ä»£ç &nbsp;ï¼›</li>\n<li>ä¸ºå…¶ä»–å¼€å‘äººå‘˜æä¾›æŒ‡å¯¼ï¼Œå‚ä¸ç®—æ³•è®¾è®¡å’Œå®ç°ã€‚</li>\n<li>è´Ÿè´£è®¾è®¡å’Œä¼˜åŒ–åè®®ã€å¼±ç½‘é€šä¿¡ã€å­˜å‚¨ã€ç½‘ç»œå¹¶å‘ã€å¹¶è¡Œè®¡ç®—ã€åŠ å¯†ä»¥åŠå®‰å…¨ç­‰ï¼›</li>\n<li>ä¿è¯å·¥ç¨‹è´¨é‡å’Œå¼€å‘æ•ˆç‡ã€‚</li>\n<li>è®¾è®¡å’Œç»´æŠ¤æ€§èƒ½æµ‹è¯•ç”¨ä¾‹ï¼›</li>\n</ol>\n<p>ã€å²—ä½è¦æ±‚ã€‘</p>\n<ol>\n<li>è®¡ç®—æœºæˆ–è€…ç›¸å…³ä¸“ä¸šæœ¬ç§‘ä»¥ä¸Šå­¦å†ï¼Œä¸¤å¹´ä»¥ä¸Šç›¸å…³å·¥ä½œç»éªŒ</li>\n<li>æŠ€æœ¯æ‰å®ï¼Œç†Ÿæ‚‰Rustè¯­è¨€ç¼–ç¨‹</li>\n<li>ç†è§£ownership, trait, asyncç­‰è¯­è¨€æœºåˆ¶ã€‚</li>\n<li>ç†Ÿç»ƒä½¿ç”¨tokioã€‚ç†Ÿç»ƒä½¿ç”¨rustå¸¸ç”¨åº“</li>\n<li>æœ‰ä¸°å¯Œçš„å¤šçº¿ç¨‹åº”ç”¨å’Œå¹³å°æ„å»ºç»éªŒï¼Œå¯ç†Ÿç»ƒæ„å»ºç¨³å®šã€é«˜æ•ˆç‡å’Œå®‰å…¨çš„ä»£ç &nbsp;ï¼›</li>\n<li>æœ‰å¼ºçƒˆçš„ä¸Šè¿›å¿ƒå’Œæ±‚çŸ¥æ¬²ï¼Œå–„äºå­¦ä¹ å’Œè¿ç”¨æ–°çŸ¥è¯†ï¼Œå–„äºæ²Ÿé€šå’Œé€»è¾‘è¡¨è¾¾ï¼Œæœ‰å¼ºçƒˆçš„å›¢é˜Ÿæ„è¯†å’Œæ‰§è¡ŒåŠ›ã€‚</li>\n<li>ç†Ÿæ‚‰Linuxä¸‹å¤šçº¿ç¨‹/å¤šè¿›ç¨‹ç¼–ç¨‹æ¨¡å‹ï¼Œè¿›ç¨‹é—´é€šè®¯ï¼Œæ¶ˆæ¯äº‹ä»¶é€šçŸ¥ï¼ŒåŒæ­¥/å¼‚æ­¥ã€‚</li>\n<li>ç†Ÿæ‚‰Linuxä¸‹å†…å­˜ç®¡ç†æœºåˆ¶ï¼Œä½å»¶è¿Ÿã€é«˜å¹¶å‘æ— é”åŒ–ç¼–ç¨‹ã€‚</li>\n</ol>\n<p>ã€ç‰¹åˆ«å¤‡æ³¨ã€‘</p>\n<ol>\n<li>äº†è§£å®‰å…¨åŠ å¯†ç›¸å…³ç®—æ³•è€…ä¼˜å…ˆ&nbsp;ï¼›</li>\n<li>æœ‰ä¸°å¯Œçš„c++ã€pythonç¼–ç¨‹ç»éªŒè€…ä¼˜å…ˆ</li>\n<li>å‚ä¸å¤§å‹ç³»ç»Ÿçš„å¼€å‘ï¼Œå¹¶æˆåŠŸéƒ¨ç½²ã€å¹¿æ³›åº”ç”¨è€…ä¼˜å…ˆï¼›</li>\n<li>ç†Ÿæ‚‰å¤§æ•°æ®ã€æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œå¦‚:sparkï¼Œflink, tensorflowè€…ä¼˜å…ˆã€‚</li>\n</ol>\n<p>ã€å·¥ä½œåœ°ç‚¹ã€‘\nbase1: æ­å·å¸‚è¥¿æ¹–åŒºä¸­ç”µä¸‡è°·å›­åŒº\nbase2: ä¸Šæµ·å¸‚æµ¦ä¸œæ–°åŒºå‰æ»©ä¸œæ–¹å¹¿åœºä¸€æœŸ\næ­å·ä¸Šæµ·å‡æœ‰å²—ä½ã€‚</p>\n<p>è”ç³»æ–¹å¼ï¼šæœæ­Œ13732914991ï¼ˆå¾®ä¿¡åŒå·ï¼‰ é‚®ç®±ï¼šzhaoge@fudata.cn</p>\n<p>ã€å…¬å¸ä»‹ç»ã€‘\nä¸Šæµ·å¯Œæ•°ç§‘æŠ€æœ‰é™å…¬å¸ ç®€ç§°â€œå¯Œæ•°ç§‘æŠ€â€ï¼Œæ˜¯å›½å†…é¢†å…ˆçš„é‡‘èAIå’Œå®‰å…¨è®¡ç®—æŠ€æœ¯é¢†è·‘è€…ï¼Œæ ¸å¿ƒå›¢é˜Ÿæ¥è‡ªCapitalOneï¼ŒAlibabaå’ŒIBMï¼Œå…¬å¸è‡ª2016å¹´æˆç«‹ä»¥æ¥å—å›½å†…é¡¶çº§é£æŠ•é’çï¼Œå·²å®ŒæˆCè½®èèµ„ã€‚å¯Œæ•°ç§‘æŠ€åšæŒä»¥â€œä»¥æ•°æ®å®‰å…¨é©±åŠ¨äººå·¥æ™ºèƒ½â€ï¼Œä¾æ‰˜äºå®‰å…¨è®¡ç®—å’Œæœºå™¨å­¦ä¹ AIæŠ€æœ¯ï¼ŒåŠ©åŠ›é‡‘èå’Œå„è¡Œä¸šæœºæ„ç»„ç»‡æé«˜æ™ºèƒ½é£æ§ã€è¥é”€å’Œè¿è¥çš„æ•ˆç‡ï¼Œå®ç°æ•°æ®åˆè§„å®‰å…¨åœ°èåˆè®¡ç®—å’Œä»·å€¼æµé€šã€‚</p>\n<p>å¯Œæ•°ç§‘æŠ€æ˜¯ä¸­å›½é€šä¿¡æ ‡å‡†åŒ–åä¼šä¼šå‘˜ã€å·¥ä¿¡éƒ¨ä¿¡é€šé™¢å¤§æ•°æ®å®‰å…¨åŠæµé€šæ ‡å‡†ç»„æˆå‘˜ã€å®‰å…¨å¤šæ–¹è®¡ç®—æ ‡å‡†å‚ä¸æ–¹ï¼Œä¸ºè¡Œä¸šè§„èŒƒæ ‡å‡†åˆ¶å®šè´¡çŒ®åˆ›æ–°æŠ€æœ¯æˆæœã€‚å¯Œæ•°ç§‘æŠ€ç»“åˆæœ€æ–°å¯†ç å­¦å’ŒåŒºå—é“¾æŠ€æœ¯ç ”å‘åˆ›æ–°ï¼Œå…¶å®‰å…¨è®¡ç®—å’Œè”é‚¦å­¦ä¹ å¼€åˆ›æ€§åœ°é‡‡ç”¨â€œæ¾å¼›è¿­ä»£æ³•â€ï¼Œåœ¨æ™ºèƒ½åˆçº¦ã€MLç®—æ³•ä¼˜åŒ–ã€ä»£ç ç¼–è¯‘å’Œè®¡ç®—ç¡¬ä»¶èŠ¯ç‰‡èåˆæ–¹é¢æ”¹å–„æ€§èƒ½ï¼Œåœ¨åŒç­‰æ¡ä»¶ä¸‹å®ç°äº†æ”¶æ•›é€Ÿåº¦çš„å¤§å¹…æå‡ï¼Œç²¾åº¦å’Œå‡†ç¡®åº¦æŸå¤±ä½äº1%ï¼Œé€Ÿåº¦è¾ƒè¡Œä¸šæ°´å¹³æé«˜äº†3å€ã€‚</p>\n<p>å¯Œæ•°ç§‘æŠ€è‡´åŠ›äºé©±åŠ¨å®‰å…¨å¯ä¿¡çš„äººå·¥æ™ºèƒ½ç§‘æŠ€ä¸å„è¡Œä¸šåœºæ™¯çš„æ·±åº¦èåˆèµ‹èƒ½ï¼Œåœ¨å…¼é¡¾éšç§ä¿æŠ¤ä¸‹å‘æŒ¥å¤§æ•°æ®çš„å•†ä¸šä»·å€¼ã€‚å¯Œæ•°ç§‘æŠ€è‡ª2017å¹´æŠ•å…¥æ•°æ®å®‰å…¨è®¡ç®—é¢†åŸŸç ”å‘åˆ›æ–°ï¼Œæ‹¥æœ‰å¤šé¡¹ä¸“åˆ©å‘æ˜å’Œè½¯è‘—ï¼Œå¹¶ä¸å›½å†…å¤–é‡‘èæœºæ„å’Œç§‘ç ”æœºæ„ï¼ˆä¸Šæµ·äº¤å¤§ç­‰ï¼‰è”åˆç ”å‘å’Œæ¨åŠ¨å·¥ç¨‹åŒ–å•†ä¸šåŒ–è½åœ°ã€‚å¯Œæ•°ç§‘æŠ€å®‰å…¨è®¡ç®—è§£å†³æ–¹æ¡ˆå·²ç»è½åœ°åœ¨æ™ºèƒ½é£æ§ã€æ™ºèƒ½è¥é”€ã€ç›‘ç®¡å’Œç§‘ç ”ç»Ÿè®¡åˆ†æã€å¼‚ä¸šæˆ–åŒä¸šæ•°æ®å®‰å…¨èåˆè®¡ç®—ç­‰åœºæ™¯ï¼Œç›®å‰å·²åœ¨é“¶è¡Œã€æŒç‰Œæ¶ˆé‡‘ã€æ”¿åŠ¡ã€åŒ»ç–—ã€è¿è¥å•†ç­‰é¢†åŸŸç§¯ç´¯ä¸Šç™¾æ¡ˆä¾‹ï¼Œåœ¨å®‰å…¨çš„æœºå™¨å­¦ä¹ é¢†åŸŸå…·æœ‰çªå‡ºçš„é¢†å…ˆä¼˜åŠ¿ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-12 15:57:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rust æ—¥æŠ¥ã€‘2021-09-12 Rust çš„ Logging æ¨è","link":"https://rustcc.cn/article?id=91b91a60-cbe9-4f8a-a5ec-8825da34457b","description":"<h3>Rust çš„ Logging æ¨è</h3>\n<p>å†…å®¹æ•´ç†è‡ª Reddit çš„è®¨è®ºï¼š<a href=\"https://www.reddit.com/r/rust/comments/pmdh6a/what_is_the_current_recommendation_for_logging_in/\" rel=\"noopener noreferrer\">What is the current recommendation for logging in Rust? : rust</a>ã€‚</p>\n<p>é—®é¢˜ç®€è¿°ï¼šé™¤äº†æ ‡å‡†çš„ <code>log</code>ï¼Œè¿˜æœ‰ä¸å°‘é€‰æ‹©ï¼š<code>env_logger</code>ï¼Œ<code>tracing</code>ï¼Œ<code>slog</code>ï¼Œ<code>simplelog</code> ç­‰ç­‰ï¼Œæœ€ä½³å®è·µæ˜¯ä»€ä¹ˆï¼Ÿ</p>\n<p>æ¥è‡ª <a href=\"https://www.reddit.com/user/Koxiaet/\" rel=\"noopener noreferrer\">Koxiaet</a> çš„ç­”å¤ï¼šé€šå¸¸æœ‰ä¸¤ç±»ä¸æ—¥å¿—ç›¸å…³çš„ crateï¼šæ—¥å¿—æ¥å£å’Œæ—¥å¿—æ¶ˆè´¹è€…ã€‚æ¥å£æä¾›äº†æƒ³è¦è®°å½•æŸäº›ä¸œè¥¿æ—¶è°ƒç”¨çš„å‡½æ•°ï¼Œæ¶ˆè´¹è€…å¤„ç†å°†ç»“æ„åŒ–æ—¥å¿—æ•°æ®æ ¼å¼åŒ–åˆ°æŸä¸ªåœ°æ–¹ï¼ˆstderr æˆ–æ–‡ä»¶ï¼‰ã€‚ä¸¤ä¸ªä¸»è¦çš„æ¥å£æ˜¯ <code>log</code> å’Œ <code>tracing</code>ï¼Œåè€…åŠŸèƒ½æ›´å¼ºå¤§å› ä¸ºå®ƒæ”¯æŒç»“æ„åŒ–æ—¥å¿—è®°å½•ï¼Œä½†å‰è€…æ›´æ™®éã€‚è¿˜æœ‰å¦ä¸€ä¸ªç»“æ„åŒ–æ—¥å¿—æ¥å£ slogï¼Œæ¯” <code>tracing</code> æ›´å¤è€ä½†ç”¨çš„è¾ƒå°‘ã€‚æ¯ä¸ªæ—¥å¿—æ¥å£éƒ½æœ‰è‡ªå·±ç”Ÿæ€ç³»ç»Ÿï¼Œå¯ä»¥æ ¹æ®è‡ªå·±çš„éœ€è¦é€‰æ‹©ã€‚å¦‚æœåœ¨å†™ä¸€ä¸ªåº“ï¼Œ<code>log</code> æ˜¯ä¸ªä¸é”™çš„é€‰æ‹©ï¼Œå› ä¸ºæ‰€æœ‰çš„æ—¥å¿—è®°å½•æ¥å£éƒ½ä¸å®ƒå…¼å®¹ã€‚ä½†å¦‚æœä½ ç¡®å®éœ€è¦ç»“æ„åŒ–æ—¥å¿—è®°å½•ï¼Œåˆ™å¯ä»¥æ”¹ç”¨ <code>tracing</code>ï¼Œè¿™å–å†³äºä½ çš„éœ€æ±‚ï¼Œæ¯”å¦‚ä½ æ˜¯éœ€è¦å†™åˆ°æ–‡ä»¶è¿˜æ˜¯åªæ˜¯ç»ˆç«¯ã€‚</p>\n<p>å…¶ä»–ç½‘å‹çš„æ¨èï¼š</p>\n<ul>\n<li>File Loggingï¼š<a href=\"https://github.com/emabee/flexi_logger\" rel=\"noopener noreferrer\">emabee/flexi_logger: A flexible logger for rust programs that can write to stderr or to log files</a>ã€‚ï¼ˆæ¥è‡ª cfsamsonï¼‰</li>\n<li><code>tracing</code> çš„æ¥å£ï¼š<a href=\"https://docs.rs/tracing-log/0.1.2/tracing_log/\" rel=\"noopener noreferrer\">tracing_log - Rust</a>ï¼Œæœ‰å¤šä¸ªåŒæ—¶æ“ä½œäº¤é”™æ—¥å¿—æ¶ˆæ¯æ—¶ç‰¹åˆ«æ–¹ä¾¿ï¼Œå¯ä»¥æŒ‰æŸäº›å±æ€§å¯¹å®ƒä»¬è¿›è¡Œåˆ†ç»„å¹¶å•ç‹¬æŸ¥çœ‹å®ƒä»¬ã€‚ï¼ˆæ¥è‡ª class_two_perversionï¼‰</li>\n<li><a href=\"https://github.com/estk/log4rs\" rel=\"noopener noreferrer\">estk/log4rs: A highly configurable logging framework for Rust</a>ï¼Œlog4rs æ˜¯ä¸€ä¸ªé«˜åº¦å¯é…ç½®çš„æ—¥å¿—æ¡†æ¶ï¼Œä»¥ Java çš„ Logback å’Œ log4j åº“ä¸ºæ¨¡å‹ã€‚é€šè¿‡ Yaml é…ç½®ï¼Œåˆ° sdout å’Œæ–‡ä»¶ï¼Œå¸¦æœ‰æ–‡ä»¶å¤§å°é™åˆ¶é€‰é¡¹ï¼Œè¿˜å¯ä»¥é…ç½®ä¸åŒçº§åˆ«çš„æ—¥å¿—ã€‚ï¼ˆæ¥è‡ª tms102ï¼‰</li>\n<li><a href=\"https://crates.io/crates/tracing-appender\" rel=\"noopener noreferrer\">tracing-appender - crates.io: Rust Package Registry</a>ï¼Œæ¨èè€…æ‰€çŸ¥é“çš„å”¯ä¸€çº¿ç¨‹å¤–æ—¥å¿—è®°å½•è§£å†³æ–¹æ¡ˆï¼Œä¸ä»…é€‚ç”¨äºå¼‚æ­¥åº”ç”¨ç¨‹åºã€‚ï¼ˆæ¥è‡ª Pand9ï¼‰</li>\n<li><a href=\"https://github.com/daboross/fern\" rel=\"noopener noreferrer\">daboross/fern: Simple, efficient logging for Rust</a>ï¼Œåƒ Python çš„ <code>logging</code> å’Œ JS çš„ <code>Winston</code>ã€‚ï¼ˆæ¥è‡ª RapBeauticianï¼‰</li>\n</ul>\n<h3>Rust å…¨æ ˆ</h3>\n<p>æœ¬æ–‡æ˜¯ä¸€ç¯‡åšå®¢ç¿»è¯‘ï¼Œæ¥è‡ªï¼š<a href=\"https://www.justinm.one/blog/2021/09/11/fullstackrust/\" rel=\"noopener noreferrer\">Full Stack Rust - Blog</a>ã€‚</p>\n<p>ä¸€å¹´å‰ï¼Œæˆ‘çš„é¦–é€‰è¯­è¨€å¦‚ä¸‹ï¼š</p>\n<ul>\n<li>Python ç”¨äºé«˜çº§ä»£ç å¿«é€ŸåŸå‹è®¾è®¡ï¼Œæˆ–ç”¨äºéœ€è¦ç¬¬ä¸‰æ–¹åŠŸèƒ½çš„ä»£ç </li>\n<li>C/C++ ç”¨äºé•¿æœŸçš„ low-level é¡¹ç›®</li>\n</ul>\n<p>å½“æ—¶åªå¬è¿‡ Rust å¹¶ç®€å•ä½¿ç”¨è¿‡ï¼Œæˆ‘çš„ç»éªŒæ¥è‡ªç”¨ Rust å†™äº†ä¸€ä¸ªå¤„ç†å¤§æ–‡ä»¶ï¼ˆ&gt;4GBï¼‰çš„äº‹åŠ¡å¹¶ä»ä¸­æŒ–æ˜ä¸€äº›ç»Ÿè®¡ä¿¡æ¯çš„å°å·¥å…·ã€‚æˆ‘ç”¨äº†ä¸€ä¸ªåº“å°†æ–‡ä»¶æ˜ å°„åˆ°å†…å­˜ï¼Œç¼¤ç‘æŒ‰ç…§é¡ºåºå¯¹å…¶è¿›è¡Œåˆ†æã€‚æœ‰ä¸€äº›å¾ˆé…·çš„æ¦‚å¿µï¼Œæ¯”å¦‚ç¼–è¯‘å™¨é™æ€åœ°å¼ºåˆ¶å†…å­˜æ˜ å°„åœ¨å®ƒè¢«å–æ¶ˆæ˜ å°„åæ— æ³•è®¿é—®â€”â€”å¦‚æœä½ ä¸å°å¿ƒï¼ŒC++ ä¸­å¯èƒ½å°±ä¼šå‘ç”Ÿè¿™ç§é”™è¯¯ã€‚</p>\n<p>ä¸è¿‡å½“æ—¶å¹¶æ²¡æœ‰çœŸæ­£å¸å¼•æˆ‘ï¼Œå› ä¸ºé‚£åªæ˜¯ä¸€ä¸ªå°æ–°å¥‡ã€‚å½“æˆ‘å‘ <a href=\"https://github.com/DrChat/pdblister\" rel=\"noopener noreferrer\">pdblister</a> æ·»åŠ æ–°åŠŸèƒ½ä»¥å¹¶è¡Œè·å–æ•°åƒä¸ª PDB æ–‡ä»¶æ—¶è¯€çªæ¥äº†ã€‚ç”±äº GILï¼Œåœ¨ CPython ä¸­å‡ ä¹ä¸å¯èƒ½ï¼Œè€Œåœ¨ C/C++ ä¸­åšåˆ°ä¸é¢ä¸´å¹¶è¡Œé”™è¯¯æ˜¯æå…¶å›°éš¾çš„ã€‚ç„¶è€Œ Rust è®©è¿™å˜å¾—å®¹æ˜“ã€‚æˆ‘æ·»åŠ äº† tokio é©±åŠ¨çš„å¼‚æ­¥ï¼Œä½¿ç”¨ <code>tokio::spawn</code> ç”Ÿæˆæ–°ä»»åŠ¡æ¥ä¸‹è½½ PDBï¼Œå¹¶ä¿®å¤äº†ç¼–è¯‘å™¨æŠ¥çš„é”™è¯¯ï¼Œå®ƒå¯ä»¥æ­£å¸¸å·¥ä½œäº†ã€‚Rust ç¼–è¯‘å™¨è¾“å‡ºä¸€ä¸ªäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå®ƒå¯ä»¥åœ¨ä»»ä½•åœ°æ–¹è¿è¡Œï¼Œæ²¡æœ‰è¿è¡Œæ—¶ä¾èµ–ã€‚</p>\n<p><strong>å–ä»£ Python</strong></p>\n<p>è¿™æ˜¯ç¬¬ä¸€ç‚¹ï¼ŒRust æ˜¯ Python ä½œä¸ºä¸­é•¿æœŸå·¥å…·è¯­è¨€çš„ç»ä½³æ›¿ä»£å“ã€‚Python çš„å¥½å¤„æ˜¯åºå¤§çš„åº“å’Œç”Ÿæ€ç³»ç»Ÿï¼Œé€šè¿‡ pip å¯ä»¥ç›´æ¥æ‹¿åˆ°ï¼Œæƒ³è¦å¿«é€Ÿåˆ¶ä½œä¸ API äº¤äº’çš„åŸå‹ï¼Œå¯ä»¥ä½¿ç”¨ <code>requests</code>ï¼Œåªè¦ <code>import requests</code> å°±å¯ä»¥ä½¿ç”¨äº†ã€‚Rust çš„ <code>reqwest\t</code> ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œåªè¦è¾“å…¥ <code>cargo add reqwest</code> å°±å¯ä»¥åœ¨ä»£ç ä¸­ä½¿ç”¨å®ƒã€‚</p>\n<p>ç„¶è€Œå½“è¿›å…¥æ›´é•¿æœŸçš„ç”Ÿå‘½å‘¨æœŸæ—¶ï¼ŒPython å°±æ˜¾ç¤ºå‡ºåŠ£åŠ¿ï¼Œ<code>requests</code> æ˜¯ç¨‹åºçš„ä¾èµ–ï¼Œç”¨æˆ·éœ€è¦åå»åæ‰èƒ½ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œç”±äºå¼±ç±»å‹å’Œé”™è¯¯å¤„ç†èƒ½åŠ›ï¼ˆä¸ Rust æ¯”ï¼‰ï¼ŒPython å˜å¾—æ›´åŠ åŠ£åŠ¿ã€‚è¿™ä¸€ç‚¹ä¸Šï¼Œæˆ‘å¯ä»¥ä½¿ç”¨ Rust æ¯”ä½¿ç”¨ Python æ›´å¿«åœ°ç¼–å†™åŸå‹å·¥å…·ï¼Œå¹¶ä¸”æˆ‘å¯ä»¥è‡ªä¿¡åœ°çŸ¥é“æˆ‘çš„å·¥å…·æ¯”ç­‰æ•ˆçš„ Python æ›´æ˜“äºç»´æŠ¤ä¸”å¯¿å‘½æ›´é•¿ã€‚ä½†æ˜¯ï¼Œå¯¹äºçŸ­æœŸå·¥å…·ï¼ŒPython å¯èƒ½ä»ç„¶æ›´å¥½ï¼Œå› ä¸ºå®ƒä¸éœ€è¦å¯åŠ¨é¡¹ç›®å³å¯åœ¨ VSCode ä¸­è·å¾—æ™ºèƒ½æ„ŸçŸ¥æ”¯æŒã€‚ Rust çš„ cargo-script æ¥è¿‘å°† Rust æ¨å…¥è„šæœ¬è¯­è¨€çš„é¢†åŸŸï¼Œä½†ä¸å¹¸çš„æ˜¯ï¼Œæˆ‘è¿˜æ²¡æœ‰åœ¨ VSCode ä¸­æ‰¾åˆ°ä¸ä¹‹é›†æˆçš„æ’ä»¶ã€‚</p>\n<p><strong>å–ä»£ C</strong></p>\n<p>Rust ä¹Ÿæ˜¯ C çš„ç›´æ¥æ›¿ä»£å“ï¼Œå®ƒåœ¨å„æ–¹é¢éƒ½æ›´å¥½ï¼Œå¹¶ä¸”å¯ä»¥ä¸é—ç•™ C ä»£ç åŸç”Ÿäº’æ“ä½œä»¥è¿›è¡Œå¢é‡æ›¿æ¢ã€‚Rust æœ€å¤§çš„æ”¹è¿›æ˜¯ç”Ÿæ€ç³»ç»Ÿï¼šå¦‚ä¸Šæ‰€è¿°ï¼Œåˆ©ç”¨ Rust ç”Ÿæ€ä¸­å·²æœ‰çš„åº“æ˜¯å¾ˆå®¹æ˜“çš„ã€‚å¦‚æœä½ ä»æœªä½¿ç”¨è¿‡ Cï¼Œé‚£å¾ˆå¹¸è¿ï¼Œå®é™…ä¸Š C ä¸­ä½¿ç”¨é«˜çº§åŠŸèƒ½çš„æœ€ä½³æ–¹æ³•æ˜¯è‡ªå·±å†™ã€‚</p>\n<p>C ç”Ÿæ€ç³»ç»Ÿæ˜¯æ”¯ç¦»ç ´ç¢çš„ï¼Œè€Œä¸”å¾ˆè„†å¼±ã€‚ABI æˆ–æ„å»ºç³»ç»Ÿæ²¡æœ‰ä¸€è‡´çš„æ ‡å‡†ï¼š</p>\n<ul>\n<li>ç”±äºç¼ºä¹ ABI ä¸€è‡´æ€§ï¼Œä½ ä¸èƒ½è·¨å¹³å°æˆ–æ“ä½œç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„äºŒè¿›åˆ¶æ–‡ä»¶ã€‚  æ‰€ä»¥ä½ å¿…é¡»ä»æºä»£ç æ„å»ºã€‚</li>\n<li>ç”±äºç¼ºä¹ä¸€è‡´çš„æ„å»ºç³»ç»Ÿï¼Œä½ ä¸èƒ½ç®€å•åœ°å’Œåº”ç”¨ç¨‹åºä¸€èµ·æ„å»º C åº“ï¼Œå¿…é¡»ä¿®è¡¥æˆ–é‡å†™è¦ä½¿å…¶ä¸ä½ çš„åº“å…¼å®¹çš„åº“çš„æ„å»ºç³»ç»Ÿã€‚</li>\n<li>C åº“å¾ˆå°‘è·¨å¹³å°å…¼å®¹ï¼Œå› ä¸ºå®ƒä»¬ç¼ºä¹å¯ä»¥ä¾èµ–çš„å…±äº«æŠ½è±¡ã€‚</li>\n</ul>\n<p>ç„¶åè¿˜æœ‰ Rust æœ€ç‰¹è‰²çš„å®‰å…¨æ”¹è¿›â€”â€”æˆ‘å°±ä¸å±•å¼€äº†ã€‚ä½†æ ¹æ®æˆ‘çš„ç»éªŒ - å®‰å…¨æ€§åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯ä¸€ç§å·¥å…·ï¼Œå¯ä»¥è®©ç¬¬ä¸‰æ–¹åº“å¼€å‘äººå‘˜æ›´å®¹æ˜“å¼ºè¿«æˆ‘æ­£ç¡®ä½¿ç”¨ä»–ä»¬çš„åº“ï¼Œè¿™æ˜¯ C åº“ä¸èƒ½åšçš„äº‹æƒ…ã€‚</p>\n<p><strong>å…¨æ ˆ Rust</strong></p>\n<p>æ€»è€Œè¨€ä¹‹ï¼Œåœ¨è¿‡å»çš„ä¸€å¹´ä¸­ï¼Œæˆ‘ä¸€ç›´åœ¨å †æ ˆçš„æ‰€æœ‰éƒ¨åˆ†ä½¿ç”¨ Rustï¼Œè€Œæˆ‘ä¹‹å‰ä½¿ç”¨è¿‡å…¶ä»–è¯­è¨€ã€‚æˆ‘å·²ç»ä½¿ç”¨ Rust æ¥å®ç°å¼•å¯¼åŠ è½½ç¨‹åºï¼š<a href=\"https://github.com/xenia-project/xell-rs\" rel=\"noopener noreferrer\">xenia-project/xell-rs: Xell Bootloader, rewritten in Rust because Â¯_(ãƒ„)_/Â¯ï¼Œ</a>æˆ‘å·²ç»ä½¿ç”¨å®ƒé€šè¿‡ <a href=\"https://github.com/DrChat/pdblister\" rel=\"noopener noreferrer\">pdblister</a> å’Œ <a href=\"https://github.com/panamax-rs/panamax\" rel=\"noopener noreferrer\">panamax</a> ä¸­çš„é«˜çº§ HTTP/HTTPS å’Œå…¶ä»–æŠ€æœ¯æ¥é•œåƒæ–‡ä»¶ã€‚æˆ‘åˆ©ç”¨å¹¶è´¡çŒ®äº†ä¼˜ç§€çš„ <a href=\"https://github.com/DrChat/gdbstub\" rel=\"noopener noreferrer\">gdbstub</a> åº“ï¼Œç”¨äºæ§åˆ¶ç”±è‡ªå®šä¹‰ VMM è¿è¡Œçš„ VMã€‚è¿™äº›é¡¹ç›®éƒ½æ˜¯åœ¨å †æ ˆçš„ä¸åŒçº§åˆ«å®Œæˆçš„ï¼Œè€Œ Rust éå¸¸é€‚åˆæ‰€æœ‰çº§åˆ«ã€‚  æˆ‘å·²ç»å¼€å§‹åœ¨æˆ‘çš„ä¸ªäººé¡¹ç›®ä¸­ä¸“é—¨ä½¿ç”¨ Rustï¼Œå¹¶åœ¨é€‚åˆçš„æ—¶å€™æ¨åŠ¨å®ƒåœ¨æˆ‘çš„å·¥ä½œä¸­ä½¿ç”¨ã€‚</p>\n<h3>tagged_cellï¼šå¿«é€Ÿã€å¯åˆå§‹åŒ–å’Œçº¿ç¨‹å®‰å…¨çš„é™æ€å˜é‡</h3>\n<p>é€šè¿‡ <code>TaggedCell</code> å’Œ <code>Tag</code> ç±»å‹å®ç°ï¼Œä¸ºäº†å®‰å…¨æ“ä½œï¼Œ<code>TaggedCell</code> çš„æ¯ä¸ªå®ä¾‹éƒ½å¿…é¡»æ˜¯å”¯ä¸€çš„ã€‚ç„¶åå¿…é¡»é€šè¿‡ <code>TaggedCell::init ()</code> åˆå§‹åŒ– <code>TaggedCell</code>ï¼Œå®ƒä½¿ç”¨ç”¨æˆ·æä¾›çš„å‡½æ•°æˆ–é—­åŒ…åˆå§‹åŒ–åº•å±‚æ•°æ®ï¼Œç„¶åè¿”å›ä¸€ä¸ªç‰¹æ®Šçš„é›¶å¤§å°çš„ <code>Init&lt;Tag&gt;</code> ç”¨äºè®¿é—® Cell çš„æ•°æ®ã€‚ä¸ºäº†ç¡®ä¿æ¯ä¸ªå•å…ƒæ ¼ä½¿ç”¨å”¯ä¸€çš„æ ‡ç­¾ç±»å‹ï¼Œ<code>tagged_cell!</code> æä¾›å®ã€‚è¯¥å®æ ¹æ®å˜é‡çš„åç§°åˆ›å»ºä¸€ä¸ªæ–°çš„æ ‡è®°ç±»å‹ï¼Œå¹¶å°†å…¶åº”ç”¨åˆ°å£°æ˜ä¸­ã€‚</p>\n<pre><code>use tagged_cell::tagged_cell;\ntagged_cell!{\n   static BAR: TaggedCell&lt;Vec&lt;usize&gt;, _&gt; = TaggedCell::new();\n}\n\nlet tag = BAR.init(|| vec![0, 10, 20]);\nlet vec = BAR.get(tag);\n\nassert_eq!(vec[2], 20);\n</code></pre>\n<p>ä¸ºäº†å…è®¸è·¨çº¿ç¨‹ä½¿ç”¨ï¼Œåªæœ‰ç¬¬ä¸€æ¬¡è°ƒç”¨ <code>TaggedCell::init</code> æ‰ä¼šåˆå§‹åŒ– Cell çš„æ•°æ®ã€‚æ‰€æœ‰æœªæ¥çš„ <code>TaggedCell::init</code> è°ƒç”¨éƒ½å°†è¿”å›ä¸€ä¸ªæ–°æ ‡ç­¾ã€‚æœªç¡®å®šå“ªä¸ªçº¿ç¨‹å°†åˆå§‹åŒ– Cell çš„æ•°æ®ã€‚</p>\n<pre><code>use std::thread;\nuse tagged_cell::tagged_cell;\n\ntagged_cell!{\n    static TABLE: TaggedCell&lt;Vec&lt;usize&gt;, _&gt; = TaggedCell::new();\n}\n\nthread::spawn(move || {\n    let tag = TABLE.init(|| vec![0, 10, 20]);\n    let table = TABLE.get(tag);\n    assert_eq!(table[2], 20);\n});\n\nthread::spawn(move || {\n    let tag = TABLE.init(|| vec![0, 10, 20]);\n    let table = TABLE.get(tag);\n    assert_eq!(table[1], 10);\n});\n</code></pre>\n<p>GitHubï¼š<a href=\"https://github.com/Dasch0/tagged_cell\" rel=\"noopener noreferrer\">Dasch0/tagged_cell: Fast, initializable, and thread safe static variables</a></p>\n<h3>ukanren-rsï¼šÂµKanren çš„ Rust å®ç°</h3>\n<p>ÂµKanren æ˜¯ä¸€ç§è½»é‡çº§å…³ç³»ç¼–ç¨‹è¯­è¨€</p>\n<ul>\n<li>åŸå§‹çš„ Schema å®ç°åœ¨è¿™é‡Œï¼š<a href=\"https://github.com/jasonhemann/microKanren\" rel=\"noopener noreferrer\">jasonhemann/microKanren: The implementation of microKanren, a featherweight relational programming language</a></li>\n<li>ç›¸å…³å‚è€ƒï¼š<a href=\"http://minikanren.org/\" rel=\"noopener noreferrer\">miniKanren.org</a></li>\n</ul>\n<pre><code>use ukanren::*;\n\nfn appendo(first: Value, second: Value, out: Value) -&gt; BoxedGoal&lt;impl Iterator&lt;Item = State&gt;&gt; {\n    eq(&amp;first, &amp;())\n        .and(eq(&amp;second, &amp;out))\n        .or(fresh(move |a: Value, d: Value, res: Value| {\n            eq(&amp;(a.clone(), d.clone()), &amp;first)\n                .and(eq(&amp;(a.clone(), res.clone()), &amp;out))\n                .and(appendo(d.clone(), second.clone(), res))\n        }))\n        .boxed()\n}\n\nlet goal = fresh(|x, y| appendo(x, y, [1, 2, 3, 4, 5].to_value()));\nassert_eq!(\n    goal.run(2).collect::&lt;Vec&lt;_&gt;&gt;(),\n    vec![\n        state![(), [1, 2, 3, 4, 5]],\n        state![[1], [2, 3, 4, 5]],\n        state![[1, 2], [3, 4, 5]],\n        state![[1, 2, 3], [4, 5]],\n        state![[1, 2, 3, 4], [5]],\n        state![[1, 2, 3, 4, 5], ()],\n    ],\n);\n</code></pre>\n<p>GitHubï¼š<a href=\"https://github.com/ekzhang/ukanren-rs\" rel=\"noopener noreferrer\">ekzhang/ukanren-rs: Rust implementation of ÂµKanren, a featherweight relational programming language.</a></p>\n<h3>rust-counter-stringsï¼šå¿«é€Ÿå®šä½å­—ç¬¦ä¸²ä½ç½®</h3>\n<p>å­—ç¬¦ä¸²ä¸­çš„æ¯ä¸ªæ˜Ÿå·éƒ½å‡ºç°åœ¨ç”±ç´§æ¥å‰é¢çš„æ•°å­—æŒ‡å®šçš„ä½ç½®ã€‚å› æ­¤ï¼Œ29 åé¢çš„æ˜Ÿå·æ˜¯è¯¥å­—ç¬¦ä¸²ä¸­çš„ç¬¬ 29 ä¸ªå­—ç¬¦ã€‚å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹ç æ‰å­—ç¬¦ä¸²çš„æœ«å°¾ï¼Œå¹¶ä¸”ç¡®åˆ‡åœ°çŸ¥é“å®ƒåœ¨å“ªé‡Œè¢«å‰ªæ‰äº†ã€‚æ¯”å¦‚ä¸ç”¨æ•°å°±çŸ¥é“å­—ç¬¦ä¸² <code>2*4*6*8*11*14*17*2</code> æ­£å¥½æœ‰ 18 ä¸ªå­—ç¬¦ã€‚å½“å¤„ç† 50 ä¸‡ä¸ªå­—ç¬¦æ—¶ä¼šæ¯”è¾ƒçœäº‹ã€‚</p>\n<pre><code>$ ./rust-counter-strings 50\n# 2*4*6*8*11*14*17*20*23*26*29*32*35*38*41*44*47*50*\n</code></pre>\n<p>è¿™å°±æ˜¯ä¸ªå°å·¥å…·ï¼Œä»£ç ä¹Ÿåªæœ‰å‡ åè¡Œã€‚</p>\n<p>GitHubï¼š<a href=\"https://github.com/thomaschaplin/rust-counter-strings\" rel=\"noopener noreferrer\">thomaschaplin/rust-counter-strings: ğŸ§µ Generate self-describing strings of a given length to help aid software testing</a></p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ é•¿ç´</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc è®ºå›ï¼šæ”¯æŒ rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRust è¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-12 14:30:38","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-09-11 Tabled å‘å¸ƒv0.3, bma-benchmark, ferros, Velorenå‘å¸ƒv0.11","link":"https://rustcc.cn/article?id=db077b1a-5af6-4fb2-b065-ff8be974fd62","description":"<h3>Tabled å‘å¸ƒv0.3</h3>\n<p>Tabled æ˜¯ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„åº“ï¼Œç”¨äºç¾åŒ– Rust ç»“æ„å’Œæšä¸¾çš„è¾“å‡ºã€‚</p>\n<p>Github<a href=\"https://github.com/zhiburt/tabled\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼Œhttps://github.com/zhiburt/tabled</p>\n<h3>bma-benchmark ä¸€ä¸ªå‹å¥½çš„åŸºå‡†æµ‹è¯•å·¥å…·</h3>\n<p>ä½¿ç”¨ <code>bma_benchmark</code></p>\n<pre><code>#[macro_use]\nextern crate bma_benchmark;\n\nuse std::sync::Mutex;\n\nlet n = 100_000_000;\nlet mutex = Mutex::new(0);\nbenchmark_start!();\nfor _ in 0..n {\n    let _a = mutex.lock().unwrap();\n}\nbenchmark_print!(n);\n</code></pre>\n<p>ä½¿ç”¨å® <code>benchmark!</code></p>\n<pre><code>#[macro_use]\nextern crate bma_benchmark;\n\nuse std::sync::Mutex;\n\nlet mutex = Mutex::new(0);\nbenchmark!(100_000_000, {\n    let _a = mutex.lock().unwrap();\n    });\n</code></pre>\n<p><img src=\"https://raw.githubusercontent.com/alttch/bma-benchmark/main/simple.png\" alt=\"ç»“æœ\"></p>\n<p>Crate <a href=\"https://crates.io/crates/bma-benchmark\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼Œhttps://crates.io/crates/bma-benchmark</p>\n<h3>ferros</h3>\n<p>seL4 æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºæ“ä½œç³»ç»Ÿå’ŒåµŒå…¥å¼ç¨‹åºçš„å·¥å…·åŒ…ï¼Œè¿™ä¸ªå¼€æºé¡¹ç›®æ˜¯ä½¿ Rust ä¸­çš„ seL4 ç¼–ç¨‹å˜å¾—æ›´å¥½ã€‚</p>\n<p>ä»¥ä¸‹ä»£ç æ¼”ç»ƒå‡å®šä½¿ç”¨ç¤ºä¾‹ sel4_start åº“æ‰§è¡Œ selfeï¼Œå¹¶ä»‹ç»äº† ferros çš„æŸäº›æ–¹é¢ã€‚</p>\n<pre><code>use selfe_sys;\nuse ferros::alloc::{self, micro_alloc, smart_alloc};\nuse ferros::userland::{root_cnode, BootInfo};\n\n// The raw boot info is provided by the sel4_start library\nlet raw_boot_info: &amp;'static selfe_sys::seL4_BootInfo = unsafe { &amp;*sel4_start::BOOTINFO };\n\n\n// Utility for finding and claiming `Untyped` instances supplied by the boot info.\nlet mut allocator = micro_alloc::Allocator::bootstrap(&amp;raw_boot_info)?;\nlet initial_untyped = allocator\n    .get_untyped::&lt;U20&gt;() // The size of the Untyped instance, as bits\n    .expect(\"Couldn't find an untyped instance of the desired size\");\n\n// Create the top-level CNode wrapper with type-level-tracked remaining slot capacity\nlet (root_cnode, local_slots) = root_cnode(&amp;raw_boot_info);\n\n// Once we have an initial Untyped instance, memory distribution from it\n// can be tracked with compile-time checks. The smart_alloc macro synthesizes\n// the allocation code, and the capacity bounds are statically verified by\n// the type checker. The effect is that you can write 'slots' in the macro body \n// anywhere you need some slots, and you'll get the right number allocated\n// with type inference. A reference to 'ut' does the same for untyped memory. \nsmart_alloc!(|slots from local_slots, ut from uts| {\n\n    // Create a page table seL4 kernel object and return a capability pointer to it.\n    // Here we use a variable binding type annotation and Rust's type system can figure out\n    // if it can allocate a large enough Untyped instance and enough cnode slots\n    // to represent this particular kernel object.\n    let example_page_table: LocalCap&lt;UnmappedPageTable&gt; = retype(ut, slots)?;\n\n    // Create a resource-tracking wrapper around the raw boot info to assist in\n    // virtual memory related operations.\n    let boot_info  = BootInfo::wrap(raw_boot_info, ut, slots);\n    let (root_page_table, boot_info) = boot_info.map_page_table(root_page_table)?;\n});\n\n</code></pre>\n<p>Github<a href=\"https://github.com/auxoncorp/ferros\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼Œhttps://github.com/auxoncorp/ferros</p>\n<h3>Velorenå‘å¸ƒv0.11</h3>\n<p>ä»Šå¤©ï¼ŒVeloren å‘å¸ƒäº† 0.11ã€‚ è¿™ä¸ªç‰ˆæœ¬å·²ç»åˆ¶ä½œäº† 3 ä¸ªæœˆï¼Œå…¶ä¸€å¤§é‡ç‚¹æ˜¯è®©ä¸–ç•Œå„åœ°çš„æˆ˜æ–—æ›´å…·æ´»åŠ›ã€‚è¿™æ˜¯ä»¥æ–°çš„åœ°ç‚¹ç³»ç»Ÿçš„å½¢å¼å‡ºç°ï¼Œä»¥åŠ NPC å’Œç”Ÿç‰©å¦‚ä½•ä¸ä¸–ç•Œäº’åŠ¨ã€‚</p>\n<p>è¦äº†è§£è¿˜æœ‰å“ªäº›æ–°åŠŸèƒ½ï¼è¯·ç»§ç»­é˜…è¯» V0.11 å˜æ›´æ—¥å¿—<a href=\"https://veloren.net/release-0-11/\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼Œhttps://veloren.net/release-0-11/</p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ <a href=\"https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929\" rel=\"noopener noreferrer\">æ´‹èŠ‹</a></p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 15:33:08","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"cargo fix å¦‚ä½•è‡ªåŠ¨ä¿®å¤warningï¼Ÿ","link":"https://rustcc.cn/article?id=0d747849-2064-4ba2-a725-2fe64f6ab556","description":"<p>ä»£ç ä¸­æœ‰äº›å¤–é¢copyè¿‡æ¥çš„enumï¼Œå¯¼è‡´å¾ˆå¤šçš„â€œshould have an upper camel case nameâ€warningã€‚å°±æ˜¯ç¼–ç é£æ ¼çš„é—®é¢˜ï¼Œå¯æ˜¯<code>cargo fix</code>æ²¡æœ‰åŠæ³•æŒ‰ç…§rustç»™çš„å»ºè®®è‡ªåŠ¨å¸®æˆ‘æ”¹æ‰ã€‚ã€‚ã€‚ã€‚</p>\n<p>æœ‰åŠæ³•å—ï¼Ÿ</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-11 14:31:22","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust åŸ¹å…»æé«˜è®¡åˆ’ Vol. 7 - 8 | Rust é¡¹ç›®å·¥ç¨‹æ¥äº†","link":"https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b","description":"<p>æˆ‘ä»¬çš„ Rust å…¬å¼€è¯¾è¿›è¡Œäº† 6 æœŸäº†ï¼Œå¸¦å¤§å®¶äº†è§£äº† ï¼š</p>\n<ol>\n<li>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€</li>\n<li>ç†è§£ Rust æ‰€æœ‰æƒ</li>\n<li>é€šè¿‡å®æˆ˜ç†è§£ Rust å®</li>\n<li>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª</li>\n<li>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1</li>\n<li>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 2</li>\n</ol>\n<p>ç›®å‰è§†é¢‘å›æ”¾ä¼ åˆ° B ç«™æ”¶è·è®¸å¤šå¥½è¯„ï¼Œèµï¼Œä¹Ÿç»™æˆ‘ä»¬å¾ˆå¤§çš„é¼“åŠ±ã€‚å¸Œæœ›æˆ‘ä»¬çš„ Rust åŸ¹å…»æé«˜è®¡åˆ’ | Datafuse å¯ä»¥å¸®åŠ©æ›´å¤šçš„æœ‹å‹å¿«é€Ÿçš„ä½¿ç”¨ä¸Š Rust ã€‚\næœ¬å‘¨ç»™å¤§å®¶æ’ä¸¤ä¸ªå…¬å¼€è¯¾ï¼šå‘¨å››æ™šä¸Šï¼Œå‘¨æ—¥æ™šä¸Šã€‚æˆ‘ä»¬ Rust åŸ¹å…»æé«˜è®¡åˆ’é‚€è¯·åˆ°ç¬¬äºŒä½åˆ†äº«å˜‰å®¾ è‘£æ³½æ¶¦è€å¸ˆï¼Œ å¦å¤– Rust åŸ¹å…»æé«˜è®¡åˆ’ çš„å†…å®¹ä¸Šä¹Ÿåšäº†ä¸€äº›è°ƒæ•´ã€‚</p>\n<hr>\n<p>åˆ†äº«ä¸»é¢˜ï¼šã€Šæ·±å…¥äº†è§£rust é—­åŒ…ã€‹ | Vol. 7</p>\n<p>åˆ†äº«æ—¶é—´ï¼š å‘¨å››æ™šä¸Š2021-09-09 20:00-21:00</p>\n<p>åˆ†äº«è®²å¸ˆï¼š è‘£æ³½æ¶¦</p>\n<p>å†…å®¹ä»‹ç»ï¼š æ·±å…¥æµ…å‡ºäº†è§£ rust é—­åŒ…å·¥ä½œåŸç†ï¼Œè®©å¤§å®¶äº†è§£åº•å±‚å®ç°\nè®²å¸ˆä»‹ç»ï¼š\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<hr>\n<p>åˆ†äº«ä¸»é¢˜ï¼šã€Šåˆ©ç”¨ Tokio å®ç°ä¸€ä¸ªé«˜æ€§èƒ½ Mini Http serverã€‹ | Vol. 8</p>\n<p>åˆ†äº«æ—¶é—´ï¼š  å‘¨æ—¥æ™šä¸Š2021-09-12 20:00-21:00</p>\n<p>åˆ†äº«è®²å¸ˆï¼š è‹æ—</p>\n<p>é¦–å…ˆæ„Ÿè°¢è‹æ—è€å¸ˆçš„åšæŒä»˜å‡ºï¼Œ å¸¦æˆ‘ä»¬å­¦ä¹  Rust çš„é‡ç‚¹çŸ¥è¯†ã€‚ ç»è¿‡å’Œè‹ç³è€å¸ˆæ²Ÿé€šï¼Œæˆ‘ä»¬åç»­çš„è¯¾ç¨‹ï¼Œä¼šæ›´åŠ å¾€å®æˆ˜æ–¹å‘è½¬å˜ã€‚æ¥ä¸‹æ˜¯ä¸€ä¸ªç³»åˆ—çš„å†…å®¹ï¼š</p>\n<ol>\n<li>åˆ©ç”¨ Tokio å®ç°ä¸€ä¸ª Mini Http server</li>\n<li>åŸºäº Http serveræä¾›å†…å®¹åŠ¨æ€çš„ API ç½‘å…³</li>\n<li>åˆ©ç”¨ Redis å®ç°å¯¹ API ç½‘å…³åŠ é€Ÿ</li>\n<li>å­¦ä¹  Rust RPC è°ƒç”¨ï¼Œå®ç°å¾®æœåŠ¡è°ƒç”¨</li>\n</ol>\n<p>è¿™ä¸ªå†…å®¹å¯èƒ½éœ€è¦4æ¬¡å·¦å³çš„å…¬å¼€è¯¾ï¼Œç›®çš„æ˜¯å¸¦ç€å¤§å®¶åšä¸€äº›å°é¡¹ç›®ï¼Œå¸¦å¤§å®¶ç†Ÿæ‚‰ä¸€ä¸‹ Rust å·¥ç¨‹ï¼Œè®©å¤§å®¶å¯ä»¥å¿«é€ŸæŠŠ Rust ç”¨åˆ°åç«¯å¼€å‘ä¸­ã€‚</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<p>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1   | Vol. 5\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<p>Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 2  | Vol. 6\nhttps://www.bilibili.com/video/bv1oy4y1G7jC</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust å¼‚æ­¥ç¼–ç¨‹æ•™æï¼šhttps://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 02:23:16","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust å­¦ä¹ éšç¬”","link":"https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8","description":"<h1>åˆ‡æ¢é•œåƒæº</h1>\n<p>crm =&gt; https://github.com/wtklbm/crm</p>\n<p>å¸¸ç”¨å‘½ä»¤å°±æ˜¯ <code>crm best</code></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 14:35:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"pretree è¡¥å…¨æ–‡æ¡£å‘å¸ƒäº†,å†æ¬¡è°¢è°¢å¤§ç¥çš„æŒ‡ç‚¹ç»ˆäºå…¥é—¨äº†ã€‚","link":"https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827","description":"<h1>Pretree</h1>\n<p>pretree is a package for storing and querying routing rules with prefix tree .</p>\n<p>pretree æ˜¯ä¸€ä¸ªç”¨äºå­˜å‚¨å’ŒæŸ¥è¯¢è·¯ç”±è§„åˆ™çš„åŒ…ã€‚å®ƒç”¨å‰ç¼€æ ‘å­˜å‚¨è·¯ç”±è§„åˆ™ï¼Œæ”¯æŒåŒ…å«å˜é‡çš„è·¯ç”±ã€‚</p>\n<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>\n<p>Inspired by <a href=\"https://github.com/obity/pretree\" rel=\"noopener noreferrer\">obity/pretree</a> (golang)</p>\n<h1>Doc</h1>\n<p>See this document at <a href=\"https://docs.rs/pretree\" rel=\"noopener noreferrer\">API documentation</a></p>\n<h1>Install</h1>\n<p>Add the following line to your Cargo.toml file:</p>\n<pre><code>pretree = \"1.0.0\"\n</code></pre>\n<h1>Example</h1>\n<pre><code>use pretree::Pretree;\nlet mut p = Pretree::new();\np.store(\"GET\",\"account/{id}/info/:name\");\np.store(\"GET\",\"account/:id/login\");\np.store(\"GET\",\"account/{id}\");\np.store(\"GET\",\"bacteria/count_number_by_month\");\nlet (ok,rule,vars) = p.query(\"GET\",\"account/929239\");\nprintln!(\"ok:{} rule:{} vars:{:#?}\",ok,rule,vars);\n\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 09:37:30","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust å¼‚æ­¥ç¼–ç¨‹äºŒ: Tokio å…¥é—¨è¿è¡Œæ—¶ä»‹ç» | Rust åŸ¹å…»æé«˜è®¡åˆ’ Vol. 6","link":"https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a","description":"<h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹äºŒ: Tokio å…¥é—¨è¿è¡Œæ—¶ä»‹ç»ã€‹|Vol. 6</h3>\n<p><strong>è¯¾ç¨‹æ—¶é—´:</strong>  2021å¹´9æœˆ5æ—¥ 20:00-21:00</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»:</strong>  ä¸Šå‘¨å…¬å¼€è¯¾æˆ‘ä»¬è®²è§£äº† Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹ï¼ˆ å±äºä¸€ä¸ªéå¸¸ç»å…¸çš„å†…å®¹ï¼Œå»ºè®®è§‚çœ‹ ï¼‰, å¤§å®¶å¯¹ Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹æœ‰äº†ä¸€ä¸ªåˆæ­¥è®¤è¯†,  Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹é‡Œéœ€è¦ Executorã€Reactorã€Future ç­‰, æœ¬å‘¨å…¬å¼€è¯¾å°†ä»¥ Tokio æ¡†æ¶ä¸ºåŸºç¡€, å’Œå¤§å®¶ä¸€èµ·èŠèŠ Tokio é‡Œçš„ Executorã€Reactorã€Future æ˜¯ä»€ä¹ˆ?</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<p>1ã€å›é¡¾ Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>\n<p>2ã€è°ˆè°ˆå¯¹ Rust å¼‚æ­¥æ¡†æ¶çš„è®¤è¯† ( futures-rsã€async-stdã€tokio ) .</p>\n<p>3ã€Tokio ä»‹ç».</p>\n<p>4ã€Tokio é‡Œçš„ Executorã€Reactorã€Future å¦‚ä½•ä½¿ç”¨.</p>\n<p>5ã€ä½¿ç”¨ Tokio å®ç°ä¸€ä¸ªç®€å•çš„æœåŠ¡ç«¯ä¸å®¢æˆ·ç«¯ç¨‹åº.</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/\nRust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1  å›æ”¾åœ°å€ï¼š\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust å¼‚æ­¥ç¼–ç¨‹æ•™æï¼šhttps://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 08:40:15","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</h3>\n<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´8æœˆ29æ—¥ 20:00-21:00</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»:</strong>  è®²åˆ° Rust ä½¿ç”¨ Future å¼‚æ­¥ç¼–ç¨‹ï¼Œå°±ä¸å¾—ä¸è¯´ futures å’Œ tokio è¿™ä¸¤ä¸ª crateï¼Œå…¶å®æ ‡å‡†åº“ä¸­çš„ futureï¼Œä»¥åŠ async/await å°±æ˜¯ä» futures åº“ä¸­æ•´åˆè¿›æ ‡å‡†åº“çš„, Tokio æ‹¥æœ‰æå¿«çš„æ€§èƒ½ï¼Œæ˜¯å¤§éƒ¨åˆ†ç³»ç»Ÿå¼‚æ­¥å¤„ç†çš„é€‰æ‹©ï¼Œå…¶æ„å»ºäº future ä¹‹ä¸Šã€‚Future æ˜¯  Rust å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒåŸºç¡€ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<p>1ã€ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥.</p>\n<p>2ã€ç†è§£å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>\n<p>3ã€Future ç¼–ç¨‹æ¨¡å‹è®²è§£.</p>\n<p>4ã€å¸¦é¢†å¤§å®¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ future , å†æ¬¡å¸®å¿™å¤§å®¶ç†è§£</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-19 -- Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</h3>\n<p>å·²ç»åœ¨ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªcrate ä¸Šæµ‹è¯•äº†ç‰ˆæœ¬è¿ç§»,å¹¶ä¸”å°†æµ‹è¯•æ‰€æœ‰å…¬å…±çš„ crateã€‚</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>å¼‚æ­¥å¼•æ“ C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D æ¸¸æˆå¼•æ“</h3>\n<ul>\n<li><strong>PCï¼ˆWindowsã€Linuxã€macOSï¼‰å’Œ Web (WebAssembly)</strong> æ”¯æŒã€‚</li>\n<li><strong>å»¶è¿Ÿç€è‰²</strong></li>\n<li><strong>å†…ç½®ä¿å­˜/åŠ è½½</strong></li>\n<li><strong>ç‹¬ç«‹åœºæ™¯ç¼–è¾‘å™¨</strong></li>\n<li><strong>é«˜çº§ç‰©ç†æ¨¡å‹</strong></li>\n<li><strong>åˆ†å±‚æ¨¡å‹èµ„æº</strong></li>\n<li><strong>å‡ ä½•å®ä¾‹åŒ–</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ å†°å±±ä¸Šçš„ mook &amp;&amp; æŒºè‚¥</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾: é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Šé€šè¿‡Datafuseç†è§£å…¨é“¾è·¯è·Ÿè¸ªã€‹| Vol. 4</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ22æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong> æ•°æ®åº“ç³»ç»Ÿä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚ï¼Œåºå¤§çš„ç³»ç»Ÿã€‚ç‰¹åˆ«æ˜¯åœ¨è°ƒè¯•å’Œè§‚å¯ŸSQLæ‰§è¡Œï¼Œå¤šçº¿ç¨‹ä»»åŠ¡åˆ‡æ¢ï¼Œå› ä¸ºæ²¡æœ‰å†…å­˜è°ƒç”¨æˆ–å †æ ˆè·Ÿè¸ªï¼Œè¿™ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¿½è¸ªçš„ç”±æ¥ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°å¤šè¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ªä¸ºæè¿°å’Œåˆ†æè·¨è¿›ç¨‹äº‹åŠ¡æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚Google Dapper(Dapper: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿé“¾è·¯è¿½è¸ªåŸºç¡€è®¾æ–½)è®ºæ–‡(å„tracerçš„åŸºç¡€)ä¸­æè¿°äº†åˆ†å¸ƒå¼è¿½è¸ªçš„ä¸€äº›ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­ç¨³æ€é—®é¢˜ã€åˆ†å¸ƒå¼åˆ†æã€èµ„æºå±æ€§å’Œå¾®æœåŠ¡çš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ã€‚</p>\n<p>æœ¬æ¬¡å…¬å¼€è¯¾é€š Google çš„ OpenTraceing ä»‹ç»ï¼Œç»“åˆRustçš„ tokio-rs/tracing ä½¿ç”¨ï¼Œæœ€ç»ˆç»“åˆ Datafuse é¡¹ç›®ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹å¤§å‹åº”ç”¨çš„å…¨é“¾è·¯è·Ÿè¸ªåˆ†æè¿‡ç¨‹ã€‚</p>\n<p>å…³äºDatafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ol>\n<li>\n<p>ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»ŸOpenTracingåŠåº”ç”¨åœºæ™¯</p>\n</li>\n<li>\n<p>ä»‹ç» tokio-rs/tracing åŠåœ¨ç¨‹åºå¼€å‘ä¸­çš„ä½œç”¨</p>\n</li>\n<li>\n<p>ä¸ºä»€ä¹ˆéœ€è¦tokio-rs/tracingåº“</p>\n</li>\n<li>\n<p>æ¼”ç¤ºDatafuseé¡¹ç›®ä¸­tokio-rs/tracingçš„ä½¿ç”¨</p>\n</li>\n</ol>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"è®ºå›githubè´¦æˆ·æ— æ³•ç™»å½•è§£å†³ç¬”è®°","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>æœ‰åæ˜ è¿™ä¸¤å¤©githubè´¦æˆ·æ— æ³•ç™»å½•äº†ã€‚</p>\n<p>æŠ¥è¿™ä¸ªé”™ï¼š</p>\n<pre><code>get github user info err\n</code></pre>\n<p>æŸ¥äº†å‡ ä¸ªåœ°æ–¹ï¼š</p>\n<ol>\n<li>ä»£ç æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼šOk</li>\n<li>httpsä»£ç†æ˜¯å¦æ­£å¸¸ï¼šOk</li>\n<li>æ£€æŸ¥äº†githubè¿”å›æ—¥å¿—ï¼Œå‘ç°æ˜¯ï¼š</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>è¿›å…¥è¿™ä¸ªåœ°å€ä¸€çœ‹ï¼š<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>åŸæ¥2020å¹´2æœˆå°±å·²ç»è¯´äº†ï¼Œè¦æ”¹è¦æ”¹ã€‚ä¸è¿‡æˆ‘ç¡®å®æ²¡ç•™æ„åˆ°è¿™ä¸ªä¿¡æ¯ã€‚ï¼šï¼ˆ</p>\n<p>æ„æ€å°±æ˜¯è¯´access_tokenä¸è¦æ”¾åœ¨queryå‚æ•°ä¸­ï¼Œè€Œæ˜¯è¦æ”¾åœ¨headeré‡Œé¢ã€‚ç…§å®ƒè¯´çš„ï¼Œæ”¹äº†åå°±å¥½äº†ã€‚</p>\n<p>ç‰¹æ­¤è®°å½•ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust çš„ Future ä¸ Javascript çš„ Promise åŠŸèƒ½å¯¹ç…§å‚è€ƒ","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>çš„<code>Future</code>ä¸<code>Javascript</code>çš„<code>Promise</code>åŠŸèƒ½å¯¹ç…§å‚è€ƒ</h1>\n<p>å­¦ä¹ æ–°é²œæŠ€æœ¯æ—¶ï¼Œæˆ‘æ€»æ˜¯ä¼šä¹ æƒ¯æ€§å‘æ›¾ç»ç†Ÿæ‚‰çš„å†…å®¹ä¸Šé ï¼Œç”šè‡³å¥—ç”¨ç°æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¿™æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œå¯¹ç…§<code>Javascript - Promise/A+ API</code>æ¥è®°å¿†ä¸€éƒ¨åˆ†<code>Rust Future</code>å¸¸ç”¨<code>API</code>ã€‚</p>\n<blockquote>\n<p>æ³¨æ„ï¼šæ‰€æœ‰çš„<code>Rust - Future</code>æ“ä½œéƒ½æ˜¯ä»¥<code>.await</code>ç»“å°¾çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œä¸åŒäº<code>Javascript - Promise/A+</code>ï¼Œ<code>Rust - Future</code>æ˜¯æƒ°æ€§çš„ã€‚åªæœ‰è¢«<code>.await</code>æŒ‡ä»¤æ¿€æ´»åï¼Œåœ¨<code>Rust - Future</code>å†…å°è£…çš„æ“ä½œæ‰ä¼šè¢«çœŸæ­£åœ°æ‰§è¡Œã€‚</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">æè¿°</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* ä»€ä¹ˆéƒ½ä¸åš */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll ä¸èƒ½è¢«ç”¨æ¥æ„é€ åŒ…å«äº†å¼‚æ­¥æ“ä½œçš„ Future å®ä¾‹ï¼Œå› ä¸ºã€å›è°ƒé—­åŒ…ã€‘å†…çš„ã€å¯ä¿®æ”¹å¼•ç”¨ã€‘&amp;mut Context&lt;'_&gt; ä¸èƒ½è¢«  ï¼ˆ1ï¼‰è·¨çº¿ç¨‹ä¼ é€’  ï¼ˆ2ï¼‰ä¼ é€’å‡ºé—­åŒ…ä½œç”¨åŸŸ2. task::spawn_blocking() ã€å›è°ƒé—­åŒ…ã€‘è¾“å…¥å‚æ•°å†…çš„ thread::sleep() ä¸æ˜¯é˜»å¡è¿è¡Œ task::spawn_blocking() çš„ä¸»çº¿ç¨‹ï¼Œè€Œæ˜¯é˜»å¡ä»ã€é˜»å¡ä»»åŠ¡çº¿ç¨‹æ± ã€‘ä¸­åˆ†é…æ¥è¿è¡Œé˜»å¡ä»»åŠ¡çš„ã€å·¥ä½œçº¿ç¨‹ã€‘ã€‚</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. æœ‰ä¸€ä¸ª promise/future å¤±è´¥å°±æ•´ä½“æ€§åœ°å¤±è´¥ã€‚2. try_join æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future çš„æˆåŠŸä¸å¤±è´¥ç»“æœéƒ½æ”¶é›†2. è¿”å›ç»“æœï¼š(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. ä»…åªæ”¶é›†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ promise/future2. try_race æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. æ”¶é›†ç¬¬ä¸€ä¸ªç»“æŸçš„ promise/futureï¼Œæ— è®ºå®ƒæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯å¤±è´¥æ”¶åœºã€‚2. è¿”å›ç»“æœï¼šT</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼š</strong>ã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹</p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ15æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong></p>\n<p>å¦‚æœæƒ³ç”¨ Rust å¼€å‘å¤§å‹ç›®ï¼Œæˆ–è€…å­¦ä¹ å¤§å‹é¡¹ç›®ä»£ç ï¼Œç‰¹åˆ«æ˜¯æ¡†æ¶çº§åˆ«çš„é¡¹ç›®ï¼Œé‚£ä¹ˆ Rust çš„å®æœºåˆ¶è‚¯å®šæ˜¯ä¸€ä¸ªå¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚ ä¾‹å¦‚ datafuse ä¸­çš„ä¸€äº›é…ç½®ç®¡ç†ï¼š\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>è¿™å°±æ˜¯é€šè¿‡å®å®ç°é…ç½®çš„ç»Ÿä¸€è¡Œä¸ºï¼Œä»£ç å‚è€ƒï¼š\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust è¯­è¨€å¼ºå¤§çš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¯ä»¥åˆ›å»ºå’Œåˆ©ç”¨å®ï¼Œä¸è¿‡åˆ›å»ºå®çœ‹èµ·æ¥æŒºå¤æ‚ï¼Œå¸¸å¸¸ä»¤åˆšæ¥è§¦ Rust çš„å¼€å‘è€…ç”Ÿç•æƒ§ã€‚ åœ¨æœ¬æ¬¡å…¬å¼€è¯¾ä¸­å¸®åŠ©ä½ ç†è§£ Rust Macro çš„åŸºæœ¬åŸç†ï¼Œå­¦ä¹ å¦‚ä½•åˆ›è‡ªå·²çš„ Rust å®ï¼Œä»¥åŠæŸ¥çœ‹æºç å­¦ä¹ å®çš„å®ç°ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ul>\n<li>ä»€ä¹ˆæ˜¯ Rust å®</li>\n<li>ä»€ä¹ˆæ˜¯å®è¿è¡ŒåŸç†</li>\n<li>å¦‚ä½•åˆ›å»º Rust å®è¿‡ç¨‹</li>\n<li>é˜…è¯» datafuse é¡¹ç›®æºç ï¼Œ å­¦ä¹ é¡¹ç›®ä¸­å®çš„å®ç°</li>\n</ul>\n<p><strong>è®²å¸ˆä»‹ç»</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-09-15T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"The Emergence of the Shape Bias Results from Communicative Efficiency. (arXiv:2109.06232v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06232","description":"<p>By the age of two, children tend to assume that new word categories are based\non objects' shape, rather than their color or texture; this assumption is\ncalled the shape bias. They are thought to learn this bias by observing that\ntheir caregiver's language is biased towards shape based categories. This\npresents a chicken and egg problem: if the shape bias must be present in the\nlanguage in order for children to learn it, how did it arise in language in the\nfirst place? In this paper, we propose that communicative efficiency explains\nboth how the shape bias emerged and why it persists across generations. We\nmodel this process with neural emergent language agents that learn to\ncommunicate about raw pixelated images. First, we show that the shape bias\nemerges as a result of efficient communication strategies employed by agents.\nSecond, we show that pressure brought on by communicative need is also\nnecessary for it to persist across generations; simply having a shape bias in\nan agent's input language is insufficient. These results suggest that, over and\nabove the operation of other learning strategies, the shape bias in human\nlearners may emerge and be sustained by communicative pressures.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Portelance_E/0/1/0/all/0/1\">Eva Portelance</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_M/0/1/0/all/0/1\">Michael C. Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1\">Romain Laroche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KroneckerBERT: Learning Kronecker Decomposition for Pre-trained Language Models via Knowledge Distillation. (arXiv:2109.06243v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06243","description":"<p>The development of over-parameterized pre-trained language models has made a\nsignificant contribution toward the success of natural language processing.\nWhile over-parameterization of these models is the key to their generalization\npower, it makes them unsuitable for deployment on low-capacity devices. We push\nthe limits of state-of-the-art Transformer-based pre-trained language model\ncompression using Kronecker decomposition. We use this decomposition for\ncompression of the embedding layer, all linear mappings in the multi-head\nattention, and the feed-forward network modules in the Transformer layer. We\nperform intermediate-layer knowledge distillation using the uncompressed model\nas the teacher to improve the performance of the compressed model. We present\nour KroneckerBERT, a compressed version of the BERT_BASE model obtained using\nthis framework. We evaluate the performance of KroneckerBERT on well-known NLP\nbenchmarks and show that for a high compression factor of 19 (5% of the size of\nthe BERT_BASE model), our KroneckerBERT outperforms state-of-the-art\ncompression methods on the GLUE. Our experiments indicate that the proposed\nmodel has promising out-of-distribution robustness and is superior to the\nstate-of-the-art compression methods on SQuAD.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tahaei_M/0/1/0/all/0/1\">Marzieh S. Tahaei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Charlaix_E/0/1/0/all/0/1\">Ella Charlaix</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nia_V/0/1/0/all/0/1\">Vahid Partovi Nia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghodsi_A/0/1/0/all/0/1\">Ali Ghodsi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1\">Mehdi Rezagholizadeh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multi-Sentence Resampling: A Simple Approach to Alleviate Dataset Length Bias and Beam-Search Degradation. (arXiv:2109.06253v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06253","description":"<p>Neural Machine Translation (NMT) is known to suffer from a beam-search\nproblem: after a certain point, increasing beam size causes an overall drop in\ntranslation quality. This effect is especially pronounced for long sentences.\nWhile much work was done analyzing this phenomenon, primarily for\nautoregressive NMT models, there is still no consensus on its underlying cause.\nIn this work, we analyze errors that cause major quality degradation with large\nbeams in NMT and Automatic Speech Recognition (ASR). We show that a factor that\nstrongly contributes to the quality degradation with large beams is\n\\textit{dataset length-bias} - \\textit{NMT datasets are strongly biased towards\nshort sentences}. To mitigate this issue, we propose a new data augmentation\ntechnique -- \\textit{Multi-Sentence Resampling (MSR)}. This technique extends\nthe training examples by concatenating several sentences from the original\ndataset to make a long training example. We demonstrate that MSR significantly\nreduces degradation with growing beam size and improves final translation\nquality on the IWSTL$15$ En-Vi, IWSTL$17$ En-Fr, and WMT$14$ En-De datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Provilkov_I/0/1/0/all/0/1\">Ivan Provilkov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malinin_A/0/1/0/all/0/1\">Andrey Malinin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Multiway Multilingual NMT in the Turkic Languages. (arXiv:2109.06262v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06262","description":"<p>Despite the increasing number of large and comprehensive machine translation\n(MT) systems, evaluation of these methods in various languages has been\nrestrained by the lack of high-quality parallel corpora as well as engagement\nwith the people that speak these languages. In this study, we present an\nevaluation of state-of-the-art approaches to training and evaluating MT systems\nin 22 languages from the Turkic language family, most of which being extremely\nunder-explored. First, we adopt the TIL Corpus with a few key improvements to\nthe training and the evaluation sets. Then, we train 26 bilingual baselines as\nwell as a multi-way neural MT (MNMT) model using the corpus and perform an\nextensive analysis using automatic metrics as well as human evaluations. We\nfind that the MNMT model outperforms almost all bilingual baselines in the\nout-of-domain test sets and finetuning the model on a downstream task of a\nsingle pair also results in a huge performance boost in both low- and\nhigh-resource scenarios. Our attentive analysis of evaluation criteria for MT\nmodels in Turkic languages also points to the necessity for further research in\nthis direction. We release the corpus splits, test sets as well as models to\nthe public.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mirzakhalov_J/0/1/0/all/0/1\">Jamshidbek Mirzakhalov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Babu_A/0/1/0/all/0/1\">Anoop Babu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kunafin_A/0/1/0/all/0/1\">Aigiz Kunafin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wahab_A/0/1/0/all/0/1\">Ahsan Wahab</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moydinboyev_B/0/1/0/all/0/1\">Behzod Moydinboyev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ivanova_S/0/1/0/all/0/1\">Sardana Ivanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uzokova_M/0/1/0/all/0/1\">Mokhiyakhon Uzokova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pulatova_S/0/1/0/all/0/1\">Shaxnoza Pulatova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ataman_D/0/1/0/all/0/1\">Duygu Ataman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kreutzer_J/0/1/0/all/0/1\">Julia Kreutzer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tyers_F/0/1/0/all/0/1\">Francis Tyers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Firat_O/0/1/0/all/0/1\">Orhan Firat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Licato_J/0/1/0/all/0/1\">John Licato</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chellappan_S/0/1/0/all/0/1\">Sriram Chellappan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Post-OCR Document Correction with large Ensembles of Character Sequence Models. (arXiv:2109.06264v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06264","description":"<p>In this paper, we propose a novel method based on character\nsequence-to-sequence models to correct documents already processed with Optical\nCharacter Recognition (OCR) systems. The main contribution of this paper is a\nset of strategies to accurately process strings much longer than the ones used\nto train the sequence model while being sample- and resource-efficient,\nsupported by thorough experimentation. The strategy with the best performance\ninvolves splitting the input document in character n-grams and combining their\nindividual corrections into the final output using a voting scheme that is\nequivalent to an ensemble of a large number of sequence models. We further\ninvestigate how to weigh the contributions from each one of the members of this\nensemble. We test our method on nine languages of the ICDAR 2019 competition on\npost-OCR text correction and achieve a new state-of-the-art performance in five\nof them. Our code for post-OCR correction is shared at\nhttps://github.com/jarobyte91/post_ocr_correction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramirez_Orta_J/0/1/0/all/0/1\">Juan Ramirez-Orta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xamena_E/0/1/0/all/0/1\">Eduardo Xamena</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maguitman_A/0/1/0/all/0/1\">Ana Maguitman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Milios_E/0/1/0/all/0/1\">Evangelos Milios</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Soto_A/0/1/0/all/0/1\">Axel J. Soto</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"STraTA: Self-Training with Task Augmentation for Better Few-shot Learning. (arXiv:2109.06270v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06270","description":"<p>Despite their recent successes in tackling many NLP tasks, large-scale\npre-trained language models do not perform as well in few-shot settings where\nonly a handful of training examples are available. To address this shortcoming,\nwe propose STraTA, which stands for Self-Training with Task Augmentation, an\napproach that builds on two key ideas for effective leverage of unlabeled data.\nFirst, STraTA uses task augmentation, a novel technique that synthesizes a\nlarge amount of data for auxiliary-task fine-tuning from target-task unlabeled\ntexts. Second, STraTA performs self-training by further fine-tuning the strong\nbase model created by task augmentation on a broad distribution of\npseudo-labeled data. Our experiments demonstrate that STraTA can substantially\nimprove sample efficiency across 12 few-shot benchmarks. Remarkably, on the\nSST-2 sentiment dataset, STraTA, with only 8 training examples per class,\nachieves comparable results to standard fine-tuning with 67K training examples.\nOur analyses reveal that task augmentation and self-training are both\ncomplementary and independently effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1\">Tu Vu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luong_M/0/1/0/all/0/1\">Minh-Thang Luong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simon_G/0/1/0/all/0/1\">Grady Simon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MindCraft: Theory of Mind Modeling for Situated Dialogue in Collaborative Tasks. (arXiv:2109.06275v1 [cs.AI])","link":"http://arxiv.org/abs/2109.06275","description":"<p>An ideal integration of autonomous agents in a human world implies that they\nare able to collaborate on human terms. In particular, theory of mind plays an\nimportant role in maintaining common ground during human collaboration and\ncommunication. To enable theory of mind modeling in situated interactions, we\nintroduce a fine-grained dataset of collaborative tasks performed by pairs of\nhuman subjects in the 3D virtual blocks world of Minecraft. It provides\ninformation that captures partners' beliefs of the world and of each other as\nan interaction unfolds, bringing abundant opportunities to study human\ncollaborative behaviors in situated language communication. As a first step\ntowards our goal of developing embodied AI agents able to infer belief states\nof collaborative partners in situ, we build and present results on\ncomputational models for several theory of mind tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bara_C/0/1/0/all/0/1\">Cristian-Paul Bara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+CH_Wang_S/0/1/0/all/0/1\">Sky CH-Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1\">Joyce Chai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Graph Algorithms for Multiparallel Word Alignment. (arXiv:2109.06283v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06283","description":"<p>With the advent of end-to-end deep learning approaches in machine\ntranslation, interest in word alignments initially decreased; however, they\nhave again become a focus of research more recently. Alignments are useful for\ntypological research, transferring formatting like markup to translated texts,\nand can be used in the decoding of machine translation systems. At the same\ntime, massively multilingual processing is becoming an important NLP scenario,\nand pretrained language and machine translation models that are truly\nmultilingual are proposed. However, most alignment algorithms rely on bitexts\nonly and do not leverage the fact that many parallel corpora are multiparallel.\nIn this work, we exploit the multiparallelity of corpora by representing an\ninitial set of bilingual alignments as a graph and then predicting additional\nedges in the graph. We present two graph algorithms for edge prediction: one\ninspired by recommender systems and one based on network link prediction. Our\nexperimental results show absolute improvements in $F_1$ of up to 28% over the\nbaseline bilingual word aligner in different datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Imani_A/0/1/0/all/0/1\">Ayyoob Imani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sabet_M/0/1/0/all/0/1\">Masoud Jalili Sabet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Senel_L/0/1/0/all/0/1\">L&#xfc;tfi Kerem &#x15e;enel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dufter_P/0/1/0/all/0/1\">Philipp Dufter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yvon_F/0/1/0/all/0/1\">Fran&#xe7;ois Yvon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1\">Hinrich Sch&#xfc;tze</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Phrase-BERT: Improved Phrase Embeddings from BERT with an Application to Corpus Exploration. (arXiv:2109.06304v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06304","description":"<p>Phrase representations derived from BERT often do not exhibit complex phrasal\ncompositionality, as the model relies instead on lexical similarity to\ndetermine semantic relatedness. In this paper, we propose a contrastive\nfine-tuning objective that enables BERT to produce more powerful phrase\nembeddings. Our approach (Phrase-BERT) relies on a dataset of diverse phrasal\nparaphrases, which is automatically generated using a paraphrase generation\nmodel, as well as a large-scale dataset of phrases in context mined from the\nBooks3 corpus. Phrase-BERT outperforms baselines across a variety of\nphrase-level similarity tasks, while also demonstrating increased lexical\ndiversity between nearest neighbors in the vector space. Finally, as a case\nstudy, we show that Phrase-BERT embeddings can be easily integrated with a\nsimple autoencoder to build a phrase-based neural topic model that interprets\ntopics as mixtures of words and phrases by performing a nearest neighbor search\nin the embedding space. Crowdsourced evaluations demonstrate that this\nphrase-based topic model produces more coherent and meaningful topics than\nbaseline word and phrase-level topic models, further validating the utility of\nPhrase-BERT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shufan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thompson_L/0/1/0/all/0/1\">Laure Thompson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Catastrophic Forgetting in Scheduled Sampling with Elastic Weight Consolidation in Neural Machine Translation. (arXiv:2109.06308v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06308","description":"<p>Despite strong performance in many sequence-to-sequence tasks, autoregressive\nmodels trained with maximum likelihood estimation suffer from exposure bias,\ni.e. a discrepancy between the ground-truth prefixes used during training and\nthe model-generated prefixes used at inference time. Scheduled sampling is a\nsimple and often empirically successful approach which addresses this issue by\nincorporating model-generated prefixes into the training process. However, it\nhas been argued that it is an inconsistent training objective leading to models\nignoring the prefixes altogether. In this paper, we conduct systematic\nexperiments and find that it ameliorates exposure bias by increasing model\nreliance on the input sequence. We also observe that as a side-effect, it\nworsens performance when the model-generated prefix is correct, a form of\ncatastrophic forgetting. We propose using Elastic Weight Consolidation as\ntrade-off between mitigating exposure bias and retaining output quality.\nExperiments on two IWSLT'14 translation tasks demonstrate that our approach\nalleviates catastrophic forgetting and significantly improves BLEU compared to\nstandard scheduled sampling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korakakis_M/0/1/0/all/0/1\">Michalis Korakakis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1\">Andreas Vlachos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Constraints and Descriptive Segmentation for Subevent Detection. (arXiv:2109.06316v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06316","description":"<p>Event mentions in text correspond to real-world events of varying degrees of\ngranularity. The task of subevent detection aims to resolve this granularity\nissue, recognizing the membership of multi-granular events in event complexes.\nSince knowing the span of descriptive contexts of event complexes helps infer\nthe membership of events, we propose the task of event-based text segmentation\n(EventSeg) as an auxiliary task to improve the learning for subevent detection.\nTo bridge the two tasks together, we propose an approach to learning and\nenforcing constraints that capture dependencies between subevent detection and\nEventSeg prediction, as well as guiding the model to make globally consistent\ninference. Specifically, we adopt Rectifier Networks for constraint learning\nand then convert the learned constraints to a regularization term in the loss\nfunction of the neural model. Experimental results show that the proposed\nmethod outperforms baseline methods by 2.3% and 2.5% on benchmark datasets for\nsubevent detection, HiEve and IC, respectively, while achieving a decent\nperformance on EventSeg prediction.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haoyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Hongming Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1\">Muhao Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1\">Dan Roth</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Massively Multilingual Analysis of Cross-linguality in Shared Embedding Space. (arXiv:2109.06324v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06324","description":"<p>In cross-lingual language models, representations for many different\nlanguages live in the same space. Here, we investigate the linguistic and\nnon-linguistic factors affecting sentence-level alignment in cross-lingual\npretrained language models for 101 languages and 5,050 language pairs. Using\nBERT-based LaBSE and BiLSTM-based LASER as our models, and the Bible as our\ncorpus, we compute a task-based measure of cross-lingual alignment in the form\nof bitext retrieval performance, as well as four intrinsic measures of vector\nspace alignment and isomorphism. We then examine a range of linguistic,\nquasi-linguistic, and training-related features as potential predictors of\nthese alignment metrics. The results of our analyses show that word order\nagreement and agreement in morphological complexity are two of the strongest\nlinguistic predictors of cross-linguality. We also note in-family training data\nas a stronger predictor than language-specific training data across the board.\nWe verify some of our linguistic findings by looking at the effect of\nmorphological segmentation on English-Inuktitut alignment, in addition to\nexamining the effect of word order agreement on isomorphism for 66 zero-shot\nlanguage pairs from a different corpus. We make the data and code for our\nexperiments publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jones_A/0/1/0/all/0/1\">Alex Jones</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1\">Kyle Mahowald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating Transferability of BERT Models on Uralic Languages. (arXiv:2109.06327v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06327","description":"<p>Transformer-based language models such as BERT have outperformed previous\nmodels on a large number of English benchmarks, but their evaluation is often\nlimited to English or a small number of well-resourced languages. In this work,\nwe evaluate monolingual, multilingual, and randomly initialized language models\nfrom the BERT family on a variety of Uralic languages including Estonian,\nFinnish, Hungarian, Erzya, Moksha, Karelian, Livvi, Komi Permyak, Komi Zyrian,\nNorthern S\\'ami, and Skolt S\\'ami. When monolingual models are available\n(currently only et, fi, hu), these perform better on their native language, but\nin general they transfer worse than multilingual models or models of\ngenetically unrelated languages that share the same character set. Remarkably,\nstraightforward transfer of high-resource models, even without special efforts\ntoward hyperparameter optimization, yields what appear to be state of the art\nPOS and NER tools for the minority Uralic languages where there is sufficient\ndata for finetuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Acs_J/0/1/0/all/0/1\">Judit &#xc1;cs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levai_D/0/1/0/all/0/1\">D&#xe1;niel L&#xe9;vai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kornai_A/0/1/0/all/0/1\">Andr&#xe1;s Kornai</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Old BERT, New Tricks: Artificial Language Learning for Pre-Trained Language Models. (arXiv:2109.06333v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06333","description":"<p>We extend the artificial language learning experimental paradigm from\npsycholinguistics and apply it to pre-trained language models -- specifically,\nBERT (Devlin et al., 2019). We treat the model as a subject in an artificial\nlanguage learning experimental setting: in order to learn the relation between\ntwo linguistic properties A and B, we introduce a set of new, non-existent,\nlinguistic items, give the model information about their variation along\nproperty A, then measure to what extent the model learns property B for these\nitems as a result of training. We show this method at work for degree modifiers\n(expressions like \"slightly\", \"very\", \"rather\", \"extremely\") and test the\nhypothesis that the degree expressed by modifiers (low, medium or high degree)\nis related to their sensitivity to sentence polarity (whether they show\npreference for affirmative or negative sentences or neither). Our experimental\nresults are compatible with existing linguistic observations that relate degree\nsemantics to polarity-sensitivity, including the main one: low degree semantics\nleads to positive polarity sensitivity (that is, to preference towards\naffirmative contexts). The method can be used in linguistics to elaborate on\nhypotheses and interpret experimental results, as well as for more insightful\nevaluation of linguistic representations in language models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bylinina_L/0/1/0/all/0/1\">Lisa Bylinina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tikhonov_A/0/1/0/all/0/1\">Alexey Tikhonov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garmash_E/0/1/0/all/0/1\">Ekaterina Garmash</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Few-Shot Intent Detection via Contrastive Pre-Training and Fine-Tuning. (arXiv:2109.06349v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06349","description":"<p>In this work, we focus on a more challenging few-shot intent detection\nscenario where many intents are fine-grained and semantically similar. We\npresent a simple yet effective few-shot intent detection schema via contrastive\npre-training and fine-tuning. Specifically, we first conduct self-supervised\ncontrastive pre-training on collected intent datasets, which implicitly learns\nto discriminate semantically similar utterances without using any labels. We\nthen perform few-shot intent detection together with supervised contrastive\nlearning, which explicitly pulls utterances from the same intent closer and\npushes utterances across different intents farther. Experimental results show\nthat our proposed method achieves state-of-the-art performance on three\nchallenging intent detection datasets under 5-shot and 10-shot settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jianguo Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bui_T/0/1/0/all/0/1\">Trung Bui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Seunghyun Yoon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiwei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1\">Congying Xia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tran_Q/0/1/0/all/0/1\">Quan Hung Tran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_W/0/1/0/all/0/1\">Walter Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncertainty-Aware Machine Translation Evaluation. (arXiv:2109.06352v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06352","description":"<p>Several neural-based metrics have been recently proposed to evaluate machine\ntranslation quality. However, all of them resort to point estimates, which\nprovide limited information at segment level. This is made worse as they are\ntrained on noisy, biased and scarce human judgements, often resulting in\nunreliable quality predictions. In this paper, we introduce uncertainty-aware\nMT evaluation and analyze the trustworthiness of the predicted quality. We\ncombine the COMET framework with two uncertainty estimation methods, Monte\nCarlo dropout and deep ensembles, to obtain quality scores along with\nconfidence intervals. We compare the performance of our uncertainty-aware MT\nevaluation methods across multiple language pairs from the QT21 dataset and the\nWMT20 metrics task, augmented with MQM annotations. We experiment with varying\nnumbers of references and further discuss the usefulness of uncertainty-aware\nquality estimation (without references) to flag possibly critical translation\nmistakes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glushkova_T/0/1/0/all/0/1\">Taisiya Glushkova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zerva_C/0/1/0/all/0/1\">Chrysoula Zerva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rei_R/0/1/0/all/0/1\">Ricardo Rei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martins_A/0/1/0/all/0/1\">Andr&#xe9; F. T. Martins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hunspell for Sorani Kurdish Spell Checking and Morphological Analysis. (arXiv:2109.06374v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06374","description":"<p>Spell checking and morphological analysis are two fundamental tasks in text\nand natural language processing and are addressed in the early stages of the\ndevelopment of language technology. Despite the previous efforts, there is no\nprogress in open-source to create such tools for Sorani Kurdish, also known as\nCentral Kurdish, as a less-resourced language. In this paper, we present our\nefforts in annotating a lexicon with morphosyntactic tags and also, extracting\nmorphological rules of Sorani Kurdish to build a morphological analyzer, a\nstemmer and a spell-checking system using Hunspell. This implementation can be\nused for further developments in the field by researchers and also, be\nintegrated into text editors under a publicly available license.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahmadi_S/0/1/0/all/0/1\">Sina Ahmadi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation. (arXiv:2109.06379v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06379","description":"<p>Natural language generation (NLG) spans a broad range of tasks, each of which\nserves for specific objectives and desires different properties of generated\ntext. The complexity makes automatic evaluation of NLG particularly\nchallenging. Previous work has typically focused on a single task and developed\nindividual evaluation metrics based on specific intuitions. In this paper, we\npropose a unifying perspective based on the nature of information change in NLG\ntasks, including compression (e.g., summarization), transduction (e.g., text\nrewriting), and creation (e.g., dialog). Information alignment between input,\ncontext, and output text plays a common central role in characterizing the\ngeneration. With automatic alignment prediction models, we develop a family of\ninterpretable metrics that are suitable for evaluating key aspects of different\nNLG tasks, often without need of gold reference data. Experiments show the\nuniformly designed metrics achieve stronger or comparable correlations with\nhuman judgement compared to state-of-the-art metrics in each of diverse tasks,\nincluding text summarization, style transfer, and knowledge-grounded dialog.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Deng_M/0/1/0/all/0/1\">Mingkai Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_B/0/1/0/all/0/1\">Bowen Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rationales for Sequential Predictions. (arXiv:2109.06387v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06387","description":"<p>Sequence models are a critical component of modern NLP systems, but their\npredictions are difficult to explain. We consider model explanations though\nrationales, subsets of context that can explain individual model predictions.\nWe find sequential rationales by solving a combinatorial optimization: the best\nrationale is the smallest subset of input tokens that would predict the same\noutput as the full sequence. Enumerating all subsets is intractable, so we\npropose an efficient greedy algorithm to approximate this objective. The\nalgorithm, which is called greedy rationalization, applies to any model. For\nthis approach to be effective, the model should form compatible conditional\ndistributions when making predictions on incomplete subsets of the context.\nThis condition can be enforced with a short fine-tuning step. We study greedy\nrationalization on language modeling and machine translation. Compared to\nexisting baselines, greedy rationalization is best at optimizing the\ncombinatorial objective and provides the most faithful rationales. On a new\ndataset of annotated sequential rationales, greedy rationales are most similar\nto human rationales.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vafa_K/0/1/0/all/0/1\">Keyon Vafa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1\">Yuntian Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blei_D/0/1/0/all/0/1\">David M. Blei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rush_A/0/1/0/all/0/1\">Alexander M. Rush</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Proposal Generation Network for Temporal Sentence Localization in Videos. (arXiv:2109.06398v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06398","description":"<p>We address the problem of temporal sentence localization in videos (TSLV).\nTraditional methods follow a top-down framework which localizes the target\nsegment with pre-defined segment proposals. Although they have achieved decent\nperformance, the proposals are handcrafted and redundant. Recently, bottom-up\nframework attracts increasing attention due to its superior efficiency. It\ndirectly predicts the probabilities for each frame as a boundary. However, the\nperformance of bottom-up model is inferior to the top-down counterpart as it\nfails to exploit the segment-level interaction. In this paper, we propose an\nAdaptive Proposal Generation Network (APGN) to maintain the segment-level\ninteraction while speeding up the efficiency. Specifically, we first perform a\nforeground-background classification upon the video and regress on the\nforeground frames to adaptively generate proposals. In this way, the\nhandcrafted proposal design is discarded and the redundant proposals are\ndecreased. Then, a proposal consolidation module is further developed to\nenhance the semantic of the generated proposals. Finally, we locate the target\nmoments with these generated proposals following the top-down framework.\nExtensive experiments on three challenging benchmarks show that our proposed\nAPGN significantly outperforms previous state-of-the-art methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1\">Jianfeng Dong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Progressively Guide to Attend: An Iterative Alignment Framework for Temporal Sentence Grounding. (arXiv:2109.06400v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06400","description":"<p>A key solution to temporal sentence grounding (TSG) exists in how to learn\neffective alignment between vision and language features extracted from an\nuntrimmed video and a sentence description. Existing methods mainly leverage\nvanilla soft attention to perform the alignment in a single-step process.\nHowever, such single-step attention is insufficient in practice, since\ncomplicated relations between inter- and intra-modality are usually obtained\nthrough multi-step reasoning. In this paper, we propose an Iterative Alignment\nNetwork (IA-Net) for TSG task, which iteratively interacts inter- and\nintra-modal features within multiple steps for more accurate grounding.\nSpecifically, during the iterative reasoning process, we pad multi-modal\nfeatures with learnable parameters to alleviate the nowhere-to-attend problem\nof non-matched frame-word pairs, and enhance the basic co-attention mechanism\nin a parallel manner. To further calibrate the misaligned attention caused by\neach reasoning step, we also devise a calibration module following each\nattention module to refine the alignment knowledge. With such iterative\nalignment scheme, our IA-Net can robustly capture the fine-grained relations\nbetween vision and language domains step-by-step for progressively reasoning\nthe temporal boundaries. Extensive experiments conducted on three challenging\nbenchmarks demonstrate that our proposed model performs better than the\nstate-of-the-arts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1\">Daizong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1\">Xiaoye Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pan Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Personality and Online Social Engagement: An Investigation of MBTI Users on Twitter. (arXiv:2109.06402v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06402","description":"<p>Text-based personality prediction by computational models is an emerging\nfield with the potential to significantly improve on key weaknesses of\nsurvey-based personality assessment. We investigate 3848 profiles from Twitter\nwith self-labeled Myers-Briggs personality traits (MBTI) - a framework closely\nrelated to the Five Factor Model of personality - to better understand how\ntext-based digital traces from social engagement online can be used to predict\nuser personality traits. We leverage BERT, a state-of-the-art NLP architecture\nbased on deep learning, to analyze various sources of text that hold most\npredictive power for our task. We find that biographies, statuses, and liked\ntweets contain significant predictive power for all dimensions of the MBTI\nsystem. We discuss our findings and their implications for the validity of the\nMBTI and the lexical hypothesis, a foundational theory underlying the Five\nFactor Model that links language use and behavior. Our results hold optimistic\nimplications for personality psychologists, computational linguists, and other\nsocial scientists aiming to predict personality from observational text data\nand explore the links between language and core behavioral traits.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kadambi_P/0/1/0/all/0/1\">Partha Kadambi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradient Imitation Reinforcement Learning for Low Resource Relation Extraction. (arXiv:2109.06415v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06415","description":"<p>Low-resource Relation Extraction (LRE) aims to extract relation facts from\nlimited labeled corpora when human annotation is scarce. Existing works either\nutilize self-training scheme to generate pseudo labels that will cause the\ngradual drift problem, or leverage meta-learning scheme which does not solicit\nfeedback explicitly. To alleviate selection bias due to the lack of feedback\nloops in existing LRE learning paradigms, we developed a Gradient Imitation\nReinforcement Learning method to encourage pseudo label data to imitate the\ngradient descent direction on labeled data and bootstrap its optimization\ncapability through trial and error. We also propose a framework called GradLRE,\nwhich handles two major scenarios in low-resource relation extraction. Besides\nthe scenario where unlabeled data is sufficient, GradLRE handles the situation\nwhere no unlabeled data is available, by exploiting a contextualized\naugmentation method to generate data. Experimental results on two public\ndatasets demonstrate the effectiveness of GradLRE on low resource relation\nextraction when comparing with baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1\">Xuming Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chenwei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yawen Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaohe Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1\">Li Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1\">Lijie Wen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1\">Philip S. Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-document Event Identity via Dense Annotation. (arXiv:2109.06417v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06417","description":"<p>In this paper, we study the identity of textual events from different\ndocuments. While the complex nature of event identity is previously studied\n(Hovy et al., 2013), the case of events across documents is unclear. Prior work\non cross-document event coreference has two main drawbacks. First, they\nrestrict the annotations to a limited set of event types. Second, they\ninsufficiently tackle the concept of event identity. Such annotation setup\nreduces the pool of event mentions and prevents one from considering the\npossibility of quasi-identity relations. We propose a dense annotation approach\nfor cross-document event coreference, comprising a rich source of event\nmentions and a dense annotation effort between related document pairs. To this\nend, we design a new annotation workflow with careful quality control and an\neasy-to-use annotation interface. In addition to the links, we further collect\noverlapping event contexts, including time, location, and participants, to shed\nsome light on the relation between identity decisions and context. We present\nan open-access dataset for cross-document event coreference, CDEC-WN, collected\nfrom English Wikinews and open-source our annotation toolkit to encourage\nfurther research on cross-document tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pratapa_A/0/1/0/all/0/1\">Adithya Pratapa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hasegawa_K/0/1/0/all/0/1\">Kimihiro Hasegawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamakawa_Y/0/1/0/all/0/1\">Yukari Yamakawa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Commonsense-Focused Dialogues for Response Generation: An Empirical Study. (arXiv:2109.06427v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06427","description":"<p>Smooth and effective communication requires the ability to perform latent or\nexplicit commonsense inference. Prior commonsense reasoning benchmarks (such as\nSocialIQA and CommonsenseQA) mainly focus on the discriminative task of\nchoosing the right answer from a set of candidates, and do not involve\ninteractive language generation as in dialogue. Moreover, existing dialogue\ndatasets do not explicitly focus on exhibiting commonsense as a facet. In this\npaper, we present an empirical study of commonsense in dialogue response\ngeneration. We first auto-extract commonsensical dialogues from existing\ndialogue datasets by leveraging ConceptNet, a commonsense knowledge graph.\nFurthermore, building on social contexts/situations in SocialIQA, we collect a\nnew dialogue dataset with 25K dialogues aimed at exhibiting social commonsense\nin an interactive setting. We evaluate response generation models trained using\nthese datasets and find that models trained on both extracted and our collected\ndata produce responses that consistently exhibit more commonsense than\nbaselines. Finally we propose an approach for automatic evaluation of\ncommonsense that relies on features derived from ConceptNet and pre-trained\nlanguage and dialog models, and show reasonable correlation with human\nevaluation of responses' commonsense quality. We are releasing a subset of our\ncollected data, Commonsense-Dialogues, containing about 11K dialogs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Karthik Gopalakrishnan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hedayatnia_B/0/1/0/all/0/1\">Behnam Hedayatnia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Seokhwan Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yang Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hakkani_Tur_D/0/1/0/all/0/1\">Dilek Hakkani-Tur</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"YES SIR!Optimizing Semantic Space of Negatives with Self-Involvement Ranker. (arXiv:2109.06436v1 [cs.IR])","link":"http://arxiv.org/abs/2109.06436","description":"<p>Pre-trained model such as BERT has been proved to be an effective tool for\ndealing with Information Retrieval (IR) problems. Due to its inspiring\nperformance, it has been widely used to tackle with real-world IR problems such\nas document ranking. Recently, researchers have found that selecting \"hard\"\nrather than \"random\" negative samples would be beneficial for fine-tuning\npre-trained models on ranking tasks. However, it remains elusive how to\nleverage hard negative samples in a principled way. To address the\naforementioned issues, we propose a fine-tuning strategy for document ranking,\nnamely Self-Involvement Ranker (SIR), to dynamically select hard negative\nsamples to construct high-quality semantic space for training a high-quality\nranking model. Specifically, SIR consists of sequential compressors implemented\nwith pre-trained models. Front compressor selects hard negative samples for\nrear compressor. Moreover, SIR leverages supervisory signal to adaptively\nadjust semantic space of negative samples. Finally, supervisory signal in rear\ncompressor is computed based on condition probability and thus can control\nsample dynamic and further enhance the model performance. SIR is a lightweight\nand general framework for pre-trained models, which simplifies the ranking\nprocess in industry practice. We test our proposed solution on MS MARCO with\ndocument ranking setting, and the results show that SIR can significantly\nimprove the ranking performance of various pre-trained models. Moreover, our\nmethod became the new SOTA model anonymously on MS MARCO Document ranking\nleaderboard in May 2021.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pu_R/0/1/0/all/0/1\">Ruizhi Pu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lai_R/0/1/0/all/0/1\">Ruofei Lai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1\">Zikai Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yinxia Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1\">Hao Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yongkang Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1\">Yantao Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhao Cao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Uncovering Implicit Gender Bias in Narratives through Commonsense Inference. (arXiv:2109.06437v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06437","description":"<p>Pre-trained language models learn socially harmful biases from their training\ncorpora, and may repeat these biases when used for generation. We study gender\nbiases associated with the protagonist in model-generated stories. Such biases\nmay be expressed either explicitly (\"women can't park\") or implicitly (e.g. an\nunsolicited male character guides her into a parking space). We focus on\nimplicit biases, and use a commonsense reasoning engine to uncover them.\nSpecifically, we infer and analyze the protagonist's motivations, attributes,\nmental states, and implications on others. Our findings regarding implicit\nbiases are in line with prior work that studied explicit biases, for example\nshowing that female characters' portrayal is centered around appearance, while\nmale figures' focus on intellect.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1\">Tenghao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-adaptive Pre-training and Self-training are Complementary for Natural Language Understanding. (arXiv:2109.06466v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06466","description":"<p>Task-adaptive pre-training (TAPT) and Self-training (ST) have emerged as the\nmajor semi-supervised approaches to improve natural language understanding\n(NLU) tasks with massive amount of unlabeled data. However, it's unclear\nwhether they learn similar representations or they can be effectively combined.\nIn this paper, we show that TAPT and ST can be complementary with simple TFS\nprotocol by following TAPT -&gt; Finetuning -&gt; Self-training (TFS) process.\nExperimental results show that TFS protocol can effectively utilize unlabeled\ndata to achieve strong combined gains consistently across six datasets covering\nsentiment classification, paraphrase identification, natural language\ninference, named entity recognition and dialogue slot classification. We\ninvestigate various semi-supervised settings and consistently show that gains\nfrom TAPT and ST can be strongly additive by following TFS procedure. We hope\nthat TFS could serve as an important semi-supervised baseline for future NLP\nstudies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1\">Shiyang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1\">Wenhu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1\">Xifeng Yan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Identifying Untrustworthy Samples: Data Filtering for Open-domain Dialogues with Bayesian Optimization. (arXiv:2109.06471v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06471","description":"<p>Being able to reply with a related, fluent, and informative response is an\nindispensable requirement for building high-quality conversational agents. In\norder to generate better responses, some approaches have been proposed, such as\nfeeding extra information by collecting large-scale datasets with human\nannotations, designing neural conversational models (NCMs) with complex\narchitecture and loss functions, or filtering out untrustworthy samples based\non a dialogue attribute, e.g., Relatedness or Genericness. In this paper, we\nfollow the third research branch and present a data filtering method for\nopen-domain dialogues, which identifies untrustworthy samples from training\ndata with a quality measure that linearly combines seven dialogue attributes.\nThe attribute weights are obtained via Bayesian Optimization (BayesOpt) that\naims to optimize an objective function for dialogue generation iteratively on\nthe validation set. Then we score training samples with the quality measure,\nsort them in descending order, and filter out those at the bottom. Furthermore,\nto accelerate the \"filter-train-evaluate\" iterations involved in BayesOpt on\nlarge-scale datasets, we propose a training framework that integrates maximum\nlikelihood estimation (MLE) and negative training method (NEG). The training\nmethod updates parameters of a trained NCMs on two small sets with newly\nmaintained and removed samples, respectively. Specifically, MLE is applied to\nmaximize the log-likelihood of newly maintained samples, while NEG is used to\nminimize the log-likelihood of newly removed ones. Experimental results on two\ndatasets show that our method can effectively identify untrustworthy samples,\nand NCMs trained on the filtered datasets achieve better performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1\">Lei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1\">Haolan Zhan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1\">Xin Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hongshen Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofang Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1\">Xiaodan Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Logic-level Evidence Retrieval and Graph-based Verification Network for Table-based Fact Verification. (arXiv:2109.06480v1 [cs.AI])","link":"http://arxiv.org/abs/2109.06480","description":"<p>Table-based fact verification task aims to verify whether the given statement\nis supported by the given semi-structured table. Symbolic reasoning with\nlogical operations plays a crucial role in this task. Existing methods leverage\nprograms that contain rich logical information to enhance the verification\nprocess. However, due to the lack of fully supervised signals in the program\ngeneration process, spurious programs can be derived and employed, which leads\nto the inability of the model to catch helpful logical operations. To address\nthe aforementioned problems, in this work, we formulate the table-based fact\nverification task as an evidence retrieval and reasoning framework, proposing\nthe Logic-level Evidence Retrieval and Graph-based Verification network\n(LERGV). Specifically, we first retrieve logic-level program-like evidence from\nthe given table and statement as supplementary evidence for the table. After\nthat, we construct a logic-level graph to capture the logical relations between\nentities and functions in the retrieved evidence, and design a graph-based\nverification network to perform logic-level graph-based reasoning based on the\nconstructed graph to classify the final entailment relation. Experimental\nresults on the large-scale benchmark TABFACT show the effectiveness of the\nproposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1\">Qi Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_Q/0/1/0/all/0/1\">Qingyu Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1\">Ting Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AligNART: Non-autoregressive Neural Machine Translation by Jointly Learning to Estimate Alignment and Translate. (arXiv:2109.06481v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06481","description":"<p>Non-autoregressive neural machine translation (NART) models suffer from the\nmulti-modality problem which causes translation inconsistency such as token\nrepetition. Most recent approaches have attempted to solve this problem by\nimplicitly modeling dependencies between outputs. In this paper, we introduce\nAligNART, which leverages full alignment information to explicitly reduce the\nmodality of the target distribution. AligNART divides the machine translation\ntask into $(i)$ alignment estimation and $(ii)$ translation with aligned\ndecoder inputs, guiding the decoder to focus on simplified one-to-one\ntranslation. To alleviate the alignment estimation problem, we further propose\na novel alignment decomposition method. Our experiments show that AligNART\noutperforms previous non-iterative NART models that focus on explicit modality\nreduction on WMT14 En$\\leftrightarrow$De and WMT16 Ro$\\rightarrow$En.\nFurthermore, AligNART achieves BLEU scores comparable to those of the\nstate-of-the-art connectionist temporal classification based models on WMT14\nEn$\\leftrightarrow$De. We also observe that AligNART effectively addresses the\ntoken repetition problem even without sequence-level knowledge distillation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1\">Jongyoon Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungwon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1\">Sungroh Yoon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multilevel profiling of situation and dialogue-based deep networks for movie genre classification using movie trailers. (arXiv:2109.06488v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06488","description":"<p>Automated movie genre classification has emerged as an active and essential\narea of research and exploration. Short duration movie trailers provide useful\ninsights about the movie as video content consists of the cognitive and the\naffective level features. Previous approaches were focused upon either\ncognitive or affective content analysis. In this paper, we propose a novel\nmulti-modality: situation, dialogue, and metadata-based movie genre\nclassification framework that takes both cognition and affect-based features\ninto consideration. A pre-features fusion-based framework that takes into\naccount: situation-based features from a regular snapshot of a trailer that\nincludes nouns and verbs providing the useful affect-based mapping with the\ncorresponding genres, dialogue (speech) based feature from audio, metadata\nwhich together provides the relevant information for cognitive and affect based\nvideo analysis. We also develop the English movie trailer dataset (EMTD), which\ncontains 2000 Hollywood movie trailers belonging to five popular genres:\nAction, Romance, Comedy, Horror, and Science Fiction, and perform\ncross-validation on the standard LMTD-9 dataset for validating the proposed\nframework. The results demonstrate that the proposed methodology for movie\ngenre classification has performed excellently as depicted by the F1 scores,\nprecision, recall, and area under the precision-recall curves.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vishwakarma_D/0/1/0/all/0/1\">Dinesh Kumar Vishwakarma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jindal_M/0/1/0/all/0/1\">Mayank Jindal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1\">Ayush Mittal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1\">Aditya Sharma</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"conSultantBERT: Fine-tuned Siamese Sentence-BERT for Matching Jobs and Job Seekers. (arXiv:2109.06501v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06501","description":"<p>In this paper we focus on constructing useful embeddings of textual\ninformation in vacancies and resumes, which we aim to incorporate as features\ninto job to job seeker matching models alongside other features. We explain our\ntask where noisy data from parsed resumes, heterogeneous nature of the\ndifferent sources of data, and crosslinguality and multilinguality present\ndomain-specific challenges.\n</p>\n<p>We address these challenges by fine-tuning a Siamese Sentence-BERT (SBERT)\nmodel, which we call conSultantBERT, using a large-scale, real-world, and high\nquality dataset of over 270,000 resume-vacancy pairs labeled by our staffing\nconsultants. We show how our fine-tuned model significantly outperforms\nunsupervised and supervised baselines that rely on TF-IDF-weighted feature\nvectors and BERT embeddings. In addition, we find our model successfully\nmatches cross-lingual and multilingual textual content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lavi_D/0/1/0/all/0/1\">Dor Lavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Medentsiy_V/0/1/0/all/0/1\">Volodymyr Medentsiy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Graus_D/0/1/0/all/0/1\">David Graus</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tribrid: Stance Classification with Neural Inconsistency Detection. (arXiv:2109.06508v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06508","description":"<p>We study the problem of performing automatic stance classification on social\nmedia with neural architectures such as BERT. Although these architectures\ndeliver impressive results, their level is not yet comparable to the one of\nhumans and they might produce errors that have a significant impact on the\ndownstream task (e.g., fact-checking). To improve the performance, we present a\nnew neural architecture where the input also includes automatically generated\nnegated perspectives over a given claim. The model is jointly learned to make\nsimultaneously multiple predictions, which can be used either to improve the\nclassification of the original perspective or to filter out doubtful\npredictions. In the first case, we propose a weakly supervised method for\ncombining the predictions into a final one. In the second case, we show that\nusing the confidence scores to remove doubtful predictions allows our method to\nachieve human-like performance over the retained information, which is still a\nsizable part of the original input.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1\">Song Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Urbani_J/0/1/0/all/0/1\">Jacopo Urbani</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Prompt-based Few-shot Learning for Grounded Dialog Generation. (arXiv:2109.06513v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06513","description":"<p>Dialog grounding enables conversational models to make full use of external\ninformation to establish multiple desired qualities, such as knowledgeable,\nengaging and empathetic. However, naturally grounded dialog corpora are usually\nnot directly available, which puts forward requirements for the few-shot\nlearning ability of conversational models. Motivated by recent advances in\npre-trained language models and prompt-based learning, in this paper we explore\nprompt-based few-shot learning for grounded dialog generation (GDG). We first\nformulate the prompt construction for GDG tasks, based on which we then conduct\ncomprehensive empirical analysis on two common types of prompting methods:\ntemplate-based prompting and soft-prompting. We demonstrate the potential of\nprompt-based methods in few-shot learning for GDG and provide directions of\nimprovement for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Netmarble AI Center's WMT21 Automatic Post-Editing Shared Task Submission. (arXiv:2109.06515v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06515","description":"<p>This paper describes Netmarble's submission to WMT21 Automatic Post-Editing\n(APE) Shared Task for the English-German language pair. First, we propose a\nCurriculum Training Strategy in training stages. Facebook Fair's WMT19 news\ntranslation model was chosen to engage the large and powerful pre-trained\nneural networks. Then, we post-train the translation model with different\nlevels of data at each training stages. As the training stages go on, we make\nthe system learn to solve multiple tasks by adding extra information at\ndifferent training stages gradually. We also show a way to utilize the\nadditional data in large volume for APE tasks. For further improvement, we\napply Multi-Task Learning Strategy with the Dynamic Weight Average during the\nfine-tuning stage. To fine-tune the APE corpus with limited data, we add some\nrelated subtasks to learn a unified representation. Finally, for better\nperformance, we leverage external translations as augmented machine translation\n(MT) during the post-training and fine-tuning. As experimental results show,\nour APE system significantly improves the translations of provided MT results\nby -2.848 and +3.74 on the development dataset in terms of TER and BLEU,\nrespectively. It also demonstrates its effectiveness on the test dataset with\nhigher quality than the development dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1\">Shinhyeok Oh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1\">Sion Jang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Hu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1\">Shounan An</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_I/0/1/0/all/0/1\">Insoo Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Sampling of Dependency Structures. (arXiv:2109.06521v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06521","description":"<p>Probabilistic distributions over spanning trees in directed graphs are a\nfundamental model of dependency structure in natural language processing,\nsyntactic dependency trees. In NLP, dependency trees often have an additional\nroot constraint: only one edge may emanate from the root. However, no sampling\nalgorithm has been presented in the literature to account for this additional\nconstraint. In this paper, we adapt two spanning tree sampling algorithms to\nfaithfully sample dependency trees from a graph subject to the root constraint.\nWilson (1996)'s sampling algorithm has a running time of $\\mathcal{O}(H)$ where\n$H$ is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm\nhas a running time of $\\mathcal{O}(N^3)$, which is often greater than the mean\nhitting time of a directed graph. Additionally, we build upon Colbourn's\nalgorithm and present a novel extension that can sample $K$ trees without\nreplacement in $\\mathcal{O}(K N^3 + K^2 N)$ time. To the best of our knowledge,\nno algorithm has been given for sampling spanning trees without replacement\nfrom a directed graph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zmigrod_R/0/1/0/all/0/1\">Ran Zmigrod</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vieira_T/0/1/0/all/0/1\">Tim Vieira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1\">Ryan Cotterell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Different Strokes for Different Folks: Investigating Appropriate Further Pre-training Approaches for Diverse Dialogue Tasks. (arXiv:2109.06524v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06524","description":"<p>Loading models pre-trained on the large-scale corpus in the general domain\nand fine-tuning them on specific downstream tasks is gradually becoming a\nparadigm in Natural Language Processing. Previous investigations prove that\nintroducing a further pre-training phase between pre-training and fine-tuning\nphases to adapt the model on the domain-specific unlabeled data can bring\npositive effects. However, most of these further pre-training works just keep\nrunning the conventional pre-training task, e.g., masked language model, which\ncan be regarded as the domain adaptation to bridge the data distribution gap.\nAfter observing diverse downstream tasks, we suggest that different tasks may\nalso need a further pre-training phase with appropriate training tasks to\nbridge the task formulation gap. To investigate this, we carry out a study for\nimproving multiple task-oriented dialogue downstream tasks through designing\nvarious tasks at the further pre-training phase. The experiment shows that\ndifferent downstream tasks prefer different further pre-training tasks, which\nhave intrinsic correlation and most further pre-training tasks significantly\nimprove certain target tasks rather than all. Our investigation indicates that\nit is of great importance and effectiveness to design appropriate further\npre-training tasks modeling specific information that benefit downstream tasks.\nBesides, we present multiple constructive empirical conclusions for enhancing\ntask-oriented dialogues.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Bill Similarity with Annotated and Augmented Corpora of Bills. (arXiv:2109.06527v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06527","description":"<p>Bill writing is a critical element of representative democracy. However, it\nis often overlooked that most legislative bills are derived, or even directly\ncopied, from other bills. Despite the significance of bill-to-bill linkages for\nunderstanding the legislative process, existing approaches fail to address\nsemantic similarities across bills, let alone reordering or paraphrasing which\nare prevalent in legal document writing. In this paper, we overcome these\nlimitations by proposing a 5-class classification task that closely reflects\nthe nature of the bill generation process. In doing so, we construct a\nhuman-labeled dataset of 4,721 bill-to-bill relationships at the\nsubsection-level and release this annotated dataset to the research community.\nTo augment the dataset, we generate synthetic data with varying degrees of\nsimilarity, mimicking the complex bill writing process. We use BERT variants\nand apply multi-stage training, sequentially fine-tuning our models with\nsynthetic and human-labeled datasets. We find that the predictive performance\nsignificantly improves when training with both human-labeled and synthetic\ndata. Finally, we apply our trained model to infer section- and bill-level\nsimilarities. Our analysis shows that the proposed methodology successfully\ncaptures the similarities across legal documents at various levels of\naggregation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiseon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Griggs_E/0/1/0/all/0/1\">Elden Griggs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1\">In Song Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Gradient-based Adversarial Training for Text Classification by Contrastive Learning and Auto-Encoder. (arXiv:2109.06536v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06536","description":"<p>Recent work has proposed several efficient approaches for generating\ngradient-based adversarial perturbations on embeddings and proved that the\nmodel's performance and robustness can be improved when they are trained with\nthese contaminated embeddings. While they paid little attention to how to help\nthe model to learn these adversarial samples more efficiently. In this work, we\nfocus on enhancing the model's ability to defend gradient-based adversarial\nattack during the model's training process and propose two novel adversarial\ntraining approaches: (1) CARL narrows the original sample and its adversarial\nsample in the representation space while enlarging their distance from\ndifferent labeled samples. (2) RAR forces the model to reconstruct the original\nsample from its adversarial representation. Experiments show that the proposed\ntwo approaches outperform strong baselines on various text classification\ndatasets. Analysis experiments find that when using our approaches, the\nsemantic representation of the input sentence won't be significantly affected\nby adversarial perturbations, and the model's performance drops less under\nadversarial attack. That is to say, our approaches can effectively improve the\nrobustness of the model. Besides, RAR can also be used to generate text-form\nadversarial samples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Challenging Instances are Worth Learning: Generating Valuable Negative Samples for Response Selection Training. (arXiv:2109.06538v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06538","description":"<p>Retrieval-based chatbot selects the appropriate response from candidates\naccording to the context, which heavily depends on a response selection module.\nA response selection module is generally a scoring model to evaluate candidates\nand is usually trained on the annotated positive response and sampled negative\nresponses. Sampling negative responses lead to two risks: a). The sampled\nnegative instances, especially that from random sampling methods, are mostly\nirrelevant to the dialogue context and too easy to be fitted at the training\nstage while causing a weak model in the real scenario. b). The so-called\nnegative instances may be positive, which is known as the fake negative\nproblem. To address the above issue, we employ pre-trained language models,\nsuch as the DialoGPT to construct more challenging negative instances to\nenhance the model robustness. Specifically, we provide garbled context to the\npre-trained model to generate responses and filter the fake negative ones. In\nthis way, our negative instances are fluent, context-related, and more\nchallenging for the model to learn, while can not be positive. Extensive\nexperiments show that our method brings significant and stable improvements on\nthe dialogue response selection capacity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1\">Huiying Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Talking Space: inference from spatial linguistic meanings. (arXiv:2109.06554v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06554","description":"<p>This paper concerns the intersection of natural language and the physical\nspace around us in which we live, that we observe and/or imagine things within.\nMany important features of language have spatial connotations, for example,\nmany prepositions (like in, next to, after, on, etc.) are fundamentally\nspatial. Space is also a key factor of the meanings of many\nwords/phrases/sentences/text, and space is a, if not the key, context for\nreferencing (e.g. pointing) and embodiment.\n</p>\n<p>We propose a mechanism for how space and linguistic structure can be made to\ninteract in a matching compositional fashion. Examples include Cartesian space,\nsubway stations, chesspieces on a chess-board, and Penrose's staircase. The\nstarting point for our construction is the DisCoCat model of compositional\nnatural language meaning, which we relax to accommodate physical space. We\naddress the issue of having multiple agents/objects in a space, including the\ncase that each agent has different capabilities with respect to that space,\ne.g., the specific moves each chesspiece can make, or the different velocities\none may be able to reach.\n</p>\n<p>Once our model is in place, we show how inferences drawing from the structure\nof physical space can be made. We also how how linguistic model of space can\ninteract with other such models related to our senses and/or embodiment, such\nas the conceptual spaces of colour, taste and smell, resulting in a rich\ncompositional model of meaning that is close to human experience and embodiment\nin the world.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Mascianica_V/0/1/0/all/0/1\">Vincent Wang-Mascianica</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coecke_B/0/1/0/all/0/1\">Bob Coecke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Just What do You Think You're Doing, Dave?' A Checklist for Responsible Data Use in NLP. (arXiv:2109.06598v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06598","description":"<p>A key part of the NLP ethics movement is responsible use of data, but exactly\nwhat that means or how it can be best achieved remain unclear. This position\npaper discusses the core legal and ethical principles for collection and\nsharing of textual data, and the tensions between them. We propose a potential\nchecklist for responsible data (re-)use that could both standardise the peer\nreview of conference submissions, as well as enable a more in-depth view of\npublished research across the community. Our proposal aims to contribute to the\ndevelopment of a consistent standard for data (re-)use, embraced across NLP\nconferences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rogers_A/0/1/0/all/0/1\">Anna Rogers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Tim Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leins_K/0/1/0/all/0/1\">Kobi Leins</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-Parametric Unsupervised Domain Adaptation for Neural Machine Translation. (arXiv:2109.06604v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06604","description":"<p>Recently, $k$NN-MT has shown the promising capability of directly\nincorporating the pre-trained neural machine translation (NMT) model with\ndomain-specific token-level $k$-nearest-neighbor ($k$NN) retrieval to achieve\ndomain adaptation without retraining. Despite being conceptually attractive, it\nheavily relies on high-quality in-domain parallel corpora, limiting its\ncapability on unsupervised domain adaptation, where in-domain parallel corpora\nare scarce or nonexistent. In this paper, we propose a novel framework that\ndirectly uses in-domain monolingual sentences in the target language to\nconstruct an effective datastore for $k$-nearest-neighbor retrieval. To this\nend, we first introduce an autoencoder task based on the target language, and\nthen insert lightweight adapters into the original NMT model to map the\ntoken-level representation of this task to the ideal representation of\ntranslation task. Experiments on multi-domain datasets demonstrate that our\nproposed approach significantly improves the translation accuracy with\ntarget-side monolingual data, while achieving comparable performance with\nback-translation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1\">Xin Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhirui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1\">Shujian Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1\">Boxing Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1\">Jun Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1\">Weihua Luo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Jiajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model. (arXiv:2109.06605v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06605","description":"<p>Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a\nlanguage model on domain-specific text, improves the modelling of text for\ndownstream tasks within the domain. Numerous real-world applications are based\non domain-specific text, e.g. working with financial or biomedical documents,\nand these applications often need to support multiple languages. However,\nlarge-scale domain-specific multilingual pretraining data for such scenarios\ncan be difficult to obtain, due to regulations, legislation, or simply a lack\nof language- and domain-specific text. One solution is to train a single\nmultilingual model, taking advantage of the data available in as many languages\nas possible. In this work, we explore the benefits of domain adaptive\npretraining with a focus on adapting to multiple languages within a specific\ndomain. We propose different techniques to compose pretraining corpora that\nenable a language model to both become domain-specific and multilingual.\nEvaluation on nine domain-specific datasets-for biomedical named entity\nrecognition and financial sentence classification-covering seven different\nlanguages show that a single multilingual domain-specific model can outperform\nthe general multilingual model, and performs close to its monolingual\ncounterpart. This finding holds across two different pretraining methods,\nadapter-based pretraining and full model pretraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jorgensen_R/0/1/0/all/0/1\">Rasmus K&#xe6;r J&#xf8;rgensen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hartmann_M/0/1/0/all/0/1\">Mareike Hartmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1\">Xiang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elliott_D/0/1/0/all/0/1\">Desmond Elliott</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Scalable Font Reconstruction with Dual Latent Manifolds. (arXiv:2109.06627v1 [cs.CV])","link":"http://arxiv.org/abs/2109.06627","description":"<p>We propose a deep generative model that performs typography analysis and font\nreconstruction by learning disentangled manifolds of both font style and\ncharacter shape. Our approach enables us to massively scale up the number of\ncharacter types we can effectively model compared to previous methods.\nSpecifically, we infer separate latent variables representing character and\nfont via a pair of inference networks which take as input sets of glyphs that\neither all share a character type, or belong to the same font. This design\nallows our model to generalize to characters that were not observed during\ntraining time, an important task in light of the relative sparsity of most\nfonts. We also put forward a new loss, adapted from prior work that measures\nlikelihood using an adaptive distribution in a projected space, resulting in\nmore natural images without requiring a discriminator. We evaluate on the task\nof font reconstruction over various datasets representing character types of\nmany languages, and compare favorably to modern style transfer systems\naccording to both automatic and manually-evaluated metrics.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Srivatsan_N/0/1/0/all/0/1\">Nikita Srivatsan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Si Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Barron_J/0/1/0/all/0/1\">Jonathan T. Barron</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Berg_Kirkpatrick_T/0/1/0/all/0/1\">Taylor Berg-Kirkpatrick</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An MRC Framework for Semantic Role Labeling. (arXiv:2109.06660v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06660","description":"<p>Semantic Role Labeling (SRL) aims at recognizing the predicate-argument\nstructure of a sentence and can be decomposed into two subtasks: predicate\ndisambiguation and argument labeling. Prior work deals with these two tasks\nindependently, which ignores the semantic connection between the two tasks. In\nthis paper, we propose to use the machine reading comprehension (MRC) framework\nto bridge this gap. We formalize predicate disambiguation as multiple-choice\nmachine reading comprehension, where the descriptions of candidate senses of a\ngiven predicate are used as options to select the correct sense. The chosen\npredicate sense is then used to determine the semantic roles for that\npredicate, and these semantic roles are used to construct the query for another\nMRC model for argument labeling. In this way, we are able to leverage both the\npredicate semantics and the semantic role semantics for argument labeling. We\nalso propose to select a subset of all the possible semantic roles for\ncomputational efficiency. Experiments show that the proposed framework achieves\nstate-of-the-art results on both span and dependency benchmarks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Nan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jiwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1\">Yuxian Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiaofei Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1\">Jun He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Expert Knowledge-Guided Length-Variant Hierarchical Label Generation for Proposal Classification. (arXiv:2109.06661v1 [cs.LG])","link":"http://arxiv.org/abs/2109.06661","description":"<p>To advance the development of science and technology, research proposals are\nsubmitted to open-court competitive programs developed by government agencies\n(e.g., NSF). Proposal classification is one of the most important tasks to\nachieve effective and fair review assignments. Proposal classification aims to\nclassify a proposal into a length-variant sequence of labels. In this paper, we\nformulate the proposal classification problem into a hierarchical multi-label\nclassification task. Although there are certain prior studies, proposal\nclassification exhibit unique features: 1) the classification result of a\nproposal is in a hierarchical discipline structure with different levels of\ngranularity; 2) proposals contain multiple types of documents; 3) domain\nexperts can empirically provide partial labels that can be leveraged to improve\ntask performances. In this paper, we focus on developing a new deep proposal\nclassification framework to jointly model the three features. In particular, to\nsequentially generate labels, we leverage previously-generated labels to\npredict the label of next level; to integrate partial labels from experts, we\nuse the embedding of these empirical partial labels to initialize the state of\nneural networks. Our model can automatically identify the best length of label\nsequence to stop next label prediction. Finally, we present extensive results\nto demonstrate that our method can jointly model partial labels, textual\ninformation, and semantic dependencies in label sequences, and, thus, achieve\nadvanced performances.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1\">Meng Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiao_Z/0/1/0/all/0/1\">Ziyue Qiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1\">Yanjie Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1\">Yi Du</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1\">Pengyang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Inference for Multilingual Neural Machine Translation. (arXiv:2109.06679v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06679","description":"<p>Multilingual NMT has become an attractive solution for MT deployment in\nproduction. But to match bilingual quality, it comes at the cost of larger and\nslower models. In this work, we consider several ways to make multilingual NMT\nfaster at inference without degrading its quality. We experiment with several\n\"light decoder\" architectures in two 20-language multi-parallel settings:\nsmall-scale on TED Talks and large-scale on ParaCrawl. Our experiments\ndemonstrate that combining a shallow decoder with vocabulary filtering leads to\nmore than twice faster inference with no loss in translation quality. We\nvalidate our findings with BLEU and chrF (on 380 language pairs), robustness\nevaluation and human evaluation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Berard_A/0/1/0/all/0/1\">Alexandre Berard</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dain Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clinchant_S/0/1/0/all/0/1\">St&#xe9;phane Clinchant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1\">Kweonwoo Jung</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikoulina_V/0/1/0/all/0/1\">Vassilina Nikoulina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Non-autoregressive Transformer with Unified Bidirectional Decoder for Automatic Speech Recognition. (arXiv:2109.06684v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06684","description":"<p>Non-autoregressive (NAR) transformer models have been studied intensively in\nautomatic speech recognition (ASR), and a substantial part of NAR transformer\nmodels is to use the casual mask to limit token dependencies. However, the\ncasual mask is designed for the left-to-right decoding process of the\nnon-parallel autoregressive (AR) transformer, which is inappropriate for the\nparallel NAR transformer since it ignores the right-to-left contexts. Some\nmodels are proposed to utilize right-to-left contexts with an extra decoder,\nbut these methods increase the model complexity. To tackle the above problems,\nwe propose a new non-autoregressive transformer with a unified bidirectional\ndecoder (NAT-UBD), which can simultaneously utilize left-to-right and\nright-to-left contexts. However, direct use of bidirectional contexts will\ncause information leakage, which means the decoder output can be affected by\nthe character information from the input of the same position. To avoid\ninformation leakage, we propose a novel attention mask and modify vanilla\nqueries, keys, and values matrices for NAT-UBD. Experimental results verify\nthat NAT-UBD can achieve character error rates (CERs) of 5.0%/5.5% on the\nAishell1 dev/test sets, outperforming all previous NAR transformer models.\nMoreover, NAT-UBD can run 49.8x faster than the AR transformer baseline when\ndecoding in a single step.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chuan-Fei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1\">Tian-Hao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1\">Song-Lu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1\">Xu-Cheng Yin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A system for information extraction from scientific texts in Russian. (arXiv:2109.06703v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06703","description":"<p>In this paper, we present a system for information extraction from scientific\ntexts in the Russian language. The system performs several tasks in an\nend-to-end manner: term recognition, extraction of relations between terms, and\nterm linking with entities from the knowledge base. These tasks are extremely\nimportant for information retrieval, recommendation systems, and\nclassification. The advantage of the implemented methods is that the system\ndoes not require a large amount of labeled data, which saves time and effort\nfor data labeling and therefore can be applied in low- and mid-resource\nsettings. The source code is publicly available and can be used for different\nresearch purposes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bruches_E/0/1/0/all/0/1\">Elena Bruches</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mezentseva_A/0/1/0/all/0/1\">Anastasia Mezentseva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Batura_T/0/1/0/all/0/1\">Tatiana Batura</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"KFCNet: Knowledge Filtering and Contrastive Learning Network for Generative Commonsense Reasoning. (arXiv:2109.06704v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06704","description":"<p>Pre-trained language models have led to substantial gains over a broad range\nof natural language processing (NLP) tasks, but have been shown to have\nlimitations for natural language generation tasks with high-quality\nrequirements on the output, such as commonsense generation and ad keyword\ngeneration. In this work, we present a novel Knowledge Filtering and\nContrastive learning Network (KFCNet) which references external knowledge and\nachieves better generation performance. Specifically, we propose a BERT-based\nfilter model to remove low-quality candidates, and apply contrastive learning\nseparately to each of the encoder and decoder, within a general\nencoder--decoder architecture. The encoder contrastive module helps to capture\nglobal target semantics during encoding, and the decoder contrastive module\nenhances the utility of retrieved prototypes while learning general features.\nExtensive experiments on the CommonGen benchmark show that our model\noutperforms the previous state of the art by a large margin: +6.6 points (42.5\nvs. 35.9) for BLEU-4, +3.7 points (33.3 vs. 29.6) for SPICE, and +1.3 points\n(18.3 vs. 17.0) for CIDEr. We further verify the effectiveness of the proposed\ncontrastive module on ad keyword generation, and show that our model has\npotential commercial value.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Haonan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1\">Yeyun Gong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1\">Jian Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1\">Ruofei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1\">Nan Duan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Novel Global Feature-Oriented Relational Triple Extraction Model based on Table Filling. (arXiv:2109.06705v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06705","description":"<p>Table filling based relational triple extraction methods are attracting\ngrowing research interests due to their promising performance and their\nabilities on extracting triples from complex sentences. However, this kind of\nmethods are far from their full potential because most of them only focus on\nusing local features but ignore the global associations of relations and of\ntoken pairs, which increases the possibility of overlooking some important\ninformation during triple extraction. To overcome this deficiency, we propose a\nglobal feature-oriented triple extraction model that makes full use of the\nmentioned two kinds of global associations. Specifically, we first generate a\ntable feature for each relation. Then two kinds of global associations are\nmined from the generated table features. Next, the mined global associations\nare integrated into the table feature of each relation. This\n\"generate-mine-integrate\" process is performed multiple times so that the table\nfeature of each relation is refined step by step. Finally, each relation's\ntable is filled based on its refined table feature, and all triples linked to\nthis relation are extracted based on its filled table. We evaluate the proposed\nmodel on three benchmark datasets. Experimental results show our model is\neffective and it achieves state-of-the-art results on all of these datasets.\nThe source code of our work is available at: https://github.com/neukg/GRTE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1\">Feiliang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1\">Longhui Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_S/0/1/0/all/0/1\">Shujuan Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1\">Xiaofeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1\">Shilei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1\">Bochao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yaduo Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Answer Type Prediction using BERT: IAI at the ISWC SMART Task 2020. (arXiv:2109.06714v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06714","description":"<p>This paper summarizes our participation in the SMART Task of the ISWC 2020\nChallenge. A particular question we are interested in answering is how well\nneural methods, and specifically transformer models, such as BERT, perform on\nthe answer type prediction task compared to traditional approaches. Our main\nfinding is that coarse-grained answer types can be identified effectively with\nstandard text classification methods, with over 95% accuracy, and BERT can\nbring only marginal improvements. For fine-grained type detection, on the other\nhand, BERT clearly outperforms previous retrieval-based approaches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Setty_V/0/1/0/all/0/1\">Vinay Setty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balog_K/0/1/0/all/0/1\">Krisztian Balog</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Controllable Dialogue Generation with Disentangled Multi-grained Style Specification and Attribute Consistency Reward. (arXiv:2109.06717v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06717","description":"<p>Controllable text generation is an appealing but challenging task, which\nallows users to specify particular attributes of the generated outputs. In this\npaper, we propose a controllable dialogue generation model to steer response\ngeneration under multi-attribute constraints. Specifically, we define and\ncategorize the commonly used control attributes into global and local ones,\nwhich possess different granularities of effects on response generation. Then,\nwe significantly extend the conventional seq2seq framework by introducing a\nnovel two-stage decoder, which first uses a multi-grained style specification\nlayer to impose the stylistic constraints and determine word-level control\nstates of responses based on the attributes, and then employs a response\ngeneration layer to generate final responses maintaining both semantic\nrelevancy to the contexts and fidelity to the attributes. Furthermore, we train\nour model with an attribute consistency reward to promote response control with\nexplicit supervision signals. Extensive experiments and in-depth analyses on\ntwo datasets indicate that our model can significantly outperform competitive\nbaselines in terms of response quality, content diversity and controllability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhe Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1\">Zhiwei Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1\">Hou Pong Chan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiachen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sparse Fuzzy Attention for Structural Sentiment Analysis. (arXiv:2109.06719v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06719","description":"<p>Attention scorers have achieved success in parsing tasks like semantic and\nsyntactic dependency parsing. However, in tasks modeled into parsing, like\nstructural sentiment analysis, \"dependency edges\" are very sparse which hinders\nparser performance. Thus we propose a sparse and fuzzy attention scorer with\npooling layers which improves parser performance and sets the new\nstate-of-the-art on structural sentiment analysis. We further explore the\nparsing modeling on structural sentiment analysis with second-order parsing and\nintroduce a novel sparse second-order edge building procedure that leads to\nsignificant improvement in parsing performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1\">Letain Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1\">Zuchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequential Modelling with Applications to Music Recommendation, Fact-Checking, and Speed Reading. (arXiv:2109.06736v1 [cs.IR])","link":"http://arxiv.org/abs/2109.06736","description":"<p>Sequential modelling entails making sense of sequential data, which naturally\noccurs in a wide array of domains. One example is systems that interact with\nusers, log user actions and behaviour, and make recommendations of items of\npotential interest to users on the basis of their previous interactions. In\nsuch cases, the sequential order of user interactions is often indicative of\nwhat the user is interested in next. Similarly, for systems that automatically\ninfer the semantics of text, capturing the sequential order of words in a\nsentence is essential, as even a slight re-ordering could significantly alter\nits original meaning. This thesis makes methodological contributions and new\ninvestigations of sequential modelling for the specific application areas of\nsystems that recommend music tracks to listeners and systems that process text\nsemantics in order to automatically fact-check claims, or \"speed read\" text for\nefficient further classification. (Rest of abstract omitted due to arXiv\nabstract limit)\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hansen_C/0/1/0/all/0/1\">Christian Hansen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adaptive Information Seeking for Open-Domain Question Answering. (arXiv:2109.06747v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06747","description":"<p>Information seeking is an essential step for open-domain question answering\nto efficiently gather evidence from a large corpus. Recently, iterative\napproaches have been proven to be effective for complex questions, by\nrecursively retrieving new evidence at each step. However, almost all existing\niterative approaches use predefined strategies, either applying the same\nretrieval function multiple times or fixing the order of different retrieval\nfunctions, which cannot fulfill the diverse requirements of various questions.\nIn this paper, we propose a novel adaptive information-seeking strategy for\nopen-domain question answering, namely AISO. Specifically, the whole retrieval\nand answer process is modeled as a partially observed Markov decision process,\nwhere three types of retrieval operations (e.g., BM25, DPR, and hyperlink) and\none answer operation are defined as actions. According to the learned policy,\nAISO could adaptively select a proper retrieval action to seek the missing\nevidence at each step, based on the collected evidence and the reformulated\nquery, or directly output the answer when the evidence set is sufficient for\nthe question. Experiments on SQuAD Open and HotpotQA fullwiki, which serve as\nsingle-hop and multi-hop open-domain QA benchmarks, show that AISO outperforms\nall baseline methods with predefined strategies in terms of both retrieval and\nanswer evaluations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunchang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1\">Huawei Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Zero-shot Cross-lingual Transfer between Closely Related Languages by injecting Character-level Noise. (arXiv:2109.06772v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06772","description":"<p>Cross-lingual transfer between a high-resource language and its dialects or\nclosely related language varieties should be facilitated by their similarity,\nbut current approaches that operate in the embedding space do not take surface\nsimilarity into account. In this work, we present a simple yet effective\nstrategy to improve cross-lingual transfer between closely related varieties by\naugmenting the data of the high-resource parent language with character-level\nnoise to make the model more robust towards spelling variations. Our strategy\nshows consistent improvements over several languages and tasks: Zero-shot\ntransfer of POS tagging and topic identification between language varieties\nfrom the Germanic, Uralic, and Romance language genera. Our work provides\nevidence for the usefulness of simple surface-level noise in improving transfer\nbetween language varieties.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Aepli_N/0/1/0/all/0/1\">No&#xeb;mi Aepli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Everything Is All It Takes: A Multipronged Strategy for Zero-Shot Cross-Lingual Information Extraction. (arXiv:2109.06798v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06798","description":"<p>Zero-shot cross-lingual information extraction (IE) describes the\nconstruction of an IE model for some target language, given existing\nannotations exclusively in some other language, typically English. While the\nadvance of pretrained multilingual encoders suggests an easy optimism of \"train\non English, run on any language\", we find through a thorough exploration and\nextension of techniques that a combination of approaches, both new and old,\nleads to better performance than any one cross-lingual strategy in particular.\nWe explore techniques including data projection and self-training, and how\ndifferent pretrained encoders impact them. We use English-to-Arabic IE as our\ninitial example, demonstrating strong performance in this setting for event\nextraction, named entity recognition, part-of-speech tagging, and dependency\nparsing. We then apply data projection and self-training to three tasks across\neight target languages. Because no single set of techniques performs the best\nacross all tasks, we encourage practitioners to explore various configurations\nof the techniques described in this work when seeking to improve on zero-shot\ntraining.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yarmohammadi_M/0/1/0/all/0/1\">Mahsa Yarmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1\">Shijie Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Marone_M/0/1/0/all/0/1\">Marc Marone</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebner_S/0/1/0/all/0/1\">Seth Ebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1\">Guanghui Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yunmo Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1\">Jialiang Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Harman_C/0/1/0/all/0/1\">Craig Harman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dredze_M/0/1/0/all/0/1\">Mark Dredze</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Temporal Variational Model for Story Generation. (arXiv:2109.06807v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06807","description":"<p>Recent language models can generate interesting and grammatically correct\ntext in story generation but often lack plot development and long-term\ncoherence. This paper experiments with a latent vector planning approach based\non a TD-VAE (Temporal Difference Variational Autoencoder), using the model for\nconditioning and reranking for text generation. The results demonstrate strong\nperformance in automatic cloze and swapping evaluations. The human judgments\nshow stories generated with TD-VAE reranking improve on a GPT-2 medium baseline\nand show comparable performance to a hierarchical LSTM reranking model.\nConditioning on the latent vectors proves disappointing and deteriorates\nperformance in human evaluation because it reduces the diversity of generation,\nand the models don't learn to progress the narrative. This highlights an\nimportant difference between technical task performance (e.g. cloze) and\ngenerating interesting stories.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilmot_D/0/1/0/all/0/1\">David Wilmot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"What are the attackers doing now? Automating cyber threat intelligence extraction from text on pace with the changing threat landscape: A survey. (arXiv:2109.06808v1 [cs.CR])","link":"http://arxiv.org/abs/2109.06808","description":"<p>Cybersecurity researchers have contributed to the automated extraction of CTI\nfrom textual sources, such as threat reports and online articles, where\ncyberattack strategies, procedures, and tools are described. The goal of this\narticle is to aid cybersecurity researchers understand the current techniques\nused for cyberthreat intelligence extraction from text through a survey of\nrelevant studies in the literature. We systematically collect \"CTI extraction\nfrom text\"-related studies from the literature and categorize the CTI\nextraction purposes. We propose a CTI extraction pipeline abstracted from these\nstudies. We identify the data sources, techniques, and CTI sharing formats\nutilized in the context of the proposed pipeline. Our work finds ten types of\nextraction purposes, such as extraction indicators of compromise extraction,\nTTPs (tactics, techniques, procedures of attack), and cybersecurity keywords.\nWe also identify seven types of textual sources for CTI extraction, and textual\ndata obtained from hacker forums, threat reports, social media posts, and\nonline news articles have been used by almost 90% of the studies. Natural\nlanguage processing along with both supervised and unsupervised machine\nlearning techniques such as named entity recognition, topic modelling,\ndependency parsing, supervised classification, and clustering are used for CTI\nextraction. We observe the technical challenges associated with these studies\nrelated to obtaining available clean, labelled data which could assure\nreplication, validation, and further extension of the studies. As we find the\nstudies focusing on CTI information extraction from text, we advocate for\nbuilding upon the current CTI extraction work to help cybersecurity\npractitioners with proactive decision making such as threat prioritization,\nautomated threat modelling to utilize knowledge from past cybersecurity\nincidents.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1\">Md Rayhanur Rahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahdavi_Hezaveh_R/0/1/0/all/0/1\">Rezvan Mahdavi-Hezaveh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_L/0/1/0/all/0/1\">Laurie Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LM-Critic: Language Models for Unsupervised Grammatical Error Correction. (arXiv:2109.06822v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06822","description":"<p>Training a model for grammatical error correction (GEC) requires a set of\nlabeled ungrammatical / grammatical sentence pairs, but manually annotating\nsuch pairs can be expensive. Recently, the Break-It-Fix-It (BIFI) framework has\ndemonstrated strong results on learning to repair a broken program without any\nlabeled examples, but this relies on a perfect critic (e.g., a compiler) that\nreturns whether an example is valid or not, which does not exist for the GEC\ntask. In this work, we show how to leverage a pretrained language model (LM) in\ndefining an LM-Critic, which judges a sentence to be grammatical if the LM\nassigns it a higher probability than its local perturbations. We apply this\nLM-Critic and BIFI along with a large set of unlabeled sentences to bootstrap\nrealistic ungrammatical / grammatical pairs for training a corrector. We\nevaluate our approach on GEC datasets across multiple domains (CoNLL-2014,\nBEA-2019, GMEG-wiki and GMEG-yahoo) and show that it outperforms existing\nmethods in both the unsupervised setting (+7.7 F0.5) and the supervised setting\n(+0.5 F0.5).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yasunaga_M/0/1/0/all/0/1\">Michihiro Yasunaga</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1\">Jure Leskovec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1\">Percy Liang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Types of Out-of-Distribution Texts and How to Detect Them. (arXiv:2109.06827v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06827","description":"<p>Despite agreement on the importance of detecting out-of-distribution (OOD)\nexamples, there is little consensus on the formal definition of OOD examples\nand how to best detect them. We categorize these examples by whether they\nexhibit a background shift or a semantic shift, and find that the two major\napproaches to OOD detection, model calibration and density estimation (language\nmodeling for text), have distinct behavior on these types of OOD data. Across\n14 pairs of in-distribution and OOD English natural language understanding\ndatasets, we find that density estimation methods consistently beat calibration\nmethods in background shift settings, while performing worse in semantic shift\nsettings. In addition, we find that both methods generally fail to detect\nexamples from challenge data, highlighting a weak spot for current methods.\nSince no single method works well across all settings, our results call for an\nexplicit definition of OOD examples when evaluating different detection\nmethods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arora_U/0/1/0/all/0/1\">Udit Arora</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">William Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1\">He He</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Perils of Using Mechanical Turk to Evaluate Open-Ended Text Generation. (arXiv:2109.06835v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06835","description":"<p>Recent text generation research has increasingly focused on open-ended\ndomains such as story and poetry generation. Because models built for such\ntasks are difficult to evaluate automatically, most researchers in the space\njustify their modeling choices by collecting crowdsourced human judgments of\ntext quality (e.g., Likert scores of coherence or grammaticality) from Amazon\nMechanical Turk (AMT). In this paper, we first conduct a survey of 45\nopen-ended text generation papers and find that the vast majority of them fail\nto report crucial details about their AMT tasks, hindering reproducibility. We\nthen run a series of story evaluation experiments with both AMT workers and\nEnglish teachers and discover that even with strict qualification filters, AMT\nworkers (unlike teachers) fail to distinguish between model-generated text and\nhuman-generated references. We show that AMT worker judgments improve when they\nare shown model-generated output alongside human-generated references, which\nenables the workers to better calibrate their ratings. Finally, interviews with\nthe English teachers provide deeper insights into the challenges of the\nevaluation process, particularly when rating model-generated text.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karpinska_M/0/1/0/all/0/1\">Marzena Karpinska</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Akoury_N/0/1/0/all/0/1\">Nader Akoury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Iyyer_M/0/1/0/all/0/1\">Mohit Iyyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ePiC: Employing Proverbs in Context as a Benchmark for Abstract Language Understanding. (arXiv:2109.06838v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06838","description":"<p>While large language models have shown exciting progress on several NLP\nbenchmarks, evaluating their ability for complex analogical reasoning remains\nunder-explored. Here, we introduce a high-quality crowdsourced dataset of\nnarratives for employing proverbs in context as a benchmark for abstract\nlanguage understanding. The dataset provides fine-grained annotation of aligned\nspans between proverbs and narratives, and contains minimal lexical overlaps\nbetween narratives and proverbs, ensuring that models need to go beyond\nsurface-level reasoning to succeed. We explore three tasks: (1) proverb\nrecommendation and alignment prediction, (2) narrative generation for a given\nproverb and topic, and (3) identifying narratives with similar motifs. Our\nexperiments show that neural language models struggle in our tasks compared to\nhumans, and the tasks pose multiple learning challenges.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"BenchIE: Open Information Extraction Evaluation Based on Facts, Not Tokens. (arXiv:2109.06850v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06850","description":"<p>Intrinsic evaluations of OIE systems are carried out either manually -- with\nhuman evaluators judging the correctness of extractions -- or automatically, on\nstandardized benchmarks. The latter, while much more cost-effective, is less\nreliable, primarily because of the incompleteness of the existing OIE\nbenchmarks: the ground truth extractions do not include all acceptable variants\nof the same fact, leading to unreliable assessment of models' performance.\nMoreover, the existing OIE benchmarks are available for English only. In this\nwork, we introduce BenchIE: a benchmark and evaluation framework for\ncomprehensive evaluation of OIE systems for English, Chinese and German. In\ncontrast to existing OIE benchmarks, BenchIE takes into account informational\nequivalence of extractions: our gold standard consists of fact synsets,\nclusters in which we exhaustively list all surface forms of the same fact. We\nbenchmark several state-of-the-art OIE systems using BenchIE and demonstrate\nthat these systems are significantly less effective than indicated by existing\nOIE benchmarks. We make BenchIE (data and evaluation code) publicly available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gashteovski_K/0/1/0/all/0/1\">Kiril Gashteovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1\">Mingying Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kotnis_B/0/1/0/all/0/1\">Bhushan Kotnis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lawrence_C/0/1/0/all/0/1\">Carolin Lawrence</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1\">Goran Glavas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1\">Mathias Niepert</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Summarize-then-Answer: Generating Concise Explanations for Multi-hop Reading Comprehension. (arXiv:2109.06853v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06853","description":"<p>How can we generate concise explanations for multi-hop Reading Comprehension\n(RC)? The current strategies of identifying supporting sentences can be seen as\nan extractive question-focused summarization of the input text. However, these\nextractive explanations are not necessarily concise i.e. not minimally\nsufficient for answering a question. Instead, we advocate for an abstractive\napproach, where we propose to generate a question-focused, abstractive summary\nof input paragraphs and then feed it to an RC system. Given a limited amount of\nhuman-annotated abstractive explanations, we train the abstractive explainer in\na semi-supervised manner, where we start from the supervised model and then\ntrain it further through trial and error maximizing a conciseness-promoted\nreward function. Our experiments demonstrate that the proposed abstractive\nexplainer can generate more compact explanations than an extractive explainer\nwith limited supervision (only 2k instances) while maintaining sufficiency.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Inoue_N/0/1/0/all/0/1\">Naoya Inoue</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1\">Harsh Trivedi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sinha_S/0/1/0/all/0/1\">Steven Sinha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1\">Niranjan Balasubramanian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Inui_K/0/1/0/all/0/1\">Kentaro Inui</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Broaden the Vision: Geo-Diverse Visual Commonsense Reasoning. (arXiv:2109.06860v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06860","description":"<p>Commonsense is defined as the knowledge that is shared by everyone. However,\ncertain types of commonsense knowledge are correlated with culture and\ngeographic locations and they are only shared locally. For example, the\nscenarios of wedding ceremonies vary across regions due to different customs\ninfluenced by historical and religious factors. Such regional characteristics,\nhowever, are generally omitted in prior work. In this paper, we construct a\nGeo-Diverse Visual Commonsense Reasoning dataset (GD-VCR) to test\nvision-and-language models' ability to understand cultural and\ngeo-location-specific commonsense. In particular, we study two state-of-the-art\nVision-and-Language models, VisualBERT and ViLBERT trained on VCR, a standard\nmultimodal commonsense benchmark with images primarily from Western regions. We\nthen evaluate how well the trained models can generalize to answering the\nquestions in GD-VCR. We find that the performance of both models for\nnon-Western regions including East Asia, South Asia, and Africa is\nsignificantly lower than that for Western region. We analyze the reasons behind\nthe performance disparity and find that the performance gap is larger on QA\npairs that: 1) are concerned with culture-related scenarios, e.g., weddings,\nreligious activities, and festivals; 2) require high-level geo-diverse\ncommonsense reasoning rather than low-order perception and recognition. Dataset\nand code are released at https://github.com/WadeYin9712/GD-VCR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1\">Da Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liunian Harold Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Ziniu Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Legal Transformer Models May Not Always Help. (arXiv:2109.06862v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06862","description":"<p>Deep learning-based Natural Language Processing methods, especially\ntransformers, have achieved impressive performance in the last few years.\nApplying those state-of-the-art NLP methods to legal activities to automate or\nsimplify some simple work is of great value. This work investigates the value\nof domain adaptive pre-training and language adapters in legal NLP tasks. By\ncomparing the performance of language models with domain adaptive pre-training\non different tasks and different dataset splits, we show that domain adaptive\npre-training is only helpful with low-resource downstream tasks, thus far from\nbeing a panacea. We also benchmark the performance of adapters in a typical\nlegal NLP task and show that they can yield similar performance to full model\ntuning with much smaller training costs. As an additional result, we release\nLegalRoBERTa, a RoBERTa model further pre-trained on legal corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1\">Sakbo Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lebret_R/0/1/0/all/0/1\">R&#xe9;mi Lebret</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aberer_K/0/1/0/all/0/1\">Karl Aberer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Performance-Efficiency Trade-offs in Unsupervised Pre-training for Speech Recognition. (arXiv:2109.06870v1 [cs.CL])","link":"http://arxiv.org/abs/2109.06870","description":"<p>This paper is a study of performance-efficiency trade-offs in pre-trained\nmodels for automatic speech recognition (ASR). We focus on wav2vec 2.0, and\nformalize several architecture designs that influence both the model\nperformance and its efficiency. Putting together all our observations, we\nintroduce SEW (Squeezed and Efficient Wav2vec), a pre-trained model\narchitecture with significant improvements along both performance and\nefficiency dimensions across a variety of training setups. For example, under\nthe 100h-960h semi-supervised setup on LibriSpeech, SEW achieves a 1.9x\ninference speedup compared to wav2vec 2.0, with a 13.5% relative reduction in\nword error rate. With a similar inference time, SEW reduces word error rate by\n25-50% across different model sizes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Felix Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1\">Kwangyoun Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1\">Jing Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1\">Kyu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weinberger_K/0/1/0/all/0/1\">Kilian Q. Weinberger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1\">Yoav Artzi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Hyper-Parameter Optimization for Neural Machine Translation on GPU Architectures. (arXiv:1805.02094v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/1805.02094","description":"<p>Neural machine translation (NMT) has been accelerated by deep learning neural\nnetworks over statistical-based approaches, due to the plethora and\nprogrammability of commodity heterogeneous computing architectures such as\nFPGAs and GPUs and the massive amount of training corpuses generated from news\noutlets, government agencies and social media. Training a learning classifier\nfor neural networks entails tuning hyper-parameters that would yield the best\nperformance. Unfortunately, the number of parameters for machine translation\ninclude discrete categories as well as continuous options, which makes for a\ncombinatorial explosive problem. This research explores optimizing\nhyper-parameters when training deep learning neural networks for machine\ntranslation. Specifically, our work investigates training a language model with\nMarian NMT. Results compare NMT under various hyper-parameter settings across a\nvariety of modern GPU architecture generations in single node and multi-node\nsettings, revealing insights on which hyper-parameters matter most in terms of\nperformance, such as words processed per second, convergence rates, and\ntranslation accuracy, and provides insights on how to best achieve\nhigh-performing NMT systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lim_R/0/1/0/all/0/1\">Robert Lim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Heafield_K/0/1/0/all/0/1\">Kenneth Heafield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoang_H/0/1/0/all/0/1\">Hieu Hoang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briers_M/0/1/0/all/0/1\">Mark Briers</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Malony_A/0/1/0/all/0/1\">Allen Malony</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. (arXiv:2005.00033v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2005.00033","description":"<p>With the emergence of the COVID-19 pandemic, the political and the medical\naspects of disinformation merged as the problem got elevated to a whole new\nlevel to become the first global infodemic. Fighting this infodemic has been\ndeclared one of the most important focus areas of the World Health\nOrganization, with dangers ranging from promoting fake cures, rumors, and\nconspiracy theories to spreading xenophobia and panic. Ad-dressing the issue\nrequires solving a number of challenging problems such as identifying messages\ncontaining claims, determining their check-worthiness and factuality, and their\npotential to do harm as well as the nature of that harm, to mention just a few.\nTo address this gap, we release a large dataset of 16K manually annotated\ntweets for fine-grained disinformation analysis that (i) focuses on COVID-19,\n(ii) combines the perspectives and the interests of journalists, fact-checkers,\nsocial media platforms, policy makers, and society, and (iii) covers Arabic,\nBulgarian, Dutch, and English. Finally, we show strong evaluation results using\npretrained Transformers, thus con-firming the practical utility of the dataset\nin monolingual vs. multilingual, and single task vs. multitask settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Alam_F/0/1/0/all/0/1\">Firoj Alam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shaar_S/0/1/0/all/0/1\">Shaden Shaar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dalvi_F/0/1/0/all/0/1\">Fahim Dalvi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sajjad_H/0/1/0/all/0/1\">Hassan Sajjad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nikolov_A/0/1/0/all/0/1\">Alex Nikolov</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mubarak_H/0/1/0/all/0/1\">Hamdy Mubarak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Martino_G/0/1/0/all/0/1\">Giovanni Da San Martino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Abdelali_A/0/1/0/all/0/1\">Ahmed Abdelali</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrani_N/0/1/0/all/0/1\">Nadir Durrani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Darwish_K/0/1/0/all/0/1\">Kareem Darwish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Homaid_A/0/1/0/all/0/1\">Abdulaziz Al-Homaid</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaghouani_W/0/1/0/all/0/1\">Wajdi Zaghouani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Caselli_T/0/1/0/all/0/1\">Tommaso Caselli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Danoe_G/0/1/0/all/0/1\">Gijs Danoe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stolk_F/0/1/0/all/0/1\">Friso Stolk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bruntink_B/0/1/0/all/0/1\">Britt Bruntink</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1\">Preslav Nakov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"L2R2: Leveraging Ranking for Abductive Reasoning. (arXiv:2005.11223v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2005.11223","description":"<p>The abductive natural language inference task ($\\alpha$NLI) is proposed to\nevaluate the abductive reasoning ability of a learning system. In the\n$\\alpha$NLI task, two observations are given and the most plausible hypothesis\nis asked to pick out from the candidates. Existing methods simply formulate it\nas a classification problem, thus a cross-entropy log-loss objective is used\nduring training. However, discriminating true from false does not measure the\nplausibility of a hypothesis, for all the hypotheses have a chance to happen,\nonly the probabilities are different. To fill this gap, we switch to a ranking\nperspective that sorts the hypotheses in order of their plausibilities. With\nthis new perspective, a novel $L2R^2$ approach is proposed under the\nlearning-to-rank framework. Firstly, training samples are reorganized into a\nranking form, where two observations and their hypotheses are treated as the\nquery and a set of candidate documents respectively. Then, an ESIM model or\npre-trained language model, e.g. BERT or RoBERTa, is obtained as the scoring\nfunction. Finally, the loss functions for the ranking task can be either\npair-wise or list-wise for training. The experimental results on the ART\ndataset reach the state-of-the-art in the public leaderboard.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yunchang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1\">Liang Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yanyan Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1\">Xueqi Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Document Graph for Neural Machine Translation. (arXiv:2012.03477v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.03477","description":"<p>Previous works have shown that contextual information can improve the\nperformance of neural machine translation (NMT). However, most existing\ndocument-level NMT methods only consider a few number of previous sentences.\nHow to make use of the whole document as global contexts is still a challenge.\nTo address this issue, we hypothesize that a document can be represented as a\ngraph that connects relevant contexts regardless of their distances. We employ\nseveral types of relations, including adjacency, syntactic dependency, lexical\nconsistency, and coreference, to construct the document graph. Then, we\nincorporate both source and target graphs into the conventional Transformer\narchitecture with graph convolutional networks. Experiments on various NMT\nbenchmarks, including IWSLT English--French, Chinese-English, WMT\nEnglish--German and Opensubtitle English--Russian, demonstrate that using\ndocument graphs can significantly improve the translation quality. Extensive\nanalysis verifies that the document graph is beneficial for capturing discourse\nphenomena.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1\">Mingzhou Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Liangyou Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_D/0/1/0/all/0/1\">Derek. F. Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1\">Lidia S. Chao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Quantum Mathematics in Artificial Intelligence. (arXiv:2101.04255v4 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2101.04255","description":"<p>In the decade since 2010, successes in artificial intelligence have been at\nthe forefront of computer science and technology, and vector space models have\nsolidified a position at the forefront of artificial intelligence. At the same\ntime, quantum computers have become much more powerful, and announcements of\nmajor advances are frequently in the news.\n</p>\n<p>The mathematical techniques underlying both these areas have more in common\nthan is sometimes realized. Vector spaces took a position at the axiomatic\nheart of quantum mechanics in the 1930s, and this adoption was a key motivation\nfor the derivation of logic and probability from the linear geometry of vector\nspaces. Quantum interactions between particles are modelled using the tensor\nproduct, which is also used to express objects and operations in artificial\nneural networks.\n</p>\n<p>This paper describes some of these common mathematical areas, including\nexamples of how they are used in artificial intelligence (AI), particularly in\nautomated reasoning and natural language processing (NLP). Techniques discussed\ninclude vector spaces, scalar products, subspaces and implication, orthogonal\nprojection and negation, dual vectors, density matrices, positive operators,\nand tensor products. Application areas include information retrieval,\ncategorization and implication, modelling word-senses and disambiguation,\ninference in knowledge bases, and semantic composition.\n</p>\n<p>Some of these approaches can potentially be implemented on quantum hardware.\nMany of the practical steps in this implementation are in early stages, and\nsome are already realized. Explaining some of the common mathematical tools can\nhelp researchers in both AI and quantum computing further exploit these\noverlaps, recognizing and exploring new directions along the way.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Widdows_D/0/1/0/all/0/1\">Dominic Widdows</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kitto_K/0/1/0/all/0/1\">Kirsty Kitto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohen_T/0/1/0/all/0/1\">Trevor Cohen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fused Acoustic and Text Encoding for Multimodal Bilingual Pretraining and Speech Translation. (arXiv:2102.05766v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.05766","description":"<p>Recently, representation learning for text and speech has successfully\nimproved many language related tasks. However, all existing methods suffer from\ntwo limitations: (a) they only learn from one input modality, while a unified\nrepresentation for both speech and text is needed by tasks such as end-to-end\nspeech translation, and as a result,(b) they can not exploit various\nlarge-scale text and speech data and their performance is limited by the\nscarcity of parallel speech translation data.To address these problems, we\npropose a Fused Acoustic and Text Masked Language Model (FAT-MLM) which jointly\nlearns a unified representation for both acoustic and text input from various\ntypes of corpora including parallel data for speech recognition and machine\ntranslation, and even pure speech and text data. Within this cross-modal\nrepresentation learning framework, we further present an end-to-end model for\nFused Acoustic and Text Speech Translation (FAT-ST). Experiments on three\ntranslation directions show that by fine-tuning from FAT-MLM, our proposed\nspeech translation models substantially improve translation quality by up to\n+5.9 BLEU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1\">Renjie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1\">Junkun Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1\">Mingbo Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1\">Liang Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contrastive Explanations for Model Interpretability. (arXiv:2103.01378v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01378","description":"<p>Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model's decision.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1\">Alon Jacovi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Swayamdipta_S/0/1/0/all/0/1\">Swabha Swayamdipta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ravfogel_S/0/1/0/all/0/1\">Shauli Ravfogel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1\">Yanai Elazar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1\">Yoav Goldberg</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving and Simplifying Pattern Exploiting Training. (arXiv:2103.11955v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11955","description":"<p>Recently, pre-trained language models (LMs) have achieved strong performance\nwhen fine-tuned on difficult benchmarks like SuperGLUE. However, performance\ncan suffer when there are very few labeled examples available for fine-tuning.\nPattern Exploiting Training (PET) is a recent approach that leverages patterns\nfor few-shot learning. However, PET uses task-specific unlabeled data. In this\npaper, we focus on few-shot learning without any unlabeled data and introduce\nADAPET, which modifies PET's objective to provide denser supervision during\nfine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any\ntask-specific unlabeled data. Our code can be found at\nhttps://github.com/rrmenon10/ADAPET.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tam_D/0/1/0/all/0/1\">Derek Tam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menon_R/0/1/0/all/0/1\">Rakesh R Menon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raffel_C/0/1/0/all/0/1\">Colin Raffel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Leveraging pre-trained representations to improve access to untranscribed speech from endangered languages. (arXiv:2103.14583v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.14583","description":"<p>Pre-trained speech representations like wav2vec 2.0 are a powerful tool for\nautomatic speech recognition (ASR). Yet many endangered languages lack\nsufficient data for pre-training such models, or are predominantly oral\nvernaculars without a standardised writing system, precluding fine-tuning.\nQuery-by-example spoken term detection (QbE-STD) offers an alternative for\niteratively indexing untranscribed speech corpora by locating spoken query\nterms. Using data from 7 Australian Aboriginal languages and a regional variety\nof Dutch, all of which are endangered or vulnerable, we show that QbE-STD can\nbe improved by leveraging representations developed for ASR (wav2vec 2.0: the\nEnglish monolingual model and XLSR53 multilingual model). Surprisingly, the\nEnglish model outperformed the multilingual model on 4 Australian language\ndatasets, raising questions around how to optimally leverage self-supervised\nspeech representations for QbE-STD. Nevertheless, we find that wav2vec 2.0\nrepresentations (either English or XLSR53) offer large improvements (56-86%\nrelative) over state-of-the-art approaches on our endangered language datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+San_N/0/1/0/all/0/1\">Nay San</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bartelds_M/0/1/0/all/0/1\">Martijn Bartelds</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Browne_M/0/1/0/all/0/1\">Mitchell Browne</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clifford_L/0/1/0/all/0/1\">Lily Clifford</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gibson_F/0/1/0/all/0/1\">Fiona Gibson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mansfield_J/0/1/0/all/0/1\">John Mansfield</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nash_D/0/1/0/all/0/1\">David Nash</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Simpson_J/0/1/0/all/0/1\">Jane Simpson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Turpin_M/0/1/0/all/0/1\">Myfany Turpin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vollmer_M/0/1/0/all/0/1\">Maria Vollmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wilmoth_S/0/1/0/all/0/1\">Sasha Wilmoth</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1\">Dan Jurafsky</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Connecting Attributions and QA Model Behavior on Realistic Counterfactuals. (arXiv:2104.04515v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04515","description":"<p>When a model attribution technique highlights a particular part of the input,\na user might understand this highlight as making a statement about\ncounterfactuals (Miller, 2019): if that part of the input were to change, the\nmodel's prediction might change as well. This paper investigates how well\ndifferent attribution techniques align with this assumption on realistic\ncounterfactuals in the case of reading comprehension (RC). RC is a particularly\nchallenging test case, as token-level attributions that have been extensively\nstudied in other NLP tasks such as sentiment analysis are less suitable to\nrepresent the reasoning that RC models perform. We construct counterfactual\nsets for three different RC settings, and through heuristics that can connect\nattribution methods' outputs to high-level model behavior, we can evaluate how\nuseful different attribution methods and even different formats are for\nunderstanding counterfactuals. We find that pairwise attributions are better\nsuited to RC than token-level attributions across these different RC settings,\nwith our best performance coming from a modification that we propose to an\nexisting pairwise attribution method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nair_R/0/1/0/all/0/1\">Rohan Nair</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1\">Greg Durrett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning Zero-Shot Multifaceted Visually Grounded Word Embeddings via Multi-Task Training. (arXiv:2104.07500v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07500","description":"<p>Language grounding aims at linking the symbolic representation of language\n(e.g., words) into the rich perceptual knowledge of the outside world. The\ngeneral approach is to embed both textual and visual information into a common\nspace -the grounded space-confined by an explicit relationship between both\nmodalities. We argue that this approach sacrifices the abstract knowledge\nobtained from linguistic co-occurrence statistics in the process of acquiring\nperceptual information. The focus of this paper is to solve this issue by\nimplicitly grounding the word embeddings. Rather than learning two mappings\ninto a joint space, our approach integrates modalities by determining a\nreversible grounded mapping between the textual and the grounded space by means\nof multi-task learning. Evaluations on intrinsic and extrinsic tasks show that\nour embeddings are highly beneficial for both abstract and concrete words. They\nare strongly correlated with human judgments and outperform previous works on a\nwide range of benchmarks. Our grounded embeddings are publicly available here.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shahmohammadi_H/0/1/0/all/0/1\">Hassan Shahmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lensch_H/0/1/0/all/0/1\">Hendrik P. A. Lensch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baayen_R/0/1/0/all/0/1\">R. Harald Baayen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Path Hunter: Reducing Hallucination in Dialogue Systems via Path Grounding. (arXiv:2104.08455v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08455","description":"<p>Dialogue systems powered by large pre-trained language models (LM) exhibit an\ninnate ability to deliver fluent and natural-looking responses. Despite their\nimpressive generation performance, these models can often generate factually\nincorrect statements impeding their widespread adoption. In this paper, we\nfocus on the task of improving the faithfulness -- and thus reduce\nhallucination -- of Neural Dialogue Systems to known facts supplied by a\nKnowledge Graph (KG). We propose Neural Path Hunter which follows a\ngenerate-then-refine strategy whereby a generated response is amended using the\nk-hop subgraph of a KG. Neural Path Hunter leverages a separate token-level\nfact critic to identify plausible sources of hallucination followed by a\nrefinement stage consisting of a chain of two neural LM's that retrieves\ncorrect entities by crafting a query signal that is propagated over the k-hop\nsubgraph. Our proposed model can easily be applied to any dialogue generated\nresponses without retraining the model. We empirically validate our proposed\napproach on the OpenDialKG dataset against a suite of metrics and report a\nrelative improvement of faithfulness over dialogue responses by 20.35% based on\nFeQA (Durmus et al., 2020).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dziri_N/0/1/0/all/0/1\">Nouha Dziri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1\">Andrea Madotto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zaiane_O/0/1/0/all/0/1\">Osmar Zaiane</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bose_A/0/1/0/all/0/1\">Avishek Joey Bose</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-Attention is All You Need: Adapting Pretrained Transformers for Machine Translation. (arXiv:2104.08771v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08771","description":"<p>We study the power of cross-attention in the Transformer architecture within\nthe context of transfer learning for machine translation, and extend the\nfindings of studies into cross-attention when training from scratch. We conduct\na series of experiments through fine-tuning a translation model on data where\neither the source or target language has changed. These experiments reveal that\nfine-tuning only the cross-attention parameters is nearly as effective as\nfine-tuning all parameters (i.e., the entire translation model). We provide\ninsights into why this is the case and observe that limiting fine-tuning in\nthis manner yields cross-lingually aligned embeddings. The implications of this\nfinding for researchers and practitioners include a mitigation of catastrophic\nforgetting, the potential for zero-shot translation, and the ability to extend\nmachine translation models to several new language pairs with reduced parameter\nstorage overhead.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gheini_M/0/1/0/all/0/1\">Mozhdeh Gheini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1\">Jonathan May</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Low-Dimensional Linear Geometry of Contextualized Word Representations. (arXiv:2105.07109v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.07109","description":"<p>Black-box probing models can reliably extract linguistic features like tense,\nnumber, and syntactic role from pretrained word representations. However, the\nmanner in which these features are encoded in representations remains poorly\nunderstood. We present a systematic study of the linear geometry of\ncontextualized word representations in ELMO and BERT. We show that a variety of\nlinguistic features (including structured dependency relationships) are encoded\nin low-dimensional subspaces. We then refine this geometric picture, showing\nthat there are hierarchical relations between the subspaces encoding general\nlinguistic categories and more specific ones, and that low-dimensional feature\nencodings are distributed rather than aligned to individual neurons. Finally,\nwe demonstrate that these linear subspaces are causally related to model\nbehavior, and can be used to perform fine-grained manipulation of BERT's output\ndistribution.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hernandez_E/0/1/0/all/0/1\">Evan Hernandez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1\">Jacob Andreas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coreference-Aware Dialogue Summarization. (arXiv:2106.08556v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2106.08556","description":"<p>Summarizing conversations via neural approaches has been gaining research\ntraction lately, yet it is still challenging to obtain practical solutions.\nExamples of such challenges include unstructured information exchange in\ndialogues, informal interactions between speakers, and dynamic role changes of\nspeakers as the dialogue evolves. Many of such challenges result in complex\ncoreference links. Therefore, in this work, we investigate different approaches\nto explicitly incorporate coreference information in neural abstractive\ndialogue summarization models to tackle the aforementioned challenges.\nExperimental results show that the proposed approaches achieve state-of-the-art\nperformance, implying it is useful to utilize coreference information in\ndialogue summarization. Evaluation results on factual correctness suggest such\ncoreference-aware models are better at tracing the information flow among\ninterlocutors and associating accurate status/actions with the corresponding\ninterlocutors and person mentions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_K/0/1/0/all/0/1\">Ke Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1\">Nancy F. Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"End-to-End Natural Language Understanding Pipeline for Bangla Conversational Agents. (arXiv:2107.05541v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.05541","description":"<p>Chatbots are intelligent software built to be used as a replacement for human\ninteraction. However, existing studies typically do not provide enough support\nfor low-resource languages like Bangla. Moreover, due to the increasing\npopularity of social media, we can also see the rise of interactions in Bangla\ntransliteration (mostly in English) among the native Bangla speakers. In this\npaper, we propose a novel approach to build a Bangla chatbot aimed to be used\nas a business assistant which can communicate in Bangla and Bangla\nTransliteration in English with high confidence consistently. Since annotated\ndata was not available for this purpose, we had to work on the whole machine\nlearning life cycle (data preparation, machine learning modeling, and model\ndeployment) using Rasa Open Source Framework, fastText embeddings, Polyglot\nembeddings, Flask, and other systems as building blocks. While working with the\nskewed annotated dataset, we try out different setups and pipelines to evaluate\nwhich works best and provide possible reasoning behind the observed results.\nFinally, we present a pipeline for intent classification and entity extraction\nwhich achieves reasonable performance (accuracy: 83.02%, precision: 80.82%,\nrecall: 83.02%, F1-score: 80%).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1\">Fahim Shahriar Khan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mushabbir_M/0/1/0/all/0/1\">Mueeze Al Mushabbir</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Irbaz_M/0/1/0/all/0/1\">Mohammad Sabik Irbaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nasim_M/0/1/0/all/0/1\">MD Abdullah Al Nasim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To Ship or Not to Ship: An Extensive Evaluation of Automatic Metrics for Machine Translation. (arXiv:2107.10821v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2107.10821","description":"<p>Automatic metrics are commonly used as the exclusive tool for declaring the\nsuperiority of one machine translation system's quality over another. The\ncommunity choice of automatic metric guides research directions and industrial\ndevelopments by deciding which models are deemed better. Evaluating metrics\ncorrelations with sets of human judgements has been limited by the size of\nthese sets. In this paper, we corroborate how reliable metrics are in contrast\nto human judgements on -- to the best of our knowledge -- the largest\ncollection of judgements reported in the literature. Arguably, pairwise\nrankings of two systems are the most common evaluation tasks in research or\ndeployment scenarios. Taking human judgement as a gold standard, we investigate\nwhich metrics have the highest accuracy in predicting translation quality\nrankings for such system pairs. Furthermore, we evaluate the performance of\nvarious metrics across different language pairs and domains. Lastly, we show\nthat the sole use of BLEU impeded the development of improved models leading to\nbad deployment decisions. We release the collection of 2.3M sentence-level\nhuman judgements for 4380 systems for further analysis and replication of our\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kocmi_T/0/1/0/all/0/1\">Tom Kocmi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Federmann_C/0/1/0/all/0/1\">Christian Federmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grundkiewicz_R/0/1/0/all/0/1\">Roman Grundkiewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Junczys_Dowmunt_M/0/1/0/all/0/1\">Marcin Junczys-Dowmunt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Matsushita_H/0/1/0/all/0/1\">Hitokazu Matsushita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menezes_A/0/1/0/all/0/1\">Arul Menezes</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"UNIQORN: Unified Question Answering over RDF Knowledge Graphs and Natural Language Text. (arXiv:2108.08614v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2108.08614","description":"<p>Question answering over knowledge graphs and other RDF data has been greatly\nadvanced, with a number of good systems providing crisp answers for natural\nlanguage questions or telegraphic queries. Some of these systems incorporate\ntextual sources as additional evidence for the answering process, but cannot\ncompute answers that are present in text alone. Conversely, systems from the IR\nand NLP communities have addressed QA over text, but barely utilize semantic\ndata and knowledge. This paper presents the first QA system that can seamlessly\noperate over RDF datasets and text corpora, or both together, in a unified\nframework. Our method, called UNIQORN, builds a context graph on the fly, by\nretrieving question-relevant triples from the RDF data and/or the text corpus,\nwhere the latter case is handled by automatic information extraction. The\nresulting graph is typically rich but highly noisy. UNIQORN copes with this\ninput by advanced graph algorithms for Group Steiner Trees, that identify the\nbest answer candidates in the context graph. Experimental results on several\nbenchmarks of complex questions with multiple entities and relations, show that\nUNIQORN, an unsupervised method with only five parameters, produces results\ncomparable to the state-of-the-art on KGs, text corpora, and heterogeneous\nsources. The graph-based methodology provides user-interpretable evidence for\nthe complete answering process.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pramanik_S/0/1/0/all/0/1\">Soumajit Pramanik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alabi_J/0/1/0/all/0/1\">Jesujoba Alabi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1\">Rishiraj Saha Roy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weikum_G/0/1/0/all/0/1\">Gerhard Weikum</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DEGREE: A Data-Efficient Generative Event Extraction Model. (arXiv:2108.12724v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12724","description":"<p>Event extraction (EE) aims to identify structured events, including event\ntriggers and their corresponding arguments, from unstructured text. Most of the\nexisting works rely on a large number of labeled instances to train models,\nwhile the labeled data could be expensive to be obtained. In this work, we\npresent a data-efficient event extraction method by formulating event\nextraction as a natural language generation problem. The formulation allows us\nto inject knowledge of label semantics, event structure, and output\ndependencies into the model. Given a passage and an event type, our model\nlearns to summarize this passage into a templated sentence in a predefined\nstructure. The template is event-type-specific, manually created, and contains\nevent trigger and argument information. Lastly, a rule-based algorithm is used\nto derive the trigger and argument predictions from the generated sentence. Our\nmethod inherently enjoys the following benefits: (1) The pretraining of the\ngenerative language models help incorporate the semantics of the labels for\ngenerative EE. (2) The autoregressive generation process and our end-to-end\ndesign for extracting triggers and arguments force the model to capture the\ndependencies among the output triggers and their arguments. (3) The predefined\ntemplates form concrete yet flexible rules to hint the models about the valid\npatterns for each event type, reducing the models' burden to learn structures\nfrom the data. Empirical results show that our model achieves superior\nperformance over strong baselines on EE tasks in the low data regime and\nachieves competitive results to the current state-of-the-art when more data\nbecomes available.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hsu_I/0/1/0/all/0/1\">I-Hung Hsu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1\">Kuan-Hao Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boschee_E/0/1/0/all/0/1\">Elizabeth Boschee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Miller_S/0/1/0/all/0/1\">Scott Miller</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Natarajan_P/0/1/0/all/0/1\">Prem Natarajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Robust Retrieval Augmented Generation for Zero-shot Slot Filling. (arXiv:2108.13934v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.13934","description":"<p>Automatically inducing high quality knowledge graphs from a given collection\nof documents still remains a challenging problem in AI. One way to make headway\nfor this problem is through advancements in a related task known as slot\nfilling. In this task, given an entity query in form of [Entity, Slot, ?], a\nsystem is asked to fill the slot by generating or extracting the missing value\nexploiting evidence extracted from relevant passage(s) in the given document\ncollection. The recent works in the field try to solve this task in an\nend-to-end fashion using retrieval-based language models. In this paper, we\npresent a novel approach to zero-shot slot filling that extends dense passage\nretrieval with hard negatives and robust training procedures for retrieval\naugmented generation models. Our model reports large improvements on both T-REx\nand zsRE slot filling datasets, improving both passage retrieval and slot value\ngeneration, and ranking at the top-1 position in the KILT leaderboard.\nMoreover, we demonstrate the robustness of our system showing its domain\nadaptation capability on a new variant of the TACRED dataset for slot filling,\nthrough a combination of zero/few-shot learning. We release the source code and\npre-trained models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Glass_M/0/1/0/all/0/1\">Michael Glass</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rossiello_G/0/1/0/all/0/1\">Gaetano Rossiello</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1\">Md Faisal Mahbub Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gliozzo_A/0/1/0/all/0/1\">Alfio Gliozzo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mixup Decoding for Diverse Machine Translation. (arXiv:2109.03402v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03402","description":"<p>Diverse machine translation aims at generating various target language\ntranslations for a given source language sentence. Leveraging the linear\nrelationship in the sentence latent space introduced by the mixup training, we\npropose a novel method, MixDiversity, to generate different translations for\nthe input sentence by linearly interpolating it with different sentence pairs\nsampled from the training corpus when decoding. To further improve the\nfaithfulness and diversity of the translations, we propose two simple but\neffective approaches to select diverse sentence pairs in the training corpus\nand adjust the interpolation weight for each pair correspondingly. Moreover, by\ncontrolling the interpolation weight, our method can achieve the trade-off\nbetween faithfulness and diversity without any additional training, which is\nrequired in most of the previous methods. Experiments on WMT'16 en-ro, WMT'14\nen-de, and WMT'17 zh-en are conducted to show that our method substantially\noutperforms all previous diverse machine translation methods.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jicheng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xuanfu Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1\">Zhongjun He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Memory and Knowledge Augmented Language Models for Inferring Salience in Long-Form Stories. (arXiv:2109.03754v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03754","description":"<p>Measuring event salience is essential in the understanding of stories. This\npaper takes a recent unsupervised method for salience detection derived from\nBarthes Cardinal Functions and theories of surprise and applies it to longer\nnarrative forms. We improve the standard transformer language model by\nincorporating an external knowledgebase (derived from Retrieval Augmented\nGeneration) and adding a memory mechanism to enhance performance on longer\nworks. We use a novel approach to derive salience annotation using\nchapter-aligned summaries from the Shmoop corpus for classic literary works.\nOur evaluation against this data demonstrates that our salience detection model\nimproves performance over and above a non-knowledgebase and memory augmented\nlanguage model, both of which are crucial to this improvement.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wilmot_D/0/1/0/all/0/1\">David Wilmot</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keller_F/0/1/0/all/0/1\">Frank Keller</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"PPT: Pre-trained Prompt Tuning for Few-shot Learning. (arXiv:2109.04332v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04332","description":"<p>Prompts for pre-trained language models (PLMs) have shown remarkable\nperformance by bridging the gap between pre-training tasks and various\ndownstream tasks. Among these methods, prompt tuning, which freezes PLMs and\nonly tunes soft prompts, provides an efficient and effective solution for\nadapting large-scale PLMs to downstream tasks. However, prompt tuning is yet to\nbe fully explored. In our pilot experiments, we find that prompt tuning\nperforms comparably with conventional full-model fine-tuning when downstream\ndata are sufficient, whereas it performs much worse under few-shot learning\nsettings, which may hinder the application of prompt tuning in practice. We\nattribute this low performance to the manner of initializing soft prompts.\nTherefore, in this work, we propose to pre-train prompts by adding soft prompts\ninto the pre-training stage to obtain a better initialization. We name this\nPre-trained Prompt Tuning framework \"PPT\". To ensure the generalization of PPT,\nwe formulate similar classification tasks into a unified task form and\npre-train soft prompts for this unified task. Extensive experiments show that\ntuning pre-trained prompts for downstream tasks can reach or even outperform\nfull-model fine-tuning under both full-data and few-shot settings. Our approach\nis effective and efficient for using large-scale PLMs in practice.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yuxian Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xu Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1\">Minlie Huang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How May I Help You? Using Neural Text Simplification to Improve Downstream NLP Tasks. (arXiv:2109.04604v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04604","description":"<p>The general goal of text simplification (TS) is to reduce text complexity for\nhuman consumption. This paper investigates another potential use of neural TS:\nassisting machines performing natural language processing (NLP) tasks. We\nevaluate the use of neural TS in two ways: simplifying input texts at\nprediction time and augmenting data to provide machines with additional\ninformation during training. We demonstrate that the latter scenario provides\npositive effects on machine performance on two separate datasets. In\nparticular, the latter use of TS improves the performances of LSTM (1.82-1.98%)\nand SpanBERT (0.7-1.3%) extractors on TACRED, a complex, large-scale,\nreal-world relation extraction task. Further, the same setting yields\nimprovements of up to 0.65% matched and 0.62% mismatched accuracies for a BERT\ntext classifier on MNLI, a practical natural language inference dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Van_H/0/1/0/all/0/1\">Hoang Van</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1\">Zheng Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1\">Mihai Surdeanu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CINS: Comprehensive Instruction for Few-shot Learning in Task-oriented Dialog Systems. (arXiv:2109.04645v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04645","description":"<p>As labeling cost for different modules in task-oriented dialog (ToD) systems\nis high, a major challenge in practice is to learn different tasks with the\nleast amount of labeled data. Recently, prompting methods over pre-trained\nlanguage models (PLMs) have shown promising results for few-shot learning in\nToD. To better utilize the power of PLMs, this paper proposes Comprehensive\nInstruction (CINS) that exploits PLMs with extra task-specific instructions. We\ndesign a schema (definition, constraint, prompt) of instructions and their\ncustomized realizations for three important downstream tasks in ToD, i.e.\nintent classification, dialog state tracking, and natural language generation.\nA sequence-to-sequence model (T5) is adopted to solve these three tasks in a\nunified framework. Extensive experiments are conducted on these ToD tasks in\nrealistic few-shot learning scenarios with small validation data. Empirical\nresults demonstrate that the proposed CINS approach consistently improves\ntechniques that finetune PLMs with raw input or short prompts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1\">Fei Mi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yitong Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yasheng Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1\">Xin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qun Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RoR: Read-over-Read for Long Document Machine Reading Comprehension. (arXiv:2109.04780v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04780","description":"<p>Transformer-based pre-trained models, such as BERT, have achieved remarkable\nresults on machine reading comprehension. However, due to the constraint of\nencoding length (e.g., 512 WordPiece tokens), a long document is usually split\ninto multiple chunks that are independently read. It results in the reading\nfield being limited to individual chunks without information collaboration for\nlong document machine reading comprehension. To address this problem, we\npropose RoR, a read-over-read method, which expands the reading field from\nchunk to document. Specifically, RoR includes a chunk reader and a document\nreader. The former first predicts a set of regional answers for each chunk,\nwhich are then compacted into a highly-condensed version of the original\ndocument, guaranteeing to be encoded once. The latter further predicts the\nglobal answers from this condensed document. Eventually, a voting strategy is\nutilized to aggregate and rerank the regional and global answers for final\nprediction. Extensive experiments on two benchmarks QuAC and TriviaQA\ndemonstrate the effectiveness of RoR for long document reading. Notably, RoR\nranks 1st place on the QuAC leaderboard (https://quac.ai/) at the time of\nsubmission (May 17th, 2021).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1\">Jing Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Junwei Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yifan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yongwei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Youzheng Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1\">Xiaodong He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1\">Bowen Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Universal Simultaneous Machine Translation with Mixture-of-Experts Wait-k Policy. (arXiv:2109.05238v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05238","description":"<p>Simultaneous machine translation (SiMT) generates translation before reading\nthe entire source sentence and hence it has to trade off between translation\nquality and latency. To fulfill the requirements of different translation\nquality and latency in practical applications, the previous methods usually\nneed to train multiple SiMT models for different latency levels, resulting in\nlarge computational costs. In this paper, we propose a universal SiMT model\nwith Mixture-of-Experts Wait-k Policy to achieve the best translation quality\nunder arbitrary latency with only one trained model. Specifically, our method\nemploys multi-head attention to accomplish the mixture of experts where each\nhead is treated as a wait-k expert with its own waiting words number, and given\na test latency and source inputs, the weights of the experts are accordingly\nadjusted to produce the best translation. Experiments on three datasets show\nthat our method outperforms all the strong baselines under different latency,\nincluding the state-of-the-art adaptive policy.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Concentrated Cross-Attention for Neural Machine Translation with Gaussian Mixture Model. (arXiv:2109.05244v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05244","description":"<p>Cross-attention is an important component of neural machine translation\n(NMT), which is always realized by dot-product attention in previous methods.\nHowever, dot-product attention only considers the pair-wise correlation between\nwords, resulting in dispersion when dealing with long sentences and neglect of\nsource neighboring relationships. Inspired by linguistics, the above issues are\ncaused by ignoring a type of cross-attention, called concentrated attention,\nwhich focuses on several central words and then spreads around them. In this\nwork, we apply Gaussian Mixture Model (GMM) to model the concentrated attention\nin cross-attention. Experiments and analyses we conducted on three datasets\nshow that the proposed method outperforms the baseline and has significant\nimprovement on alignment quality, N-gram accuracy, and long sentence\ntranslation.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shaolei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1\">Yang Feng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora. (arXiv:2109.05406v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05406","description":"<p>Human conversations consist of reasonable and natural topic flows, which are\nobserved as the shifts of the mentioned concepts across utterances. Previous\nchatbots that incorporate the external commonsense knowledge graph prove that\nmodeling the concept shifts can effectively alleviate the dull and\nuninformative response dilemma. However, there still exists a gap between the\nconcept relations in the natural conversation and those in the external\ncommonsense knowledge graph, which is an issue to solve. Specifically, the\nconcept relations in the external commonsense knowledge graph are not\nintuitively built from the conversational scenario but the world knowledge,\nwhich makes them insufficient for the chatbot construction. To bridge the above\ngap, we propose the method to supply more concept relations extracted from the\nconversational corpora and reconstruct an enhanced concept graph for the\nchatbot construction. In addition, we present a novel, powerful, and fast graph\nencoding architecture named the Edge-Transformer to replace the traditional GNN\narchitecture. Experimental results on the Reddit conversation dataset indicate\nour proposed method significantly outperforms strong baseline systems and\nachieves new SOTA results. Further analysis individually proves the\neffectiveness of the enhanced concept graph and the Edge-Transformer\narchitecture.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Si_P/0/1/0/all/0/1\">Pengda Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1\">Yao Qiu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1\">Yujiu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Language-Dependent Ethnic Bias in BERT. (arXiv:2109.05704v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05704","description":"<p>BERT and other large-scale language models (LMs) contain gender and racial\nbias. They also exhibit other dimensions of social bias, most of which have not\nbeen studied in depth, and some of which vary depending on the language. In\nthis paper, we study ethnic bias and how it varies across languages by\nanalyzing and mitigating ethnic bias in monolingual BERT for English, German,\nSpanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we\ndevelop a novel metric called Categorical Bias score. Then we propose two\nmethods for mitigation; first using a multilingual model, and second using\ncontextual word alignment of two monolingual models. We compare our proposed\nmethods with monolingual BERT and show that these methods effectively alleviate\nthe ethnic bias. Which of the two methods works better depends on the amount of\nNLP resources available for that language. We additionally experiment with\nArabic and Greek to verify that our proposed methods work for a wider variety\nof languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1\">Jaimeen Ahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oh_A/0/1/0/all/0/1\">Alice Oh</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CPT: A Pre-Trained Unbalanced Transformer for Both Chinese Language Understanding and Generation. (arXiv:2109.05729v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05729","description":"<p>In this paper, we take the advantage of previous pre-trained models (PTMs)\nand propose a novel Chinese Pre-trained Unbalanced Transformer (CPT). Different\nfrom previous Chinese PTMs, CPT is designed for both natural language\nunderstanding (NLU) and natural language generation (NLG) tasks. CPT consists\nof three parts: a shared encoder, an understanding decoder, and a generation\ndecoder. Two specific decoders with a shared encoder are pre-trained with\nmasked language modeling (MLM) and denoising auto-encoding (DAE) tasks,\nrespectively. With the partially shared architecture and multi-task\npre-training, CPT can (1) learn specific knowledge of both NLU or NLG tasks\nwith two decoders and (2) be fine-tuned flexibly that fully exploits the\npotential of the model. Moreover, the unbalanced Transformer saves the\ncomputational and storage cost, which makes CPT competitive and greatly\naccelerates the inference of text generation. Experimental results on a wide\nrange of Chinese NLU and NLG tasks show the effectiveness of CPT.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1\">Yunfan Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1\">Zhichao Geng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yitao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1\">Junqi Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1\">Fei Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhe_L/0/1/0/all/0/1\">Li Zhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_H/0/1/0/all/0/1\">Hujun Bao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1\">Xipeng Qiu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Categorical Semantics of Reversible Pattern-Matching. (arXiv:2109.05837v2 [cs.LO] UPDATED)","link":"http://arxiv.org/abs/2109.05837","description":"<p>This paper is concerned with categorical structures for reversible\ncomputation. In particular, we focus on a typed, functional reversible language\nbased on Theseus. We discuss how join inverse rig categories do not in general\ncapture pattern-matching, the core construct Theseus uses to enforce\nreversibility. We then derive a categorical structure to add to join inverse\nrig categories in order to capture pattern-matching. We show how such a\nstructure makes an adequate model for reversible pattern-matching.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lemonnier_L/0/1/0/all/0/1\">Louis Lemonnier</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chardonnet_K/0/1/0/all/0/1\">Kostia Chardonnet</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Valiron_B/0/1/0/all/0/1\">Beno&#xee;t Valiron</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Question Answering over Electronic Devices: A New Benchmark Dataset and a Multi-Task Learning based QA Framework. (arXiv:2109.05897v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05897","description":"<p>Answering questions asked from instructional corpora such as E-manuals,\nrecipe books, etc., has been far less studied than open-domain factoid\ncontext-based question answering. This can be primarily attributed to the\nabsence of standard benchmark datasets. In this paper we meticulously create a\nlarge amount of data connected with E-manuals and develop suitable algorithm to\nexploit it. We collect E-Manual Corpus, a huge corpus of 307,957 E-manuals and\npretrain RoBERTa on this large corpus. We create various benchmark QA datasets\nwhich include question answer pairs curated by experts based upon two\nE-manuals, real user questions from Community Question Answering Forum\npertaining to E-manuals etc. We introduce EMQAP (E-Manual Question Answering\nPipeline) that answers questions pertaining to electronics devices. Built upon\nthe pretrained RoBERTa, it harbors a supervised multi-task learning framework\nwhich efficiently performs the dual tasks of identifying the section in the\nE-manual where the answer can be found and the exact answer span within that\nsection. For E-Manual annotated question-answer pairs, we show an improvement\nof about 40% in ROUGE-L F1 scores over the most competitive baseline. We\nperform a detailed ablation study and establish the versatility of EMQAP across\ndifferent circumstances. The code and datasets are shared at\nhttps://github.com/abhi1nandy2/EMNLP-2021-Findings, and the corresponding\nproject website is https://sites.google.com/view/emanualqa/home.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nandy_A/0/1/0/all/0/1\">Abhilash Nandy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1\">Soumya Sharma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maddhashiya_S/0/1/0/all/0/1\">Shubham Maddhashiya</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sachdeva_K/0/1/0/all/0/1\">Kapil Sachdeva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1\">Pawan Goyal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ganguly_N/0/1/0/all/0/1\">Niloy Ganguly</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can Language Models Encode Perceptual Structure Without Grounding? A Case Study in Color. (arXiv:2109.06129v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2109.06129","description":"<p>Pretrained language models have been shown to encode relational information,\nsuch as the relations between entities or concepts in knowledge-bases --\n(Paris, Capital, France). However, simple relations of this type can often be\nrecovered heuristically and the extent to which models implicitly reflect\ntopological structure that is grounded in world, such as perceptual structure,\nis unknown. To explore this question, we conduct a thorough case study on\ncolor. Namely, we employ a dataset of monolexemic color terms and color chips\nrepresented in CIELAB, a color space with a perceptually meaningful distance\nmetric.\n</p>\n<p>Using two methods of evaluating the structural alignment of colors in this\nspace with text-derived color term representations, we find significant\ncorrespondence. Analyzing the differences in alignment across the color\nspectrum, we find that warmer colors are, on average, better aligned to the\nperceptual color space than cooler ones, suggesting an intriguing connection to\nfindings from recent work on efficient communication in color naming. Further\nanalysis suggests that differences in alignment are, in part, mediated by\ncollocationality and differences in syntactic usage, posing questions as to the\nrelationship between color perception and usage and context.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Abdou_M/0/1/0/all/0/1\">Mostafa Abdou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kulmizev_A/0/1/0/all/0/1\">Artur Kulmizev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hershcovich_D/0/1/0/all/0/1\">Daniel Hershcovich</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Frank_S/0/1/0/all/0/1\">Stella Frank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1\">Ellie Pavlick</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sogaard_A/0/1/0/all/0/1\">Anders S&#xf8;gaard</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-14T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}