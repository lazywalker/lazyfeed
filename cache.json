{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-08-30T01:49:53.272297370Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"æ„å»ºå®‰å…¨æ˜“ç”¨çš„é“¾è¡¨","link":"https://rustcc.cn/article?id=273831e7-932d-476f-9d31-323151afb123","description":"<p>å†™äº†ä¸€ä¸ªé“¾è¡¨çš„Crateï¼Œæ„¿æ™¯æ˜¯æ„å»ºå®‰å…¨ä¸”æ˜“ç”¨çš„é“¾è¡¨ã€‚</p>\n<p>æ¬¢è¿å¤§å®¶æ¥æ‰¾èŒ¬ï¼ˆBugï¼‰æˆ–æéœ€æ±‚ :)</p>\n<p>Crate IOé“¾æ¥ï¼š<a href=\"https://crates.io/crates/cyclic_list\" rel=\"noopener noreferrer\">https://crates.io/crates/cyclic_list</a>;</p>\n<p>GitHubé“¾æ¥ï¼š<a href=\"https://github.com/whjpji/cyclic_list\" rel=\"noopener noreferrer\">https://github.com/whjpji/cyclic_list</a></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-29 15:10:34","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rust æ—¥æŠ¥ã€‘2021-08-29 Tangramï¼šè®­ç»ƒã€éƒ¨ç½²å’Œç›‘æ§æœºå™¨å­¦ä¹ æ¨¡å‹","link":"https://rustcc.cn/article?id=4a218f6c-3c77-4aa0-84d6-90ac2bf1fc7c","description":"<h3>Embedded Rust ç¬¬ä¸€æ­¥ï¼šé€‰æ‹©ä¸€å—æ¿å­</h3>\n<p>å†…å®¹æ•´ç†è‡ª <a href=\"https://github.com/robyoung\" rel=\"noopener noreferrer\">robyoung (Rob Young)</a> çš„æ–‡ç« ï¼šFirst steps with Embedded Rust: Selecting a board</p>\n<p>æœ‰è¿™ä¹ˆå¤šä»¤äººçœ¼èŠ±ç¼­ä¹±çš„å¾®æ§åˆ¶å™¨å’Œé¡¹ç›®ï¼Œå¯¹äºåµŒå…¥å¼ç»éªŒå¾ˆå°‘çš„äººæ¥è¯´åº”è¯¥ä»å“ªé‡Œå¼€å§‹ï¼Ÿ</p>\n<p><strong>æˆ‘ä»¬åœ¨å¼€å‘æ¿ä¸­æƒ³è¦ä»€ä¹ˆï¼Ÿ</strong></p>\n<ul>\n<li>è‰¯å¥½çš„æ¶æ„æ”¯æŒ</li>\n<li>è‰¯å¥½çš„èŠ¯ç‰‡æ”¯æŒ</li>\n<li>æ´»è·ƒçš„ç¤¾åŒº</li>\n<li>å†…ç½®è°ƒè¯•å™¨</li>\n</ul>\n<p><strong>æˆ‘ä»¬éœ€è¦ä»€ä¹ˆæ¶æ„ï¼Ÿ</strong></p>\n<p>æ‹¥æœ‰æœ€å®Œæ•´åº“ã€æœ€è¯¦å°½æŒ‡å—å’Œæœ€å¤§ç¤¾åŒºçš„æ¶æ„æ˜¯ ARM Cortex-Mã€‚ ARM Cortex-M æ˜¯é¢å‘å¾®æ§åˆ¶å™¨åº”ç”¨çš„ä½åŠŸè€—ã€ä½æˆæœ¬å¤„ç†å™¨ã€‚ æŸ¥çœ‹ crates.io ä¸Šçš„ä¸‹è½½é‡è™½è¯´ä¸æ˜¯ä¸€ä¸ªå®Œç¾çš„æŒ‡æ ‡ï¼Œä½†å¯ä»¥è®©æˆ‘ä»¬äº†è§£è§„æ¨¡ä¸Šçš„å·®å¼‚ã€‚åœ¨è¿‡å»çš„ 90 å¤©å†…ï¼Œcortex-m çš„ä¸‹è½½é‡è¶…è¿‡ 250kã€‚ RISC-Vã€AVR æˆ– Xtensa æœ€å¤šæœ‰ 3k æ¬¡ä¸‹è½½ï¼Œcortex-a æœ‰å¤§çº¦ 18k æ¬¡ä¸‹è½½ã€‚ARM Cortex-M ç‹¬æ ‘ä¸€å¸œã€‚</p>\n<ul>\n<li>AVRï¼šAVR æ˜¯ç”¨äºåµŒå…¥å¼ç³»ç»Ÿçš„ 8 ä½å¾®æ§åˆ¶å™¨ç³»åˆ—ã€‚åœ¨ Rust ç”Ÿæ€ç³»ç»Ÿä¸­ï¼Œå®ƒä»¬å¹¶æ²¡æœ‰å¾—åˆ°å¾ˆå¥½çš„æ”¯æŒã€‚ç›´åˆ°æœ€è¿‘ï¼Œè¿˜éœ€è¦ä½¿ç”¨ rustc çš„ä¸€ä¸ªåˆ†æ”¯æ¥æ„å»º AVRã€‚ ç°åœ¨æœ‰å‡ ä¸ªä¸åŒçš„é€‰æ‹©ï¼Œawesome-avr-rust æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚</li>\n<li>ARM Cortex-Aï¼šæ›´å¼ºå¤§çš„å¤šæ ¸ ARM å¤„ç†å™¨ï¼Œä¸“ä¸ºè¿è¡Œæ›´å¤§çš„ä¸œè¥¿è€Œè®¾è®¡ã€‚ é€šå¸¸ä¼šåœ¨å®ƒä»¬ä¸Šè¿è¡Œå®Œæ•´çš„æ“ä½œç³»ç»Ÿã€‚  ä¾‹å¦‚è¿™æ˜¯å¤§å¤šæ•°æ™ºèƒ½æ‰‹æœºå’ŒæŒä¸Šæ¸¸æˆæœºä¸­ä½¿ç”¨çš„æ¶æ„ã€‚æŸ¥çœ‹ <a href=\"https://crates.io/crates/cortex-a\" rel=\"noopener noreferrer\">cortex-a - crates.io: Rust Package Registry</a> äº†è§£æ›´å¤šã€‚</li>\n<li>RISC-Vï¼šä¼¼ä¹æ˜¯æœºå™¨æ¶æ„çš„æ–°çƒ­ç‚¹ï¼Œå®ƒæ˜¯ä¸€ç§å…è´¹ä¸”å¼€æ”¾çš„æŒ‡ä»¤é›†æ¶æ„ (ISA)ã€‚  å®ƒä¹Ÿä»ä¸€å¼€å§‹å°±è¢«è®¾è®¡æˆæ¨¡å—åŒ–çš„ï¼Œè¿™æ„å‘³ç€èŠ¯ç‰‡è®¾è®¡äººå‘˜å¯ä»¥åˆ›å»ºå„ç§å„æ ·çš„ä¸“ç”¨èŠ¯ç‰‡ï¼Œè™½ç„¶ç›®å‰å¼€å‘æ¿çš„èŒƒå›´å¾ˆå°ã€‚æœ‰ä¸€ä¸ªæ´»è·ƒçš„ Rust RISC-V ç¤¾åŒºï¼ŒSiFive æˆ– www.riscv.org éƒ½æ˜¯ä¸é”™çš„èµ·ç‚¹ï¼ŒRust æ–¹é¢ï¼Œå¯ä»¥æŸ¥çœ‹ riscv crateã€‚</li>\n<li>Xtensaï¼šæœ€å—æ¬¢è¿çš„ä¸»æ¿ç»„æ˜¯æ¥è‡ª Espressif çš„ ESP32 ç³»åˆ—èŠ¯ç‰‡ã€‚å®ƒä»¬æ˜¯å°å‹ã€å»‰ä»·ã€æ”¯æŒ WiFi çš„ç”µè·¯æ¿ã€‚  éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¹¶éæ‰€æœ‰ ESP32 å¼€å‘æ¿éƒ½ä½¿ç”¨ Xtensa èŠ¯ç‰‡ï¼Œæ–°çš„ ESP32-C3 æ˜¯åŸºäº RISC-V çš„ã€‚åœ¨ Xtensa èŠ¯ç‰‡ä¸Šä½¿ç”¨ Rust çš„æœ€å¤§éšœç¢å¯èƒ½æ˜¯ llvm ä¸æ”¯æŒå®ƒï¼Œå› æ­¤éœ€è¦æ„å»º Rust çš„ forkï¼š<a href=\"https://github.com/esp-rs/rust\" rel=\"noopener noreferrer\">esp-rs/rust</a>ã€‚</li>\n</ul>\n<p><strong>æˆ‘ä»¬éœ€è¦ä»€ä¹ˆèŠ¯ç‰‡ï¼Ÿ</strong></p>\n<p>å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ ARM Cortex-Mã€‚  è¿™ç¼©å°äº†æœç´¢èŒƒå›´ï¼Œä½†ä»æœ‰å¾ˆå¤šé€‰æ‹©ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹ cortex-m <a href=\"https://crates.io/crates/cortex-m/reverse_dependencies\" rel=\"noopener noreferrer\">crate</a> çš„ä¾èµ–é¡¹ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æœ‰ä¸¤ç»„èŠ¯ç‰‡æ¯”å…¶ä»–ä»»ä½•ä¸€ç»„éƒ½ä½¿ç”¨å¾—æ›´å¤šï¼› <a href=\"https://www.st.com/content/st_com/en/products/microcontrollers-microprocessors/stm32-32-bit-arm-cortex-mcus.html\" rel=\"noopener noreferrer\">STM32</a> ç³»åˆ—èŠ¯ç‰‡å’Œ <a href=\"https://www.nordicsemi.com/Products/Bluetooth-Low-Energy\" rel=\"noopener noreferrer\">nRF5</a> ç³»åˆ—ï¼Œè¿™æ˜¯æˆ‘ä»¬è¦é‡ç‚¹æœç´¢çš„åœ°æ–¹ã€‚</p>\n<ul>\n<li>STM32ï¼šSTM32 ç³»åˆ—èŠ¯ç‰‡å¯èƒ½æ˜¯åº”ç”¨æœ€å¹¿æ³›çš„åµŒå…¥å¼ Rust ARM Cortex-M èŠ¯ç‰‡ã€‚ä¸¤ç§æœ€å—æ¬¢è¿çš„ STM32 æ¿æ˜¯ Blue Pill å’Œ Black Pillã€‚ä¸»è¦çš„ç¼ºç‚¹æ˜¯æ²¡æœ‰æ¿è½½è°ƒè¯•å™¨ã€‚å¦‚æœæƒ³è¦å¸¦æœ‰è°ƒè¯•å™¨çš„åŸºäº STM32 çš„ç”µè·¯æ¿ï¼Œé‚£ä¹ˆè·å¾— STMicroelectronics <a href=\"https://www.st.com/en/evaluation-tools/stm32-discovery-kits.html#overview\" rel=\"noopener noreferrer\">å®˜æ–¹å¥—ä»¶</a>æ˜¯ä¸€ä¸ªä¸é”™çš„é€‰æ‹©ï¼ˆSTM32F3 æˆ– STM32F4 æ˜¯ä¸é”™çš„é€‰æ‹©ï¼‰ã€‚Rust Embedded Discovery ä¹¦çš„åŸå§‹ç‰ˆæœ¬æ˜¯é’ˆå¯¹ STM32F3 æ¿ç¼–å†™çš„ï¼Œå› æ­¤æœ‰éå¸¸é«˜è´¨é‡çš„åˆå­¦è€…æ–‡æ¡£ï¼Œå¯ä»¥ä»é‚£é‡Œå¼€å§‹ã€‚</li>\n<li>nRF5ï¼šç”¨äºåµŒå…¥å¼ Rust çš„ç¬¬äºŒä¸ªæœ€å¹¿æ³›ä½¿ç”¨çš„ ARM Cortex-M èŠ¯ç‰‡ç³»åˆ—æ˜¯ Nordic Semiconductor çš„ <a href=\"https://www.nordicsemi.com/Products/Bluetooth-Low-Energy\" rel=\"noopener noreferrer\">nRF5 ç³»åˆ—</a>ã€‚å®˜æ–¹å¼€å‘<a href=\"https://www.nordicsemi.com/Products/Bluetooth-Low-Energy/Development-hardware\" rel=\"noopener noreferrer\">å¥—ä»¶</a> (DK) æ˜¯å¾ˆæ£’çš„å…¥é—¨æ¿ã€‚ Ferrous Systems çš„ Knurling-rs ä¼šè®®ä½¿ç”¨ nRF52840 <a href=\"https://www.nordicsemi.com/Products/Development-hardware/nRF52840-DK\" rel=\"noopener noreferrer\">å¼€å‘å¥—ä»¶</a>ã€‚Knurling è¯¾ç¨‹è´¨é‡éå¸¸é«˜ï¼Œæ‰‹æŠŠæ‰‹æŒ‡å¯¼ï¼Œé€šè¿‡æœ‰è¶£å¥½ç©çš„é¡¹ç›®æ•™æˆåµŒå…¥ Rustï¼Œæ˜¯ä½¿ç”¨ Rust è¿›è¡ŒåµŒå…¥å¼å¼€å‘çš„æœ€ä½³åˆ‡å…¥ç‚¹ã€‚å¦ä¸€ä¸ªå¾ˆæ£’çš„åŸºäº nRF çš„å¼€å‘æ¿æ˜¯ <a href=\"https://www.microbit.org/\" rel=\"noopener noreferrer\">BBC micro:bit</a>ã€‚å®ƒé…å¤‡äº†æ¿è½½è°ƒè¯•å™¨å’Œä¸€ç³»åˆ—æœ‰è¶£çš„æ¿è½½å¤–å›´è®¾å¤‡ï¼Œå¦‚æ¿ä¸Šçš„ LED æ˜¾ç¤ºå±ã€æŒ‰é’®å’Œä¼ æ„Ÿå™¨ã€‚BBC micro:bit è¢«è®¾è®¡ä¸ºä¸€ä¸ªæ•™è‚²å¹³å°ï¼Œå› æ­¤ç¡¬ä»¶åœ¨ä»–ä»¬çš„<a href=\"https://tech.microbit.org/\" rel=\"noopener noreferrer\">å¼€å‘è€…ç¤¾åŒº</a>ä¸­ä»¥éå¸¸é€‚åˆåˆå­¦è€…çš„æ–¹å¼è¿›è¡Œè®°å½•ï¼Œå¹¶ä¸”äº’è”ç½‘ä¸Šæœ‰å¤§é‡é¡¹ç›®åˆ›æ„ã€‚</li>\n<li>RP2040ï¼š<a href=\"https://www.raspberrypi.org/documentation/rp2040/getting-started/\" rel=\"noopener noreferrer\">RP2040</a> äº 2020 å¹´åº•å‘å¸ƒï¼Œæ˜¯ Raspberry Pi åŸºé‡‘ä¼šé¦–æ¬¡å°è¯•è®¾è®¡è‡ªå·±çš„èŠ¯ç‰‡ã€‚ç”±äºå¦‚æ­¤æ–°ï¼ŒRust å¯¹å®ƒçš„æ”¯æŒä»åœ¨å¼€å‘ä¸­ã€‚ä¸ BBC micro:bit ä¸€æ ·ï¼ŒRP2040 æ—¨åœ¨æˆä¸ºä¸€ä¸ªæ•™è‚²å¹³å°ï¼Œå› æ­¤ç¡¬ä»¶æ–‡æ¡£æ˜¯ä¸€æµçš„ï¼Œå¹¶ä¸”æœ‰å¤§é‡åˆå­¦è€…å‹å¥½çš„ä»£ç ç¤ºä¾‹å’Œå…¶ä»–ç¼–ç¨‹è¯­è¨€çš„åº“ï¼ˆæ²¡æœ‰å¤šå°‘é€‚åˆåˆå­¦è€…çš„åµŒå…¥å¼ Rust æ–‡æ¡£ï¼‰ã€‚è¿™æ˜¯ä¸€ä¸ªéå¸¸ä»¤äººå…´å¥‹çš„å¹³å°ï¼Œå¹¶ä¸”åœ¨ Embedded Rust ç¤¾åŒºä¸­å›´ç»•å®ƒè¿›è¡Œäº†å¤§é‡æ´»åŠ¨ï¼Œæ‰€ä»¥ä¸€å®šè¦å¯†åˆ‡å…³æ³¨ï¼Œä½†å®ƒå¯èƒ½ä¸é€‚åˆä½œä¸ºå…¥é—¨ç¬¬ä¸€å—æ¿ã€‚</li>\n</ul>\n<p><strong>æ¿è½½è°ƒè¯•å™¨ï¼Ÿ</strong></p>\n<p>åœ¨ä¸»æœºä¸Šè¿è¡Œç¨‹åºæ—¶ï¼Œå¯ä»¥åœ¨ shell ä¸­è¿è¡Œå®ƒå¹¶æŸ¥çœ‹æ‰“å°è¾“å‡ºã€‚è¿™åœ¨åµŒå…¥å¼ç›®æ ‡ä¸Šæ›´åŠ å›°éš¾ï¼Œè°ƒè¯•å™¨å¡«è¡¥äº†è¿™ä¸€ç©ºç™½ã€‚é™¤äº†å…è®¸å•æ­¥è°ƒè¯•ã€æ–­ç‚¹è°ƒè¯•å¤–ï¼Œå®ƒè¿˜å…è®¸å°†ç¨‹åºåŠ è½½åˆ°è®¾å¤‡ä¸Šå¹¶è½»æ¾æŸ¥çœ‹è¾“å‡ºã€‚ä¸è¿‡æœ‰ä¸€ä¸ªé—®é¢˜ï¼Œå®ƒé€šå¸¸æ˜¯è¿æ¥åˆ°ä¸»æœºç„¶åè¿æ¥åˆ°ç›®æ ‡è®¾å¤‡çš„å•ç‹¬è®¾å¤‡ã€‚ç¬¬ä¸€æ¬¡å¼€å§‹æ—¶ï¼Œè¿™æ˜¯ä¸€ç¬”ä¸å¯å¿½è§†çš„è´¹ç”¨ï¼Œä¹Ÿæ˜¯å¿…é¡»æ­£ç¡®è®¾ç½®çš„å¦ä¸€ä»¶äº‹ã€‚å¹¸è¿çš„æ˜¯ï¼Œæœ‰äº›è®¾å¤‡å¸¦æœ‰å†…ç½®è°ƒè¯•å™¨ï¼Œå°†å®ƒä»¬ç›´æ¥æ’å…¥ä¸»æœºå¹¶åœ¨ç¬é—´æ¢æµ‹è¿è¡Œçš„ä»£ç ï¼ˆé€šå¸¸éœ€è¦åœ¨ä¸»æœºä¸Šè¿›è¡Œä¸€äº›è®¾ç½®æ‰èƒ½ä½¿è°ƒè¯•å™¨æ­£å¸¸å·¥ä½œï¼Œferrous æœ‰ä¸€ä¸ªå¾ˆå¥½çš„è®¾ç½®<a href=\"https://session20q4.ferrous-systems.com/sessions/installation.html\" rel=\"noopener noreferrer\">æŒ‡å—</a>ï¼‰ã€‚</p>\n<p><strong>ç»“è®º</strong></p>\n<p>ä»¥ä¸‹è¿™äº›æ¿éƒ½æœ‰å¾ˆæ£’çš„ HAL å’Œ BSP crateã€æ´»è·ƒå‹å¥½çš„ç¤¾åŒºå’Œæ¿è½½è°ƒè¯•å™¨ã€‚</p>\n<ul>\n<li><a href=\"https://www.microbit.org/\" rel=\"noopener noreferrer\">BBC micro:bit</a>ï¼ˆçº¦ 13 è‹±é•‘ï¼‰ï¼šå®ƒæ˜¯æ–°ç‰ˆ Rust Embedded Discovery ä¹¦ä¸­ä½¿ç”¨çš„æ¿ã€‚</li>\n<li><a href=\"https://www.nordicsemi.com/Products/Development-hardware/nRF52840-DK\" rel=\"noopener noreferrer\">nRF52840 å¼€å‘å¥—ä»¶</a>ï¼ˆçº¦ 35 è‹±é•‘ï¼‰ï¼›  å®ƒæ˜¯ Ferrous Systems åœ¨ Kunrling ä¼šè®®å’ŒåŸ¹è®­ä¸­ä½¿ç”¨çš„æ¿ã€‚</li>\n<li><a href=\"https://www.st.com/en/evaluation-tools/stm32f3discovery.html\" rel=\"noopener noreferrer\">STM32F3 æ¢ç´¢å¥—ä»¶</a>ï¼ˆçº¦ 14 è‹±é•‘ï¼‰ï¼›  å®ƒæ˜¯ Rust Embedded Discovery ä¹¦çš„ç¬¬ä¸€ç‰ˆä¸­ä½¿ç”¨çš„æ¿ã€‚</li>\n</ul>\n<p>å¯†åˆ‡å…³æ³¨ï¼š</p>\n<ul>\n<li><a href=\"https://www.raspberrypi.org/products/raspberry-pi-pico/\" rel=\"noopener noreferrer\">Raspberry Pi Pico</a>ï¼ˆçº¦ 6 è‹±é•‘ï¼Œå¸¦é¢„ç„Šå¼•è„šï¼‰ï¼› ARM Cortex-M ä½†æ²¡æœ‰å†…ç½®è°ƒè¯•å™¨ï¼ŒHAL ä»åœ¨å¼€å‘ä¸­ã€‚ä¸è¿‡ç›®å‰æœ‰å¾ˆå¤šæ´»åŠ¨ï¼Œè¿›å±•å¾ˆå¿«ã€‚</li>\n<li><a href=\"https://www.sifive.com/boards/hifive1-rev-b\" rel=\"noopener noreferrer\">HiFive1 Rev B</a>ï¼ˆçº¦ 50 è‹±é•‘ï¼‰ï¼› RISC-V æ˜¯æ–°çš„çƒ­ç‚¹ã€‚ Rust ä¸­ä¼¼ä¹æœ‰å¾ˆå¤šå›´ç»•å®ƒçš„æ´»åŠ¨ï¼Œä½†å®ƒç›®å‰è¿˜æ²¡æœ‰ ARM Cortex-M çš„æ”¯æŒã€‚  å…¶ä»–éœ€è¦å…³æ³¨çš„å¼€å‘æ¿æ˜¯ <a href=\"https://longan.sipeed.com/en/\" rel=\"noopener noreferrer\">Logan Nano</a> å’Œ <a href=\"https://hackaday.com/2021/02/08/hands-on-the-risc-v-esp32-c3-will-be-your-new-esp8266/\" rel=\"noopener noreferrer\">ESP32-C3</a>ã€‚</li>\n</ul>\n<p>éƒ¨åˆ†å†…å®¹ç•¥æœ‰è½»å¾®è°ƒæ•´ï¼Œæ›´å¤šå¯é˜…è¯»åŸæ–‡ï¼š<a href=\"https://robyoung.digital/blog/embedded-rust-selecting-a-board/\" rel=\"noopener noreferrer\">Rob Young | digital</a></p>\n<h3>Tangramï¼šè®­ç»ƒã€éƒ¨ç½²å’Œç›‘æ§æœºå™¨å­¦ä¹ æ¨¡å‹</h3>\n<p>ä¸€ä¸ªæœºå™¨å­¦ä¹ å¥—ä»¶ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š</p>\n<pre><code># è®­ç»ƒ\n$ tangram train --file heart_disease.csv --target diagnosis --output heart_disease.tangram\n</code></pre>\n<p>æ¨ç†æ”¯æŒå¤šç§è¯­è¨€ï¼š<a href=\"https://hex.pm/packages/tangram\" rel=\"noopener noreferrer\">Elixir</a>, <a href=\"https://pkg.go.dev/github.com/tangramdotdev/tangram-go\" rel=\"noopener noreferrer\">Go</a>, <a href=\"https://www.npmjs.com/package/@tangramdotdev/tangram\" rel=\"noopener noreferrer\">JavaScript</a>, <a href=\"https://pypi.org/project/tangram\" rel=\"noopener noreferrer\">Python</a>, <a href=\"https://rubygems.org/gems/tangram\" rel=\"noopener noreferrer\">Ruby</a> å’Œ <a href=\"https://lib.rs/tangram\" rel=\"noopener noreferrer\">Rust</a>ï¼Œä»¥ Rust ä¸ºä¾‹ï¼š</p>\n<pre><code>let model: tangram::Model = tangram::Model::from_path(\"heart_disease.tangram\", None).unwrap();\n\nlet input = tangram::predict_input! {\n  \"age\": 63.0,\n  \"gender\": \"male\",\n  // ...\n};\n\nlet output = model.predict_one(input, None);\n# { className: 'Negative', probability: 0.9381780624389648 }\n</code></pre>\n<p>å¾ˆå¥½å¥‡è®­ç»ƒçš„æ—¶å€™å±…ç„¶æ²¡æœ‰è¦æŒ‡å®šæ¨¡å‹ï¼Œå‘ç°å…¶å°†æ¨¡å‹å…±åˆ†ä¸ºä¸‰ç±»ï¼šå›å½’ã€äºŒåˆ†ç±»å’Œå¤šåˆ†ç±»ï¼Œè®­ç»ƒæ—¶ä¼šæ ¹æ®æ•°æ®è‡ªåŠ¨é€‰æ‹©åˆé€‚ï¼ˆä½¿ç”¨è¯„ä¼°æ–¹æ³•ï¼‰çš„æ¨¡å‹ï¼Œæ¯ç§æ¨¡å‹åˆæœ‰ä¸¤ç§ä¸åŒçš„è®­ç»ƒæ–¹æ³•ï¼šçº¿æ€§æ–¹æ³•å’Œæ ‘æ–¹æ³•ã€‚</p>\n<p>è‡ªå¸¦çš„ç›‘æ§åŠŸèƒ½çœ‹èµ·æ¥è¿˜ä¸é”™ï¼Œæ¯”å¦‚ä¸‹é¢è¿™å¼ å¯ä»¥å±•ç¤ºç‰¹å¾å¯¹è¾“å‡ºçš„è´¡çŒ®ï¼š</p>\n<p><img src=\"https://github.com/tangramdotdev/tangram/raw/main/readme/predictions.png\" alt=\"\"></p>\n<p>é¡¹ç›®ç†è®ºä¸Šå¯ä»¥ç”¨åœ¨ç®€å•æœºå™¨å­¦ä¹ åœºæ™¯ä¸‹ï¼Œå°¤å…¶æ˜¯é‚£äº›è¿˜æ²¡æœ‰æ”¯æŒæœºå™¨å­¦ä¹ çš„è¯­è¨€ï¼Œä¸è¿‡æ¨ç†å¹¶æ²¡æœ‰ Benchmarkï¼Œç”Ÿäº§ä¸­ä½¿ç”¨éœ€è¦åšå¥½æ€§èƒ½æµ‹è¯•ã€‚</p>\n<p>GitHubï¼š<a href=\"https://github.com/tangramdotdev/tangram\" rel=\"noopener noreferrer\">tangramdotdev/tangram: Tangram makes it easy for programmers to train, deploy, and monitor machine learning models.</a></p>\n<p>æ–‡æ¡£ï¼š<a href=\"https://www.tangram.dev/docs/\" rel=\"noopener noreferrer\">Tangram</a></p>\n<h3>lateralï¼šä¸€ä¸ªåœ¨ x86_64 ä¸Šå¯åŠ¨çš„æ¨¡å—åŒ–å†…æ ¸</h3>\n<p>åœ¨æœ¬åœ°æ‰§è¡Œï¼š</p>\n<pre><code>$ make run-release ARCH=x86_64\n</code></pre>\n<p>å¯ä»¥æ ¹æ®è‡ªå·±çš„æƒ…å†µè°ƒæ•´ Makefile ç¬¬ä¸€è¡Œ Bash çš„é…ç½®ã€‚æ‰§è¡Œåå¦‚æœæœ‰å®‰è£… QEMU çš„è¯ä¼šè‡ªåŠ¨åŠ è½½ï¼š</p>\n<p><img src=\"http://qnimg.lovevivian.cn/tmp-os-1.jpg\" alt=\"\"></p>\n<p>æ¯ä¸ªç»„ä»¶éƒ½å»ºç«‹åœ¨çª—å£ç®¡ç†å™¨ä¹‹ä¸Šï¼Œè€Œä¸æ˜¯åƒå¤§å¤šæ•°æ“ä½œç³»ç»Ÿé‚£æ ·å»ºç«‹åœ¨ç»ˆç«¯ä¹‹ä¸Šã€‚</p>\n<p>GitHubï¼š<a href=\"https://github.com/carterisonline/lateral\" rel=\"noopener noreferrer\">carterisonline/lateral: A clean, custom-built modular kernel ready to boot on x86_64.</a></p>\n<h3>tvï¼šæ˜¾ç¤ºè¡¨æ ¼çš„ cli å·¥å…·</h3>\n<p>å°±æ˜¯æŠŠ json æˆ– csv æ˜¾ç¤ºæˆè¡¨æ ¼ï¼Œçœ‹èµ·æ¥å¾ˆä¸é”™ï¼š</p>\n<pre><code>$ cat test.json\n[\n  {\n    \"name\": \"test\",\n    \"age\": 10,\n    \"lang\": \"ja\"\n  },\n  {\n    \"name\": \"uzimaru\",\n    \"age\": 23,\n    \"lang\": \"ja\"\n  },\n  {\n    \"name\": \"hogehoge\",\n    \"age\": 21,\n    \"lang\": \"en\"\n  },\n  {\n    \"name\": \"hugehuge\",\n    \"age\": 32,\n    \"lang\": \"en\"\n  }\n]\n\n$ tv test.json\n|age|lang|    name|\n|---|----|--------|\n| 10|  ja|    test|\n| 23|  ja| uzimaru|\n| 21|  en|hogehoge|\n| 32|  en|hugehuge|\n\n$ cat test.csv\nname,age,lang\ntest,10,ja\nuzimaru,23,ja\nhogehoge,21,en\nhugehuge,32,en\n\n$ tv test.csv\n|age|lang|    name|\n|---|----|--------|\n| 10|  ja|    test|\n| 23|  ja| uzimaru|\n| 21|  en|hogehoge|\n| 32|  en|hugehuge|\n</code></pre>\n<p>Mac ç”¨æˆ· brew å®‰è£…ï¼š</p>\n<pre><code>$ brew install uzimaru0000/tap/tv\n</code></pre>\n<p>GitHubï¼š<a href=\"https://github.com/uzimaru0000/tv\" rel=\"noopener noreferrer\">uzimaru0000/tv: CLI tool for displaying table</a></p>\n<h3>minesweeperï¼šä½¿ç”¨ Rustï¼ŒWebAssembly å’Œ Canvas çš„æ‰«é›·æ¸¸æˆ</h3>\n<p>ç•Œé¢é•¿è¿™æ ·ï¼š</p>\n<p><img src=\"https://github.com/KarthikNedunchezhiyan/minesweeper/raw/main/www/assets/stage_bomb_triggered.png\" alt=\"\"></p>\n<p>æ˜¯å¾ˆå¥½çš„å­¦ä¹ èµ„æ–™ã€‚åœ¨è¿™é‡Œç©å„¿ï¼š<a href=\"https://karthiknedunchezhiyan.me/minesweeper/\" rel=\"noopener noreferrer\">Minesweeper</a></p>\n<p>GitHubï¼š<a href=\"https://github.com/karthikNedunchezhiyan/minesweeper\" rel=\"noopener noreferrer\">KarthikNedunchezhiyan/minesweeper: Minesweeper game developed with Rust, WebAssembly (Wasm), and Canvas</a></p>\n<h3>copy-translatorï¼šåˆ’è¯ç¿»è¯‘</h3>\n<p>å¤åˆ¶åç¿»è¯‘ï¼Œä½¿ç”¨ DeepL çš„ APIï¼Œä¸è¿‡ç›®å‰åªæœ‰ Local ç‰ˆæœ¬å¥½ç”¨ï¼š</p>\n<p><img src=\"http://qnimg.lovevivian.cn/tmp-rust-1.jpg\" alt=\"\"></p>\n<p>å½“ç„¶ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨ Eudicï¼ˆæ¬§è·¯è¯å…¸ï¼‰ã€‚</p>\n<p>GitHubï¼š<a href=\"https://github.com/zu1k/copy-translator\" rel=\"noopener noreferrer\">zu1k/copy-translator: Copy Translator, using DeepL api</a></p>\n<h3>veccentricï¼šå°å·§çš„ 2-D å‘é‡ Library</h3>\n<p>é¡¹ç›®å— <a href=\"https://p5js.org/reference/#/p5.Vector\" rel=\"noopener noreferrer\">p5.Vector</a> å¯å‘ï¼Œä½¿ç”¨æ–¹æ³•å¦‚ä¸‹ï¼š</p>\n<pre><code>use veccentric::Vecc;\n\nlet a = Vecc::new(3_i32, 4);\nlet b = a * 5;\nlet c = Vecc::new(-10, -8);\nlet d = b - c;\nlet e = -d;\n</code></pre>\n<p>GitHubï¼š<a href=\"https://github.com/micouy/veccentric\" rel=\"noopener noreferrer\">micouy/veccentric: Tiny 2D vector library. Inspired by p5.js's p5.Vector.</a></p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ é•¿ç´</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc è®ºå›ï¼šæ”¯æŒ rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRust è¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-29 12:17:36","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"axumå¦‚ä½•ä½¿ç”¨é™æ€æ–‡ä»¶ç›®å½•","link":"https://rustcc.cn/article?id=f3fa9c8e-004b-4d95-8d5f-bdf6609c2e8e","description":"<p>å†é‡æ„ä¸€ä¸ªç®€å•çš„å•é¡µé¢å°ç¨‹åºçš„æ—¶å€™æ‰“ç®—ç”¨<code>axum</code>ä»£æ›¿<code>warp</code>çš„æ—¶å€™é‡åˆ°äº†ä¸ªé—®é¢˜ã€‚</p>\n<pre><code>    let post = warp::post()\n        .and(warp::body::bytes())\n        .map(move |content: Bytes| {\n            Response::builder().body(server::handle_post_request(content))\n        });\n\n    let routers = warp::get().and(warp::fs::dir(\"./wwwroot\")).or(post);\n\n    warp::serve(routers).run(([127, 0, 0, 1], 3030)).await;\n</code></pre>\n<p>æœ‰å¦‚ä¸Šçš„ç®€å•ä»£ç ï¼Œä½¿ç”¨<code>wwwroot</code>æ–‡ä»¶å¤¹ç›®å½•æ¥ç”Ÿæˆé¡µé¢ï¼Œæ–‡ä»¶å¤¹é‡ŒåŒ…å«æœ‰<code>index.html</code>,JSå’ŒCSSæ–‡ä»¶ï¼Œæ€ä¹ˆä½¿ç”¨<code>axum</code>æ”¹å†™å‘¢ï¼Ÿçœ‹äº†ä¸‹docï¼Œåªçœ‹åˆ°</p>\n<pre><code>let app = Router::new()\n    // this route cannot fail\n    .route(\"/foo\", get(|| async {}))\n    // this route can fail with io::Error\n    .route(\n        \"/\",\n        service::get(service_fn(|_req: Request&lt;Body&gt;| async {\n            let contents = tokio::fs::read_to_string(\"some_file\").await?;\n            Ok::&lt;_, io::Error&gt;(Response::new(Body::from(contents)))\n        }))\n        .handle_error(handle_io_error),\n    );\n\nfn handle_io_error(error: io::Error) -&gt; Result&lt;impl IntoResponse, Infallible&gt; {\n    // ...\n}\n</code></pre>\n<p>è¿™ç§å†™æ³•ã€‚çœ‹ç€å¤´å¤§ä¸è¯´ï¼Œé‚£ä¸ª<code>some_file</code>åªæ˜¯ç®€å•è¯»å–æ–‡ä»¶ï¼Œå®Œæˆä¸äº†æˆ‘çš„è¦æ±‚ã€‚</p>\n<p>æœ‰å¤§ç¥è¯´è¯´axumå®Œæˆäº†è¿™ä¸ªéƒ¨åˆ†äº†å—ï¼Ÿè¿™æ¡†æ¶çš„ä»£ç çœ‹ç€æ„Ÿè§‰æœ‰ç‚¹è¿‡äºå¤æ‚äº†ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-29 07:28:13","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"è‡ªå·±ç®¡ç†å†…å­˜çš„æµ‹è¯•æ–¹æ³•","link":"https://rustcc.cn/article?id=d4e8f317-f43f-44ea-a041-f39dd3ce1578","description":"<p>å¾ˆæ¼‚äº®çš„ä¸€æ®µcaseï¼Œæ¥è‡ªstd</p>\n<p>library/alloc/tests/linked_list.rs</p>\n<pre><code>#[test]\nfn test_drop() {\n    static mut DROPS: i32 = 0;\n    struct Elem;\n    impl Drop for Elem {\n        fn drop(&amp;mut self) {\n            unsafe {\n                DROPS += 1;\n            }\n        }\n    }\n\n    let mut ring = LinkedList::new();\n    ring.push_back(Elem);\n    ring.push_front(Elem);\n    ring.push_back(Elem);\n    ring.push_front(Elem);\n    drop(ring);\n\n    assert_eq!(unsafe { DROPS }, 4);\n}\n\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-29 07:14:22","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"é—®ä¸€ä¸ªDisplay traitçš„é—®é¢˜","link":"https://rustcc.cn/article?id=6b59dfb1-87d8-4b79-8820-e9d5397f178a","description":"<p>è¯·é—®&amp;str, &amp;&amp;str, &amp;&amp;&amp;str å¹¶æ²¡æœ‰å®ç°Display çš„trait, ä¸ºä»€ä¹ˆè¿™ä¸ªå‡½æ•°è°ƒç”¨æ²¡é—®é¢˜?</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-29 06:12:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-28 å¼€æºæ“ä½œç³»ç»Ÿå¤ä»¤è¥æœ€ç»ˆæŠ¥å‘Šä¼šå®‰æ’","link":"https://rustcc.cn/article?id=ef3dd4e8-a8e8-4fec-bc7e-75703e1117ff","description":"<h3>å¼€æºæ“ä½œç³»ç»Ÿå¤ä»¤è¥æœ€ç»ˆæŠ¥å‘Šä¼šå®‰æ’</h3>\n<p>ä¼šè®®ä¸»é¢˜ï¼šå¼€æºæ“ä½œç³»ç»Ÿå¤ä»¤è¥æœ€ç»ˆæŠ¥å‘Šä¼š\nä¼šè®®æ—¶é—´ï¼š2021/08/29 09:00-11:30 (GMT+08:00) ä¸­å›½æ ‡å‡†æ—¶é—´ - åŒ—äº¬\nç‚¹å‡»é“¾æ¥å…¥ä¼šï¼Œæˆ–æ·»åŠ è‡³ä¼šè®®åˆ—è¡¨ï¼š https://meeting.tencent.com/dm/Mp7T1h5zeQOk?rs=25\nä¼šè®® IDï¼š635 194 989</p>\n<p>ä¸‹é¢æ˜¯9ä½å…¨ç¨‹å‚ä¸å¤ä»¤è¥æ´»åŠ¨åŒå­¦çš„æŠ¥å‘Šé¡ºåºã€‚æ¯äººæŠ¥å‘Šæ—¶é—´æœ€é•¿15åˆ†é’Ÿã€‚</p>\n<ol>\n<li>æ¨äº‘æ« ç‹æ¶› Rustsbiçš„å“ªå’å¼€å‘ç‰ˆç§»æ¤</li>\n<li>å…°é™ˆæ˜• zCoreå›¾å½¢æ”¯æŒ</li>\n<li>éƒ½ç§‰ç”² å®¹å™¨æŠ€æœ¯å­¦ä¹ </li>\n<li>è–›æ½‡å· RVM çš„ RISC-V æ”¯æŒ</li>\n<li>é™ˆä¹ å…±äº«è°ƒåº¦å™¨</li>\n<li>å´éå‡¡ åŸºäºç”¨æˆ·æ€ä¸­æ–­çš„å¼‚æ­¥ç³»ç»Ÿè°ƒç”¨è®¾è®¡ä¸å®ç°</li>\n<li>å½­æ·³æ¯… é™ˆå¿—æ‰¬ åŸºäºrCore-Tutorialçš„æ€§èƒ½åˆ†æè½¯ä»¶å®ç°</li>\n</ol>\n<h3>crates.liveï¼šå¯è§†åŒ– Rust crates ä¾èµ–é¡¹</h3>\n<p>crates.live æ˜¯æ¥è‡ª crates.io çš„ Rust crates çš„ä¾èµ–å¯è§†åŒ–å·¥å…·ã€‚ å®ƒæ˜¾ç¤ºäº† Rust cratesï¼ˆåŒ…ï¼‰çš„ä¾èµ–æ ‘ã€‚åŠŸèƒ½åŒ…æ‹¬ï¼š</p>\n<ul>\n<li>ä¾èµ–è§£æï¼Œ crates.live å¼•æ“é€šè¿‡åŒ¹é…ä¾èµ–ç‰ˆæœ¬æ¥å®Œæˆå®Œæ•´çš„ä¾èµ–è§£æã€‚</li>\n<li>äº¤äº’å¼å›¾è¡¨ï¼Œå¸¦æœ‰æ ‡è®°çš„æ¿æ¡ç®±çš„å¯ç¼©æ”¾äº¤äº’å¼å›¾è¡¨ã€‚</li>\n<li>å›¾åƒå¯¼å‡ºï¼Œ å°†å›¾å½¢å¯¼å‡ºä¸º PNGã€‚</li>\n<li>å¼€æ”¾ APIï¼šï¼ˆå³å°†æ¨å‡ºï¼‰GraphQL APIã€‚</li>\n</ul>\n<p>crates.live ä½¿ç”¨äº†ä¸€å †æŠ€æœ¯æ¡†æ¶ï¼ŒæŠ€æœ¯æ ˆåŒ…æ‹¬ï¼š</p>\n<ul>\n<li>Rustï¼Œ crates.live åç«¯å’Œçˆ¬è™«æ˜¯ç”¨ Rust å’Œå¼€æº Rust åº“å¼€å‘çš„ã€‚</li>\n<li>GraphQlï¼Œ WASM é©±åŠ¨çš„ GraphQL æœåŠ¡å™¨ã€‚</li>\n<li>React/Bulmaï¼Œ å‰ç«¯åº“ã€‚</li>\n<li>Terraformï¼Œ å¸®åŠ©å¯åŠ¨å’Œç»´æŠ¤æˆ‘ä»¬çš„åŸºç¡€è®¾æ–½ã€‚</li>\n<li>Cloudflareï¼Œ Cloudflare å·¥ä½œäººå‘˜è¿è¡Œ WASM åç«¯ã€‚</li>\n</ul>\n<p>å¦‚æœåœ¨ä½¿ç”¨æ­¤åº”ç”¨ç¨‹åºæ—¶æœ‰ä»»ä½•ç–‘é—®ã€å»ºè®®æˆ–é—®é¢˜ï¼› å¯ä»¥é€šè¿‡ contact@crates.live è”ç³»ã€‚ crates.live ç”± Abid Omar å¼€å‘ï¼Œå¯é€šè¿‡ contact@omarabid.com è”ç³»ã€‚</p>\n<p><a href=\"https://crates.live/\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼šhttps://crates.live/</p>\n<h3>Obakeï¼Œç‰ˆæœ¬åŒ–æ•°æ®ç»“æ„</h3>\n<p>Obake æ˜¯ä¸€ä¸ªç”¨äºå£°æ˜å’Œç»´æŠ¤ç‰ˆæœ¬åŒ–æ•°æ®ç»“æ„çš„è¿‡ç¨‹å®ã€‚ â€œobakeâ€è¿™ä¸ªåå­—å–è‡ªæ—¥è¯­â€œãŠåŒ–ã‘ï¼ˆãŠã°ã‘ï¼‰â€ï¼Œè¿™æ˜¯æ—¥æœ¬æ°‘é—´ä¼ è¯´ä¸­ä¸€ç±»ä¼šå˜å½¢çš„è¶…è‡ªç„¶ç”Ÿç‰©ã€‚</p>\n<p>åœ¨å¼€å‘åº”ç”¨ç¨‹åºæ—¶ï¼Œé…ç½®æ ¼å¼å’Œå†…éƒ¨æ•°æ®ç»“æ„é€šå¸¸ä¼šåœ¨ç‰ˆæœ¬ä¹‹é—´æ¼”å˜ã€‚ ç„¶è€Œï¼Œä¿æŒè¿™äº›ç‰ˆæœ¬ä¹‹é—´çš„å‘åå…¼å®¹æ€§éœ€è¦å£°æ˜å’Œç»´æŠ¤é—ç•™æ ¼å¼çš„æ•°æ®ç»“æ„å’Œç”¨äºåœ¨å®ƒä»¬ä¹‹é—´è¿ç§»çš„ä»£ç ã€‚ Obake çš„ç›®æ ‡æ˜¯è®©è¿™ä¸ªè¿‡ç¨‹å˜å¾—è½»æ¾ã€‚</p>\n<pre><code>#[obake::versioned]                 // create a versioned data-structure\n#[obake(version(\"0.1.0\"))]          // declare some versions\n#[obake(version(\"0.2.0\"))]\n#[derive(PartialEq, Eq, Hash)]      // additional attributes are applied to all versions\nstruct Foo {\n    #[obake(cfg(\"0.1.0\"))]          // enable fields for specific versions with\n    foo: String,                    // semantic version constraints\n   \n    #[obake(cfg(\"&gt;=0.2, &lt;=0.3.0\"))] // any semantic version constraint can appear in\n    bar: u32,                       // a `cfg` attribute \n   \n    #[obake(cfg(\"0.1.0\"))]          // multiple `cfg` attributes are treated as a\n    #[obake(cfg(\"&gt;=0.3\"))]          // disjunction over version constraints\n    baz: char,\n}\n\n// describe migrations between versions using the `From` trait\n// and an automatically generated type-level macro for referring to\n// specific versions of `Foo`\nimpl From&lt;Foo![\"0.1.0\"]&gt; for Foo![\"0.2.0\"] {\n    fn from(foo: Foo![\"0.1.0\"]) -&gt; Self {\n        Self { bar: 0 }\n    }\n}\n\n// an enumeration of all versions of `Foo` is accessed using the\n// `obake::Versioned` trait:\nlet versioned_example: &lt;Foo as obake::Versioned&gt;::Versioned = unimplemented!();\n\n// this enumeration implements `Into&lt;Foo&gt;`, where `Foo` is the latest declared\n// version of `Foo` (in this case, `Foo![\"0.2.0\"]`)\nlet example: Foo = versioned_example.into();\n</code></pre>\n<p>Github<a href=\"https://github.com/doctorn/obake\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼šhttps://github.com/doctorn/obake</p>\n<h3>icedï¼Œè·¨å¹³å° GUI åº“</h3>\n<p>icedï¼ŒRust çš„è·¨å¹³å° GUI åº“ï¼Œä¸“æ³¨äºç®€å•æ€§å’Œç±»å‹å®‰å…¨ã€‚ çµæ„Ÿæ¥è‡ª<a href=\"https://elm-lang.org/\" rel=\"noopener noreferrer\">Elm</a>ã€‚</p>\n<p><img src=\"https://raw.githubusercontent.com/hecrj/iced/master/docs/graphs/ecosystem.png\" alt=\"eco\"></p>\n<p>Github<a href=\"https://github.com/hecrj/iced/\" rel=\"noopener noreferrer\">é“¾æ¥</a>ï¼šhttps://github.com/hecrj/iced/</p>\n<p>ç¤ºä¾‹ï¼šhttps://github.com/hecrj/iced/tree/master/examples</p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ <a href=\"https://rustcc.cn/blog_with_author?author_id=207704d2-4f5e-4219-a631-6ab4ab4d8929\" rel=\"noopener noreferrer\">æ´‹èŠ‹</a></p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-28 15:42:07","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rust æ—¥æŠ¥ã€‘2021-8-27 Rudra Rust çš„å†…å­˜å®‰å…¨å’Œæœªå®šä¹‰è¡Œä¸ºæ£€æµ‹å·¥å…·","link":"https://rustcc.cn/article?id=ce7eb559-fdda-45d7-a53e-293af787a813","description":"<h4>Rudra Rust çš„å†…å­˜å®‰å…¨å’Œæœªå®šä¹‰è¡Œä¸ºæ£€æµ‹å·¥å…·</h4>\n<p>Rudra æ˜¯ä¸€ä¸ªé™æ€åˆ†æå™¨ï¼Œç”¨äºæ£€æµ‹ Rust ç¨‹åºä¸­å¸¸è§çš„æœªå®šä¹‰è¡Œä¸ºã€‚å®ƒèƒ½å¤Ÿåˆ†æå•ä¸ª Rust åŒ…ä»¥åŠ crates.io ä¸Šçš„æ‰€æœ‰åŒ…ã€‚Rudra åŠå…¶ç›¸å…³è®ºæ–‡å°†åœ¨ Proceedings of the 28th ACM Symposium on Operating Systems Principles 2021 (SOSP '21) ä¸Šå‘è¡¨ã€‚</p>\n<ul>\n<li>https://github.com/sslab-gatech/Rudra#readme</li>\n</ul>\n<h4>nom 7.0 ç‰ˆæœ¬å‘å¸ƒ</h4>\n<p>nom æ˜¯ä¸€ä¸ªç”¨ Rust ç¼–å†™çš„è§£æå™¨ç»„åˆåº“ã€‚å®ƒçš„ç›®æ ‡æ˜¯æä¾›å·¥å…·æ¥æ„å»ºå®‰å…¨çš„è§£æå™¨ï¼Œè€Œä¸ä¼šå½±å“é€Ÿåº¦æˆ–å†…å­˜æ¶ˆè€—ã€‚ä¸ºæ­¤ï¼Œå®ƒå¹¿æ³›ä½¿ç”¨ Rust çš„å¼ºç±»å‹å’Œå†…å­˜å®‰å…¨æ¥ç”Ÿæˆå¿«é€Ÿä¸”æ­£ç¡®çš„è§£æå™¨ï¼Œå¹¶æä¾›å‡½æ•°ã€å®å’Œç‰¹å¾æ¥æŠ½è±¡å¤§éƒ¨åˆ†å®¹æ˜“å‡ºé”™çš„ç®¡é“ã€‚ç›®å‰7.0å·²ç»å‘å¸ƒ</p>\n<ul>\n<li>https://crates.io/crates/nom</li>\n</ul>\n<h4>egui 0.14 ç‰ˆæœ¬å‘å¸ƒ</h4>\n<p>egui æ˜¯ä¸€ä¸ªæ˜“äºä½¿ç”¨çš„çº¯ Rust å›¾å½¢ç”¨æˆ·ç•Œé¢ã€‚egui å¯ä»¥åœ¨ Web ä¸Šã€æœ¬æœºä¸Šä»¥åŠæ‚¨æœ€å–œæ¬¢çš„æ¸¸æˆå¼•æ“ä¸­è¿è¡Œã€‚egui æ—¨åœ¨æˆä¸ºæœ€å®¹æ˜“ä½¿ç”¨çš„ Rust GUI åº“ï¼Œä»¥åŠåœ¨ Rust ä¸­åˆ¶ä½œ Web åº”ç”¨ç¨‹åºçš„æœ€ç®€å•æ–¹æ³•ï¼Œå®ƒå¯ä»¥åœ¨ä»»ä½•å¯ä»¥ç»˜åˆ¶çº¹ç†ä¸‰è§’å½¢çš„åœ°æ–¹ä½¿ç”¨ï¼Œè¿™æ„å‘³ç€æ‚¨å¯ä»¥è½»æ¾åœ°å°†å…¶é›†æˆåˆ°æ‚¨é€‰æ‹©çš„æ¸¸æˆå¼•æ“ä¸­ã€‚</p>\n<ul>\n<li>æ¼”ç¤ºæ–‡æ¡£ï¼šhttps://emilk.github.io/egui/</li>\n<li>https://github.com/emilk/egui</li>\n</ul>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ åŒ—çº¬27åº¦ï¼Œä¾¯ç››é‘«</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-27 14:27:47","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å¼€æºé¡¹ç›®xiuç™»ä¸Šäº†GitHub rust trendingæ¦œ","link":"https://rustcc.cn/article?id=86c83d9a-8370-42cf-8993-ef15af6932c4","description":"<p><a href=\"https://github.com/harlanc/xiu\" rel=\"noopener noreferrer\">https://github.com/harlanc/xiu</a></p>\n<p><a href=\"https://github.com/trending/rust?since=daily\" rel=\"noopener noreferrer\">https://github.com/trending/rust?since=daily</a></p>\n<p>æ„Ÿè°¢å¤§å®¶çš„æ”¯æŒï¼ï¼</p>\n<p>PSï¼š</p>\n<p>å‰ä¸‰åæœ‰ä¸¤ä¸ªéƒ½åœ¨è®ºå›é‡Œå‘è¿‡ï¼Œè¿™ä¸ªè®ºå›æœ‰ç‚¹ç‹ ï¼Œå“ˆå“ˆ</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-27 10:43:48","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Salvo - ä¸€ä¸ªç®€å•çš„ Web åç«¯æ¡†æ¶","link":"https://rustcc.cn/article?id=e5dc5be9-b1ab-488f-8944-dd7cd97b0128","description":"<h2>ä¸ºä»€ä¹ˆè¦å†™è¿™ä¸ªæ¡†æ¶</h2>\n<p>å› ä¸ºæˆ‘ç¬¨ï¼Œæ— æ³•å­¦ä¼šä½¿ç”¨ actix-web ç­‰ç°å­˜çš„æ¡†æ¶ã€‚å½“æˆ‘æƒ³æŠŠä»¥å‰çš„ go çš„ web æœåŠ¡ä½¿ç”¨ rust å®ç°æ—¶ï¼Œä¸€çœ¼çœ‹å»ï¼Œä¼¼ä¹æ¯ä¸ªæ¡†æ¶éƒ½æ¯” go é‡Œå­˜åœ¨æ¡†æ¶å¤æ‚, æœ¬æ¥ Rust çš„å­¦ä¹ æ›²çº¿å°±å¤Ÿé™¡å³­çš„äº†, åˆä½•è‹¦æŠŠ Web æ¡†æ¶æ•´å¾—é‚£ä¹ˆå¤æ‚?</p>\n<h2>å¦‚ä½•åšåˆ°è¶³å¤Ÿç®€å•</h2>\n<p>å¾ˆå¤šåº•å±‚çš„å®ç° Hyper éƒ½å·²ç»å®ç°ï¼Œæ‰€ä»¥ï¼Œä¸€èˆ¬éœ€æ±‚ï¼ŒåŸºäº Hyper å®ç°åº”è¯¥æ²¡æœ‰é”™ã€‚Salvo ä¹Ÿæ˜¯ä¸€æ ·ã€‚ æ ¸å¿ƒåŠŸèƒ½æ˜¯æä¾›è¿˜ç”¨ç®€å•çš„APIï¼Œä»¥åŠä¸€ä¸ªåŠŸèƒ½å¼ºå¤§å¹¶ä¸”çµæ´»çš„è·¯ç”±ç³»ç»Ÿã€‚</p>\n<p>Salvo é‡Œç»Ÿä¸€äº† Handler å’Œ Middleware. Middleware å°±æ˜¯ Handler. é€šè¿‡è·¯ç”±çš„ before æˆ–è€… after æ·»åŠ åˆ° Router ä¸Šã€‚æœ¬è´¨ä¸Š, Middleware å’Œ Handler éƒ½æ˜¯å¤„ç† Request è¯·æ±‚ï¼Œå¹¶ä¸”å¯èƒ½å‘ Response å†™å…¥æ•°æ®ã€‚è€Œ Handler æ¥æ”¶çš„å‚æ•°æ˜¯ Request, Depot, Response ä¸‰ä¸ª, å…¶ä¸­ Depot ç”¨äºå­˜å‚¨è¯·æ±‚å¤„ç†è¿‡ç¨‹ä¸­çš„ä¸´æ—¶æ•°æ®. ä¸ºæ–¹ä¾¿ä¹¦å†™, åœ¨ç”¨ä¸ç€çš„æƒ…å†µä¸‹å¯ä»¥çœç•¥æ‰æŸäº›å‚æ•°.</p>\n<pre><code>use Salvo::prelude::*;\n\n#[fn_handler]\nasync fn hello_world(_req: &amp;mut Request, _depot: &amp;mut Depot, res: &amp;mut Response) {\n    res.render_plain_text(\"Hello World\");\n}\n#[fn_handler]\nasync fn hello_world2(res: &amp;mut Response) {\n    res.render_plain_text(\"Hello World\");\n}\n</code></pre>\n<p>å¦å¤–è·¯ç”±ç³»ç»Ÿæä¾›çš„ API ä¹Ÿæ˜¯æå…¶ç®€å•çš„, ä½†æ˜¯, åŠŸèƒ½å´æ˜¯å¼ºå¤§çš„. æ­£å¸¸ä½¿ç”¨éœ€æ±‚ä¸‹, åŸºæœ¬ä¸Šå°±æ˜¯åªå…³æ³¨ Router ä¸€ä¸ªç±»å‹å³å¯.</p>\n<h3>è·¯ç”±ç³»ç»Ÿ</h3>\n<p>æˆ‘è‡ªå·±æ„Ÿè§‰è·¯ç”±ç³»ç»Ÿæ˜¯è·Ÿå…¶ä»–çš„æ¡†æ¶ä¸å¤ªä¸€æ ·çš„. Router å¯ä»¥å†™å¹³ï¼Œä¹Ÿå¯ä»¥å†™æˆæ ‘çŠ¶ã€‚è¿™é‡ŒåŒºä¸šåŠ¡é€»è¾‘æ ‘ä¸è®¿é—®ç›®å½•æ ‘ã€‚ä¸šåŠ¡é€»è¾‘æ ‘æ˜¯æ ¹æ®ä¸šåŠ¡é€»è¾‘éœ€æ±‚ï¼Œåˆ’åˆ† router ç»“æ„ï¼Œå½¢æˆ router æ ‘ï¼Œå®ƒä¸ä¸€å®šä¸è®¿é—®ç›®å½•æ ‘ä¸€è‡´ã€‚</p>\n<p>æ­£å¸¸æƒ…å†µä¸‹æˆ‘ä»¬æ˜¯è¿™æ ·å†™è·¯ç”±çš„ï¼š</p>\n<pre><code>Router::new().path(\"articles\").get(list_articles).post(create_article);\nRouter::new()\n    .path(\"articles/&lt;id&gt;\")\n    .get(show_article)\n    .patch(edit_article)\n    .delete(delete_article);\n</code></pre>\n<p>å¾€å¾€æŸ¥çœ‹æ–‡ç« å’Œæ–‡ç« åˆ—è¡¨æ˜¯ä¸éœ€è¦ç”¨æˆ·ç™»å½•çš„, ä½†æ˜¯åˆ›å»º, ç¼–è¾‘, åˆ é™¤æ–‡ç« ç­‰éœ€è¦ç”¨æˆ·ç™»å½•è®¤è¯æƒé™æ‰å¯ä»¥. Salvo ä¸­æ”¯æŒåµŒå¥—çš„è·¯ç”±ç³»ç»Ÿå¯ä»¥å¾ˆå¥½åœ°æ»¡è¶³è¿™ç§éœ€æ±‚. æˆ‘ä»¬å¯ä»¥æŠŠä¸éœ€è¦ç”¨æˆ·ç™»å½•çš„è·¯ç”±å†™åˆ°ä¸€èµ·ï¼š</p>\n<pre><code>Router::new()\n    .path(\"articles\")\n    .get(list_articles)\n    .push(Router::new().path(\"&lt;id&gt;\").get(show_article));\n</code></pre>\n<p>ç„¶åæŠŠéœ€è¦ç”¨æˆ·ç™»å½•çš„è·¯ç”±å†™åˆ°ä¸€èµ·ï¼Œ å¹¶ä¸”ä½¿ç”¨ç›¸åº”çš„ä¸­é—´ä»¶éªŒè¯ç”¨æˆ·æ˜¯å¦ç™»å½•ï¼š</p>\n<pre><code>Router::new()\n    .path(\"articles\")\n    .before(auth_check)\n    .post(list_articles)\n    .push(Router::new().path(\"&lt;id&gt;\").patch(edit_article).delete(delete_article));\n</code></pre>\n<p>è™½ç„¶è¿™ä¸¤ä¸ªè·¯ç”±éƒ½æœ‰è¿™åŒæ ·çš„ <code>path(\"articles\")</code>, ç„¶è€Œå®ƒä»¬ä¾ç„¶å¯ä»¥è¢«åŒæ—¶æ·»åŠ åˆ°åŒä¸€ä¸ªçˆ¶è·¯ç”±, æ‰€ä»¥æœ€åçš„è·¯ç”±é•¿æˆäº†è¿™ä¸ªæ ·å­:</p>\n<pre><code>Router::new()\n    .push(\n        Router::new()\n            .path(\"articles\")\n            .get(list_articles)\n            .push(Router::new().path(\"&lt;id&gt;\").get(show_article)),\n    )\n    .push(\n        Router::new()\n            .path(\"articles\")\n            .before(auth_check)\n            .post(list_articles)\n            .push(Router::new().path(\"&lt;id&gt;\").patch(edit_article).delete(delete_article)),\n    );\n</code></pre>\n<p><code>&lt;id&gt;</code>åŒ¹é…äº†è·¯å¾„ä¸­çš„ä¸€ä¸ªç‰‡æ®µ, æ­£å¸¸æƒ…å†µä¸‹æ–‡ç« çš„ <code>id</code> åªæ˜¯ä¸€ä¸ªæ•°å­—, è¿™æ˜¯æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼é™åˆ¶ <code>id</code> çš„åŒ¹é…è§„åˆ™, <code>r\"&lt;id:/\\d+/&gt;\"</code>.</p>\n<p>æ›´å¤šä¿¡æ¯å¯ä»¥æŸ¥çœ‹ç½‘ç«™ https://salvo.rs</p>\n<p>æºç åœ°å€: https://github.com/salvo-rs/salvo</p>\n<p>éå¸¸æ¬¢è¿å¤§å®¶ä¸ºé¡¹ç›®è´¡çŒ®åŠ›é‡ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹æ³•ä¸ºé¡¹ç›®ä½œå‡ºè´¡çŒ®:</p>\n<ul>\n<li>åœ¨ issue ä¸­æäº¤åŠŸèƒ½éœ€æ±‚å’Œ bug report;</li>\n<li>åœ¨ issues æˆ–è€… require feedback ä¸‹ç•™ä¸‹è‡ªå·±çš„æ„è§;</li>\n<li>é€šè¿‡ pull requests æäº¤ä»£ç ;</li>\n<li>åœ¨åšå®¢æˆ–è€…æŠ€æœ¯å¹³å°å‘è¡¨ Salvo ç›¸å…³çš„æŠ€æœ¯æ–‡ç« ã€‚</li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-27 00:23:31","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"[å·²è§£å†³]println! ä¸¥é‡æ‹–å»¶æ•ˆèƒ½ï¼Œä»…åˆ—å°ä¸€è¡Œ","link":"https://rustcc.cn/article?id=ab0d06cb-d33d-4e18-b7b3-0b3e889f7b11","description":"<p>å½“æŠŠcallå‡½æ•°æ³¨è§£åï¼Œæˆ–æ˜¯æ³¨è§£println! éƒ½å¯ä»¥å¿«é€Ÿè¿è¡Œã€‚</p>\n<p>åœ¨ https://play.rust-lang.org/ ä¸Šæœ‰æ—¶å€™å¯ä»¥ \"\"ä½¿ç”¨println! \"\" è€Œä¸”ä¾ç„¶ç¼–è¯‘çš„å¾ˆå¿«ï¼Œæœ‰æ—¶å€™åˆ™ä¸è¡Œï¼Œæˆ‘è‡ªå·±æœ¬åœ°ç”µè„‘éƒ½ä¸è¡Œã€‚</p>\n<p>è¿™æ•ˆèƒ½å·®äº†åä¸‡å…«åƒé‡Œï¼Œè¯·å¤§å®¶å¸®å¿™ï¼Œæ–°æ‰‹æ€»æ˜¯åœ¨ println! è·Œå‘ã€‚</p>\n<p>è¿™è¾¹ä½¿ç”¨ <code>cargo run --release</code> ç¼–è¯‘</p>\n<pre><code>use std::time::{Duration, Instant};\n\nstruct Struct {\n    a: String,\n    b: bool,\n}\ntrait Dyn {}\nimpl Dyn for Struct {}\n\nfn main() {\n    let start = Instant::now();\n    let mut count = 0;\n    let count_end = 100_000_000i64;\n\n    while count &lt;= count_end {\n        let m: Box&lt;Struct&gt; = Box::new(Struct {\n            b: false,\n            a: \"str\".to_string(),\n        });\n        if count == count_end {\n            call();               // ---- è¿™å„¿\n            m.b;\n            m.a;\n        }\n        count += 1;\n    }\n\n    let duration = start.elapsed();\n    println!(\"Time: {:?}\", duration);\n}\n\nfn call(){\n    println!(\"run call()\\n\");     // ---- é‡ç‚¹åœ¨è¿™å„¿ï¼Œæ³¨è§£åå˜è¶…å¿«\n}\n</code></pre>\n<p>Time:</p>\n<table>\n<thead>\n<tr>\n<th align=\"right\">ğŸ˜«ä½¿ç”¨println!</th>\n<th align=\"right\">ğŸ˜„æ³¨è§£//println!</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td align=\"right\">12.863911s</td>\n<td align=\"right\">2.8486ms</td>\n</tr>\n<tr>\n<td align=\"right\">13.2101748s</td>\n<td align=\"right\">2.4661ms</td>\n</tr>\n<tr>\n<td align=\"right\">13.5353751s</td>\n<td align=\"right\">2.0433ms</td>\n</tr>\n<tr>\n<td align=\"right\">13.4852107s</td>\n<td align=\"right\">1.7869ms</td>\n</tr>\n<tr>\n<td align=\"right\">â€”â€”â€”â€”â€”â€”â€”â€”</td>\n<td align=\"right\">â€”â€”â€”â€”â€”â€”â€”â€”</td>\n</tr>\n</tbody>\n</table>\n<hr>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-26 16:17:11","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</h3>\n<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´8æœˆ29æ—¥ 20:00-21:00</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»:</strong>  è®²åˆ° Rust ä½¿ç”¨ Future å¼‚æ­¥ç¼–ç¨‹ï¼Œå°±ä¸å¾—ä¸è¯´ futures å’Œ tokio è¿™ä¸¤ä¸ª crateï¼Œå…¶å®æ ‡å‡†åº“ä¸­çš„ futureï¼Œä»¥åŠ async/await å°±æ˜¯ä» futures åº“ä¸­æ•´åˆè¿›æ ‡å‡†åº“çš„, Tokio æ‹¥æœ‰æå¿«çš„æ€§èƒ½ï¼Œæ˜¯å¤§éƒ¨åˆ†ç³»ç»Ÿå¼‚æ­¥å¤„ç†çš„é€‰æ‹©ï¼Œå…¶æ„å»ºäº future ä¹‹ä¸Šã€‚Future æ˜¯  Rust å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒåŸºç¡€ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<p>1ã€ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥.</p>\n<p>2ã€ç†è§£å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>\n<p>3ã€Future ç¼–ç¨‹æ¨¡å‹è®²è§£.</p>\n<p>4ã€å¸¦é¢†å¤§å®¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ future , å†æ¬¡å¸®å¿™å¤§å®¶ç†è§£</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-19 -- Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</h3>\n<p>å·²ç»åœ¨ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªcrate ä¸Šæµ‹è¯•äº†ç‰ˆæœ¬è¿ç§»,å¹¶ä¸”å°†æµ‹è¯•æ‰€æœ‰å…¬å…±çš„ crateã€‚</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>å¼‚æ­¥å¼•æ“ C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D æ¸¸æˆå¼•æ“</h3>\n<ul>\n<li><strong>PCï¼ˆWindowsã€Linuxã€macOSï¼‰å’Œ Web (WebAssembly)</strong> æ”¯æŒã€‚</li>\n<li><strong>å»¶è¿Ÿç€è‰²</strong></li>\n<li><strong>å†…ç½®ä¿å­˜/åŠ è½½</strong></li>\n<li><strong>ç‹¬ç«‹åœºæ™¯ç¼–è¾‘å™¨</strong></li>\n<li><strong>é«˜çº§ç‰©ç†æ¨¡å‹</strong></li>\n<li><strong>åˆ†å±‚æ¨¡å‹èµ„æº</strong></li>\n<li><strong>å‡ ä½•å®ä¾‹åŒ–</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ å†°å±±ä¸Šçš„ mook &amp;&amp; æŒºè‚¥</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾: é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Šé€šè¿‡Datafuseç†è§£å…¨é“¾è·¯è·Ÿè¸ªã€‹| Vol. 4</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ22æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong> æ•°æ®åº“ç³»ç»Ÿä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚ï¼Œåºå¤§çš„ç³»ç»Ÿã€‚ç‰¹åˆ«æ˜¯åœ¨è°ƒè¯•å’Œè§‚å¯ŸSQLæ‰§è¡Œï¼Œå¤šçº¿ç¨‹ä»»åŠ¡åˆ‡æ¢ï¼Œå› ä¸ºæ²¡æœ‰å†…å­˜è°ƒç”¨æˆ–å †æ ˆè·Ÿè¸ªï¼Œè¿™ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¿½è¸ªçš„ç”±æ¥ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°å¤šè¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ªä¸ºæè¿°å’Œåˆ†æè·¨è¿›ç¨‹äº‹åŠ¡æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚Google Dapper(Dapper: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿé“¾è·¯è¿½è¸ªåŸºç¡€è®¾æ–½)è®ºæ–‡(å„tracerçš„åŸºç¡€)ä¸­æè¿°äº†åˆ†å¸ƒå¼è¿½è¸ªçš„ä¸€äº›ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­ç¨³æ€é—®é¢˜ã€åˆ†å¸ƒå¼åˆ†æã€èµ„æºå±æ€§å’Œå¾®æœåŠ¡çš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ã€‚</p>\n<p>æœ¬æ¬¡å…¬å¼€è¯¾é€š Google çš„ OpenTraceing ä»‹ç»ï¼Œç»“åˆRustçš„ tokio-rs/tracing ä½¿ç”¨ï¼Œæœ€ç»ˆç»“åˆ Datafuse é¡¹ç›®ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹å¤§å‹åº”ç”¨çš„å…¨é“¾è·¯è·Ÿè¸ªåˆ†æè¿‡ç¨‹ã€‚</p>\n<p>å…³äºDatafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ol>\n<li>\n<p>ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»ŸOpenTracingåŠåº”ç”¨åœºæ™¯</p>\n</li>\n<li>\n<p>ä»‹ç» tokio-rs/tracing åŠåœ¨ç¨‹åºå¼€å‘ä¸­çš„ä½œç”¨</p>\n</li>\n<li>\n<p>ä¸ºä»€ä¹ˆéœ€è¦tokio-rs/tracingåº“</p>\n</li>\n<li>\n<p>æ¼”ç¤ºDatafuseé¡¹ç›®ä¸­tokio-rs/tracingçš„ä½¿ç”¨</p>\n</li>\n</ol>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"è®ºå›githubè´¦æˆ·æ— æ³•ç™»å½•è§£å†³ç¬”è®°","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>æœ‰åæ˜ è¿™ä¸¤å¤©githubè´¦æˆ·æ— æ³•ç™»å½•äº†ã€‚</p>\n<p>æŠ¥è¿™ä¸ªé”™ï¼š</p>\n<pre><code>get github user info err\n</code></pre>\n<p>æŸ¥äº†å‡ ä¸ªåœ°æ–¹ï¼š</p>\n<ol>\n<li>ä»£ç æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼šOk</li>\n<li>httpsä»£ç†æ˜¯å¦æ­£å¸¸ï¼šOk</li>\n<li>æ£€æŸ¥äº†githubè¿”å›æ—¥å¿—ï¼Œå‘ç°æ˜¯ï¼š</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>è¿›å…¥è¿™ä¸ªåœ°å€ä¸€çœ‹ï¼š<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>åŸæ¥2020å¹´2æœˆå°±å·²ç»è¯´äº†ï¼Œè¦æ”¹è¦æ”¹ã€‚ä¸è¿‡æˆ‘ç¡®å®æ²¡ç•™æ„åˆ°è¿™ä¸ªä¿¡æ¯ã€‚ï¼šï¼ˆ</p>\n<p>æ„æ€å°±æ˜¯è¯´access_tokenä¸è¦æ”¾åœ¨queryå‚æ•°ä¸­ï¼Œè€Œæ˜¯è¦æ”¾åœ¨headeré‡Œé¢ã€‚ç…§å®ƒè¯´çš„ï¼Œæ”¹äº†åå°±å¥½äº†ã€‚</p>\n<p>ç‰¹æ­¤è®°å½•ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust çš„ Future ä¸ Javascript çš„ Promise åŠŸèƒ½å¯¹ç…§å‚è€ƒ","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>çš„<code>Future</code>ä¸<code>Javascript</code>çš„<code>Promise</code>åŠŸèƒ½å¯¹ç…§å‚è€ƒ</h1>\n<p>å­¦ä¹ æ–°é²œæŠ€æœ¯æ—¶ï¼Œæˆ‘æ€»æ˜¯ä¼šä¹ æƒ¯æ€§å‘æ›¾ç»ç†Ÿæ‚‰çš„å†…å®¹ä¸Šé ï¼Œç”šè‡³å¥—ç”¨ç°æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¿™æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œå¯¹ç…§<code>Javascript - Promise/A+ API</code>æ¥è®°å¿†ä¸€éƒ¨åˆ†<code>Rust Future</code>å¸¸ç”¨<code>API</code>ã€‚</p>\n<blockquote>\n<p>æ³¨æ„ï¼šæ‰€æœ‰çš„<code>Rust - Future</code>æ“ä½œéƒ½æ˜¯ä»¥<code>.await</code>ç»“å°¾çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œä¸åŒäº<code>Javascript - Promise/A+</code>ï¼Œ<code>Rust - Future</code>æ˜¯æƒ°æ€§çš„ã€‚åªæœ‰è¢«<code>.await</code>æŒ‡ä»¤æ¿€æ´»åï¼Œåœ¨<code>Rust - Future</code>å†…å°è£…çš„æ“ä½œæ‰ä¼šè¢«çœŸæ­£åœ°æ‰§è¡Œã€‚</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">æè¿°</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* ä»€ä¹ˆéƒ½ä¸åš */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll ä¸èƒ½è¢«ç”¨æ¥æ„é€ åŒ…å«äº†å¼‚æ­¥æ“ä½œçš„ Future å®ä¾‹ï¼Œå› ä¸ºã€å›è°ƒé—­åŒ…ã€‘å†…çš„ã€å¯ä¿®æ”¹å¼•ç”¨ã€‘&amp;mut Context&lt;'_&gt; ä¸èƒ½è¢«  ï¼ˆ1ï¼‰è·¨çº¿ç¨‹ä¼ é€’  ï¼ˆ2ï¼‰ä¼ é€’å‡ºé—­åŒ…ä½œç”¨åŸŸ2. task::spawn_blocking() ã€å›è°ƒé—­åŒ…ã€‘è¾“å…¥å‚æ•°å†…çš„ thread::sleep() ä¸æ˜¯é˜»å¡è¿è¡Œ task::spawn_blocking() çš„ä¸»çº¿ç¨‹ï¼Œè€Œæ˜¯é˜»å¡ä»ã€é˜»å¡ä»»åŠ¡çº¿ç¨‹æ± ã€‘ä¸­åˆ†é…æ¥è¿è¡Œé˜»å¡ä»»åŠ¡çš„ã€å·¥ä½œçº¿ç¨‹ã€‘ã€‚</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. æœ‰ä¸€ä¸ª promise/future å¤±è´¥å°±æ•´ä½“æ€§åœ°å¤±è´¥ã€‚2. try_join æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future çš„æˆåŠŸä¸å¤±è´¥ç»“æœéƒ½æ”¶é›†2. è¿”å›ç»“æœï¼š(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. ä»…åªæ”¶é›†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ promise/future2. try_race æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. æ”¶é›†ç¬¬ä¸€ä¸ªç»“æŸçš„ promise/futureï¼Œæ— è®ºå®ƒæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯å¤±è´¥æ”¶åœºã€‚2. è¿”å›ç»“æœï¼šT</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼š</strong>ã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹</p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ15æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong></p>\n<p>å¦‚æœæƒ³ç”¨ Rust å¼€å‘å¤§å‹ç›®ï¼Œæˆ–è€…å­¦ä¹ å¤§å‹é¡¹ç›®ä»£ç ï¼Œç‰¹åˆ«æ˜¯æ¡†æ¶çº§åˆ«çš„é¡¹ç›®ï¼Œé‚£ä¹ˆ Rust çš„å®æœºåˆ¶è‚¯å®šæ˜¯ä¸€ä¸ªå¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚ ä¾‹å¦‚ datafuse ä¸­çš„ä¸€äº›é…ç½®ç®¡ç†ï¼š\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>è¿™å°±æ˜¯é€šè¿‡å®å®ç°é…ç½®çš„ç»Ÿä¸€è¡Œä¸ºï¼Œä»£ç å‚è€ƒï¼š\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust è¯­è¨€å¼ºå¤§çš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¯ä»¥åˆ›å»ºå’Œåˆ©ç”¨å®ï¼Œä¸è¿‡åˆ›å»ºå®çœ‹èµ·æ¥æŒºå¤æ‚ï¼Œå¸¸å¸¸ä»¤åˆšæ¥è§¦ Rust çš„å¼€å‘è€…ç”Ÿç•æƒ§ã€‚ åœ¨æœ¬æ¬¡å…¬å¼€è¯¾ä¸­å¸®åŠ©ä½ ç†è§£ Rust Macro çš„åŸºæœ¬åŸç†ï¼Œå­¦ä¹ å¦‚ä½•åˆ›è‡ªå·²çš„ Rust å®ï¼Œä»¥åŠæŸ¥çœ‹æºç å­¦ä¹ å®çš„å®ç°ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ul>\n<li>ä»€ä¹ˆæ˜¯ Rust å®</li>\n<li>ä»€ä¹ˆæ˜¯å®è¿è¡ŒåŸç†</li>\n<li>å¦‚ä½•åˆ›å»º Rust å®è¿‡ç¨‹</li>\n<li>é˜…è¯» datafuse é¡¹ç›®æºç ï¼Œ å­¦ä¹ é¡¹ç›®ä¸­å®çš„å®ç°</li>\n</ul>\n<p><strong>è®²å¸ˆä»‹ç»</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šç†è§£Rustçš„æ‰€æœ‰æƒ| Vol 2","link":"https://rustcc.cn/article?id=c107b830-9fe1-43dd-94a3-9efcd5544205","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼šã€Šç†è§£Rustæ‰€æœ‰æƒã€‹</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š2021å¹´8æœˆ8æ—¥ 20:30-21:30</strong></p>\n<p><strong>å˜‰å®¾è®²å¸ˆï¼š è‹æ—</strong></p>\n<p><strong>å˜‰å®¾ä»‹ç»ï¼š</strong></p>\n<p>Rustä¸­æ–‡ç¤¾åŒºæˆå‘˜ï¼Œå¤šç‚¹DmallæŠ€æœ¯Leaderï¼Œå‰æŠ˜800äº’è”ç½‘ç ”å‘å›¢é˜Ÿè´Ÿè´£äººã€10ä½™å¹´ä¸€çº¿ç ”å‘ç»éªŒã€‚å…·æœ‰å¤šå¹´çš„è½¯ä»¶å¼€å‘ç»éªŒ, ç†Ÿç»ƒRubyã€Javaã€Rustç­‰å¼€å‘è¯­è¨€, åŒæ—¶ä¹Ÿå‚ä¸è¿‡Rustä¸­æ–‡ç¤¾åŒºæ—¥æŠ¥ç»´æŠ¤å·¥ä½œã€‚</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»</strong></p>\n<p>æœ¬æ¬¡è¯¾ç¨‹é€šè¿‡10ä¸ªå·¦å³çš„å°ä¾‹å­ï¼Œå¸¦å¤§å®¶ç†è§£ä¸€ä¸‹Rustçš„æ‰€æœ‰æƒï¼ŒRustå¼•ç”¨å’Œå€Ÿç”¨ï¼ŒRustå˜é‡å…‹éš†å’Œå¤åˆ¶çš„ç†å¿µã€‚</p>\n<p><strong>å‚åŠ è¯¾ç¨‹</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/Rust-pbc-1.jpg\" alt=\"\"></p>\n<p><strong>è¯¾ç¨‹è§„åˆ’</strong></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloudé¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-08 02:04:00","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"æ•°æ®è¡¨ Timestamp æ—¥æœŸ Serialize","link":"https://rustcc.cn/article?id=2ff8a69e-59bb-4502-87c0-c3416ffae8a0","description":"<p>ä¸»è¦å‚è€ƒï¼š<a href=\"https://github.com/rustcc/forustm\" rel=\"noopener noreferrer\">Rustccç½‘ç«™æºç åº“</a></p>\n<p>åœ¨å¤„ç†æ•°æ®è¡¨ä¸­æ—¥æœŸç›¸å…³æ•°æ®æ—¶ï¼ŒSeralizeåºåˆ—åŒ–ç›¸å…³æ“ä½œä¼šæŠ¥é”™ï¼Œæç¤º DateTime å­—æ®µä¸è¯†åˆ«ï¼Œ\næŸ¥äº† rustcc æºç æ‰å‘ç°ä¾èµ–ä¸­éœ€è¦å¼€å¯ç›¸åº”çš„featureã€‚ç‰¹æ­¤è®°å½•ã€‚</p>\n<h2>1.ä¾èµ–çš„åº“ï¼š</h2>\n<pre><code>[dependencies]\n# æ—¥æœŸæ—¶é—´å¤„ç† éœ€è¦å¼€å¯ serde ç‰¹å¾ æ”¯æŒåºåˆ—åŒ–\nchrono = { version = \"0.4.19\", features = [\"serde\"] }\n\n# æ•°æ®åº“ORM\ndiesel = { version = \"1.4.4\", features = [\"postgres\", \"chrono\", \"uuid\", \"r2d2\"] }\ndotenv = \"0.15.0\"\nserde = { version = \"1.0.127\", features = [\"derive\"] }\nserde_json = \"1.0.66\"\nuuid = { version = \"0.8.2\", features = [\"serde\", \"v4\"] }\n</code></pre>\n<h2>2.åˆ›å»ºæ•°æ®è¡¨</h2>\n<pre><code>CREATE TABLE characters (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(128) UNIQUE NOT NULL,\n    age INTEGER NOT NULL DEFAULT 0,\n    friends VARCHAR NOT NULL DEFAULT '',\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\n)\n</code></pre>\n<h2>3.æ•°æ®è¡¨å¯¹åº”çš„ model</h2>\n<pre><code>use chrono::{NaiveDateTime};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Queryable, Serialize, Deserialize, Debug)]\npub struct Characters {\n    pub id: i32,\n    pub name: String,\n    pub age: i32,\n    pub friends: String,\n    // è¿™é‡Œçš„ NaiveDateTime æ—¥æœŸæ ¼å¼åºåˆ—åŒ–éœ€è¦å¼€å¯ç›¸å…³ features\n    pub created_at: NaiveDateTime,\n}\n</code></pre>\n<h2>4.è·å–æ•°æ®</h2>\n<pre><code>use db::schema::characters;\nuse db::{get_connection};\nuse db::models::{Characters, NewCharacter};\nuse db::schema::characters::dsl::*;\nuse diesel::QueryDsl;\nuse diesel::prelude::*;\n\nfn main() {\n    let conn = get_connection();\n\n    // æŸ¥è¯¢å¹´é¾„å¤§äº30çš„10æ¡æ•°æ®\n    let arr: Vec&lt;Characters&gt; = characters.filter(characters::age.gt(30))\n        .limit(10)\n        .load::&lt;Characters&gt;(&amp;conn)\n        .expect(\"Loading Error\");\n\n    let date_arr = arr.iter()\n        .map(|item| {\n\t    // æ•°æ®æ ¼å¼åŒ–\n            let t = item.created_at.format(\"%Y-%m-%d %H:%M:%S\").to_string();\n            println!(\"{} {}\", item.name, t);\n            t\n        })\n        .collect::&lt;Vec&lt;String&gt;&gt;();\n}\n</code></pre>\n<p>è¾“å‡ºç»“æœç±»ä¼¼ï¼š</p>\n<pre><code>Box 2021-08-05 09:39:34\nBobe 2021-08-05 09:39:34\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-08 01:40:35","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Cargo workspace config","link":"https://rustcc.cn/article?id=c3dcce30-1fc0-4819-8992-142365c7e21c","description":"<p><a href=\"https://kaisery.github.io/trpl-zh-cn/ch14-03-cargo-workspaces.html\" rel=\"noopener noreferrer\">Workspace æ–‡æ¡£é“¾æ¥</a></p>\n<h2>ç›®å½•ç»“æ„</h2>\n<pre><code>workspace-test/\n    Cargo.toml\n    db/\n        src/\n            bin/\n                init.rs\n        Cargo.tml\n</code></pre>\n<h2>workspace</h2>\n<p>workspace-test/Cargo.toml</p>\n<pre><code>[workspace]\nmembers = [\"db\"]\ndefault-member = \"db\"\n</code></pre>\n<h2>å­é¡¹ç›®</h2>\n<p>workspace-test/db/Cargo.toml</p>\n<pre><code>[package]\nname = \"db\"\nversion = \"0.1.0\"\nedition = \"2018\"\n\n[dependencies]\n\n# å¯é€‰çš„å¯æ‰§è¡Œæ–‡ä»¶é…ç½®\n# [[bin]]\n# name = \"init\"\n# path = \"src/bin/init.rs\"\n</code></pre>\n<h2>æ“ä½œ</h2>\n<pre><code># è¿è¡Œ init\ncargo run --bin init\n# -p æŒ‡å®šé¡¹ç›®\ncargo run -p db --bin init\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-04 09:54:31","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰","link":"https://rustcc.cn/article?id=120035c3-944d-4a79-9b3a-8390697a6e13","description":"<h1><code>Rust</code>å¼‚æ­¥ç¼–ç¨‹æµ…æ‚Ÿï¼ˆä¸€ï¼‰</h1>\n<p>ä¸åŒäº<code>javascript</code>çš„<code>new Promise((resolve, reject) =&gt; {...})</code>æ„é€ å³è¿è¡Œï¼Œ<code>Rust</code>ä¸­çš„<code>Future</code>æ˜¯Â·æƒ°æ€§Â·çŠ¶æ€æœºã€‚è¿™ä½“ç°ä¸ºï¼š</p>\n<ol>\n<li>ã€è°ƒç”¨å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€æ‰§è¡Œå¼‚æ­¥å—ã€‘ä»…åªæ„é€ ä¸€ä¸ª<code>Future trait object</code>ã€‚</li>\n<li>å› ä¸º<code>Future</code>æ˜¯æƒ°æ€§çŠ¶æ€æœºï¼Œæ‰€ä»¥å®ƒä¸ä¼šè‡ªåŠ¨æ‰§è¡Œã€å¼‚æ­¥å‡½æ•°ã€‘æˆ–ã€å¼‚æ­¥å—ã€‘å†…çš„ä»»ä½•ä¸€è¡Œä»£ç  --- æ­¤ç‚¹ä¸<code>javascript</code>çš„Â·æ´»æ€§Â·çŠ¶æ€æœºå®Œå…¨ä¸åŒã€‚ç›¸åï¼Œéœ€è¦äººå·¥æ¿€æ´»è§¦å‘ã€‚</li>\n<li>äººå·¥å¯åŠ¨<code>Future</code>è¿è¡Œï¼Œåˆåˆ†ä¸ºä¸¤ä¸ªåœºæ™¯çš„ä¸¤ç§æƒ…å†µï¼š\n<ol>\n<li>\n<p>å·²ç»åœ¨<code>async fn</code>å†…ï¼Œ<code>Future.await</code>æ¿€æ´»ã€‚ä½†ï¼ŒåŒæ—¶<strong>é˜»å¡</strong>å½“å‰å¼‚æ­¥ç¨‹åºæ‰§è¡Œæµã€‚</p>\n</li>\n<li>\n<p>åœ¨<code>async fn</code>å¤–ï¼Œéœ€è¦å€ŸåŠ©ç”±ã€è¿è¡Œæ—¶ã€‘æä¾›çš„ã€æ‰§è¡Œå™¨ã€‘ã€‚å°±<code>async-std</code>åº“è€Œè¨€ï¼Œæœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š</p>\n<ol>\n<li><code>task::block_on(Future)</code> æ‰§è¡Œ<code>Future</code>ä¸”é˜»å¡å½“å‰çº¿ç¨‹ç›´åˆ°<code>Future</code>è¢«å®Œæˆã€‚</li>\n<li><code>task::spawn(Future)</code>ä»…æ‰§è¡Œ<code>Future</code>å’Œä¸é˜»å¡å½“å‰çº¿ç¨‹ã€‚</li>\n</ol>\n<p>æ— è®ºé€‰æ‹©ä¸Šé¢å“ªç§æ–¹å¼ï¼Œè‹¥åœ¨<code>Future</code>æ‰§è¡ŒæœŸé—´å‡ºç°äº†<code>panic</code>ï¼Œå…¶éƒ½ä¼šç»ˆæ­¢ï¼ˆ<code>abort</code>ï¼‰æ­£åœ¨å…±äº«åŒä¸€ä¸ªæ‰§è¡Œçº¿ç¨‹ï¼ˆ<code>thread</code>ï¼‰çš„æ‰€æœ‰<code>task</code>ï¼ˆÂ·æ— æ ˆÂ·åç¨‹ï¼‰çš„è¿è¡Œã€‚</p>\n</li>\n</ol>\n</li>\n</ol>\n<p>é¢˜å¤–è¯ï¼Œ</p>\n<ol>\n<li>ç»¿è‰²çº¿ç¨‹æ˜¯Â·æœ‰æ ˆÂ·åç¨‹ï¼›å¼‚æ­¥å‡½æ•°ä¸å¼‚æ­¥å—æ˜¯Â·æ— æ ˆÂ·åç¨‹ã€‚</li>\n<li>åœ¨<code>async-std</code>åº“çš„è¯æ±‡è¡¨å†…ï¼Œåç¨‹è¢«ç§°ä½œ<code>task</code>è€Œä¸æ˜¯æƒ¯ä¾‹çš„<code>coroutine</code>ã€‚</li>\n<li><code>task::spawn(Future)</code>ä¹Ÿèƒ½è¢«ä½¿ç”¨äº<code>async fn</code>æˆ–<code>async {...}</code>å†…ã€‚å®ƒè¢«ç”¨æ¥ä»£æ›¿<code>.await</code>æŒ‡ä»¤ï¼Œä»¥<strong>éé˜»å¡</strong><code>async fn</code>æˆ–<code>async {...}</code>çš„æ–¹å¼ï¼Œæ¿€æ´»ä¸æ‰§è¡Œä¸€ä¸ª<code>Future</code>å®ä¾‹ã€‚</li>\n</ol>\n<h2>ä¾‹ç¨‹</h2>\n<pre><code>async fn accept_loop(addr: impl ToSocketAddrs) -&gt; Result&lt;()&gt; {\n    // 1. TcpListener::bind(addr) è¿”å› Future\n    // 2. .await äº Future å–å¾— Result&lt;T, E&gt;\n    // 3. Result&lt;T, E&gt;? å†æ‹¿å¾— Ok&lt;T&gt; ä¸­çš„ T\n    let listener = TcpListener::bind(addr).await?; // å¼‚æ­¥å‡½æ•°å†…çš„äººå·¥å¯åŠ¨ Future\n    let mut incoming = listener.incoming();\n    // å› ä¸ºæ²¡æœ‰ä»è¯­è¨€å±‚é¢æ”¯æŒ async for loopï¼Œæ‰€ä»¥ while loop + Iterator&lt;Item = T&gt; æ¥æ¨¡æ‹Ÿä¹‹ã€‚\n    while let Some(stream) = incoming.next().await {\n        // TODO\n    }\n    Ok(())\n}\nfn main() {\n    let fut = accept_loop(\"127.0.0.1:8080\");\n    task::block_on(fut); // å¼‚æ­¥å‡½æ•°å¤–çš„äººå·¥å¯åŠ¨ Future\n}\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-03 00:01:43","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-08-30T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Enhanced Seq2Seq Autoencoder via Contrastive Learning for Abstractive Text Summarization. (arXiv:2108.11992v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11992","description":"<p>In this paper, we present a denoising sequence-to-sequence (seq2seq)\nautoencoder via contrastive learning for abstractive text summarization. Our\nmodel adopts a standard Transformer-based architecture with a multi-layer\nbi-directional encoder and an auto-regressive decoder. To enhance its denoising\nability, we incorporate self-supervised contrastive learning along with various\nsentence-level document augmentation. These two components, seq2seq autoencoder\nand contrastive learning, are jointly trained through fine-tuning, which\nimproves the performance of text summarization with regard to ROUGE scores and\nhuman evaluation. We conduct experiments on two datasets and demonstrate that\nour model outperforms many existing benchmarks and even achieves comparable\nperformance to the state-of-the-art abstractive systems trained with more\ncomplex architecture and extensive computation resources.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1\">Chujie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1\">Kunpeng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Harry Jiannan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1\">Ling Fan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zhe Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A New Sentence Ordering Method Using BERT Pretrained Model. (arXiv:2108.11994v1 [cs.CL])","link":"http://arxiv.org/abs/2108.11994","description":"<p>Building systems with capability of natural language understanding (NLU) has\nbeen one of the oldest areas of AI. An essential component of NLU is to detect\nlogical succession of events contained in a text. The task of sentence ordering\nis proposed to learn succession of events with applications in AI tasks. The\nperformance of previous works employing statistical methods is poor, while the\nneural networks-based approaches are in serious need of large corpora for model\nlearning. In this paper, we propose a method for sentence ordering which does\nnot need a training phase and consequently a large corpus for learning. To this\nend, we generate sentence embedding using BERT pre-trained model and measure\nsentence similarity using cosine similarity score. We suggest this score as an\nindicator of sequential events' level of coherence. We finally sort the\nsentences through brute-force search to maximize overall similarities of the\nsequenced sentences. Our proposed method outperformed other baselines on\nROCStories, a corpus of 5-sentence human-made stories. The method is\nspecifically more efficient than neural network-based methods when no huge\ncorpus is available. Among other advantages of this method are its\ninterpretability and needlessness to linguistic knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Golestani_M/0/1/0/all/0/1\">Melika Golestani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Razavi_S/0/1/0/all/0/1\">Seyedeh Zahra Razavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Faili_H/0/1/0/all/0/1\">Heshaam Faili</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with RoBERTa. (arXiv:2108.12009v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12009","description":"<p>We present EmoBERTa: Speaker-Aware Emotion Recognition in Conversation with\nRoBERTa, a simple yet expressive scheme of solving the ERC (emotion recognition\nin conversation) task. By simply prepending speaker names to utterances and\ninserting separation tokens between the utterances in a dialogue, EmoBERTa can\nlearn intra- and inter- speaker states and context to predict the emotion of a\ncurrent speaker, in an end-to-end manner. Our experiments show that we reach a\nnew state of the art on the two popular ERC datasets using a basic and\nstraight-forward approach. We've open sourced our code and models at\nhttps://github.com/tae898/erc.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1\">Taewoon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vossen_P/0/1/0/all/0/1\">Piek Vossen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic-based Self-Critical Training For Question Generation. (arXiv:2108.12026v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12026","description":"<p>We present in this work a fully Transformer-based reinforcement learning\ngenerator-evaluator architecture for neural question generation. Question\ngeneration is a task that consists in generating questions given a context and\nanswer. To improve the quality of the generated question, we came up with a\nsemantic-based self-critical training layout in generator-evaluator\narchitecture, which goes beyond typical maximum likelihood training. Evaluation\nmetrics for language modeling only based on n-gram overlapping do not consider\nsemantic relations between reference and candidate strings. To improve the\nevaluation step, we assess our model for both n-gram overlap using BLEU and\nsemantically using BERTScore and NUBIA, a novel state-of-the-art evaluation\nmetric for text generation. Question generation could be used in many\ndownstream applications, including in extending question answering datasets,\nconversational systems, and educational assessment systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lo%5C%22ic/0/1/0/all/0/1\">Lo&#xef;c</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dassi_K/0/1/0/all/0/1\">Kwate Dassi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Using GAN-based models to sentimental analysis on imbalanced datasets in education domain. (arXiv:2108.12061v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12061","description":"<p>While the whole world is still struggling with the COVID-19 pandemic, online\nlearning and home office become more common. Many schools transfer their\ncourses teaching to the online classroom. Therefore, it is significant to mine\nthe students' feedback and opinions from their reviews towards studies so that\nboth schools and teachers can know where they need to improve. This paper\ntrains machine learning and deep learning models using both balanced and\nimbalanced datasets for sentiment classification. Two SOTA category-aware text\ngeneration GAN models: CatGAN and SentiGAN, are utilized to synthesize text\nused to balance the highly imbalanced dataset. Results on three datasets with\ndifferent imbalance degree from distinct domains show that when using generated\ntext to balance the dataset, the F1-score of machine learning and deep learning\nmodel on sentiment classification increases 2.79% ~ 9.28%. Also, the results\nindicate that the average growth degree for CR100k is higher than CR23k, the\naverage growth degree for deep learning is more increased than machine learning\nalgorithms, and the average growth degree for more complex deep learning models\nis more increased than simpler deep learning models in experiments.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1\">Ru Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Edalati_M/0/1/0/all/0/1\">Maryam Edalati</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"4-bit Quantization of LSTM-based Speech Recognition Models. (arXiv:2108.12074v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12074","description":"<p>We investigate the impact of aggressive low-precision representations of\nweights and activations in two families of large LSTM-based architectures for\nAutomatic Speech Recognition (ASR): hybrid Deep Bidirectional LSTM - Hidden\nMarkov Models (DBLSTM-HMMs) and Recurrent Neural Network - Transducers\n(RNN-Ts). Using a 4-bit integer representation, a na\\\"ive quantization approach\napplied to the LSTM portion of these models results in significant Word Error\nRate (WER) degradation. On the other hand, we show that minimal accuracy loss\nis achievable with an appropriate choice of quantizers and initializations. In\nparticular, we customize quantization schemes depending on the local properties\nof the network, improving recognition performance while limiting computational\ntime. We demonstrate our solution on the Switchboard (SWB) and CallHome (CH)\ntest sets of the NIST Hub5-2000 evaluation. DBLSTM-HMMs trained with 300 or\n2000 hours of SWB data achieves $&lt;$0.5% and $&lt;$1% average WER degradation,\nrespectively. On the more challenging RNN-T models, our quantization strategy\nlimits degradation in 4-bit inference to 1.3%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fasoli_A/0/1/0/all/0/1\">Andrea Fasoli</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1\">Chia-Yu Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Serrano_M/0/1/0/all/0/1\">Mauricio Serrano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xiao Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1\">Naigang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Venkataramani_S/0/1/0/all/0/1\">Swagath Venkataramani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saon_G/0/1/0/all/0/1\">George Saon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1\">Xiaodong Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1\">Brian Kingsbury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wei Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tuske_Z/0/1/0/all/0/1\">Zolt&#xe1;n T&#xfc;ske</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1\">Kailash Gopalakrishnan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Harms of Gender Exclusivity and Challenges in Non-Binary Representation in Language Technologies. (arXiv:2108.12084v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12084","description":"<p>Gender is widely discussed in the context of language tasks and when\nexamining the stereotypes propagated by language models. However, current\ndiscussions primarily treat gender as binary, which can perpetuate harms such\nas the cyclical erasure of non-binary gender identities. These harms are driven\nby model and dataset biases, which are consequences of the non-recognition and\nlack of understanding of non-binary genders in society. In this paper, we\nexplain the complexity of gender and language around it, and survey non-binary\npersons to understand harms associated with the treatment of gender as binary\nin English language technologies. We also detail how current language\nrepresentations (e.g., GloVe, BERT) capture and perpetuate these harms and\nrelated challenges that need to be acknowledged and addressed for\nrepresentations to equitably encode gender information.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1\">Sunipa Dev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Monajatipoor_M/0/1/0/all/0/1\">Masoud Monajatipoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ovalle_A/0/1/0/all/0/1\">Anaelia Ovalle</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Subramonian_A/0/1/0/all/0/1\">Arjun Subramonian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1\">Jeff M Phillips</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1\">Kai-Wei Chang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Lingxi: A Diversity-aware Chinese Modern Poetry Generation System. (arXiv:2108.12108v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12108","description":"<p>Poetry generation has been a difficult task in natural language processing.\nUnlike plain neural text generation tasks, poetry has a high requirement for\nnovelty, since an easily-understood sentence with too many high frequency words\nmight not be considered as poetic, while adequately ambiguous sentences with\nlow frequency words can possibly be novel and creative. Inspired by this, we\npresent Lingxi, a diversity-aware Chinese modern poetry generation system. We\npropose nucleus sampling with randomized head (NS-RH) algorithm, which\nrandomizes the high frequency part (\"head\") of the predicted distribution, in\norder to emphasize on the \"comparatively low frequency\" words. The proposed\nalgorithm can significantly increase the novelty of generated poetry compared\nwith traditional sampling methods. The permutation of distribution is\ncontrollable by tuning the filtering parameter that determines the \"head\" to\npermutate, achieving diversity-aware sampling. We find that even when a large\nportion of filtered vocabulary is randomized, it can actually generate fluent\npoetry but with notably higher novelty. We also propose a\nsemantic-similarity-based rejection sampling algorithm, which creates longer\nand more informative context on the basis of the short input poetry title while\nmaintaining high semantic similarity to the title, alleviating the off-topic\nissue.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1\">Xinran Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1\">Maosong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiafeng Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1\">Xiaobing Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Automated Generation of Accurate \\& Fluent Medical X-ray Reports. (arXiv:2108.12126v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12126","description":"<p>Our paper focuses on automating the generation of medical reports from chest\nX-ray image inputs, a critical yet time-consuming task for radiologists. Unlike\nexisting medical re-port generation efforts that tend to produce human-readable\nreports, we aim to generate medical reports that are both fluent and clinically\naccurate. This is achieved by our fully differentiable and end-to-end paradigm\ncontaining three complementary modules: taking the chest X-ray images and\nclinical his-tory document of patients as inputs, our classification module\nproduces an internal check-list of disease-related topics, referred to as\nenriched disease embedding; the embedding representation is then passed to our\ntransformer-based generator, giving rise to the medical reports; meanwhile, our\ngenerator also pro-duces the weighted embedding representation, which is fed to\nour interpreter to ensure consistency with respect to disease-related\ntopics.Our approach achieved promising results on commonly-used metrics\nconcerning language fluency and clinical accuracy. Moreover, noticeable\nperformance gains are consistently ob-served when additional input information\nis available, such as the clinical document and extra scans of different views.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1\">Hoang T.N. Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_D/0/1/0/all/0/1\">Dong Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Badamdorj_T/0/1/0/all/0/1\">Taivanbat Badamdorj</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yujie Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yingying Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Truong_J/0/1/0/all/0/1\">Jason Truong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1\">Li Cheng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Secoco: Self-Correcting Encoding for Neural Machine Translation. (arXiv:2108.12137v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12137","description":"<p>This paper presents Self-correcting Encoding (Secoco), a framework that\neffectively deals with input noise for robust neural machine translation by\nintroducing self-correcting predictors. Different from previous robust\napproaches, Secoco enables NMT to explicitly correct noisy inputs and delete\nspecific errors simultaneously with the translation decoding process. Secoco is\nable to achieve significant improvements over strong baselines on two\nreal-world test sets and a benchmark WMT dataset with good interpretability. We\nwill make our code and dataset publicly available soon.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tao Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1\">Chengqi Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1\">Mingxuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1\">Hang Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1\">Deyi Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving callsign recognition with air-surveillance data in air-traffic communication. (arXiv:2108.12156v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12156","description":"<p>Automatic Speech Recognition (ASR) can be used as the assistance of speech\ncommunication between pilots and air-traffic controllers. Its application can\nsignificantly reduce the complexity of the task and increase the reliability of\ntransmitted information. Evidently, high accuracy predictions are needed to\nminimize the risk of errors. Especially, high accuracy is required in\nrecognition of key information, such as commands and callsigns, used to\nnavigate pilots. Our results prove that the surveillance data containing\ncallsigns can help to considerably improve the recognition of a callsign in an\nutterance when the weights of probable callsign n-grams are reduced per\nutterance. In this paper, we investigate two approaches: (1) G-boosting, when\ncallsigns weights are adjusted at language model level (G) and followed by the\ndynamic decoder with an on-the-fly composition, and (2) lattice rescoring when\ncallsign information is introduced on top of lattices generated using a\nconventional decoder. Boosting callsign n-grams with the combination of two\nmethods allowed us to gain 28.4% of absolute improvement in callsign\nrecognition accuracy and up to 74.2% of relative improvement in WER of callsign\nrecognition.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Braun_R/0/1/0/all/0/1\">Rudolf Braun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grammar Based Identification Of Speaker Role For Improving ATCO And Pilot ASR. (arXiv:2108.12175v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12175","description":"<p>Assistant Based Speech Recognition (ABSR) for air traffic control is\ngenerally trained by pooling both Air Traffic Controller (ATCO) and pilot data.\nIn practice, this is motivated by the fact that the proportion of pilot data is\nlesser compared to ATCO while their standard language of communication is\nsimilar. However, due to data imbalance of ATCO and pilot and their varying\nacoustic conditions, the ASR performance is usually significantly better for\nATCOs than pilots. In this paper, we propose to (1) split the ATCO and pilot\ndata using an automatic approach exploiting ASR transcripts, and (2) consider\nATCO and pilot ASR as two separate tasks for Acoustic Model (AM) training. For\nspeaker role classification of ATCO and pilot data, a hypothesized ASR\ntranscript is generated with a seed model, subsequently used to classify the\nspeaker role based on the knowledge extracted from grammar defined by\nInternational Civil Aviation Organization (ICAO). This approach provides an\naverage speaker role identification accuracy of 83% for ATCO and pilot.\nFinally, we show that training AMs separately for each task, or using a\nmultitask approach is well suited for this data compared to AM trained by\npooling all data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ohneiser_O/0/1/0/all/0/1\">Oliver Ohneiser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Helmke_H/0/1/0/all/0/1\">Hartmut Helmke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sarfjoo_S/0/1/0/all/0/1\">Saeed Sarfjoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Offensive Language Identification in Low-resourced Code-mixed Dravidian languages using Pseudo-labeling. (arXiv:2108.12177v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12177","description":"<p>Social media has effectively become the prime hub of communication and\ndigital marketing. As these platforms enable the free manifestation of thoughts\nand facts in text, images and video, there is an extensive need to screen them\nto protect individuals and groups from offensive content targeted at them. Our\nwork intends to classify codemixed social media comments/posts in the Dravidian\nlanguages of Tamil, Kannada, and Malayalam. We intend to improve offensive\nlanguage identification by generating pseudo-labels on the dataset. A custom\ndataset is constructed by transliterating all the code-mixed texts into the\nrespective Dravidian language, either Kannada, Malayalam, or Tamil and then\ngenerating pseudo-labels for the transliterated dataset. The two datasets are\ncombined using the generated pseudo-labels to create a custom dataset called\nCMTRA. As Dravidian languages are under-resourced, our approach increases the\namount of training data for the language models. We fine-tune several recent\npretrained language models on the newly constructed dataset. We extract the\npretrained language embeddings and pass them onto recurrent neural networks. We\nobserve that fine-tuning ULMFiT on the custom dataset yields the best results\non the code-mixed test sets of all three languages. Our approach yields the\nbest results among the benchmarked models on Tamil-English, achieving a\nweighted F1-Score of 0.7934 while scoring competitive weighted F1-Scores of\n0.9624 and 0.7306 on the code-mixed test sets of Malayalam-English and\nKannada-English, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hande_A/0/1/0/all/0/1\">Adeep Hande</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Puranik_K/0/1/0/all/0/1\">Karthik Puranik</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yasaswini_K/0/1/0/all/0/1\">Konthala Yasaswini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Priyadharshini_R/0/1/0/all/0/1\">Ruba Priyadharshini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thavareesan_S/0/1/0/all/0/1\">Sajeetha Thavareesan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sampath_A/0/1/0/all/0/1\">Anbukkarasi Sampath</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shanmugavadivel_K/0/1/0/all/0/1\">Kogilavani Shanmugavadivel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thenmozhi_D/0/1/0/all/0/1\">Durairaj Thenmozhi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chakravarthi_B/0/1/0/all/0/1\">Bharathi Raja Chakravarthi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Query-Focused Extractive Summarisation for Finding Ideal Answers to Biomedical and COVID-19 Questions. (arXiv:2108.12189v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12189","description":"<p>This paper presents Macquarie University's participation to the BioASQ\nSynergy Task, and BioASQ9b Phase B. In each of these tasks, our participation\nfocused on the use of query-focused extractive summarisation to obtain the\nideal answers to medical questions. The Synergy Task is an end-to-end question\nanswering task on COVID-19 where systems are required to return relevant\ndocuments, snippets, and answers to a given question. Given the absence of\ntraining data, we used a query-focused summarisation system that was trained\nwith the BioASQ8b training data set and we experimented with methods to\nretrieve the documents and snippets. Considering the poor quality of the\ndocuments and snippets retrieved by our system, we observed reasonably good\nquality in the answers returned. For phase B of the BioASQ9b task, the relevant\ndocuments and snippets were already included in the test data. Our system split\nthe snippets into candidate sentences and used BERT variants under a sentence\nclassification setup. The system used the question and candidate sentence as\ninput and was trained to predict the likelihood of the candidate sentence being\npart of the ideal answer. The runs obtained either the best or second best\nROUGE-F1 results of all participants to all batches of BioASQ9b. This shows\nthat using BERT in a classification setup is a very strong baseline for the\nidentification of ideal answers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Molla_D/0/1/0/all/0/1\">Diego Moll&#xe1;</a> (1 and 2), <a href=\"http://arxiv.org/find/cs/1/au:+Khanna_U/0/1/0/all/0/1\">Urvashi Khanna</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Galat_D/0/1/0/all/0/1\">Dima Galat</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1\">Vincent Nguyen</a> (2 and 3) <a href=\"http://arxiv.org/find/cs/1/au:+Rybinski_M/0/1/0/all/0/1\">Maciej Rybinski</a> (3) ( (1) Macquarie University, (2) CSIRO Data61, (3) Australian National University)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Translation Error Detection as Rationale Extraction. (arXiv:2108.12197v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12197","description":"<p>Recent Quality Estimation (QE) models based on multilingual pre-trained\nrepresentations have achieved very competitive results when predicting the\noverall quality of translated sentences. Predicting translation errors, i.e.\ndetecting specifically which words are incorrect, is a more challenging task,\nespecially with limited amounts of training data. We hypothesize that, not\nunlike humans, successful QE models rely on translation errors to predict\noverall sentence quality. By exploring a set of feature attribution methods\nthat assign relevance scores to the inputs to explain model predictions, we\nstudy the behaviour of state-of-the-art sentence-level QE models and show that\nexplanations (i.e. rationales) extracted from these models can indeed be used\nto detect translation errors. We therefore (i) introduce a novel\nsemi-supervised method for word-level QE and (ii) propose to use the QE task as\na new benchmark for evaluating the plausibility of feature attribution, i.e.\nhow interpretable model explanations are to humans.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fomicheva_M/0/1/0/all/0/1\">Marina Fomicheva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features are dependent\nupon each other. Experiment results on five public datasets show that our model\nperforms significantly better than previous approaches. The source code can be\nfound in https://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring the Capacity of a Large-scale Masked Language Model to Recognize Grammatical Errors. (arXiv:2108.12216v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12216","description":"<p>In this paper, we explore the capacity of a language model-based method for\ngrammatical error detection in detail. We first show that 5 to 10% of training\ndata are enough for a BERT-based error detection method to achieve performance\nequivalent to a non-language model-based method can achieve with the full\ntraining data; recall improves much faster with respect to training data size\nin the BERT-based method than in the non-language model method while precision\nbehaves similarly. These suggest that (i) the BERT-based method should have a\ngood knowledge of grammar required to recognize certain types of error and that\n(ii) it can transform the knowledge into error detection rules by fine-tuning\nwith a few training samples, which explains its high generalization ability in\ngrammatical error detection. We further show with pseudo error data that it\nactually exhibits such nice properties in learning rules for recognizing\nvarious types of error. Finally, based on these findings, we explore a\ncost-effective method for detecting grammatical errors with feedback comments\nexplaining relevant grammatical rules to learners.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nagata_R/0/1/0/all/0/1\">Ryo Nagata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kimura_M/0/1/0/all/0/1\">Manabu Kimura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hanawa_K/0/1/0/all/0/1\">Kazuaki Hanawa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Injecting Text in Self-Supervised Speech Pretraining. (arXiv:2108.12226v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12226","description":"<p>Self-supervised pretraining for Automated Speech Recognition (ASR) has shown\nvaried degrees of success. In this paper, we propose to jointly learn\nrepresentations during pretraining from two different modalities: speech and\ntext. The proposed method, tts4pretrain complements the power of contrastive\nlearning in self-supervision with linguistic/lexical representations derived\nfrom synthesized speech, effectively learning from untranscribed speech and\nunspoken text. Lexical learning in the speech encoder is enforced through an\nadditional sequence loss term that is coupled with contrastive loss during\npretraining. We demonstrate that this novel pretraining method yields Word\nError Rate (WER) reductions of 10% relative on the well-benchmarked,\nLibrispeech task over a state-of-the-art baseline pretrained with wav2vec2.0\nonly. The proposed method also serves as an effective strategy to compensate\nfor the lack of transcribed speech, effectively matching the performance of\n5000 hours of transcribed speech with just 100 hours of transcribed speech on\nthe AMI meeting transcription task. Finally, we demonstrate WER reductions of\nup to 15% on an in-house Voice Search task over traditional pretraining.\nIncorporating text into encoder pretraining is complimentary to rescoring with\na larger or in-domain language model, resulting in additional 6% relative\nreduction in WER.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1\">Zhehuai Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_A/0/1/0/all/0/1\">Andrew Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ramabhadran_B/0/1/0/all/0/1\">Bhuvana Ramabhadran</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Gary Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moreno_P/0/1/0/all/0/1\">Pedro Moreno</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ProtoInfoMax: Prototypical Networks with Mutual Information Maximization for Out-of-Domain Detection. (arXiv:2108.12229v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12229","description":"<p>The ability to detect Out-of-Domain (OOD) inputs has been a critical\nrequirement in many real-world NLP applications since the inclusion of\nunsupported OOD inputs may lead to catastrophic failure of systems. However, it\nremains an empirical question whether current algorithms can tackle such\nproblem reliably in a realistic scenario where zero OOD training data is\navailable. In this study, we propose ProtoInfoMax, a new architecture that\nextends Prototypical Networks to simultaneously process In-Domain (ID) and OOD\nsentences via Mutual Information Maximization (InfoMax) objective. Experimental\nresults show that our proposed method can substantially improve performance up\nto 20% for OOD detection in low resource settings of text classification. We\nalso show that ProtoInfoMax is less prone to typical over-confidence Error of\nNeural Networks, leading to more reliable ID and OOD prediction outcomes.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nimah_I/0/1/0/all/0/1\">Iftitahu Ni&#x27;mah</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1\">Meng Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1\">Vlado Menkovski</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1\">Mykola Pechenizkiy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Evaluating the Robustness of Neural Language Models to Input Perturbations. (arXiv:2108.12237v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12237","description":"<p>High-performance neural language models have obtained state-of-the-art\nresults on a wide range of Natural Language Processing (NLP) tasks. However,\nresults for common benchmark datasets often do not reflect model reliability\nand robustness when applied to noisy, real-world data. In this study, we design\nand implement various types of character-level and word-level perturbation\nmethods to simulate realistic scenarios in which input texts may be slightly\nnoisy or different from the data distribution on which NLP systems were\ntrained. Conducting comprehensive experiments on different NLP tasks, we\ninvestigate the ability of high-performance language models such as BERT,\nXLNet, RoBERTa, and ELMo in handling different types of input perturbations.\nThe results suggest that language models are sensitive to input perturbations\nand their performance can decrease even when small changes are introduced. We\nhighlight that models need to be further improved and that current benchmarks\nare not reflecting model robustness well. We argue that evaluations on\nperturbed inputs should routinely complement widely-used benchmarks in order to\nyield a more realistic understanding of NLP systems robustness.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Deep learning models are not robust against noise in clinical text. (arXiv:2108.12242v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12242","description":"<p>Artificial Intelligence (AI) systems are attracting increasing interest in\nthe medical domain due to their ability to learn complicated tasks that require\nhuman intelligence and expert knowledge. AI systems that utilize\nhigh-performance Natural Language Processing (NLP) models have achieved\nstate-of-the-art results on a wide variety of clinical text processing\nbenchmarks. They have even outperformed human accuracy on some tasks. However,\nperformance evaluation of such AI systems have been limited to accuracy\nmeasures on curated and clean benchmark datasets that may not properly reflect\nhow robustly these systems can operate in real-world situations. In order to\naddress this challenge, we introduce and implement a wide variety of\nperturbation methods that simulate different types of noise and variability in\nclinical text data. While noisy samples produced by these perturbation methods\ncan often be understood by humans, they may cause AI systems to make erroneous\ndecisions. Conducting extensive experiments on several clinical text processing\ntasks, we evaluated the robustness of high-performance NLP models against\nvarious types of character-level and word-level noise. The results revealed\nthat the NLP models performance degrades when the input contains small amounts\nof noise. This study is a significant step towards exposing vulnerabilities of\nAI models utilized in clinical text processing systems. The proposed\nperturbation methods can be used in performance evaluation tests to assess how\nrobustly clinical NLP models can operate on noisy data, in real-world settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1\">Milad Moradi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blagec_K/0/1/0/all/0/1\">Kathrin Blagec</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1\">Matthias Samwald</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Detecting Propaganda on the Sentence Level during the COVID-19 Pandemic. (arXiv:2108.12269v1 [cs.CY])","link":"http://arxiv.org/abs/2108.12269","description":"<p>The spread of misinformation, conspiracy, and questionable content and\ninformation manipulation by foreign adversaries on social media has surged\nalong with the COVID-19 pandemic. Such malicious cyber-enabled actions may\ncause increasing social polarization, health crises, and property loss. In this\npaper, using fine-tuned contextualized embedding trained on Reddit, we tackle\nthe detection of the propaganda of such user accounts and their targeted issues\non Twitter during March 2020 when the COVID-19 epidemic became recognized as a\npandemic. Our result shows that the pro-China group appeared to be tweeting 35\nto 115 times more than the neutral group. At the same time, neutral groups were\ntweeting more positive-attitude content and voicing alarm for the COVID-19\nsituation. The pro-China group was also using more call-for-action words on\npolitical issues not necessarily China-related.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_R/0/1/0/all/0/1\">Rong-Ching Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1\">Chu-Hsing Lin</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Can the Transformer Be Used as a Drop-in Replacement for RNNs in Text-Generating GANs?. (arXiv:2108.12275v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12275","description":"<p>In this paper we address the problem of fine-tuned text generation with a\nlimited computational budget. For that, we use a well-performing text\ngenerative adversarial network (GAN) architecture - Diversity-Promoting GAN\n(DPGAN), and attempted a drop-in replacement of the LSTM layer with a\nself-attention-based Transformer layer in order to leverage their efficiency.\nThe resulting Self-Attention DPGAN (SADPGAN) was evaluated for performance,\nquality and diversity of generated text and stability. Computational\nexperiments suggested that a transformer architecture is unable to drop-in\nreplace the LSTM layer, under-performing during the pre-training phase and\nundergoing a complete mode collapse during the GAN tuning phase. Our results\nsuggest that the transformer architecture need to be adapted before it can be\nused as a replacement for RNNs in text-generating GANs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Blin_K/0/1/0/all/0/1\">Kevin Blin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kucharavy_A/0/1/0/all/0/1\">Andrei Kucharavy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree Decomposition Attention for AMR-to-Text Generation. (arXiv:2108.12300v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12300","description":"<p>Text generation from AMR requires mapping a semantic graph to a string that\nit annotates. Transformer-based graph encoders, however, poorly capture vertex\ndependencies that may benefit sequence prediction. To impose order on an\nencoder, we locally constrain vertex self-attention using a graph's tree\ndecomposition. Instead of forming a full query-key bipartite graph, we restrict\nattention to vertices in parent, subtree, and same-depth bags of a vertex. This\nhierarchical context lends both sparsity and structure to vertex state updates.\nWe apply dynamic programming to derive a forest of tree decompositions,\nchoosing the most structurally similar tree to the AMR. Our system outperforms\na self-attentive baseline by 1.6 BLEU and 1.8 chrF++.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lisa Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Tree Decomposition Parsers for AMR-to-Text Generation. (arXiv:2108.12304v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12304","description":"<p>Graph encoders in AMR-to-text generation models often rely on neighborhood\nconvolutions or global vertex attention. While these approaches apply to\ngeneral graphs, AMRs may be amenable to encoders that target their tree-like\nstructure. By clustering edges into a hierarchy, a tree decomposition\nsummarizes graph structure. Our model encodes a derivation forest of tree\ndecompositions and extracts an expected tree. From tree node embeddings, it\nbuilds graph edge features used in vertex attention of the graph encoder.\nEncoding TD forests instead of shortest-pairwise paths in a self-attentive\nbaseline raises BLEU by 0.7 and chrF++ by 0.3. The forest encoder also\nsurpasses a convolutional baseline for molecular property prediction by 1.92%\nROC-AUC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lisa Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CAPE: Context-Aware Private Embeddings for Private Language Learning. (arXiv:2108.12318v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12318","description":"<p>Deep learning-based language models have achieved state-of-the-art results in\na number of applications including sentiment analysis, topic labelling, intent\nclassification and others. Obtaining text representations or embeddings using\nthese models presents the possibility of encoding personally identifiable\ninformation learned from language and context cues that may present a risk to\nreputation or privacy. To ameliorate these issues, we propose Context-Aware\nPrivate Embeddings (CAPE), a novel approach which preserves privacy during\ntraining of embeddings. To maintain the privacy of text representations, CAPE\napplies calibrated noise through differential privacy, preserving the encoded\nsemantic links while obscuring sensitive information. In addition, CAPE employs\nan adversarial training regime that obscures identified private variables.\nExperimental results demonstrate that the proposed approach reduces private\ninformation leakage better than either single intervention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Plant_R/0/1/0/all/0/1\">Richard Plant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gkatzia_D/0/1/0/all/0/1\">Dimitra Gkatzia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Giuffrida_V/0/1/0/all/0/1\">Valerio Giuffrida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DomiKnowS: A Library for Integration of Symbolic Domain Knowledge in Deep Learning. (arXiv:2108.12370v1 [cs.LG])","link":"http://arxiv.org/abs/2108.12370","description":"<p>We demonstrate a library for the integration of domain knowledge in deep\nlearning architectures. Using this library, the structure of the data is\nexpressed symbolically via graph declarations and the logical constraints over\noutputs or latent variables can be seamlessly added to the deep models. The\ndomain knowledge can be defined explicitly, which improves the models'\nexplainability in addition to the performance and generalizability in the\nlow-data regime. Several approaches for such an integration of symbolic and\nsub-symbolic models have been introduced; however, there is no library to\nfacilitate the programming for such an integration in a generic way while\nvarious underlying algorithms can be used. Our library aims to simplify\nprogramming for such an integration in both training and inference phases while\nseparating the knowledge representation from learning algorithms. We showcase\nvarious NLP benchmark tasks and beyond. The framework is publicly available at\nGithub(https://github.com/HLR/DomiKnowS).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Faghihi_H/0/1/0/all/0/1\">Hossein Rajaby Faghihi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1\">Quan Guo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Uszok_A/0/1/0/all/0/1\">Andrzej Uszok</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nafar_A/0/1/0/all/0/1\">Aliakbar Nafar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Raisi_E/0/1/0/all/0/1\">Elaheh Raisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kordjamshidi_P/0/1/0/all/0/1\">Parisa Kordjamshidi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation. (arXiv:2108.12409v1 [cs.CL])","link":"http://arxiv.org/abs/2108.12409","description":"<p>Since the introduction of the transformer model by Vaswani et al. (2017), a\nfundamental question remains open: how to achieve extrapolation at inference\ntime to longer sequences than seen during training? We first show that\nextrapolation can be improved by changing the position representation method,\nthough we find that existing proposals do not allow efficient extrapolation. We\nintroduce a simple and efficient method, Attention with Linear Biases (ALiBi),\nthat allows for extrapolation. ALiBi does not add positional embeddings to the\nword embeddings; instead, it biases the query-key attention scores with a term\nthat is proportional to their distance. We show that this method allows\ntraining a 1.3 billion parameter model on input sequences of length 1024 that\nextrapolates to input sequences of length 2048, achieving the same perplexity\nas a sinusoidal position embedding model trained on inputs of length 2048, 11%\nfaster and using 11% less memory. ALiBi's inductive bias towards recency allows\nit to outperform multiple strong position methods on the WikiText-103\nbenchmark. Finally, we provide analysis of ALiBi to understand why it leads to\nbetter performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Press_O/0/1/0/all/0/1\">Ofir Press</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1\">Noah A. Smith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1\">Mike Lewis</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Investigating Software Usage in the Social Sciences: A Knowledge Graph Approach. (arXiv:2003.10715v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2003.10715","description":"<p>Knowledge about the software used in scientific investigations is necessary\nfor different reasons, including provenance of the results, measuring software\nimpact to attribute developers, and bibliometric software citation analysis in\ngeneral. Additionally, providing information about whether and how the software\nand the source code are available allows an assessment about the state and role\nof open source software in science in general. While such analyses can be done\nmanually, large scale analyses require the application of automated methods of\ninformation extraction and linking. In this paper, we present SoftwareKG - a\nknowledge graph that contains information about software mentions from more\nthan 51,000 scientific articles from the social sciences. A silver standard\ncorpus, created by a distant and weak supervision approach, and a gold standard\ncorpus, created by manual annotation, were used to train an LSTM based neural\nnetwork to identify software mentions in scientific articles. The model\nachieves a recognition rate of .82 F-score in exact matches. As a result, we\nidentified more than 133,000 software mentions. For entity disambiguation, we\nused the public domain knowledge base DBpedia. Furthermore, we linked the\nentities of the knowledge graph to other knowledge bases such as the Microsoft\nAcademic Knowledge Graph, the Software Ontology, and Wikidata. Finally, we\nillustrate, how SoftwareKG can be used to assess the role of software in the\nsocial sciences.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Schindler_D/0/1/0/all/0/1\">David Schindler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zapilko_B/0/1/0/all/0/1\">Benjamin Zapilko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kruger_F/0/1/0/all/0/1\">Frank Kr&#xfc;ger</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A review of on-device fully neural end-to-end automatic speech recognition algorithms. (arXiv:2012.07974v3 [cs.LG] UPDATED)","link":"http://arxiv.org/abs/2012.07974","description":"<p>In this paper, we review various end-to-end automatic speech recognition\nalgorithms and their optimization techniques for on-device applications.\nConventional speech recognition systems comprise a large number of discrete\ncomponents such as an acoustic model, a language model, a pronunciation model,\na text-normalizer, an inverse-text normalizer, a decoder based on a Weighted\nFinite State Transducer (WFST), and so on. To obtain sufficiently high speech\nrecognition accuracy with such conventional speech recognition systems, a very\nlarge language model (up to 100 GB) is usually needed. Hence, the corresponding\nWFST size becomes enormous, which prohibits their on-device implementation.\nRecently, fully neural network end-to-end speech recognition algorithms have\nbeen proposed. Examples include speech recognition systems based on\nConnectionist Temporal Classification (CTC), Recurrent Neural Network\nTransducer (RNN-T), Attention-based Encoder-Decoder models (AED), Monotonic\nChunk-wise Attention (MoChA), transformer-based speech recognition systems, and\nso on. These fully neural network-based systems require much smaller memory\nfootprints compared to conventional algorithms, therefore their on-device\nimplementation has become feasible. In this paper, we review such end-to-end\nspeech recognition models. We extensively discuss their structures,\nperformance, and advantages compared to conventional algorithms.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1\">Chanwoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gowda_D/0/1/0/all/0/1\">Dhananjaya Gowda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1\">Dongsoo Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1\">Jiyeon Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1\">Ankur Kumar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1\">Sungsoo Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1\">Abhinav Garg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1\">Changwoo Han</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Modeling Disclosive Transparency in NLP Application Descriptions. (arXiv:2101.00433v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2101.00433","description":"<p>Broader disclosive transparency$-$truth and clarity in communication\nregarding the function of AI systems$-$is widely considered desirable.\nUnfortunately, it is a nebulous concept, difficult to both define and quantify.\nThis is problematic, as previous work has demonstrated possible trade-offs and\nnegative consequences to disclosive transparency, such as a confusion effect,\nwhere 'too much information' clouds a reader's understanding of what a system\ndescription means. Disclosive transparency's subjective nature has rendered\ndeep study into these problems and their remedies difficult. To improve this\nstate of affairs, We introduce neural language model-based probabilistic\nmetrics to directly model disclosive transparency, and demonstrate that they\ncorrelate with user and expert opinions of system transparency, making them a\nvalid objective proxy. Finally, we demonstrate the use of these metrics in a\npilot study quantifying the relationships between transparency, confusion, and\nuser perceptions in a corpus of real NLP system descriptions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Saxon_M/0/1/0/all/0/1\">Michael Saxon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Levy_S/0/1/0/all/0/1\">Sharon Levy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xinyi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Albalak_A/0/1/0/all/0/1\">Alon Albalak</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">William Yang Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Contextual Semi-Supervised Learning: An Approach To Leverage Air-Surveillance and Untranscribed ATC Data in ASR Systems. (arXiv:2104.03643v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.03643","description":"<p>Air traffic management and specifically air-traffic control (ATC) rely mostly\non voice communications between Air Traffic Controllers (ATCos) and pilots. In\nmost cases, these voice communications follow a well-defined grammar that could\nbe leveraged in Automatic Speech Recognition (ASR) technologies. The callsign\nused to address an airplane is an essential part of all ATCo-pilot\ncommunications. We propose a two-steps approach to add contextual knowledge\nduring semi-supervised training to reduce the ASR system error rates at\nrecognizing the part of the utterance that contains the callsign. Initially, we\nrepresent in a WFST the contextual knowledge (i.e. air-surveillance data) of an\nATCo-pilot communication. Then, during Semi-Supervised Learning (SSL) the\ncontextual knowledge is added by second-pass decoding (i.e. lattice\nre-scoring). Results show that `unseen domains' (e.g. data from airports not\npresent in the supervised training data) are further aided by contextual SSL\nwhen compared to standalone SSL. For this task, we introduce the Callsign Word\nError Rate (CA-WER) as an evaluation metric, which only assesses ASR\nperformance of the spoken callsign in an utterance. We obtained a 32.1% CA-WER\nrelative improvement applying SSL with an additional 17.5% CA-WER improvement\nby adding contextual knowledge during SSL on a challenging ATC-based test set\ngathered from LiveATC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zuluaga_Gomez_J/0/1/0/all/0/1\">Juan Zuluaga-Gomez</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nigmatulina_I/0/1/0/all/0/1\">Iuliia Nigmatulina</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Prasad_A/0/1/0/all/0/1\">Amrutha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Motlicek_P/0/1/0/all/0/1\">Petr Motlicek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vesely_K/0/1/0/all/0/1\">Karel Vesel&#xfd;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kocour_M/0/1/0/all/0/1\">Martin Kocour</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Szoke_I/0/1/0/all/0/1\">Igor Sz&#xf6;ke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learning to Remove: Towards Isotropic Pre-trained BERT Embedding. (arXiv:2104.05274v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.05274","description":"<p>Pre-trained language models such as BERT have become a more common choice of\nnatural language processing (NLP) tasks. Research in word representation shows\nthat isotropic embeddings can significantly improve performance on downstream\ntasks. However, we measure and analyze the geometry of pre-trained BERT\nembedding and find that it is far from isotropic. We find that the word vectors\nare not centered around the origin, and the average cosine similarity between\ntwo random words is much higher than zero, which indicates that the word\nvectors are distributed in a narrow cone and deteriorate the representation\ncapacity of word embedding. We propose a simple, and yet effective method to\nfix this problem: remove several dominant directions of BERT embedding with a\nset of learnable weights. We train the weights on word similarity tasks and\nshow that processed embedding is more isotropic. Our method is evaluated on\nthree standardized tasks: word similarity, word analogy, and semantic textual\nsimilarity. In all tasks, the word embedding processed by our method\nconsistently outperforms the original embedding (with average improvement of\n13% on word analogy and 16% on semantic textual similarity) and two baseline\nmethods. Our method is also proven to be more robust to changes of\nhyperparameter.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yuxin Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_R/0/1/0/all/0/1\">Rui Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1\">Jie Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1\">Jie Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Ling Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NAREOR: The Narrative Reordering Problem. (arXiv:2104.06669v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.06669","description":"<p>We propose the task of Narrative Reordering (NAREOR) which involves rewriting\na given story in a different narrative order while preserving its plot. We\npresent a dataset, NAREORC, with human rewritings of stories within ROCStories\nin non-linear orders, and conduct a detailed analysis of it. Further, we\npropose novel task-specific training methods with suitable evaluation metrics.\nWe perform experiments on NAREORC using state-of-the-art models such as BART\nand T5 and conduct extensive automatic and human evaluations. We demonstrate\nthat NAREOR is a challenging task with potential for further exploration.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gangal_V/0/1/0/all/0/1\">Varun Gangal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1\">Steven Y. Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hovy_E/0/1/0/all/0/1\">Eduard Hovy</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Surface Form Competition: Why the Highest Probability Answer Isn't Always Right. (arXiv:2104.08315v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08315","description":"<p>Large language models have shown promising results in zero-shot settings\n(Brown et al.,2020; Radford et al., 2019). For example, they can perform\nmultiple choice tasks simply by conditioning on a question and selecting the\nanswer with the highest probability.\n</p>\n<p>However, ranking by string probability can be problematic due to surface form\ncompetition-wherein different surface forms compete for probability mass, even\nif they represent the same underlying concept, e.g. \"computer\" and \"PC.\" Since\nprobability mass is finite, this lowers the probability of the correct answer,\ndue to competition from other strings that are valid answers (but not one of\nthe multiple choice options).\n</p>\n<p>We introduce Domain Conditional Pointwise Mutual Information, an alternative\nscoring function that directly compensates for surface form competition by\nsimply reweighing each option according to a term that is proportional to its a\npriori likelihood within the context of the specific zero-shot task. It\nachieves consistent gains in zero-shot performance over both calibrated (Zhao\net al., 2021) and uncalibrated scoring functions on all GPT-2 and GPT-3 models\nover a variety of multiple choice datasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Holtzman_A/0/1/0/all/0/1\">Ari Holtzman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1\">Peter West</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shwartz_V/0/1/0/all/0/1\">Vered Shwartz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zettlemoyer_L/0/1/0/all/0/1\">Luke Zettlemoyer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Learn Continually, Generalize Rapidly: Lifelong Knowledge Accumulation for Few-shot Learning. (arXiv:2104.08808v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08808","description":"<p>The ability to continuously expand knowledge over time and utilize it to\nrapidly generalize to new tasks is a key feature of human linguistic\nintelligence. Existing models that pursue rapid generalization to new tasks\n(e.g., few-shot learning methods), however, are mostly trained in a single shot\non fixed datasets, unable to dynamically expand their knowledge; while\ncontinual learning algorithms are not specifically designed for rapid\ngeneralization. We present a new learning setup, Continual Learning of Few-Shot\nLearners (CLIF), to address the challenges of both learning settings in a\nunified setup. CLIF assumes a model learns from a sequence of diverse NLP tasks\narriving sequentially, accumulating knowledge for improved generalization to\nnew tasks, while also retaining performance on the tasks learned earlier. We\nexamine how the generalization ability is affected in the continual learning\nsetup, evaluate a number of continual learning algorithms, and propose a novel\nregularized adapter generation approach. We find that catastrophic forgetting\naffects generalization ability to a less degree than performance on seen tasks;\nwhile continual learning algorithms can still bring considerable benefit to the\ngeneralization ability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1\">Xisen Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1\">Mohammad Rostami</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SciCo: Hierarchical Cross-Document Coreference for Scientific Concepts. (arXiv:2104.08809v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08809","description":"<p>Determining coreference of concept mentions across multiple documents is a\nfundamental task in natural language understanding. Work on cross-document\ncoreference resolution (CDCR) typically considers mentions of events in the\nnews, which seldom involve abstract technical concepts that are prevalent in\nscience and technology. These complex concepts take diverse or ambiguous forms\nand have many hierarchical levels of granularity (e.g., tasks and subtasks),\nposing challenges for CDCR. We present a new task of Hierarchical CDCR (H-CDCR)\nwith the goal of jointly inferring coreference clusters and hierarchy between\nthem. We create SciCo, an expert-annotated dataset for H-CDCR in scientific\npapers, 3X larger than the prominent ECB+ resource. We study strong baseline\nmodels that we customize for H-CDCR, and highlight challenges for future work.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Cattan_A/0/1/0/all/0/1\">Arie Cattan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Johnson_S/0/1/0/all/0/1\">Sophie Johnson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1\">Daniel Weld</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dagan_I/0/1/0/all/0/1\">Ido Dagan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Beltagy_I/0/1/0/all/0/1\">Iz Beltagy</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1\">Doug Downey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1\">Tom Hope</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutXLM: Multimodal Pre-training for Multilingual Visually-rich Document Understanding. (arXiv:2104.08836v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08836","description":"<p>Multimodal pre-training with text, layout, and image has achieved SOTA\nperformance for visually-rich document understanding tasks recently, which\ndemonstrates the great potential for joint learning across different\nmodalities. In this paper, we present LayoutXLM, a multimodal pre-trained model\nfor multilingual document understanding, which aims to bridge the language\nbarriers for visually-rich document understanding. To accurately evaluate\nLayoutXLM, we also introduce a multilingual form understanding benchmark\ndataset named XFUND, which includes form understanding samples in 7 languages\n(Chinese, Japanese, Spanish, French, Italian, German, Portuguese), and\nkey-value pairs are manually labeled for each language. Experiment results show\nthat the LayoutXLM model has significantly outperformed the existing SOTA\ncross-lingual pre-trained models on the XFUND dataset. The pre-trained\nLayoutXLM model and the XFUND dataset are publicly available at\nhttps://aka.ms/layoutxlm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_T/0/1/0/all/0/1\">Tengchao Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1\">Guoxin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1\">Yijuan Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Florencio_D/0/1/0/all/0/1\">Dinei Florencio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Cha Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Acoustic Data-Driven Subword Modeling for End-to-End Speech Recognition. (arXiv:2104.09106v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09106","description":"<p>Subword units are commonly used for end-to-end automatic speech recognition\n(ASR), while a fully acoustic-oriented subword modeling approach is somewhat\nmissing. We propose an acoustic data-driven subword modeling (ADSM) approach\nthat adapts the advantages of several text-based and acoustic-based subword\nmethods into one pipeline. With a fully acoustic-oriented label design and\nlearning process, ADSM produces acoustic-structured subword units and\nacoustic-matched target sequence for further ASR training. The obtained ADSM\nlabels are evaluated with different end-to-end ASR approaches including CTC,\nRNN-Transducer and attention models. Experiments on the LibriSpeech corpus show\nthat ADSM clearly outperforms both byte pair encoding (BPE) and\npronunciation-assisted subword modeling (PASM) in all cases. Detailed analysis\nshows that ADSM achieves acoustically more logical word segmentation and more\nbalanced sequence length, and thus, is suitable for both time-synchronous and\nlabel-synchronous models. We also briefly describe how to apply acoustic-based\nsubword regularization and unseen text segmentation using ADSM.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1\">Wei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeineldeen_M/0/1/0/all/0/1\">Mohammad Zeineldeen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1\">Zuoyun Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Schluter_R/0/1/0/all/0/1\">Ralf Schl&#xfc;ter</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1\">Hermann Ney</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-Guided Curriculum Learning for Neural Machine Translation. (arXiv:2105.04475v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.04475","description":"<p>In the field of machine learning, the well-trained model is assumed to be\nable to recover the training labels, i.e. the synthetic labels predicted by the\nmodel should be as close to the ground-truth labels as possible. Inspired by\nthis, we propose a self-guided curriculum strategy to encourage the learning of\nneural machine translation (NMT) models to follow the above recovery criterion,\nwhere we cast the recovery degree of each training example as its learning\ndifficulty. Specifically, we adopt the sentence level BLEU score as the proxy\nof recovery degree. Different from existing curricula relying on linguistic\nprior knowledge or third-party language models, our chosen learning difficulty\nis more suitable to measure the degree of knowledge mastery of the NMT models.\nExperiments on translation benchmarks, including WMT14\nEnglish$\\Rightarrow$German and WMT17 Chinese$\\Rightarrow$English, demonstrate\nthat our approach can consistently improve translation performance against\nstrong baseline Transformer.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1\">Lei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1\">Liang Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1\">Kevin Duh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1\">Shinji Watanabe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sasano_R/0/1/0/all/0/1\">Ryohei Sasano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1\">Koichi Takeda</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Twitter User Representation using Weakly Supervised Graph Embedding. (arXiv:2108.08988v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.08988","description":"<p>Social media platforms provide convenient means for users to participate in\nmultiple online activities on various contents and create fast widespread\ninteractions. However, this rapidly growing access has also increased the\ndiverse information, and characterizing user types to understand people's\nlifestyle decisions shared in social media is challenging. In this paper, we\npropose a weakly supervised graph embedding based framework for understanding\nuser types. We evaluate the user embedding learned using weak supervision over\nwell-being related tweets from Twitter, focusing on 'Yoga', 'Keto diet'.\nExperiments on real-world datasets demonstrate that the proposed framework\noutperforms the baselines for detecting user types. Finally, we illustrate data\nanalysis on different types of users (e.g., practitioner vs. promotional) from\nour dataset. While we focus on lifestyle-related tweets (i.e., yoga, keto), our\nmethod for constructing user representation readily generalizes to other\ndomains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1\">Tunazzina Islam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Semantic Communication with Adaptive Universal Transformer. (arXiv:2108.09119v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09119","description":"<p>With the development of deep learning (DL), natural language processing (NLP)\nmakes it possible for us to analyze and understand a large amount of language\ntexts. Accordingly, we can achieve a semantic communication in terms of joint\nsemantic source and channel coding over a noisy channel with the help of NLP.\nHowever, the existing method to realize this goal is to use a fixed transformer\nof NLP while ignoring the difference of semantic information contained in each\nsentence. To solve this problem, we propose a new semantic communication system\nbased on Universal Transformer. Compared with the traditional transformer, an\nadaptive circulation mechanism is introduced in the Universal Transformer.\nThrough the introduction of the circulation mechanism, the new semantic\ncommunication system can be more flexible to transmit sentences with different\nsemantic information, and achieve better end-to-end performance under various\nchannel conditions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1\">Qingyang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1\">Rongpeng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1\">Zhifeng Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1\">Chenghui Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Honggang Zhang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Relation Extraction from Tables using Artificially Generated Metadata. (arXiv:2108.10750v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10750","description":"<p>Relation Extraction (RE) from tables is the task of identifying relations\nbetween pairs of columns of a table. Generally, RE models for this task require\nlabelled tables for training. These labelled tables can also be generated\nartificially from a Knowledge Graph (KG), which makes the cost to acquire them\nmuch lower in comparison to manual annotations. However, unlike real tables,\nthese synthetic tables lack associated metadata, such as, column-headers,\ncaptions, etc; this is because synthetic tables are created out of KGs that do\nnot store such metadata. Meanwhile, previous works have shown that metadata is\nimportant for accurate RE from tables. To address this issue, we propose\nmethods to artificially create some of this metadata for synthetic tables.\nAfterward, we experiment with a BERT-based model, in line with recently\npublished works, that takes as input a combination of proposed artificial\nmetadata and table content. Our empirical results show that this leads to an\nimprovement of 9\\%-45\\% in F1 score, in absolute terms, over 2 tabular\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+singh_G/0/1/0/all/0/1\">Gaurav singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1\">Siffi Singh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1\">Joshua Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Saffari_A/0/1/0/all/0/1\">Amir Saffari</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Offensive Language Identification for Tamil Code-Mixed YouTube Comments and Posts. (arXiv:2108.10939v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.10939","description":"<p>Offensive Language detection in social media platforms has been an active\nfield of research over the past years. In non-native English spoken countries,\nsocial media users mostly use a code-mixed form of text in their\nposts/comments. This poses several challenges in the offensive content\nidentification tasks, and considering the low resources available for Tamil,\nthe task becomes much harder. The current study presents extensive experiments\nusing multiple deep learning, and transfer learning models to detect offensive\ncontent on YouTube. We propose a novel and flexible approach of selective\ntranslation and transliteration techniques to reap better results from\nfine-tuning and ensembling multilingual transformer networks like BERT, Distil-\nBERT, and XLM-RoBERTa. The experimental results showed that ULMFiT is the best\nmodel for this task. The best performing models were ULMFiT and mBERTBiLSTM for\nthis Tamil code-mix dataset instead of more popular transfer learning models\nsuch as Distil- BERT and XLM-RoBERTa and hybrid deep learning models. The\nproposed model ULMFiT and mBERTBiLSTM yielded good results and are promising\nfor effective offensive speech identification in low-resourced languages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vasantharajan_C/0/1/0/all/0/1\">Charangan Vasantharajan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Thayasivam_U/0/1/0/all/0/1\">Uthayasanker Thayasivam</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LayoutReader: Pre-training of Text and Layout for Reading Order Detection. (arXiv:2108.11591v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11591","description":"<p>Reading order detection is the cornerstone to understanding visually-rich\ndocuments (e.g., receipts and forms). Unfortunately, no existing work took\nadvantage of advanced deep learning models because it is too laborious to\nannotate a large enough dataset. We observe that the reading order of WORD\ndocuments is embedded in their XML metadata; meanwhile, it is easy to convert\nWORD documents to PDFs or images. Therefore, in an automated manner, we\nconstruct ReadingBank, a benchmark dataset that contains reading order, text,\nand layout information for 500,000 document images covering a wide spectrum of\ndocument types. This first-ever large-scale dataset unleashes the power of deep\nneural networks for reading order detection. Specifically, our proposed\nLayoutReader captures the text and layout information for reading order\nprediction using the seq2seq model. It performs almost perfectly in reading\norder detection and significantly improves both open-source and commercial OCR\nengines in ordering text lines in their results in our experiments. We will\nrelease the dataset and model at \\url{https://aka.ms/layoutreader}.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1\">Zilong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yiheng Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1\">Lei Cui</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1\">Jingbo Shang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1\">Furu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Rethinking Negative Sampling for Unlabeled Entity Problem in Named Entity Recognition. (arXiv:2108.11607v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.11607","description":"<p>In many situations (e.g., distant supervision), unlabeled entity problem\nseriously degrades the performances of named entity recognition (NER) models.\nRecently, this issue has been well addressed by a notable approach based on\nnegative sampling. In this work, we perform two studies along this direction.\nFirstly, we analyze why negative sampling succeeds both theoretically and\nempirically. Based on the observation that named entities are highly sparse in\ndatasets, we show a theoretical guarantee that, for a long sentence, the\nprobability of containing no unlabeled entities in sampled negatives is high.\nMissampling tests on synthetic datasets have verified our guarantee in\npractice. Secondly, to mine hard negatives and further reduce missampling\nrates, we propose a weighted and adaptive sampling distribution for negative\nsampling. Experiments on synthetic datasets and well-annotated datasets show\nthat our method significantly improves negative sampling in robustness and\neffectiveness. We also have achieved new state-of-the-art results on real-world\ndatasets.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yangming Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1\">Lemao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1\">Shuming Shi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-08-29T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","syn":"http://purl.org/rss/1.0/modules/syndication/","content":"http://purl.org/rss/1.0/modules/content/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","taxo":"http://purl.org/rss/1.0/modules/taxonomy/"}}]}]}