{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.1","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-04T01:51:47.746750354Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"arghï¼šåŸºäº  derive å®ä¸”å¯¹äºŒè¿›åˆ¶ä½“ç§¯è¿›è¡Œä¼˜åŒ–çš„å‘½ä»¤è¡Œè§£æå·¥å…·","link":"https://rustcc.cn/article?id=01a3db1f-b567-47d4-9c3b-08142a11cd3e","description":"<blockquote>\n<p>Derive-based argument parsing optimized for code size and conformance to the Fuchsia commandline tools specification.</p>\n<p>åŸºäº derive å®çš„å‚æ•°è§£æå·¥å…·ï¼Œé’ˆå¯¹ä»£ç å¤§å°è¿›è¡Œäº†ä¼˜åŒ–ï¼Œå¹¶ä¸”éµå¾ª Fuchsia å‘½ä»¤è¡Œå·¥å…·è§„èŒƒã€‚</p>\n</blockquote>\n<p>repoï¼š<a href=\"https://github.com/google/argh\" rel=\"noopener noreferrer\">https://github.com/google/argh</a></p>\n<p>ç”± Google å¼€å‘è€…ç¼–å†™ï¼Œä½†å¹¶é Google å®˜æ–¹æ”¯æŒã€‚</p>\n<p>å®˜æ–¹ç»™çš„åŸºæœ¬ä¾‹å­ï¼š</p>\n<pre><code>use argh::FromArgs;\n\n#[derive(FromArgs)]\n/// Reach new heights.\nstruct GoUp {\n    /// whether or not to jump\n    #[argh(switch, short = 'j')]\n    jump: bool,\n\n    /// how high to go\n    #[argh(option)]\n    height: usize,\n\n    /// an optional nickname for the pilot\n    #[argh(option)]\n    pilot_nickname: Option&lt;String&gt;,\n}\n\nfn main() {\n    let up: GoUp = argh::from_env();\n}\n</code></pre>\n<pre><code>Usage: cmdname [-j] --height &lt;height&gt; [--pilot-nickname &lt;pilot-nickname&gt;]\n\nReach new heights.\n\nOptions:\n  -j, --jump        whether or not to jump\n  --height          how high to go\n  --pilot-nickname  an optional nickname for the pilot\n  --help            display usage information\n</code></pre>\n<p>è¿‡ç¨‹å®-å‚æ•°ç±»å‹ï¼š</p>\n<ul>\n<li><code>switch</code>ï¼šç”¨åœ¨ bool ç±»å‹çš„å­—æ®µä¸Šï¼Œè¡¨æ˜å‘½ä»¤è¡Œå‚æ•°æ˜¯å¯é€‰çš„ï¼Œè€Œä¸”ä¸€æ—¦æä¾›è¯¥å‘½ä»¤è¡Œå‚æ•°ï¼Œåˆ™ç»™è¯¥å­—æ®µçš„å€¼èµ‹ç»™ true ã€‚</li>\n<li><code>option</code>ï¼š\n<ul>\n<li>ç”¨åœ¨ <code>Option</code> ç±»å‹ä¸Šï¼Œè¡¨æ˜å‘½ä»¤è¡Œå‚æ•°æ˜¯å¯é€‰çš„ã€‚</li>\n<li>ç”¨åœ¨ <code>Vec</code> ç±»å‹ä¸Šï¼Œè¡¨æ˜å‘½ä»¤è¡Œå‚æ•°å¯é€‰ï¼Œè€Œä¸”å¯ä»¥é‡å¤å‡ºç°ï¼Œå³è¿™ä¸ªå‚æ•°åŠå…¶å€¼å¯ä»¥åœ¨å‘½ä»¤è¡Œä¸­å‡ºç° 0 æ¬¡æˆ–æ›´å¤šæ¬¡ã€‚</li>\n<li>ç”¨åœ¨é <code>Option</code> ã€é <code>Vec</code> ç±»å‹ä¸Šï¼Œåˆ™è¡¨ç¤ºå‘½ä»¤è¡Œå‚æ•°å¿…é€‰ã€‚</li>\n</ul>\n</li>\n<li><code>positional</code>ï¼šä½ç½®å‚æ•°ï¼Œè¡¨æ˜æŒ‰ç…§ç»“æ„ä½“å£°æ˜çš„å­—æ®µé¡ºåºè§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œæ— éœ€ <code>--xx value</code> çš„ <code>--xx</code> ã€‚æœ€åä¸€ä¸ªä½ç½®å‚æ•°å¯ä»¥åŒ…å«é»˜è®¤å€¼ï¼Œä¹Ÿå¯ä»¥åŒ…è£…åœ¨ Option æˆ– Vec ä¸­æ¥æ¥æ”¶å¯é€‰ï¼ˆæŒ‡ 0 æˆ– 1 ä¸ªï¼‰æˆ–é‡å¤ï¼ˆæŒ‡ 0 æˆ–å¤šä¸ªï¼‰çš„ä½ç½®å‚æ•°ã€‚</li>\n<li><code>subcommand</code>ï¼šéœ€å®šä¹‰ä¸€ä¸ªé¡¶å±‚ç»“æ„ä½“ã€ä¸€ä¸ªè¡¨ç¤ºå­å‘½ä»¤çš„æšä¸¾ä½“ï¼ˆè¿™ä¸ªæšä¸¾ä½“åˆ—ä¸¾æ‰€æœ‰å­å‘½ä»¤ï¼Œå­å‘½ä»¤ä»¥ç»“æ„ä½“å½¢å¼å‘ˆç°ï¼Œå­å‘½ä»¤ç»“æ„ä½“è¿˜éœ€è¦ name è®¾ç½®åç§°ï¼‰</li>\n</ul>\n<p>è¿‡ç¨‹å®-å…¶ä»–è®¾ç½®ï¼š</p>\n<ul>\n<li><code>short = 'a'</code>ï¼šè§£æ <code>-a</code> å½¢å¼çš„ç®€çŸ­å‚æ•°ï¼Œåªæ”¯æŒ ascii çš„ <code>Char</code> ç±»å‹ï¼Œæ¯”å¦‚å¤§å°å†™ã€æ•°å­—ã€‚</li>\n<li><code>long = \"xx-xx\"</code>ï¼šé‡æ–°å‘½åè¿™ä¸ªå­—æ®µçš„å‚æ•°åç§°ï¼Œç”±æ­¤å¯å…è®¸å‚æ•°åç§°å¸¦è¿å­—ç¬¦ <code>--xx-xx</code>ã€‚è¿™ä¸ªè®¾ç½®çš„é»˜è®¤å€¼ä¸ºå­—æ®µåç§°ï¼Œåªæ”¯æŒ ascii å°å†™å½¢å¼çš„åç§°ï¼Œä¸æ”¯æŒå¤§å†™å’Œæ•°å­—ã€‚</li>\n<li><code>default = \"default_height()\")</code>ã€<code>default = \"String::from(\\\"only up\\\")\")</code>ï¼šé»˜è®¤å€¼ï¼Œå¼•å·å†…å¯ä»¥æ˜¯å‡½æ•°åï¼ˆå¸¦æ‹¬å·ï¼‰ã€è¡¨è¾¾å¼</li>\n<li><code>from_str_fn(always_five)</code>ï¼šé’ˆå¯¹æŸä¸ªè§£æçš„å‚æ•°è¿›è¡Œè‡ªå®šä¹‰å¤„ç†ï¼Œ<code>always_five</code> çš„å‡½æ•°ç­¾åæ–¹å¼ä¸º <code>fn(&amp;str) -&gt; Result&lt;T, String&gt;</code></li>\n<li><code>description = \"xxxxx\"</code>ï¼šç»™å‚æ•°æ·»åŠ å¸®åŠ©ä¿¡æ¯ã€‚<code>///</code> æ–‡æ¡£æ³¨é‡Šä¹Ÿå¯ä»¥æä¾›ç”¨å¸®åŠ©ä¿¡æ¯ï¼Œè€Œ <code>description</code> çš„å†…å®¹åœ¨å‘½ä»¤è¡Œå¸®åŠ©ä¿¡æ¯é‡Œä¼šè¦†ç›–æ‰ <code>///</code> æä¾›çš„ä¿¡æ¯ã€‚æ³¨æ„ï¼šæ¢è¡Œå’Œç©ºæ¢è¡Œä¼šåœ¨ --help ä¿¡æ¯é‡Œå˜æˆä¸€ä¸ªç©ºæ ¼ï¼›æè¿°ä¿¡æ¯ä¸èƒ½è¿‡é•¿ï¼Œå¦åˆ™ä¼šå‡ºç° <code>error: invalid reference to positional arguments 4 and 5 (there is 1 argument</code> ï¼ˆè¿™ä¸ªæŠ¥é”™ä¿¡æ¯ä¸å‡†ç¡®ï¼Œæˆ‘ä¹Ÿæ˜¯æ’æŸ¥äº†å¾ˆä¹…æ‰å‘ç°ï¼‰ã€‚</li>\n</ul>\n<p>traitï¼š</p>\n<ul>\n<li><code>FromArgs</code> traitï¼šç”¨äº argh å‘½ä»¤è¡Œè§£æçš„æ‰€æœ‰ç»“æ„ä½“å’Œæšä¸¾ä½“ï¼Œéƒ½å¿…é¡» derive è¿™ä¸ª trait ã€‚</li>\n<li><code>FromArgValue</code> traitï¼šç”¨äº argh å‘½ä»¤è¡Œè§£æçš„ç»“æ„ä½“å­—æ®µçš„ç±»å‹å¿…é¡»å®ç°è¿™ä¸ª trait ï¼Œargh å·²ç»ç»™æ‰€æœ‰å®ç° <code>FromStr</code> trait çš„ç±»å‹å®ç°äº†è¿™ä¸ª trait ã€‚std çš„åŸºç¡€ç±»å‹éƒ½å®ç°äº† <code>FromStr</code> trait ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥ä½¿ç”¨ std çš„åŸºç¡€ç±»å‹ï¼›è‡ªå®šä¹‰ç±»å‹éœ€è¦å®ç° <code>FromStr</code> trait å’Œ <code>FromArgValue</code> trait ã€‚</li>\n</ul>\n<p>ä¼˜ç‚¹ï¼š</p>\n<ul>\n<li>ä½¿ç”¨ç®€å•è€Œç›´è§‚ï¼Œä¸Šæ‰‹å¿«ï¼Œé€‚ç”¨äºåŸºç¡€çš„å‘½ä»¤è¡Œè§£æåœºæ™¯</li>\n<li>ç”Ÿæˆçš„ä½“ç§¯æ¯” clap å°</li>\n<li>ä¾èµ–å°‘ï¼Œç¼–è¯‘é€Ÿåº¦å¿«</li>\n<li>æ”¯æŒ unicode</li>\n</ul>\n<p>ç¼ºç‚¹ï¼š</p>\n<ul>\n<li>ç»ˆç«¯è¾“å‡ºç»“æœéå½©è‰²</li>\n<li>é»˜è®¤ä¸æ”¯æŒå¾ˆé•¿çš„ help ä¿¡æ¯ï¼›åªæ”¯æŒ <code>--help</code>  ä¸æ”¯æŒ <code>-h</code> ï¼ˆä½†æ˜¯ä¹Ÿå¸¦æ¥ä¼˜ç‚¹â€”â€”å¯ä»¥è‡ªå®šä¹‰ä¸€ä¸ªå­—æ®µï¼Œshort as <code>-h</code>ï¼Œä»è€Œæœ‰ä¸€ä»½é»˜è®¤ç®€æ´çš„ help infoï¼Œåˆæœ‰ä¸€ä»½å®Œå…¨è‡ªå®šä¹‰çš„ infoï¼Œæ¯”å¦‚ <code>#[argh(option, short = 'h')] description: Vec&lt;String&gt;</code> =&gt; <code>cmd -h arg1 arg2</code> å°±å¯ä»¥æ˜¾ç¤º arg1 å’Œ arg2 çš„è¯´æ˜ï¼‰</li>\n<li>åªæ”¯æŒ <code>--option value</code> å’Œ <code>-o value</code>ï¼Œä¸æ”¯æŒ <code>--option=value</code> å’Œ <code>-ovalue</code></li>\n</ul>\n<p>å…¶ä»– args-parserï¼š</p>\n<blockquote>\n<ul>\n<li><a href=\"https://github.com/blyxxyz/lexopt\" rel=\"noopener noreferrer\">lexopt</a>ï¼šé›¶ä¾èµ–ã€æ³¨é‡æ­£ç¡®æ€§çš„æç®€ args-parser ã€‚</li>\n<li><a href=\"https://github.com/clap-rs/clap\" rel=\"noopener noreferrer\"><code>clap</code></a>/<a href=\"https://github.com/TeXitoi/structopt\" rel=\"noopener noreferrer\"><code>structopt</code></a>: very fully-featured. The only other argument parser for Rust I know of that truly handles invalid unicode properly, if used right. Large.</li>\n<li><a href=\"https://github.com/google/argh\" rel=\"noopener noreferrer\"><code>argh</code></a> and <a href=\"https://github.com/murarth/gumdrop\" rel=\"noopener noreferrer\"><code>gumdrop</code></a>: much leaner, yet still convenient and powerful enough for most purposes. Panic on invalid unicode.\n<ul>\n<li><code>argh</code> adheres to the <a href=\"https://fuchsia.dev/fuchsia-src/concepts/api/cli#command_line_arguments\" rel=\"noopener noreferrer\">Fuchsia specification</a> and therefore does <em>not</em> support <code>--option=value</code> and <code>-ovalue</code>, only <code>--option value</code> and <code>-o value</code>.</li>\n</ul>\n</li>\n<li><a href=\"https://github.com/RazrFalcon/pico-args\" rel=\"noopener noreferrer\"><code>pico-args</code></a>: slightly smaller than lexopt and easier to use (but less rigorous).</li>\n<li><a href=\"https://docs.rs/ap\" rel=\"noopener noreferrer\"><code>ap</code></a>: I have not used this, but it seems to support iterative parsing while being less bare-bones than lexopt.</li>\n<li>libc's <a href=\"https://en.wikipedia.org/wiki/Getopt#Examples\" rel=\"noopener noreferrer\"><code>getopt</code></a>.</li>\n</ul>\n<p>src: <a href=\"https://github.com/blyxxyz/lexopt#see-also\" rel=\"noopener noreferrer\">https://github.com/blyxxyz/lexopt#see-also</a></p>\n</blockquote>\n<p>P.S. ä¸å¾—ä¸è¯´ï¼ŒRust åˆ©ç”¨æŠ½è±¡çš„ç±»å‹ç³»ç»Ÿå’Œå®ï¼Œåœ¨ args-parser æ–¹é¢å¤ªæ£’äº†ã€‚å†™ Rust æ˜¯ä¸€ç§äº«å—ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-03 11:12:15","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"çˆ±æ­»ä½ äº†ï¼Œé˜¿å…‹è‹å§†","link":"https://rustcc.cn/article?id=628500dc-3232-40ff-98f8-03540eaa5d12","description":"<p>æœ€è¿‘èŠ±äº†ç‚¹æ—¶é—´æ¥å­¦ä¹ axum, å¹¶æˆåŠŸå°†ä¸€ä¸ªç”¨warpå†™çš„é¡¹ç›®æ”¹ç”¨axumé‡å†™ã€‚axumå¤ªæ£’äº†ï¼Œå……åˆ†ä½“ç°äº†rustè¿™é—¨è¯­è¨€çš„è¡¨è¾¾èƒ½åŠ›ã€‚</p>\n<ol>\n<li>è·¯ç”±è®¾è®¡éå¸¸ç®€æ´ï¼Œæ¼”ç¤ºäº†Rustä¸ç”¨å®ï¼Œä¹Ÿå¯ä»¥æDSLçš„æ–¹æ³•ã€‚</li>\n<li>Extractorä¸AddExtensionæä¸ºçµæ´»ï¼Œç®€åŒ–äº†warpé€šè¿‡æ„å»ºå‚æ•°è·å–Requestä¸ç¯å¢ƒæ•°æ®çš„è®¾è®¡ã€‚</li>\n<li>å€Ÿç”¨Towerç”Ÿæ€æé«˜äº†ä»£ç åˆ©ç”¨ç‡ã€‚</li>\n</ol>\n<p>axuméå¸¸ç¨³å®šï¼Œå‹åŠ›æµ‹è¯•ä¸­åŒæ—¶å¼€15Kå¹¶å‘å¦¥å¦¥çš„ã€‚åœ¨axumé¢ä¸–ä¹‹å‰ï¼Œwarpæ˜¯æœ€æ£’çš„webæ¡†æ¶ï¼Œç°åœ¨è¯¥æ˜¯é˜¿å…‹è‹å§†æ‹…å½“ä¸»è§’äº†ã€‚ç”±äºä¸¤è€…éƒ½æ˜¯åŸºäºhyperå¹³å°ï¼Œä»warpç§»æ¤åˆ°axumä¹Ÿæ˜¯åˆ†åˆ†é’Ÿçš„äº‹ã€‚\nä¸‹é¢è´´å‡ºå®æˆ˜é¡¹ç›®ä¸­ä¸¤æ®µä»£ç main.rsä¸servce.rsã€‚main.rsä¸­æ¼”ç¤ºäº†å¦‚ä½•é€šè¿‡å‘½ä»¤è¡Œå‚æ•°åˆ‡æ¢ï¼Œå®ç°httpä¸httpsä¸¤ç§æœåŠ¡ï¼Œè¿˜æ¼”ç¤ºäº†å¦‚ä½•è°ƒç”¨äº†é™æ€æ–‡ä»¶æœåŠ¡åŠŸèƒ½ã€‚service.rsæ˜¯æ”¾apiçš„åœ°æ–¹ï¼Œæ¼”ç¤ºäº†å¦‚ä½•å¤„ç†getä¸postè¯·æ±‚ï¼Œå¦‚ä½•è·å–æ•°æ®åº“ä¸­çš„æ•°æ®ï¼Œå¦‚ä½•æä¾›åŠ¨æ€ä¸‹è½½å†…å®¹ç­‰åŠŸèƒ½ã€‚</p>\n<pre><code>//main.rs\nmod addr;\nmod base16;\nmod bb8_tiberius;\nmod ccb_gwk;\nmod ccb_socket;\nmod config;\nmod context;\nmod database;\nmod json_helper;\nmod json_value;\nmod parse_exp;\nmod parse_param;\nmod service;\nmod service_da;\n\nuse axum::{http::StatusCode, Router};\nuse tower_http::services::ServeDir;\n\nuse std::env::args;\n\nuse chrono::prelude::*;\nuse context::AppContext;\nuse json_helper::JsonHelper;\nuse json_value::JsonValue;\n\nconst VERSION: &amp;str = \"1.3.0\";\n\n#[tokio::main]\nasync fn main() {\n    pretty_env_logger::init_timed();\n\n    let is_https = args().nth(1).unwrap_or(\"http\".into()) == \"https\";\n\n    let context = AppContext::new().await;\n    let ctx = context.clone();\n    let config = &amp;ctx.config;\n    let server_config = &amp;config[\"config\"];\n    let ctx = context.clone();\n    let app = Router::new()\n        .nest(\n            \"/\",\n            axum::service::get(ServeDir::new(\"D:/Js/OnlyOne/public\")).handle_error(\n                |error: std::io::Error| {\n                    Ok::&lt;_, std::convert::Infallible&gt;((\n                        StatusCode::INTERNAL_SERVER_ERROR,\n                        format!(\"Unhandled internal error: {}\", error),\n                    ))\n                },\n            ),\n        )\n        .nest(\"/api\", service::api(ctx));\n\n    let addr = addr::Addr::new(server_config, is_https);\n    let now = Local::now().to_string();\n    let now = &amp;now[0..19];\n    println!(\n        \"{} HTTP{} Server V{} is starting at {:19}, {}\",\n        server_config[\"server_name\"].string(\"W3\"),\n        if is_https { \"S\" } else { \"\" },\n        VERSION,\n        now,\n        addr\n    );\n\n    let addr = addr.to_string_full();\n\n    if is_https {\n        axum_server::bind_rustls(addr)\n            .private_key_file(\"key.pem\")\n            .certificate_file(\"cert.pem\")\n            .serve(app)\n            .await\n            .unwrap();\n    } else {\n        axum_server::bind(addr).serve(app).await.unwrap();\n    }\n}\n\n</code></pre>\n<pre><code>//service.rs\nuse crate::base16;\nuse crate::ccb_gwk;\nuse crate::database;\nuse crate::parse_param;\nuse crate::service_da::{da_read_about, da_write_about, download_photos, DA_WEBP_DISABLE};\nuse crate::AppContext;\nuse crate::JsonHelper;\nuse crate::JsonValue;\nuse anyhow::{anyhow, Result};\nuse encoding::{all::GB18030, EncoderTrap, Encoding};\nuse serde_json::{json, Value};\nuse std::collections::HashMap;\nuse std::sync::Arc;\nuse tiberius::ToSql;\nuse tracing::info;\n\nuse axum::{\n    extract::{Extension, Form, Query},\n    response::Json,\n    handler::{get, post},\n    http::header::{HeaderMap, HeaderName, HeaderValue},\n    routing::BoxRoute,\n    AddExtensionLayer, Router,\n};\n\nfn string_to_gb18030bytes(string: &amp;str) -&gt; Result&lt;Vec&lt;u8&gt;&gt; {\n    GB18030\n        .encode(string, EncoderTrap::Strict)\n        .map_err(|e| anyhow!(\"string_to_gb18030bytes failure: {:?}\", e))\n}\n\npub(crate) fn api(ctx: Arc&lt;AppContext&gt;) -&gt; Router&lt;BoxRoute&gt; {\n    Router::new()\n        .route(\"/ask\", get(ask))\n        .route(\"/act\", post(act))\n        .layer(AddExtensionLayer::new(ctx))\n        .boxed()\n}\n\n\npub(crate) async fn ask(\n    Query(qs): Query&lt;HashMap&lt;String, String&gt;&gt;,\n    Extension(context): Extension&lt;Arc&lt;AppContext&gt;&gt;,\n) -&gt; (HeaderMap, Vec&lt;u8&gt;) {\n    let mut headers = HeaderMap::new();\n    //let qs=format!(\"{:?}\",qs);\n    //let bytes=Vec::from(json!({\"ask\":qs}).to_string());\n\n    let empty = String::from(\"\");\n    let dbs = context.dbs.clone();\n    let ask = qs.get(\"ask\").unwrap_or(&amp;empty).clone();\n    let params = base16::base16_decode(qs.get(\"params\").unwrap_or(&amp;empty)).unwrap();\n    let params: Value = serde_json::from_str(&amp;params).unwrap();\n    info!(\"ask={} params={}\", ask, params);\n    //let content_type = \"application/json\";\n    let p1: JsonValue;\n    let p2: JsonValue;\n    let p3: JsonValue;\n    let p4: JsonValue;\n    let p5: JsonValue;\n    let pof = |id| JsonValue::of(&amp;params[id]);\n    let static_path = context.config[\"config\"][\"static_path\"].string(\"wwwroot\");\n    let da_webp_active = context.config[\"config\"][\"da_webp_active\"].bool(false);\n    let accept_webp = params[\"acceptWebp\"].bool(false);\n    let da_webp_quality = if da_webp_active &amp;&amp; accept_webp {\n        context.config[\"config\"][\"da_webp_quality\"].i64(20) as i8\n    } else {\n        DA_WEBP_DISABLE\n    };\n    let mut sql: String = \"\".into();\n    let mut sql_params: Vec&lt;&amp;dyn ToSql&gt; = Vec::new();\n    let mut pending = true;\n    let mut result: String = \"null\".into();\n    let mut about_file: String = \"\".into();\n    let mut voucher_id: &amp;str = &amp;empty;\n    let mut attach: String = \"\".into();\n\n    if ask == \"@login\" {\n        sql = r\"EXEC TM_OnlyOneLogin @P1,@P2\".into();\n        p1 = pof(\"userId\");\n        p2 = pof(\"password\");\n        sql_params = vec![&amp;p1, &amp;p2];\n    } else if ask == \"workload\" {\n        sql = \"EXEC TM_WorkLoad @P1,@P2,@P3\".into();\n        p1 = pof(\"userName\");\n        p2 = pof(\"year\");\n        let more_where = if params[\"limitMonth\"].bool(false) {\n            let month_from = params[\"monthFrom\"].i64(1);\n            let month_to = params[\"monthTo\"].i64(13);\n            let month_to = if month_to &lt; month_from {\n                month_from\n            } else {\n                month_to\n            };\n            format!(\n                \" AND z.kjqj BETWEEN '{:02}' AND '{:02}'\",\n                month_from, month_to\n            )\n        } else {\n            \"\".to_string()\n        };\n        //println!(\"moreWhere:{}\",more_where);\n        p3 = JsonValue::new(json!(more_where));\n        sql_params = vec![&amp;p1, &amp;p2, &amp;p3];\n    } else if ask == \"wujinFH\" {\n        sql = \"EXEC dbo.TM_UpdateOracleWSZZ4WujinFH @P1,@P2\".into();\n        p1 = pof(\"p1\");\n        p2 = pof(\"p2\");\n        sql_params = vec![&amp;p1, &amp;p2];\n    } else if ask == \"salaryVoucher\" {\n        sql = \"EXEC dbo.TM_MakeSalaryVoucher @P1,@P2\".into();\n        p1 = pof(\"period\");\n        p2 = pof(\"personType\");\n        sql_params = vec![&amp;p1, &amp;p2];\n    } else if ask == \"salaryVoucherBank\" {\n        sql = \"EXEC dbo.TM_GetSalaryBankDetail @P1,@P2\".into();\n        p1 = pof(\"period\");\n        p2 = pof(\"personType\");\n        sql_params = vec![&amp;p1, &amp;p2];\n    } else if ask == \"salaryVoucherSheet\" {\n        sql = \"EXEC dbo.TM_GetSalaryVoucher @P1\".into();\n        p1 = pof(\"batch\");\n        sql_params = vec![&amp;p1];\n    } else if ask == \"checkncye\" {\n        sql = \"EXEC dbo.TM_CheckNCYE @P1,@P2\".into();\n        p1 = pof(\"year\");\n        p2 = pof(\"tblname\");\n        sql_params = vec![&amp;p1, &amp;p2];\n    } else if ask == \"py2code\" {\n        sql = \"EXEC dbo.TM_PY2Code @P1,@P2\".into();\n        p1 = pof(\"type\");\n        p2 = pof(\"code\");\n        sql_params = vec![&amp;p1, &amp;p2];\n    } else if ask == \"@aboutvoucher\" {\n        sql = \"EXEC dbo.TM_AboutVoucher @P1\".into();\n        p1 = pof(\"pznm\");\n        sql_params = vec![&amp;p1];\n    } else if ask == \"@voucherphotos\" {\n        voucher_id = params[\"voucherId\"].str(\"\");\n        match da_read_about(voucher_id, &amp;static_path, da_webp_quality).await {\n            Ok((about_file_exists, read_result, about_file_name)) =&gt; {\n                about_file = about_file_name;\n                if about_file_exists {\n                    result = format!(\"{{\\\"msg\\\":\\\"ok\\\", \\\"data\\\":{}}}\", read_result);\n                    pending = false;\n                }\n            }\n            Err(e) =&gt; {\n                result = format!(\"{{\\\"msg\\\":\\\"{:?}\\\"}}\", e);\n                pending = false;\n            }\n        }\n        if pending {\n            sql = \"EXEC dbo.TM_VoucherPhotos @P1\".into();\n            p1 = pof(\"voucherId\");\n            sql_params = vec![&amp;p1];\n        }\n    } else if ask == \"@aboutreceipt\" {\n        sql = \"EXEC dbo.TM_AboutReceipt @P1,@P2,@P3,@P4,@P5\".into();\n        p1 = pof(\"id\");\n        p2 = pof(\"checkSum\");\n        p3 = pof(\"datePaid\");\n        p4 = pof(\"amount\");\n        p5 = pof(\"checker\");\n        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4, &amp;p5];\n    } else if ask == \"payee\" {\n        sql = \"EXEC dbo.TM_QueryPayee @P1,@P2\".into();\n        p1 = pof(\"bankName\");\n        p2 = pof(\"bankAcct\");\n        sql_params = vec![&amp;p1, &amp;p2];\n    } else if ask == \"ledger\" || ask == \"voucher\" {\n        // CREATE PROCEDURE dbo.TM_QueryLedgerExt\n        // @èµ·å§‹å¹´ INT,@ç»ˆæ­¢å¹´ INT,@æŸ¥è¯¢æ¡ä»¶ VARCHAR(4096),@æ’åº VARCHAR(80)='æ—¥æœŸ,å‡­è¯å·,ç¬”å·',\n        // @å€Ÿè´·å¯¹å†² BIT=0,@éšè—è´Ÿå€¼ BIT=0,@Select VARCHAR(250)='*'\n        let params = parse_param::params_convert(&amp;context.config, &amp;ask, &amp;params);\n        let pof = |id| JsonValue::of(&amp;params[id]);\n        let only_sum_line = if ask == \"ledger\" { \",1\" } else { \",0\" };\n        sql = \"EXEC dbo.TM_QueryLedgerExt @P1,@P2,@P3,@P4,0,0,@P5\".to_string() + only_sum_line;\n        p1 = pof(\"yearFrom\");\n        p2 = pof(\"yearTo\");\n        p3 = pof(\"filter\");\n        p4 = pof(\"orderby\");\n        p5 = pof(\"select\");\n        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4, &amp;p5];\n    } else if ask == \"balance\" {\n        // CREATE PROCEDURE dbo.TM_QueryBalanceExt\n        // @èµ·å§‹å¹´ INT,@ç»ˆæ­¢å¹´ INT,@æŸ¥è¯¢æ¡ä»¶ VARCHAR(4096),@æœŸåˆæ¡ä»¶ VARCHAR(4096)=NULL,\n        // @å¹´åˆæ¡ä»¶ VARCHAR(4096)=NULL,@ä½™é¢æ¡ä»¶ VARCHAR(250)=NULL,\n        // @é¡¶å±‚ VARCHAR(10)='ç§‘ç›®1çº§',@åº•å±‚ VARCHAR(10)='ç§‘ç›®4çº§',\n        // @åˆå¹¶ INT=NULL,@åˆè®¡ BIT=0,@ä»…åº•å±‚ BIT=0,@ä»…ç¼–ç  BIT=0,@å€ç‡ INT=1,@æŸ¥é¡¹ç›®ä½™é¢ BIT=0\n        let params = parse_param::params_convert(&amp;context.config, &amp;ask, &amp;params);\n        let pof = |id| JsonValue::of(&amp;params[id]);\n        sql = format!(\n            \"EXEC dbo.TM_QueryBalanceExt @P1,@P2,@P3,@P4,@P5{}\",\n            params[\"params_in_sql\"].str(\"\")\n        );\n        p1 = pof(\"yearFrom\");\n        p2 = pof(\"yearTo\");\n        p3 = pof(\"filter\");\n        p4 = pof(\"filter_qc\");\n        p5 = pof(\"filter_nc\");\n        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4, &amp;p5];\n    }\n    if pending {\n        let result_json = if !sql.is_empty() {\n            let row_is_obj = ask.starts_with('@');\n            let result = database::query(dbs, &amp;sql, &amp;sql_params, row_is_obj).await;\n            match result {\n                Ok(result) =&gt; {\n                    if ask == \"ledger\" {\n                        let params =\n                            parse_param::params_convert(&amp;context.config, \"voucher\", &amp;params);\n                        json!({ \"msg\":\"ok\",\"voucherColDefs\":params[\"select\"], \"data\":result})\n                    } else if ask == \"@voucherphotos\" {\n                        let row_count = result[\"rowCount\"].u64(0);\n                        if row_count &gt; 0 {\n                            let result = download_photos(context, result, da_webp_quality).await;\n                            match da_write_about(&amp;about_file, &amp;result).await {\n                                Ok(_) =&gt; json!({ \"msg\":\"ok\", \"data\":result}),\n                                Err(e) =&gt; json!({ \"msg\": format!(\"{:?}\", e) }),\n                            }\n                        } else {\n                            json!({\n                                \"msg\": format!(\"æ²¡æœ‰æ‰¾åˆ°å‡­è¯{}çš„å½±åƒèµ„æ–™\", voucher_id)\n                            })\n                        }\n                    } else if ask == \"salaryVoucherSheet\" {\n                        let empty_vec: Vec&lt;Value&gt; = Vec::new();\n                        let sheet = result[\"rows\"].as_array().unwrap_or(&amp;empty_vec);\n                        let sheet = sheet\n                            .iter()\n                            .map(|x| x.get(0).unwrap_or(&amp;Value::Null).string(\"\"))\n                            .fold(\"\".to_string(), |lines, line| lines + &amp;line + \"\\r\\n\");\n                        attach = sheet;\n                        json!(\"attachment\")\n                    } else {\n                        json!({ \"msg\":\"ok\", \"data\":result})\n                    }\n                }\n                Err(err) =&gt; {\n                    json!({ \"msg\": format!(\"{:?}\", err) })\n                }\n            }\n        } else if ask == \"checkgwk\" {\n            let check_all = params[\"checkAll\"].bool(false);\n            ccb_gwk::check_gwk(&amp;context.config, check_all)\n                .await\n                .unwrap()\n        } else {\n            json!({ \"msg\": format!(\"unknown ask {} params:{}\", ask, params.to_string()) })\n        };\n        result = format!(\"{}\", result_json);\n    }\n    if ask == \"salaryVoucherSheet\" {\n        let file_name = params[\"fileName\"].str(\"å‡­è¯\");\n        let value = format!(\"attachment;filename={}.txt\", file_name);\n        let bytes: Vec&lt;u8&gt; = string_to_gb18030bytes(&amp;attach).unwrap_or_default();\n        //reply::with_header(bytes, \"Content-disposition\", value)\n        headers.insert(\n            HeaderName::from_static(\"content-type\"),\n            HeaderValue::from_static(\"text/plain\"),\n        );\n        headers.insert(\n            HeaderName::from_static(\"content-disposition\"),\n            HeaderValue::from_str(&amp;value).unwrap(),\n        );\n        (headers, bytes)\n    } else {\n        let bytes: Vec&lt;u8&gt; = result.into_bytes();\n        //reply::with_header(bytes, \"content-type\", content_type)\n        headers.insert(\n            HeaderName::from_static(\"content-type\"),\n            HeaderValue::from_static(\"application/json\"),\n        );\n        (headers, bytes)\n    }\n}\n\nasync fn act(\n    Form(qs): Form&lt;HashMap&lt;String, String&gt;&gt;,\n    Extension(context): Extension&lt;Arc&lt;AppContext&gt;&gt;,\n) -&gt; Json&lt;Value&gt; {\n    let empty = String::from(\"\");\n    let dbs = context.dbs.clone();\n    let act = qs.get(\"act\").unwrap_or(&amp;empty).clone();\n    let params = base16::base16_decode(qs.get(\"params\").unwrap_or(&amp;empty)).unwrap();\n    let params: Value = serde_json::from_str(&amp;params).unwrap_or(Value::Null);\n    info!(\"act={} params={}\", act, params);\n    //let content_type = \"application/json\";\n    let sql: String;\n    let p1: JsonValue;\n    let p2: JsonValue;\n    let p3: JsonValue;\n    let p4: JsonValue;\n    let p5: JsonValue;\n    let sql_params: Vec&lt;&amp;dyn ToSql&gt;;\n    let pof = |id| JsonValue::of(&amp;params[id]);\n    if act == \"exam\" {\n        sql = r\"EXEC dbo.TM_Exam @P1,@P2,@P3,@P4\".into();\n        p1 = pof(\"ids\");\n        p2 = pof(\"fhr\");\n        p3 = pof(\"fhrId\");\n        p4 = pof(\"isUndo\");\n        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4];\n    } else if act == \"changepayee\" {\n        sql = \"EXEC dbo.TM_ChangePayee @P1,@P2,@P3,@P4,@P5\".into();\n        p1 = pof(\"bankName\");\n        p2 = pof(\"bankAcct\");\n        p3 = pof(\"unitCode\");\n        p4 = pof(\"updateDate\");\n        p5 = pof(\"mark\");\n        sql_params = vec![&amp;p1, &amp;p2, &amp;p3, &amp;p4, &amp;p5];\n    } else if act == \"execsql\" || act == \"@execsql\" {\n        sql = params[\"sql\"].string(\"\");\n        sql_params = Vec::new();\n    } else {\n        sql = \"\".into();\n        sql_params = Vec::new();\n    }\n    let result = if !sql.is_empty() {\n        let row_is_obj = act.starts_with('@');\n        let result = database::query(dbs, &amp;sql, &amp;sql_params, row_is_obj).await;\n        match result {\n            Ok(result) =&gt; {\n                json!({ \"msg\":\"ok\", \"data\":result})\n            }\n            Err(err) =&gt; {\n                json!({ \"msg\": format!(\"{:?}\", err) })\n            }\n        }\n    } else {\n        json!({ \"msg\": format!(\"unknown act {} params:{}\", act, params.to_string()) })\n    };\n    Json(result)\n}\n\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-03 10:27:07","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-09-03 Bebop v2.3.0ï¼šä¸º Bebop åºåˆ—åŒ–æ·»åŠ  Rust æ”¯æŒ","link":"https://rustcc.cn/article?id=99b9fdf0-4026-4753-a07f-b7355caced95","description":"<h4>Bebop v2.3.0ï¼šä¸º Bebop åºåˆ—åŒ–æ·»åŠ  Rust æ”¯æŒ</h4>\n<p>Bebop æ˜¯ä¸€ç§åŸºäºæ¨¡å‹çš„äºŒè¿›åˆ¶åºåˆ—åŒ–æŠ€æœ¯ï¼Œç±»ä¼¼äº Protocol Buffers æˆ– MessagePackã€‚ç‰¹åˆ«æ˜¯ï¼ŒBebop è¯•å›¾éå¸¸é€‚åˆéœ€è¦æ¯” JSON æˆ– MessagePack æ›´å¿«ã€æ›´ç®€æ´å’Œç±»å‹å®‰å…¨çš„å®¢æˆ·ç«¯-æœåŠ¡å™¨æˆ–åˆ†å¸ƒå¼ Web åº”ç”¨ç¨‹åºã€‚Matthew Conover 2021å¹´8æœˆ30æ—¥å®£å¸ƒ Bebop æ·»åŠ äº† Rust çš„æ”¯æŒ</p>\n<ul>\n<li>https://rainway.com/blog/2021/08/30/bebop-rust/</li>\n</ul>\n<h4>å°† TensorFlow æ¨¡å‹ç§»æ¤åˆ° Rust çš„å¼€å‘æˆæœ¬</h4>\n<p>é€šè¿‡ CrowdStrike çš„å¯æ‰©å±•æ€§ï¼Œå¯ä»¥ç«‹å³å°† TensorFlow æ¨¡å‹æˆåŠŸè½¬æ¢ä¸ºçº¯ Rust ä»£ç ï¼Œæ–‡ç« ä»‹ç»äº†é€šè¿‡è¿™ä¸€æ–¹æ³•çš„æ—¶é—´å’Œç²¾åŠ›æˆæœ¬</p>\n<ul>\n<li>https://www.crowdstrike.com/blog/development-cost-of-porting-tensorflow-models-to-pure-rust/</li>\n</ul>\n<h4>Rust ä¸­çš„ç»“æ„æ›´æ–°è¯­æ³•</h4>\n<ul>\n<li>https://www.reddit.com/r/rust/comments/pchp8h/media_struct_update_syntax_in_rust/</li>\n</ul>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ åŒ—çº¬27åº¦ ä¾¯ç››é‘«</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rust.cc è®ºå›: æ”¯æŒ rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRust è¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-03 08:34:27","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"bytes::ByteMut æ— æ³•å†™å…¥æ•°æ®","link":"https://rustcc.cn/article?id=1ab18776-e162-4fbc-80b3-d23bb7402cf3","description":"<p>æƒ³ç”¨ <a href=\"https://docs.rs/bytes/1.1.0/bytes/struct.BytesMut.html\" rel=\"noopener noreferrer\">ByteMut</a> ä½œç¼“å†²åŒºï¼Œä»åŒæ­¥ioæ•°æ®æºä¸­è¯»å–æ•°æ®ï¼Œå¯æ˜¯æ— æ³•è¯»å–ï¼Œä¹Ÿæ²¡æŠ¥é”™ã€‚ã€‚ã€‚</p>\n<pre><code>fn main() -&gt; io::Result&lt;()&gt; {\n    let mut buf = BytesMut::with_capacity(10);\n\n    let mut input: Cursor&lt;Vec&lt;u8&gt;&gt; = Cursor::new({\n        (0..100).collect()\n    });\n\n    loop {\n        match input.read(&amp;mut buf)? {\n            0 =&gt; {\n                println!(\"[READ OVER]\");\n                break;\n            }\n            n =&gt; {\n                println!(\"{:?}\", &amp;buf);\n                println!(\"[READ ONCE]\");\n            }\n        }\n    }\n\n    Ok(())\n}\n</code></pre>\n<p>å¦‚æœåªç”¨æ™®é€šçš„æ•°ç»„ï¼Œæ˜¯å¯ä»¥è¯»å–æ•°æ®</p>\n<pre><code>let mut buf = [0; 10];\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-03 07:43:46","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"éå‡¸ç§‘æŠ€ ï½œå¯»æ‰¾è¡Œä¸šå†…ä¼˜ç§€çš„Rustå¼€å‘å·¥ç¨‹å¸ˆ","link":"https://rustcc.cn/article?id=ac57d61f-1742-423a-91db-2cd682843f93","description":"<p>åœ¨æœ€è¿‘å‡ºç‚‰çš„ Stack Overflow å…¨çƒå¼€å‘è€…è°ƒæŸ¥æŠ¥å‘Šä¸­ï¼Œçº¦86.69%çš„å¼€å‘äººå‘˜é€‰æ‹©Rustä½œä¸ºä»–ä»¬â€œæœ€å–œçˆ±çš„è¯­è¨€â€ã€‚Rustè¯­è¨€è™½å—å¥½è¯„ï¼Œä½†ä½¿ç”¨ç¾¤ä½“ä»æ˜¯å°‘æ•°ã€‚Ruståœ¨ç³»ç»Ÿç¼–ç¨‹é¢†åŸŸå¾ˆå—æ¬¢è¿ï¼Œå¹¶ä¸”è¿‘å¹´æ¥è¢«è®¤ä¸ºå°†æ¥æ›¿Cè¯­è¨€ç”¨äºLinuxå†…æ ¸å¼€å‘ï¼ŒåŸå› ä¹‹ä¸€æ˜¯Rustå¯ä»¥å¸®åŠ©æ¶ˆé™¤ä¸å†…å­˜ç›¸å…³çš„å®‰å…¨æ¼æ´ã€‚</p>\n<p>éå‡¸ç§‘æŠ€æ­£æˆä¸ºå›½å†…é‡‘èå¸‚åœºæ™ºèƒ½æŠ•èµ„äº¤æ˜“å¹³å°çš„å¼•é¢†è€…ï¼Œæ½œå¿ƒæ‰“é€ æ™ºèƒ½ç®—æ³•äº¤æ˜“å¹³å°ï¼Œåœ¨åŸæœ‰åŸºç¡€ä¸Šå…¨é¢å‡çº§åˆ°äº’è”ç½‘æ–°ä¸€ä»£æŠ€æœ¯æ¶æ„ï¼Œç»“åˆæœºå™¨å­¦ä¹ ç­‰æ–°å…´æŠ€æœ¯ï¼Œé€æ­¥å®Œæˆå„ç±»äº¤æ˜“ç®—æ³•çš„ç ”å‘è¿­ä»£ï¼Œæ­£æŒç»­ä¸ºåˆ¸å•†ã€é‡åŒ–ç§å‹Ÿç­‰ä¼—å¤šå¤§å‹é‡‘èæœºæ„æä¾›ä¼˜è´¨çš„ç®—æ³•æœåŠ¡ã€‚</p>\n<p>ç°é˜¶æ®µï¼Œéå‡¸ç§‘æŠ€æ­£åœ¨å¯»æ‰¾è¡Œä¸šå†…ä¼˜ç§€çš„Rustå¼€å‘å·¥ç¨‹å¸ˆï¼Œè–ªèµ„ç¦åˆ©è¶…çº§ä¼˜åšã€‚å…³é”®æ˜¯å›¢é˜Ÿæœ‰å¾ˆå¥½çš„Rustçš„å¼€å‘æ°›å›´ï¼ŒRustå¤§ç¥æ‰‹æŠŠæ‰‹è¾…å¯¼ï¼ŒåŠ©ä½ ä»Rustæ–°äººä¸æ–­å‡çº§ã€‚</p>\n<p>å²—ä½èŒè´£ï¼š\n1.è®¾è®¡å¹¶å¼€å‘åŸºäºRUSTçš„é«˜æ€§èƒ½ï¼Œä½æ—¶å»¶ç®—æ³•äº¤æ˜“ç³»ç»Ÿï¼›\n2.è®¾è®¡å¹¶å¼€å‘æ•°æ®å¤„ç†å¹³å°ï¼Œç›‘æ§è¿ç»´å¹³å°ï¼›\n3.è®¾è®¡å¹¶å¼€å‘é¢å‘å®¢æˆ·çš„é«˜å¯ç”¨äº¤æ˜“å·¥å…·ç­‰ï¼›\n4.è®¾è®¡å¹¶å¼€å‘ç­–ç•¥ç›¸å…³çš„å›æµ‹å¹³å°ã€‚</p>\n<p>å²—ä½è¦æ±‚ï¼š\n1.æœ¬ç§‘åŠä»¥ä¸Šå­¦å†ï¼ˆ985ä¼˜å…ˆï¼‰ã€‚ç¼–ç¨‹åŸºç¡€æ‰å®ï¼Œå…·æœ‰è‰¯å¥½çš„è®¡ç®—æœºç†è®ºåŸºç¡€ï¼›\n2.ç†Ÿç»ƒæŒæ¡Linuxæ“ä½œï¼Œæ€§èƒ½åˆ†æï¼Œå…·å¤‡Rust/C++/Java/Goä¸°å¯Œå¼€å‘ç»éªŒï¼Œç†Ÿæ‚‰å¸¸ç”¨çš„è®¾è®¡æ¨¡å¼ï¼Œæœ‰åˆ†å¸ƒå¼ç›¸å…³ç»éªŒåŠ åˆ†ï¼›\n3.æœ‰ç ”å‘é«˜æ€§èƒ½ï¼Œä½æ—¶å»¶ç³»ç»Ÿç»éªŒåŠ åˆ†ï¼›\n4.å¯¹æŠ€æœ¯å……æ»¡çƒ­æƒ…ï¼Œæ€è€ƒæ·±å…¥ã€‚è‡ªæˆ‘é©±åŠ¨ï¼Œèƒ½å¿«é€Ÿå­¦ä¹ æ–°é²œäº‹ç‰©ã€‚</p>\n<p>å‚è€ƒè–ªé…¬ï¼šBase 30K-60K+ï¼Œæœ‰æœŸæƒæ¿€åŠ±ã€‚\nå·¥ä½œåœ°ç‚¹ï¼šä¸Šæµ·å¸‚æ¼•æ²³æ³¾å¼€å‘åŒº å‡¯ç§‘å›½é™…å¤§å¦\næŠ•é€’é‚®ç®±ï¼šrecruit@non-convex.com\nè”ç³»äººå¾®ä¿¡å·ï¼šSweeneyTodd333333</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-03 07:24:05","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Poem-openapiå¼€æºäº†!","link":"https://rustcc.cn/article?id=334551fe-a8a9-4561-aa63-11987b10e81f","description":"<p>å°½ç®¡æœ‰ä¸å°‘æœ‹å‹å·²ç»çŸ¥é“æˆ‘è¿™å‡ å¤©åœ¨åšä»€ä¹ˆï¼Œä½†å½“Poem-openapiçš„ç¬¬ä¸€ç‰ˆå‡†æ—¶å®Œæˆï¼Œå¹¶ä¸”å®Œå…¨æŒ‰ç…§åˆšå¼€å§‹çš„æƒ³æ³•æ­£å¸¸å·¥ä½œæ—¶ï¼Œæˆ‘è¿˜æ˜¯æŒ‰æºä¸ä½å†…å¿ƒçš„æ¿€åŠ¨å¸Œæœ›è·Ÿå¤§å®¶åˆ†äº«ã€‚</p>\n<p>æ³¨æ„ï¼šPoem-openapiåªæ”¯æŒPoemï¼Œæ‰€ä»¥ä½ å¦‚æœå¸Œæœ›ä½¿ç”¨å®ƒï¼ŒPoemæ˜¯å¿…è¦çš„ä¾èµ–ï¼Œè€Œä¸”æˆ‘ä¸ä¼šè€ƒè™‘æ”¯æŒå…¶å®ƒçš„webæ¡†æ¶ã€‚ğŸ˜</p>\n<p>æ®æˆ‘æ‰€çŸ¥è¿™æ˜¯Rustè¯­è¨€é‡Œç¬¬ä¸€ä¸ªç”¨è¿‡ç¨‹å®æ¥å®ç°OpenAPIè§„èŒƒçš„åº“ï¼Œå®ƒçš„å·¥ä½œæ–¹å¼å’Œ<a href=\"https://crates.io/crates/async-graphql\" rel=\"noopener noreferrer\">Async-graphql</a>éå¸¸çš„åƒï¼Œä»¥ç±»å‹å®‰å…¨çš„ä»£ç æ¥ç¼–å†™ç¬¦åˆOpenAPIè§„èŒƒçš„APIå¹¶è‡ªåŠ¨ç”Ÿæˆæ–‡æ¡£ã€‚è¿‡ç¨‹å®çš„ä½¿ç”¨å®Œå…¨IDEå‹å¥½ï¼Œä½ ç»å¯¹ä¸ä¼šç›´æ¥ç”¨åˆ°è¿‡ç¨‹å®ç”Ÿæˆçš„ä»»ä½•ä»£ç ï¼Œé¿å…äº†IDEæ»¡å±å¹•çš„çº¢çº¿ï¼Œæˆ–è€…æ²¡æ³•è‡ªåŠ¨å®Œæˆï¼ˆæˆ‘å¾ˆçœ‹é‡è¿™ä¸ªï¼Œè„±ç¦»IDEçš„è‡ªåŠ¨å®Œæˆæˆ‘ä¸ä¼šå†™ä»£ç ï¼‰ã€‚ğŸ˜‚</p>\n<p>ä¸‹é¢æˆ‘ä»¥ä¸€ä¸ªå°ä¾‹å­æ¥ä»‹ç»å®ƒæ˜¯å¦‚ä½•ä½¿ç”¨çš„ï¼š</p>\n<p>è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç”¨æˆ·ç®¡ç†APIå®ç°ï¼ˆåˆ«ç®¡å®ƒæœ‰æ²¡æœ‰ä»€ä¹ˆå®é™…ä»·å€¼ï¼Œä¹Ÿåˆ«å‘Šè¯‰æˆ‘oaiè¿™ä¸ªåå­—å¾ˆå¥‡æ€ªï¼Œè¿™æ˜¯å®˜æ–¹èµ·çš„ç®€ç§°ï¼Œä¸æ€ªæˆ‘ï¼‰ï¼Œæˆ‘ä»¬åªçœ‹å®ƒå¦‚ä½•ä½¿ç”¨ã€‚ğŸ˜</p>\n<p>æ¯ä¸€ä¸ªæ¥å£éƒ½éœ€è¦å®šä¹‰Requestå’ŒResponseç±»å‹ï¼Œé™¤éå®ƒä¸æ¥æ”¶æˆ–è€…è¿”å›ä»»ä½•å†…å®¹ã€‚</p>\n<p><code>create_user</code>æ¥å£åˆ›å»ºä¸€ä¸ªç”¨æˆ·ï¼Œç”±äºå®ƒçš„è¯·æ±‚å¯¹è±¡ç±»å‹æ˜¯Jsonï¼Œæ‰€ä»¥å®ƒåªæ”¯æŒ<code>content-type</code>ä¸º<code>application/json</code>çš„è¯·æ±‚ã€‚è¿”å›çš„<code>CreateUserResponse</code>å®šä¹‰äº†ä¸åŒçŠ¶æ€ç å¯¹åº”çš„å“åº”ç±»å‹ã€‚</p>\n<p>æ‰€æœ‰<code>API</code>å®æè¿°çš„æ“ä½œéƒ½ä¼šè‡ªåŠ¨ç”ŸæˆOpenAPI 3.0è§„èŒƒçš„æ–‡æ¡£ï¼Œä½ å¯ä»¥cloneä»“åº“ https://github.com/poem-web/poem-openapi ï¼Œç„¶åæ‰§è¡Œ<code>cargo run --example users</code>ï¼Œæµè§ˆå™¨æ‰“å¼€<code>http://localhost:3000</code>ï¼Œå°±èƒ½çœ‹åˆ°ä¸€ä¸ªéå¸¸å¥¢åçš„Swagger UIï¼ˆå°½ç®¡æˆ‘è§‰å¾—å®ƒç¦»GraphQL Playgroundçš„æ˜“ç”¨åº¦è¿˜å·®å¾—è¿œï¼‰ã€‚ğŸ˜</p>\n<pre><code>use std::collections::HashMap;\n\nuse poem_openapi::{payload::Json, types::Password, OpenAPI, Response, Schema, API};\nuse tokio::sync::Mutex;\n\n/// Create user schema\n#[derive(Debug, Schema, Clone, Eq, PartialEq)]\nstruct User {\n    /// Id\n    id: String,\n    /// Name\n    name: String,\n    /// Password\n    password: Password,\n}\n\n/// Update user schema\n#[derive(Debug, Schema, Clone, Eq, PartialEq)]\nstruct UpdateUser {\n    /// Name\n    name: Option&lt;String&gt;,\n    /// Password\n    password: Option&lt;Password&gt;,\n}\n\n#[derive(Response)]\nenum CreateUserResponse {\n    /// Returns when the user is successfully created.\n    #[oai(status = 200)]\n    Ok,\n    /// Returns when the user already exists.\n    #[oai(status = 409)]\n    UserAlreadyExists,\n}\n\n#[derive(Response)]\nenum FindUserResponse {\n    /// Return the specified user.\n    #[oai(status = 200)]\n    Ok(Json&lt;User&gt;),\n    /// Return when the specified user is not found.\n    #[oai(status = 404)]\n    NotFound,\n}\n\n#[derive(Response)]\nenum DeleteUserResponse {\n    /// Returns when the user is successfully deleted.\n    #[oai(status = 200)]\n    Ok,\n    /// Return when the specified user is not found.\n    #[oai(status = 404)]\n    NotFound,\n}\n\n#[derive(Response)]\nenum UpdateUserResponse {\n    /// Returns when the user is successfully updated.\n    #[oai(status = 200)]\n    Ok,\n    /// Return when the specified user is not found.\n    #[oai(status = 404)]\n    NotFound,\n}\n\n#[derive(Default)]\nstruct Api {\n    users: Mutex&lt;HashMap&lt;String, User&gt;&gt;,\n}\n\n#[API]\nimpl Api {\n    /// Create a new user\n    #[oai(path = \"/users\", method = \"post\", tag = \"user\")]\n    async fn create_user(&amp;self, user: Json&lt;User&gt;) -&gt; CreateUserResponse {\n        let mut users = self.users.lock().await;\n        if users.contains_key(&amp;user.0.id) {\n            return CreateUserResponse::UserAlreadyExists;\n        }\n        users.insert(user.0.id.clone(), user.0);\n        CreateUserResponse::Ok\n    }\n\n    /// Find user by id\n    #[oai(path = \"/users/:user_id\", method = \"get\", tag = \"user\")]\n    async fn find_user(\n        &amp;self,\n        #[oai(name = \"user_id\", in = \"path\")] user_id: String,\n    ) -&gt; FindUserResponse {\n        let users = self.users.lock().await;\n        match users.get(&amp;user_id) {\n            Some(user) =&gt; FindUserResponse::Ok(Json(user.clone())),\n            None =&gt; FindUserResponse::NotFound,\n        }\n    }\n\n    /// Delete user by id\n    #[oai(path = \"/users/:user_id\", method = \"delete\", tag = \"user\")]\n    async fn delete_user(\n        &amp;self,\n        #[oai(name = \"user_id\", in = \"path\")] user_id: String,\n    ) -&gt; DeleteUserResponse {\n        let mut users = self.users.lock().await;\n        match users.remove(&amp;user_id) {\n            Some(_) =&gt; DeleteUserResponse::Ok,\n            None =&gt; DeleteUserResponse::NotFound,\n        }\n    }\n\n    /// Update user by id\n    #[oai(path = \"/users/:user_id\", method = \"put\", tag = \"user\")]\n    async fn put_user(\n        &amp;self,\n        #[oai(name = \"user_id\", in = \"path\")] user_id: String,\n        update: Json&lt;UpdateUser&gt;,\n    ) -&gt; UpdateUserResponse {\n        let mut users = self.users.lock().await;\n        match users.get_mut(&amp;user_id) {\n            Some(user) =&gt; {\n                if let Some(name) = update.0.name {\n                    user.name = name;\n                }\n                if let Some(password) = update.0.password {\n                    user.password = password;\n                }\n                UpdateUserResponse::Ok\n            }\n            None =&gt; UpdateUserResponse::NotFound,\n        }\n    }\n}\n\n#[tokio::main]\nasync fn main() {\n    poem::Server::bind(\"127.0.0.1:3000\")\n        .await\n        .unwrap()\n        .run(\n            OpenAPI::new(Api::default())\n                .title(\"poem-openapi\")\n                .version(\"0.1.0\")\n                .server_with_description(\"http://localhost:3000\", \"localhost\")\n                .tag_with_description(\"user\", \"Operations about user\")\n                .ui_path(\"/\"),\n        )\n        .await\n        .unwrap();\n}\n\n</code></pre>\n<p>è¦å®Œå…¨æ”¯æŒOpen APIè§„èŒƒä¸­å®šä¹‰çš„ç‰¹æ€§è¿˜æœ‰ä¸å°‘åŠŸèƒ½è¦åšï¼Œæ¯”å¦‚JsonSchemaçš„æ‰€æœ‰æ ¡éªŒå™¨ï¼Œè®¤è¯ï¼Œæƒé™ç­‰ç­‰ï¼Œå¦‚æœä½ è§‰å¾—è¿™ä¸ªåº“æœ‰ç”¨ï¼Œå¹¶ä¸”å¸Œæœ›èƒ½å¤Ÿä¸ºå®ƒè´¡çŒ®è‡ªå·±çš„åŠ›é‡ï¼Œæˆ‘éå¸¸æ¬¢è¿ï¼ğŸ˜</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-03 04:08:51","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-09-02 pixels 0.6.0 å‘å¸ƒ","link":"https://rustcc.cn/article?id=069ce86c-0ff8-4ded-9367-3a19651144fe","description":"<h3>pilka - ç”¨ Rust å†™æˆè·¨å¹³å°å®æ—¶ç¼–ç å·¥å…·</h3>\n<p>Pilkaæ˜¯ä¸€ç§ç”¨äºåˆ›å»ºç€è‰²å™¨ï¼ˆshaderï¼‰æ¼”ç¤ºçš„è·¨å¹³å°å®æ—¶ç¼–ç å·¥å…·ï¼Œç±»ä¼¼äº Bonzomatic æˆ– KodeLife ã€‚æ”¯æŒçƒ­é‡è½½ï¼Œèƒ½å¤Ÿåœ¨åå°æ£€æŸ¥å’Œæ›´æ–°èµ„æºã€‚</p>\n<p><a href=\"https://github.com/pudnax/pilka\" rel=\"noopener noreferrer\">GitHub</a>: https://github.com/pudnax/pilka</p>\n<h3>ritecahce - ç®€å•æ˜“ç”¨çš„ memory/disk cache</h3>\n<p>ritecache æ˜¯åœ¨æµè¡Œçš„ sccache/lru_disk_cache åŸºç¡€ä¸Šæ´¾ç”Ÿå‡ºçš„å†…å­˜/ç£ç›˜ç¼“å­˜ã€‚é»˜è®¤æä¾› <code>LruCache</code> å’Œ <code>LruDiskCache</code>ï¼Œä¹Ÿæ”¯æŒå¼€å‘è€…é€šè¿‡å®ç° <code>Cache</code> ç‰¹è´¨æ¥æ”¯æŒåŸºäºå…¶ä»–ç­–ç•¥çš„å†…å­˜/ç£ç›˜ç¼“å­˜ã€‚</p>\n<p>åŒæ—¶ï¼Œå¾—ç›Šäº ritelinked çš„æ”¯æŒï¼Œæ€§èƒ½ä¼˜äºåŸºäº linked-hash-map çš„ç‰ˆæœ¬ã€‚</p>\n<p><a href=\"https://github.com/ritelabs/ritecache\" rel=\"noopener noreferrer\">GitHub</a>: https://github.com/ritelabs/ritecache</p>\n<p><a href=\"https://crates.io/crates/ritecache\" rel=\"noopener noreferrer\">Crates.io</a>: https://crates.io/crates/ritecache</p>\n<h3>hebi - ç”± Bevy å¼•æ“é©±åŠ¨çš„è´ªåƒè›‡æ¸¸æˆ</h3>\n<p>hebi æ˜¯ä¸€ä¸ªé«˜åº¦å¯å®šåˆ¶çš„è´ªåƒè›‡æ¸¸æˆå¤åˆ»ï¼Œä½¿ç”¨ Rust å†™å°±ï¼Œç”± Bevy å¼•æ“é©±åŠ¨ï¼Œå‘½åæºäºæ—¥è¯­ä¸­çš„â€œè›‡â€ã€‚</p>\n<p><a href=\"https://github.com/ElnuDev/hebi\" rel=\"noopener noreferrer\">GitHub</a>: https://github.com/ElnuDev/hebi</p>\n<h3>pixels 0.6.0 å‘å¸ƒ</h3>\n<p>pixels æ˜¯ç”¨äºç®€å•è½¯ä»¶ä¾§å…‰æ …åŒ–çš„æ¿æ¡ç®±ã€‚å®ƒå¯ä»¥æä¾›ä¸€ä¸ªåƒç´ ç¼“å†²åŒºï¼Œç”¨äºæ’å…¥é¢œè‰²ï¼ˆåœ¨ CPU ç«¯å®Œæˆï¼‰ã€‚ç¼“å†²åŒºä½œä¸ºçº¹ç†ä¸Šè½½åˆ°GPUï¼Œæ‰€æœ‰ç¼©æ”¾å’Œå‰ªè£éƒ½ç”±é»˜è®¤ç€è‰²å™¨å¤„ç†ã€‚å¯¹äºå…¶ä»–æ§ä»¶ï¼Œå¯ä»¥æ·»åŠ è‡ªå·±çš„è‡ªå®šä¹‰ç€è‰²å™¨ä»¥è¿›è¡Œé¢„å¤„ç†å’Œåå¤„ç†ã€‚</p>\n<p><a href=\"https://github.com/parasyte/pixels\" rel=\"noopener noreferrer\">GitHub</a>: https://github.com/parasyte/pixels</p>\n<p><a href=\"https://crates.io/crates/pixels\" rel=\"noopener noreferrer\">Crates.io</a>: https://crates.io/crates/pixels</p>\n<h3>This Week in Datafuse 5</h3>\n<p>Datafuse å‘å¸ƒäº†ç¬¬ 5 æœŸå‘¨æŠ¥ï¼Œæ„Ÿå…´è¶£çš„æœ‹å‹ä»¬å¯ä»¥å…³æ³¨ Datafuse çš„æœ€æ–°è¿›å±•ã€‚</p>\n<p>Datafuse æ˜¯ä¸€ä¸ªå¼€æºã€æ˜“ç”¨ã€ä¾¿äºæ‰©å±•çš„äº‘æ•°ä»“ï¼ŒæŸ¥è¯¢é€Ÿåº¦æå¿«ï¼Œå¹¶ç»“åˆäº‘çš„å¼¹æ€§ã€ç®€å•æ€§å’Œä½æˆæœ¬ï¼Œå¸®åŠ©ç”¨æˆ·è½»æ¾äº«å—ä¸‹ä¸€ä»£æ•°æ®äº‘ã€‚</p>\n<p><a href=\"https://github.com/datafuselabs/datafuse\" rel=\"noopener noreferrer\">Datafuse</a>: https://github.com/datafuselabs/datafuse</p>\n<p><a href=\"https://datafuselabs.github.io/weekly/2021-09-01-datafuse-weekly/\" rel=\"noopener noreferrer\">This Week in Datafuse 5</a>: https://datafuselabs.github.io/weekly/2021-09-01-datafuse-weekly/</p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ <a href=\"https://github.com/PsiACE\" rel=\"noopener noreferrer\">PsiACE</a></p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rust.cc è®ºå›: æ”¯æŒ rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRust è¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 15:31:28","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"RustDeskå¯»æ‰¾ç¬¬ä¸€ä½å‘˜å·¥ï¼Œå›½å†…æˆ–è€…æ˜¯æ–°åŠ å¡ç”±ä½ å†³å®š","link":"https://rustcc.cn/article?id=ae7f2822-173b-45ba-9013-e5cb4e286810","description":"<p>rustcc.cnè§è¯äº†RustDeskä¸€è·¯çš„æˆé•¿ï¼Œåœ¨æ­¤æ¥å—äº†å¾ˆå¤šçš„é¼“åŠ±ï¼Œæˆ‘å—ç›ŠåŒªæµ…ã€‚è™½ç„¶ä¸€ç›´éƒ½æ˜¯ä¸€ä¸ªäººæˆ˜æ–—ï¼Œå¯å´æ—¶åˆ»æœŸç›¼ç€ä¼˜ç§€çš„ä½ åŠ å…¥ã€‚è™½ç„¶å…¬å¸å®ä½“è¿˜æ²¡æœ‰æ³¨å†Œå®Œæˆï¼Œä½†æ˜¯RustDeskå·²ç»æ”¶è·ä¸¤å®¶æŠ•èµ„æœºæ„çš„æŠ•èµ„æ„å‘ï¼Œä¸€å®¶å›½å†…ï¼Œä¸€å®¶å›½å¤–ï¼Œæ€»å…±è¶…è¿‡åƒä¸‡äººæ°‘å¸ã€‚å¦‚æœä½ ä¹ŸåƒæŠ•èµ„äººä¸€æ ·çœ‹å¥½RustDeskè¿™ä¸ªé¡¹ç›®ï¼Œèƒ½å¤Ÿç†è§£ä»–çš„æœªæ¥å‘å±•æ½œåŠ›ï¼Œè¯·é‚®ä»¶è”ç³»æˆ‘ï¼Œå¹¶é™„ä¸Šä½ çš„ç®€å†ã€‚è°¢è°¢ï¼ŒæœŸç›¼æˆ‘ä»¬ä¸€èµ·æˆé•¿ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 09:47:25","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"cfg-rs v0.2.3ï¼Œrusté…ç½®åº“","link":"https://rustcc.cn/article?id=529bbd2c-aed1-46e7-a5c3-ede7283c0c5d","description":"<p>å’Œconfigåº“çš„åŒºåˆ«</p>\n<ul>\n<li>æ”¯æŒå ä½ç¬¦è¡¨è¾¾å¼ï¼Œå¯ä»¥ç®€åŒ–é…ç½®è®¾è®¡ã€‚</li>\n<li>ä¸åŸºäºserdeï¼Œè®¾è®¡äº†ç‹¬ç«‹çš„traitç”¨æ¥åšé…ç½®å’Œstructçš„æ˜ å°„ï¼Œæä¾›è‡ªåŠ¨æ¨å¯¼åŠŸèƒ½ã€‚</li>\n<li>æ”¯æŒå¯æ›´æ–°å€¼ç±»å‹RefValueï¼Œåœ¨é…ç½®åˆ·æ–°æ—¶è‡ªåŠ¨æ›´æ–°ã€‚ä¸éœ€è¦å…¨å±€Configurationå®ä¾‹ã€‚</li>\n<li>é…åˆå ä½ç¬¦è¡¨è¾¾å¼ï¼Œæä¾›éšæœºå€¼è·å–ã€‚</li>\n</ul>\n<p>è¯¦æƒ…è¯·è®¿é—®\nhttps://docs.rs/cfg-rs\nhttps://github.com/leptonyu/cfg-rs</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 09:26:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"åˆ†äº«ä¸€ä¸ªvim for rust-langé…ç½®ï¼Œæ”¯æŒå¤šç§å¼€å‘è¯­è¨€ã€‚","link":"https://rustcc.cn/article?id=3c62e530-3aab-4111-925f-83e0338d3799","description":"<p>https://github.com/wandercn/go-ide-vim.conf.git\næœ‰å…´è¶£vimæµçš„å¯ä»¥è¯•è¯•ï¼Œæˆ‘è‡ªå·±ä¸€ç›´ç”¨ç€è¿˜å¯ä»¥ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-01 18:22:14","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust å¼‚æ­¥ç¼–ç¨‹äºŒ: Tokio å…¥é—¨è¿è¡Œæ—¶ä»‹ç» | Rust åŸ¹å…»æé«˜è®¡åˆ’ Vol. 6","link":"https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a","description":"<h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹äºŒ: Tokio å…¥é—¨è¿è¡Œæ—¶ä»‹ç»ã€‹|Vol. 6</h3>\n<p><strong>è¯¾ç¨‹æ—¶é—´:</strong>  2021å¹´9æœˆ5æ—¥ 20:00-21:00</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»:</strong>  ä¸Šå‘¨å…¬å¼€è¯¾æˆ‘ä»¬è®²è§£äº† Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹ï¼ˆ å±äºä¸€ä¸ªéå¸¸ç»å…¸çš„å†…å®¹ï¼Œå»ºè®®è§‚çœ‹ ï¼‰, å¤§å®¶å¯¹ Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹æœ‰äº†ä¸€ä¸ªåˆæ­¥è®¤è¯†,  Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹é‡Œéœ€è¦ Executorã€Reactorã€Future ç­‰, æœ¬å‘¨å…¬å¼€è¯¾å°†ä»¥ Tokio æ¡†æ¶ä¸ºåŸºç¡€, å’Œå¤§å®¶ä¸€èµ·èŠèŠ Tokio é‡Œçš„ Executorã€Reactorã€Future æ˜¯ä»€ä¹ˆ?</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<p>1ã€å›é¡¾ Rust å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>\n<p>2ã€è°ˆè°ˆå¯¹ Rust å¼‚æ­¥æ¡†æ¶çš„è®¤è¯† ( futures-rsã€async-stdã€tokio ) .</p>\n<p>3ã€Tokio ä»‹ç».</p>\n<p>4ã€Tokio é‡Œçš„ Executorã€Reactorã€Future å¦‚ä½•ä½¿ç”¨.</p>\n<p>5ã€ä½¿ç”¨ Tokio å®ç°ä¸€ä¸ªç®€å•çš„æœåŠ¡ç«¯ä¸å®¢æˆ·ç«¯ç¨‹åº.</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/\nRust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future Part 1  å›æ”¾åœ°å€ï¼š\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust å¼‚æ­¥ç¼–ç¨‹æ•™æï¼šhttps://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 08:40:15","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Š Rust å¼‚æ­¥ç¼–ç¨‹å…¥é—¨ Future ã€‹|Vol. 5</h3>\n<p><strong>è¯¾ç¨‹æ—¶é—´:</strong> 2021å¹´8æœˆ29æ—¥ 20:00-21:00</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»:</strong>  è®²åˆ° Rust ä½¿ç”¨ Future å¼‚æ­¥ç¼–ç¨‹ï¼Œå°±ä¸å¾—ä¸è¯´ futures å’Œ tokio è¿™ä¸¤ä¸ª crateï¼Œå…¶å®æ ‡å‡†åº“ä¸­çš„ futureï¼Œä»¥åŠ async/await å°±æ˜¯ä» futures åº“ä¸­æ•´åˆè¿›æ ‡å‡†åº“çš„, Tokio æ‹¥æœ‰æå¿«çš„æ€§èƒ½ï¼Œæ˜¯å¤§éƒ¨åˆ†ç³»ç»Ÿå¼‚æ­¥å¤„ç†çš„é€‰æ‹©ï¼Œå…¶æ„å»ºäº future ä¹‹ä¸Šã€‚Future æ˜¯  Rust å¼‚æ­¥ç¼–ç¨‹çš„æ ¸å¿ƒåŸºç¡€ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<p>1ã€ä¸ºä»€ä¹ˆéœ€è¦å¼‚æ­¥.</p>\n<p>2ã€ç†è§£å¼‚æ­¥ç¼–ç¨‹æ¨¡å‹.</p>\n<p>3ã€Future ç¼–ç¨‹æ¨¡å‹è®²è§£.</p>\n<p>4ã€å¸¦é¢†å¤§å®¶å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆçš„ future , å†æ¬¡å¸®å¿™å¤§å®¶ç†è§£</p>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>è¯¾ç¨‹ä¸­æ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"ã€Rustæ—¥æŠ¥ã€‘2021-08-19 -- Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 å¯èƒ½ä¼šå‡ºç°åœ¨ Rust 1.56ä¸­</h3>\n<p>å·²ç»åœ¨ä¸‹è½½æ¬¡æ•°æœ€å¤šçš„å‰ 10000 ä¸ªcrate ä¸Šæµ‹è¯•äº†ç‰ˆæœ¬è¿ç§»,å¹¶ä¸”å°†æµ‹è¯•æ‰€æœ‰å…¬å…±çš„ crateã€‚</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>å¼‚æ­¥å¼•æ“ C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D æ¸¸æˆå¼•æ“</h3>\n<ul>\n<li><strong>PCï¼ˆWindowsã€Linuxã€macOSï¼‰å’Œ Web (WebAssembly)</strong> æ”¯æŒã€‚</li>\n<li><strong>å»¶è¿Ÿç€è‰²</strong></li>\n<li><strong>å†…ç½®ä¿å­˜/åŠ è½½</strong></li>\n<li><strong>ç‹¬ç«‹åœºæ™¯ç¼–è¾‘å™¨</strong></li>\n<li><strong>é«˜çº§ç‰©ç†æ¨¡å‹</strong></li>\n<li><strong>åˆ†å±‚æ¨¡å‹èµ„æº</strong></li>\n<li><strong>å‡ ä½•å®ä¾‹åŒ–</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From æ—¥æŠ¥å°ç»„ å†°å±±ä¸Šçš„ mook &amp;&amp; æŒºè‚¥</p>\n<p>ç¤¾åŒºå­¦ä¹ äº¤æµå¹³å°è®¢é˜…ï¼š</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustccè®ºå›: æ”¯æŒrss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">å¾®ä¿¡å…¬ä¼—å·ï¼šRustè¯­è¨€ä¸­æ–‡ç¤¾åŒº</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"å…¬å¼€è¯¾: é€šè¿‡ Datafuse ç†è§£å…¨é“¾è·¯è·Ÿè¸ª | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>æœ¬å‘¨å…¬å¼€è¯¾ï¼šã€Šé€šè¿‡Datafuseç†è§£å…¨é“¾è·¯è·Ÿè¸ªã€‹| Vol. 4</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ22æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong> æ•°æ®åº“ç³»ç»Ÿä¹Ÿæ˜¯ä¸€ä¸ªéå¸¸å¤æ‚ï¼Œåºå¤§çš„ç³»ç»Ÿã€‚ç‰¹åˆ«æ˜¯åœ¨è°ƒè¯•å’Œè§‚å¯ŸSQLæ‰§è¡Œï¼Œå¤šçº¿ç¨‹ä»»åŠ¡åˆ‡æ¢ï¼Œå› ä¸ºæ²¡æœ‰å†…å­˜è°ƒç”¨æˆ–å †æ ˆè·Ÿè¸ªï¼Œè¿™ä¹Ÿæ˜¯åˆ†å¸ƒå¼è¿½è¸ªçš„ç”±æ¥ã€‚è¿™é‡Œé¢æ¶‰åŠåˆ°å¤šè¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ªä¸ºæè¿°å’Œåˆ†æè·¨è¿›ç¨‹äº‹åŠ¡æä¾›äº†ä¸€ç§è§£å†³æ–¹æ¡ˆã€‚Google Dapper(Dapper: å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿé“¾è·¯è¿½è¸ªåŸºç¡€è®¾æ–½)è®ºæ–‡(å„tracerçš„åŸºç¡€)ä¸­æè¿°äº†åˆ†å¸ƒå¼è¿½è¸ªçš„ä¸€äº›ä½¿ç”¨æ¡ˆä¾‹åŒ…æ‹¬å¼‚å¸¸æ£€æµ‹ã€è¯Šæ–­ç¨³æ€é—®é¢˜ã€åˆ†å¸ƒå¼åˆ†æã€èµ„æºå±æ€§å’Œå¾®æœåŠ¡çš„å·¥ä½œè´Ÿè½½å»ºæ¨¡ã€‚</p>\n<p>æœ¬æ¬¡å…¬å¼€è¯¾é€š Google çš„ OpenTraceing ä»‹ç»ï¼Œç»“åˆRustçš„ tokio-rs/tracing ä½¿ç”¨ï¼Œæœ€ç»ˆç»“åˆ Datafuse é¡¹ç›®ç»™å¤§å®¶å±•ç¤ºä¸€ä¸‹å¤§å‹åº”ç”¨çš„å…¨é“¾è·¯è·Ÿè¸ªåˆ†æè¿‡ç¨‹ã€‚</p>\n<p>å…³äºDatafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ol>\n<li>\n<p>ä»€ä¹ˆæ˜¯åˆ†å¸ƒå¼è¿½è¸ªç³»ç»ŸOpenTracingåŠåº”ç”¨åœºæ™¯</p>\n</li>\n<li>\n<p>ä»‹ç» tokio-rs/tracing åŠåœ¨ç¨‹åºå¼€å‘ä¸­çš„ä½œç”¨</p>\n</li>\n<li>\n<p>ä¸ºä»€ä¹ˆéœ€è¦tokio-rs/tracingåº“</p>\n</li>\n<li>\n<p>æ¼”ç¤ºDatafuseé¡¹ç›®ä¸­tokio-rs/tracingçš„ä½¿ç”¨</p>\n</li>\n</ol>\n<h3><strong>è®²å¸ˆä»‹ç»</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šDatafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒºã€çŸ¥æ•°å ‚ å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è·å– T-Shirt çš„æ–¹æ³•ï¼š</h3>\n<ol>\n<li>ç»™ https://github.com/datafuselabs/datafuse æ issue/pr</li>\n<li>è¿›è¡Œ Rustï¼Œå¤§æ•°æ®ï¼Œæ•°æ®åº“æ–¹é¢çš„å…¬å¼€è¯¾åˆ†äº«</li>\n<li>ç¤¾åŒºé‡Œåˆ†äº« datafuse ç›¸å…³æ–‡ç« </li>\n<li>datafuse.rs ä¸Šé¢æ–‡æ¡£ç¿»è¯‘å·¥ä½œ</li>\n</ol>\n<h3>å¾€æœŸè¯¾ç¨‹å›æ”¾</h3>\n<p>è®¤è¯†é¢å‘åŸºç¡€æ¶æ„è¯­è¨€ Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>ç†è§£ Rust çš„æ‰€æœ‰æƒ | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>é€šè¿‡å®æˆ˜ç†è§£ Rust å® | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n<p>Rustå®çš„ç»ƒä¹ é¡¹ç›®ï¼š   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"è®ºå›githubè´¦æˆ·æ— æ³•ç™»å½•è§£å†³ç¬”è®°","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>æœ‰åæ˜ è¿™ä¸¤å¤©githubè´¦æˆ·æ— æ³•ç™»å½•äº†ã€‚</p>\n<p>æŠ¥è¿™ä¸ªé”™ï¼š</p>\n<pre><code>get github user info err\n</code></pre>\n<p>æŸ¥äº†å‡ ä¸ªåœ°æ–¹ï¼š</p>\n<ol>\n<li>ä»£ç æ˜¯å¦è¿è¡Œæ­£å¸¸ï¼šOk</li>\n<li>httpsä»£ç†æ˜¯å¦æ­£å¸¸ï¼šOk</li>\n<li>æ£€æŸ¥äº†githubè¿”å›æ—¥å¿—ï¼Œå‘ç°æ˜¯ï¼š</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>è¿›å…¥è¿™ä¸ªåœ°å€ä¸€çœ‹ï¼š<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>åŸæ¥2020å¹´2æœˆå°±å·²ç»è¯´äº†ï¼Œè¦æ”¹è¦æ”¹ã€‚ä¸è¿‡æˆ‘ç¡®å®æ²¡ç•™æ„åˆ°è¿™ä¸ªä¿¡æ¯ã€‚ï¼šï¼ˆ</p>\n<p>æ„æ€å°±æ˜¯è¯´access_tokenä¸è¦æ”¾åœ¨queryå‚æ•°ä¸­ï¼Œè€Œæ˜¯è¦æ”¾åœ¨headeré‡Œé¢ã€‚ç…§å®ƒè¯´çš„ï¼Œæ”¹äº†åå°±å¥½äº†ã€‚</p>\n<p>ç‰¹æ­¤è®°å½•ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust çš„ Future ä¸ Javascript çš„ Promise åŠŸèƒ½å¯¹ç…§å‚è€ƒ","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>çš„<code>Future</code>ä¸<code>Javascript</code>çš„<code>Promise</code>åŠŸèƒ½å¯¹ç…§å‚è€ƒ</h1>\n<p>å­¦ä¹ æ–°é²œæŠ€æœ¯æ—¶ï¼Œæˆ‘æ€»æ˜¯ä¼šä¹ æƒ¯æ€§å‘æ›¾ç»ç†Ÿæ‚‰çš„å†…å®¹ä¸Šé ï¼Œç”šè‡³å¥—ç”¨ç°æœ‰çš„è®¤çŸ¥æ¨¡å‹ã€‚è¿™æ¬¡ä¹Ÿä¸ä¾‹å¤–ï¼Œå¯¹ç…§<code>Javascript - Promise/A+ API</code>æ¥è®°å¿†ä¸€éƒ¨åˆ†<code>Rust Future</code>å¸¸ç”¨<code>API</code>ã€‚</p>\n<blockquote>\n<p>æ³¨æ„ï¼šæ‰€æœ‰çš„<code>Rust - Future</code>æ“ä½œéƒ½æ˜¯ä»¥<code>.await</code>ç»“å°¾çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œä¸åŒäº<code>Javascript - Promise/A+</code>ï¼Œ<code>Rust - Future</code>æ˜¯æƒ°æ€§çš„ã€‚åªæœ‰è¢«<code>.await</code>æŒ‡ä»¤æ¿€æ´»åï¼Œåœ¨<code>Rust - Future</code>å†…å°è£…çš„æ“ä½œæ‰ä¼šè¢«çœŸæ­£åœ°æ‰§è¡Œã€‚</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">æè¿°</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">åœ¨ rust ä¸­ï¼ŒFuture è‡ªèº«ä¸åŒºåˆ†å¼‚æ­¥æˆåŠŸï¼Œè¿˜æ˜¯å¼‚æ­¥å¤±è´¥ã€‚éœ€è¦ç»™å¼‚æ­¥è®¡ç®—ç»“æœå¥—ä¸Š Result&lt;T, E&gt; é©¬ç”²ï¼Œæ¥åš resolve ä¸ reject çš„å·®åˆ«å¤„ç†ã€‚</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* ä»€ä¹ˆéƒ½ä¸åš */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll ä¸èƒ½è¢«ç”¨æ¥æ„é€ åŒ…å«äº†å¼‚æ­¥æ“ä½œçš„ Future å®ä¾‹ï¼Œå› ä¸ºã€å›è°ƒé—­åŒ…ã€‘å†…çš„ã€å¯ä¿®æ”¹å¼•ç”¨ã€‘&amp;mut Context&lt;'_&gt; ä¸èƒ½è¢«  ï¼ˆ1ï¼‰è·¨çº¿ç¨‹ä¼ é€’  ï¼ˆ2ï¼‰ä¼ é€’å‡ºé—­åŒ…ä½œç”¨åŸŸ2. task::spawn_blocking() ã€å›è°ƒé—­åŒ…ã€‘è¾“å…¥å‚æ•°å†…çš„ thread::sleep() ä¸æ˜¯é˜»å¡è¿è¡Œ task::spawn_blocking() çš„ä¸»çº¿ç¨‹ï¼Œè€Œæ˜¯é˜»å¡ä»ã€é˜»å¡ä»»åŠ¡çº¿ç¨‹æ± ã€‘ä¸­åˆ†é…æ¥è¿è¡Œé˜»å¡ä»»åŠ¡çš„ã€å·¥ä½œçº¿ç¨‹ã€‘ã€‚</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. æœ‰ä¸€ä¸ª promise/future å¤±è´¥å°±æ•´ä½“æ€§åœ°å¤±è´¥ã€‚2. try_join æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future çš„æˆåŠŸä¸å¤±è´¥ç»“æœéƒ½æ”¶é›†2. è¿”å›ç»“æœï¼š(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. ä»…åªæ”¶é›†ç¬¬ä¸€ä¸ªæˆåŠŸçš„ promise/future2. try_race æˆå‘˜æ–¹æ³•è¦æ±‚å…¶ Self ä¸º Future&lt;Output = Result&lt;T, E&gt;&gt;3. è¿”å›ç»“æœï¼šResult&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. æ”¶é›†ç¬¬ä¸€ä¸ªç»“æŸçš„ promise/futureï¼Œæ— è®ºå®ƒæ˜¯æˆåŠŸç»“æŸè¿˜æ˜¯å¤±è´¥æ”¶åœºã€‚2. è¿”å›ç»“æœï¼šT</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼š</strong>ã€Šé€šè¿‡å®æˆ˜ç†è§£ Rust å®ã€‹</p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š</strong>  2021å¹´8æœˆ15æ—¥ 20:30-21:30</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»ï¼š</strong></p>\n<p>å¦‚æœæƒ³ç”¨ Rust å¼€å‘å¤§å‹ç›®ï¼Œæˆ–è€…å­¦ä¹ å¤§å‹é¡¹ç›®ä»£ç ï¼Œç‰¹åˆ«æ˜¯æ¡†æ¶çº§åˆ«çš„é¡¹ç›®ï¼Œé‚£ä¹ˆ Rust çš„å®æœºåˆ¶è‚¯å®šæ˜¯ä¸€ä¸ªå¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚ ä¾‹å¦‚ datafuse ä¸­çš„ä¸€äº›é…ç½®ç®¡ç†ï¼š\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>è¿™å°±æ˜¯é€šè¿‡å®å®ç°é…ç½®çš„ç»Ÿä¸€è¡Œä¸ºï¼Œä»£ç å‚è€ƒï¼š\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust è¯­è¨€å¼ºå¤§çš„ä¸€ä¸ªç‰¹ç‚¹å°±æ˜¯å¯ä»¥åˆ›å»ºå’Œåˆ©ç”¨å®ï¼Œä¸è¿‡åˆ›å»ºå®çœ‹èµ·æ¥æŒºå¤æ‚ï¼Œå¸¸å¸¸ä»¤åˆšæ¥è§¦ Rust çš„å¼€å‘è€…ç”Ÿç•æƒ§ã€‚ åœ¨æœ¬æ¬¡å…¬å¼€è¯¾ä¸­å¸®åŠ©ä½ ç†è§£ Rust Macro çš„åŸºæœ¬åŸç†ï¼Œå­¦ä¹ å¦‚ä½•åˆ›è‡ªå·²çš„ Rust å®ï¼Œä»¥åŠæŸ¥çœ‹æºç å­¦ä¹ å®çš„å®ç°ã€‚</p>\n<h3>è¯¾ç¨‹å¤§çº²</h3>\n<ul>\n<li>ä»€ä¹ˆæ˜¯ Rust å®</li>\n<li>ä»€ä¹ˆæ˜¯å®è¿è¡ŒåŸç†</li>\n<li>å¦‚ä½•åˆ›å»º Rust å®è¿‡ç¨‹</li>\n<li>é˜…è¯» datafuse é¡¹ç›®æºç ï¼Œ å­¦ä¹ é¡¹ç›®ä¸­å®çš„å®ç°</li>\n</ul>\n<p><strong>è®²å¸ˆä»‹ç»</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloud é¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>è¯¾ç¨‹ä¸­è‹æ—è€å¸ˆæ¨èå…¥é—¨èµ„æ–™ï¼š</h3>\n<p>Ruståœ¨çº¿ç¼–è¾‘å™¨:                     https://play.rust-lang.org/</p>\n<p>ã€ŠRustè¯­è¨€ç¨‹åºè®¾è®¡ã€‹:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>æ‰“æ€ªé€šå…³å­¦ä¹ æ–¹å¼Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rustä¼˜ç§€é¡¹ç›®Datafuseï¼š        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rustå…¬å¼€è¯¾ï¼šç†è§£Rustçš„æ‰€æœ‰æƒ| Vol 2","link":"https://rustcc.cn/article?id=c107b830-9fe1-43dd-94a3-9efcd5544205","description":"<p><strong>è¯¾ç¨‹ä¸»é¢˜ï¼šã€Šç†è§£Rustæ‰€æœ‰æƒã€‹</strong></p>\n<p><strong>è¯¾ç¨‹æ—¶é—´ï¼š2021å¹´8æœˆ8æ—¥ 20:30-21:30</strong></p>\n<p><strong>å˜‰å®¾è®²å¸ˆï¼š è‹æ—</strong></p>\n<p><strong>å˜‰å®¾ä»‹ç»ï¼š</strong></p>\n<p>Rustä¸­æ–‡ç¤¾åŒºæˆå‘˜ï¼Œå¤šç‚¹DmallæŠ€æœ¯Leaderï¼Œå‰æŠ˜800äº’è”ç½‘ç ”å‘å›¢é˜Ÿè´Ÿè´£äººã€10ä½™å¹´ä¸€çº¿ç ”å‘ç»éªŒã€‚å…·æœ‰å¤šå¹´çš„è½¯ä»¶å¼€å‘ç»éªŒ, ç†Ÿç»ƒRubyã€Javaã€Rustç­‰å¼€å‘è¯­è¨€, åŒæ—¶ä¹Ÿå‚ä¸è¿‡Rustä¸­æ–‡ç¤¾åŒºæ—¥æŠ¥ç»´æŠ¤å·¥ä½œã€‚</p>\n<p><strong>è¯¾ç¨‹ä»‹ç»</strong></p>\n<p>æœ¬æ¬¡è¯¾ç¨‹é€šè¿‡10ä¸ªå·¦å³çš„å°ä¾‹å­ï¼Œå¸¦å¤§å®¶ç†è§£ä¸€ä¸‹Rustçš„æ‰€æœ‰æƒï¼ŒRustå¼•ç”¨å’Œå€Ÿç”¨ï¼ŒRustå˜é‡å…‹éš†å’Œå¤åˆ¶çš„ç†å¿µã€‚</p>\n<p><strong>å‚åŠ è¯¾ç¨‹</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/Rust-pbc-1.jpg\" alt=\"\"></p>\n<p><strong>è¯¾ç¨‹è§„åˆ’</strong></p>\n<p>æœ¬æ¬¡æ´»åŠ¨ç”±ï¼šçŸ¥æ•°å ‚ã€Datafuseé¡¹ç›®ã€Rustè¯­è¨€ä¸­æ–‡ç¤¾åŒº å…±åŒå‘èµ·ã€‚åæœŸä¹Ÿæ¬¢è¿Rustçˆ±å¥½è€…ï¼ŒRustä¼˜ç§€é¡¹ç›®ï¼Œ Data Cloudé¡¹ç›®æ¥åˆ†äº«ï¼Œå…¬å¼€è¯¾åˆ†äº«åˆä½œè”ç³»å¾®ä¿¡ï¼š82565387 å¤‡æ³¨ï¼šRust ã€‚ å…¬å¼€è¯¾å˜‰å®¾ &amp; Datafuse contributoréƒ½å¯ä»¥è·å–Datafuseçºªå¿µTæ¤ã€‚</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-08 02:04:00","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"æ•°æ®è¡¨ Timestamp æ—¥æœŸ Serialize","link":"https://rustcc.cn/article?id=2ff8a69e-59bb-4502-87c0-c3416ffae8a0","description":"<p>ä¸»è¦å‚è€ƒï¼š<a href=\"https://github.com/rustcc/forustm\" rel=\"noopener noreferrer\">Rustccç½‘ç«™æºç åº“</a></p>\n<p>åœ¨å¤„ç†æ•°æ®è¡¨ä¸­æ—¥æœŸç›¸å…³æ•°æ®æ—¶ï¼ŒSeralizeåºåˆ—åŒ–ç›¸å…³æ“ä½œä¼šæŠ¥é”™ï¼Œæç¤º DateTime å­—æ®µä¸è¯†åˆ«ï¼Œ\næŸ¥äº† rustcc æºç æ‰å‘ç°ä¾èµ–ä¸­éœ€è¦å¼€å¯ç›¸åº”çš„featureã€‚ç‰¹æ­¤è®°å½•ã€‚</p>\n<h2>1.ä¾èµ–çš„åº“ï¼š</h2>\n<pre><code>[dependencies]\n# æ—¥æœŸæ—¶é—´å¤„ç† éœ€è¦å¼€å¯ serde ç‰¹å¾ æ”¯æŒåºåˆ—åŒ–\nchrono = { version = \"0.4.19\", features = [\"serde\"] }\n\n# æ•°æ®åº“ORM\ndiesel = { version = \"1.4.4\", features = [\"postgres\", \"chrono\", \"uuid\", \"r2d2\"] }\ndotenv = \"0.15.0\"\nserde = { version = \"1.0.127\", features = [\"derive\"] }\nserde_json = \"1.0.66\"\nuuid = { version = \"0.8.2\", features = [\"serde\", \"v4\"] }\n</code></pre>\n<h2>2.åˆ›å»ºæ•°æ®è¡¨</h2>\n<pre><code>CREATE TABLE characters (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(128) UNIQUE NOT NULL,\n    age INTEGER NOT NULL DEFAULT 0,\n    friends VARCHAR NOT NULL DEFAULT '',\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP\n)\n</code></pre>\n<h2>3.æ•°æ®è¡¨å¯¹åº”çš„ model</h2>\n<pre><code>use chrono::{NaiveDateTime};\nuse serde::{Deserialize, Serialize};\n\n#[derive(Queryable, Serialize, Deserialize, Debug)]\npub struct Characters {\n    pub id: i32,\n    pub name: String,\n    pub age: i32,\n    pub friends: String,\n    // è¿™é‡Œçš„ NaiveDateTime æ—¥æœŸæ ¼å¼åºåˆ—åŒ–éœ€è¦å¼€å¯ç›¸å…³ features\n    pub created_at: NaiveDateTime,\n}\n</code></pre>\n<h2>4.è·å–æ•°æ®</h2>\n<pre><code>use db::schema::characters;\nuse db::{get_connection};\nuse db::models::{Characters, NewCharacter};\nuse db::schema::characters::dsl::*;\nuse diesel::QueryDsl;\nuse diesel::prelude::*;\n\nfn main() {\n    let conn = get_connection();\n\n    // æŸ¥è¯¢å¹´é¾„å¤§äº30çš„10æ¡æ•°æ®\n    let arr: Vec&lt;Characters&gt; = characters.filter(characters::age.gt(30))\n        .limit(10)\n        .load::&lt;Characters&gt;(&amp;conn)\n        .expect(\"Loading Error\");\n\n    let date_arr = arr.iter()\n        .map(|item| {\n\t    // æ•°æ®æ ¼å¼åŒ–\n            let t = item.created_at.format(\"%Y-%m-%d %H:%M:%S\").to_string();\n            println!(\"{} {}\", item.name, t);\n            t\n        })\n        .collect::&lt;Vec&lt;String&gt;&gt;();\n}\n</code></pre>\n<p>è¾“å‡ºç»“æœç±»ä¼¼ï¼š</p>\n<pre><code>Box 2021-08-05 09:39:34\nBobe 2021-08-05 09:39:34\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-08 01:40:35","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Cargo workspace config","link":"https://rustcc.cn/article?id=c3dcce30-1fc0-4819-8992-142365c7e21c","description":"<p><a href=\"https://kaisery.github.io/trpl-zh-cn/ch14-03-cargo-workspaces.html\" rel=\"noopener noreferrer\">Workspace æ–‡æ¡£é“¾æ¥</a></p>\n<h2>ç›®å½•ç»“æ„</h2>\n<pre><code>workspace-test/\n    Cargo.toml\n    db/\n        src/\n            bin/\n                init.rs\n        Cargo.tml\n</code></pre>\n<h2>workspace</h2>\n<p>workspace-test/Cargo.toml</p>\n<pre><code>[workspace]\nmembers = [\"db\"]\ndefault-member = \"db\"\n</code></pre>\n<h2>å­é¡¹ç›®</h2>\n<p>workspace-test/db/Cargo.toml</p>\n<pre><code>[package]\nname = \"db\"\nversion = \"0.1.0\"\nedition = \"2018\"\n\n[dependencies]\n\n# å¯é€‰çš„å¯æ‰§è¡Œæ–‡ä»¶é…ç½®\n# [[bin]]\n# name = \"init\"\n# path = \"src/bin/init.rs\"\n</code></pre>\n<h2>æ“ä½œ</h2>\n<pre><code># è¿è¡Œ init\ncargo run --bin init\n# -p æŒ‡å®šé¡¹ç›®\ncargo run -p db --bin init\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-04 09:54:31","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-09-03T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Don't Discard All the Biased Instances: Investigating a Core Assumption in Dataset Bias Mitigation Techniques. (arXiv:2109.00521v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00521","description":"<p>Existing techniques for mitigating dataset bias often leverage a biased model\nto identify biased instances. The role of these biased instances is then\nreduced during the training of the main model to enhance its robustness to\nout-of-distribution data. A common core assumption of these techniques is that\nthe main model handles biased instances similarly to the biased model, in that\nit will resort to biases whenever available. In this paper, we show that this\nassumption does not hold in general. We carry out a critical investigation on\ntwo well-known datasets in the domain, MNLI and FEVER, along with two biased\ninstance detection methods, partial-input and limited-capacity models. Our\nexperiments show that in around a third to a half of instances, the biased\nmodel is unable to predict the main model's behavior, highlighted by the\nsignificantly different parts of the input on which they base their decisions.\nBased on a manual validation, we also show that this estimate is highly in line\nwith human interpretation. Our findings suggest that down-weighting of\ninstances detected by bias detection methods, which is a widely-practiced\nprocedure, is an unnecessary waste of training data. We release our code to\nfacilitate reproducibility and future research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amirkhani_H/0/1/0/all/0/1\">Hossein Amirkhani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pilehvar_M/0/1/0/all/0/1\">Mohammad Taher Pilehvar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Text AutoAugment: Learning Compositional Augmentation Policy for Text Classification. (arXiv:2109.00523v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00523","description":"<p>Data augmentation aims to enrich training samples for alleviating the\noverfitting issue in low-resource or class-imbalanced situations. Traditional\nmethods first devise task-specific operations such as Synonym Substitute, then\npreset the corresponding parameters such as the substitution rate artificially,\nwhich require a lot of prior knowledge and are prone to fall into the\nsub-optimum. Besides, the number of editing operations is limited in the\nprevious methods, which decreases the diversity of the augmented data and thus\nrestricts the performance gain. To overcome the above limitations, we propose a\nframework named Text AutoAugment (TAA) to establish a compositional and\nlearnable paradigm for data augmentation. We regard a combination of various\noperations as an augmentation policy and utilize an efficient Bayesian\nOptimization algorithm to automatically search for the best policy, which\nsubstantially improves the generalization capability of models. Experiments on\nsix benchmark datasets show that TAA boosts classification accuracy in\nlow-resource and class-imbalanced regimes by an average of 8.8% and 9.7%,\nrespectively, outperforming strong baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ren_S/0/1/0/all/0/1\">Shuhuai Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1\">Xu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Search Engines with Interactive Agents. (arXiv:2109.00527v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00527","description":"<p>Can machines learn to use a search engine as an interactive tool for finding\ninformation? That would have far reaching consequences for making the world's\nknowledge more accessible. This paper presents first steps in designing agents\nthat learn meta-strategies for contextual query refinements. Our approach uses\nmachine reading to guide the selection of refinement terms from aggregated\nsearch results. Agents are then empowered with simple but effective search\noperators to exert fine-grained and transparent control over queries and search\nresults. We develop a novel way of generating synthetic search sessions, which\nleverages the power of transformer-based generative language models through\n(self-)supervised learning. We also present a reinforcement learning agent with\ndynamically constrained actions that can learn interactive search strategies\ncompletely from scratch. In both cases, we obtain significant improvements over\none-shot search with a strong information retrieval baseline. Finally, we\nprovide an in-depth analysis of the learned search policies.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Adolphs_L/0/1/0/all/0/1\">Leonard Adolphs</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Boerschinger_B/0/1/0/all/0/1\">Benjamin Boerschinger</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Buck_C/0/1/0/all/0/1\">Christian Buck</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huebscher_M/0/1/0/all/0/1\">Michelle Chen Huebscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ciaramita_M/0/1/0/all/0/1\">Massimiliano Ciaramita</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Espeholt_L/0/1/0/all/0/1\">Lasse Espeholt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hofmann_T/0/1/0/all/0/1\">Thomas Hofmann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kilcher_Y/0/1/0/all/0/1\">Yannic Kilcher</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Improving Adversarial Training of NLP Models. (arXiv:2109.00544v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00544","description":"<p>Adversarial training, a method for learning robust deep neural networks,\nconstructs adversarial examples during training. However, recent methods for\ngenerating NLP adversarial examples involve combinatorial search and expensive\nsentence encoders for constraining the generated instances. As a result, it\nremains challenging to use vanilla adversarial training to improve NLP models'\nperformance, and the benefits are mainly uninvestigated. This paper proposes a\nsimple and improved vanilla adversarial training process for NLP, which we name\nAttacking to Training ($\\texttt{A2T}$). The core part of $\\texttt{A2T}$ is a\nnew and cheaper word substitution attack optimized for vanilla adversarial\ntraining. We use $\\texttt{A2T}$ to train BERT and RoBERTa models on IMDB,\nRotten Tomatoes, Yelp, and SNLI datasets. Our results show that it is possible\nto train empirically robust NLP models using a much cheaper adversary. We\ndemonstrate that vanilla adversarial training with $\\texttt{A2T}$ can improve\nan NLP model's robustness to the attack it was originally trained with and also\ndefend the model against other types of attacks. Furthermore, we show that\n$\\texttt{A2T}$ can improve NLP models' standard accuracy, cross-domain\ngeneralization, and interpretability. Code is available at\n<a href=\"http://github.com/jinyongyoo/A2T\">this http URL</a> .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yoo_J/0/1/0/all/0/1\">Jin Yong Yoo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1\">Yanjun Qi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Knowledge Help General NLU? An Empirical Study. (arXiv:2109.00563v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00563","description":"<p>It is often observed in knowledge-centric tasks (e.g., common sense question\nand answering, relation classification) that the integration of external\nknowledge such as entity representation into language models can help provide\nuseful information to boost the performance. However, it is still unclear\nwhether this benefit can extend to general natural language understanding (NLU)\ntasks. In this work, we empirically investigated the contribution of external\nknowledge by measuring the end-to-end performance of language models with\nvarious knowledge integration methods. We find that the introduction of\nknowledge can significantly improve the results on certain tasks while having\nno adverse effects on other tasks. We then employ mutual information to reflect\nthe difference brought by knowledge and a neural interpretation model to reveal\nhow a language model utilizes external knowledge. Our study provides valuable\ninsights and guidance for practitioners to equip NLP models with knowledge.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1\">Ruochen Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1\">Michael Zeng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"DILBERT: Customized Pre-Training for Domain Adaptation withCategory Shift, with an Application to Aspect Extraction. (arXiv:2109.00571v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00571","description":"<p>The rise of pre-trained language models has yielded substantial progress in\nthe vast majority of Natural Language Processing (NLP) tasks. However, a\ngeneric approach towards the pre-training procedure can naturally be\nsub-optimal in some cases. Particularly, fine-tuning a pre-trained language\nmodel on a source domain and then applying it to a different target domain,\nresults in a sharp performance decline of the eventual classifier for many\nsource-target domain pairs. Moreover, in some NLP tasks, the output categories\nsubstantially differ between domains, making adaptation even more challenging.\nThis, for example, happens in the task of aspect extraction, where the aspects\nof interest of reviews of, e.g., restaurants or electronic devices may be very\ndifferent. This paper presents a new fine-tuning scheme for BERT, which aims to\naddress the above challenges. We name this scheme DILBERT: Domain Invariant\nLearning with BERT, and customize it for aspect extraction in the unsupervised\ndomain adaptation setting. DILBERT harnesses the categorical information of\nboth the source and the target domains to guide the pre-training process\ntowards a more domain and category invariant representation, thus closing the\ngap between the domains. We show that DILBERT yields substantial improvements\nover state-of-the-art baselines while using a fraction of the unlabeled data,\nparticularly in more challenging domain adaptation setups.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lekhtman_E/0/1/0/all/0/1\">Entony Lekhtman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ziser_Y/0/1/0/all/0/1\">Yftah Ziser</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fight Fire with Fire: Fine-tuning Hate Detectors using Large Samples of Generated Hate Speech. (arXiv:2109.00591v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00591","description":"<p>Automatic hate speech detection is hampered by the scarcity of labeled\ndatasetd, leading to poor generalization. We employ pretrained language models\n(LMs) to alleviate this data bottleneck. We utilize the GPT LM for generating\nlarge amounts of synthetic hate speech sequences from available labeled\nexamples, and leverage the generated data in fine-tuning large pretrained LMs\non hate detection. An empirical study using the models of BERT, RoBERTa and\nALBERT, shows that this approach improves generalization significantly and\nconsistently within and across data distributions. In fact, we find that\ngenerating relevant labeled hate speech sequences is preferable to using\nout-of-domain, and sometimes also within-domain, human-labeled examples.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wullach_T/0/1/0/all/0/1\">Tomer Wullach</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adler_A/0/1/0/all/0/1\">Amir Adler</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Minkov_E/0/1/0/all/0/1\">Einat Minkov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latin writing styles analysis with Machine Learning: New approach to old questions. (arXiv:2109.00601v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00601","description":"<p>In the Middle Ages texts were learned by heart and spread using oral means of\ncommunication from generation to generation. Adaptation of the art of prose and\npoems allowed keeping particular descriptions and compositions characteristic\nfor many literary genres. Taking into account such a specific construction of\nliterature composed in Latin, we can search for and indicate the probability\npatterns of familiar sources of specific narrative texts. Consideration of\nNatural Language Processing tools allowed us the transformation of textual\nobjects into numerical ones and then application of machine learning algorithms\nto extract information from the dataset. We carried out the task consisting of\nthe practical use of those concepts and observation to create a tool for\nanalyzing narrative texts basing on open-source databases. The tool focused on\ncreating specific search tools resources which could enable us detailed\nsearching throughout the text. The main objectives of the study take into\naccount finding similarities between sentences and between documents. Next, we\napplied machine learning algorithms on chosen texts to calculate specific\nfeatures of them (for instance authorship or centuries) and to recognize\nsources of anonymous texts with a certain percentage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bernardo_A/0/1/0/all/0/1\">Arianna Di Bernardo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poetto_S/0/1/0/all/0/1\">Simone Poetto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sillano_P/0/1/0/all/0/1\">Pietro Sillano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villata_B/0/1/0/all/0/1\">Beatrice Villata</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sojka_W/0/1/0/all/0/1\">Weronika S&#xf3;jka</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pietka_Danilewicz_Z/0/1/0/all/0/1\">Zofia Pi&#x119;tka-Danilewicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pranke_P/0/1/0/all/0/1\">Piotr Pranke</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Point-of-Interest Type Prediction using Text and Images. (arXiv:2109.00602v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00602","description":"<p>Point-of-interest (POI) type prediction is the task of inferring the type of\na place from where a social media post was shared. Inferring a POI's type is\nuseful for studies in computational social science including sociolinguistics,\ngeosemiotics, and cultural geography, and has applications in geosocial\nnetworking technologies such as recommendation and visualization systems. Prior\nefforts in POI type prediction focus solely on text, without taking visual\ninformation into account. However in reality, the variety of modalities, as\nwell as their semiotic relationships with one another, shape communication and\ninteractions in social media. This paper presents a study on POI type\nprediction using multimodal information from text and images available at\nposting time. For that purpose, we enrich a currently available data set for\nPOI type prediction with the images that accompany the text messages. Our\nproposed method extracts relevant information from each modality to effectively\ncapture interactions between text and image achieving a macro F1 of 47.21\nacross eight categories significantly outperforming the state-of-the-art method\nfor POI type prediction based on text-only methods. Finally, we provide a\ndetailed analysis to shed light on cross-modal interactions and the limitations\nof our best performing model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Villegas_D/0/1/0/all/0/1\">Danae S&#xe1;nchez Villegas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Aletras_N/0/1/0/all/0/1\">Nikolaos Aletras</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An unsupervised framework for tracing textual sources of moral change. (arXiv:2109.00608v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00608","description":"<p>Morality plays an important role in social well-being, but people's moral\nperception is not stable and changes over time. Recent advances in natural\nlanguage processing have shown that text is an effective medium for informing\nmoral change, but no attempt has been made to quantify the origins of these\nchanges. We present a novel unsupervised framework for tracing textual sources\nof moral change toward entities through time. We characterize moral change with\nprobabilistic topical distributions and infer the source text that exerts\nprominent influence on the moral time course. We evaluate our framework on a\ndiverse set of data ranging from social media to news articles. We show that\nour framework not only captures fine-grained human moral judgments, but also\nidentifies coherent source topics of moral change triggered by historical\nevents. We apply our methodology to analyze the news in the COVID-19 pandemic\nand demonstrate its utility in identifying sources of moral change in\nhigh-impact and real-time social events.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ramezani_A/0/1/0/all/0/1\">Aida Ramezani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1\">Zining Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rudzicz_F/0/1/0/all/0/1\">Frank Rudzicz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1\">Yang Xu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Algorithme de recherche approximative dans un dictionnaire fond\\'e sur une distance d'\\'edition d\\'efinie par blocs. (arXiv:2109.00624v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00624","description":"<p>We propose an algorithm for approximative dictionary lookup, where altered\nstrings are matched against reference forms. The algorithm makes use of a\ndivergence function between strings -- broadly belonging to the family of edit\ndistances; it finds dictionary entries whose distance to the search string is\nbelow a certain threshold. The divergence function is not the classical edit\ndistance (DL distance); it is adaptable to a particular corpus, and is based on\nelementary alteration costs defined on character blocks, rather than on\nindividual characters.\n</p>\n<p>Nous proposons un algorithme de recherche approximative de cha\\^ines dans un\ndictionnaire \\`a partir de formes alt\\'er\\'ees. Cet algorithme est fond\\'e sur\nune fonction de divergence entre cha\\^ines~ -- une sorte de distance\nd'\\'edition: il recherche des entr\\'ees pour lesquelles la distance \\`a la\ncha\\^ine cherch\\'ee est inf\\'erieure \\`a un certain seuil. La fonction\nutilis\\'ee n'est pas la distance d'\\'edition classique (distance DL); elle est\nadapt\\'ee \\`a un corpus, et se fonde sur la prise en compte de co\\^uts\nd'alt\\'eration \\'el\\'ementaires d\\'efinis non pas sur des caract\\`eres, mais\nsur des sous-cha\\^ines (des blocs de caract\\`eres).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Vaillant_P/0/1/0/all/0/1\">Pascal Vaillant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00627","description":"<p>Contextual knowledge is important for real-world automatic speech recognition\n(ASR) applications. In this paper, a novel tree-constrained pointer generator\n(TCPGen) component is proposed that incorporates such knowledge as a list of\nbiasing words into both attention-based encoder-decoder and transducer\nend-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing\nwords into an efficient prefix tree to serve as its symbolic input and creates\na neural shortcut between the tree and the final ASR output distribution to\nfacilitate recognising biasing words during decoding. Systems were trained and\nevaluated on the Librispeech corpus where biasing words were extracted at the\nscales of an utterance, a chapter, or a book to simulate different application\nscenarios. Experimental results showed that TCPGen consistently improved word\nerror rates (WERs) compared to the baselines, and in particular, achieved\nsignificant WER reductions on the biasing words. TCPGen is highly efficient: it\ncan handle 5,000 biasing words and distractors and only add a small overhead to\nmemory use and computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Ensemble Approach for Annotating Source Code Identifiers with Part-of-speech Tags. (arXiv:2109.00629v1 [cs.SE])","link":"http://arxiv.org/abs/2109.00629","description":"<p>This paper presents an ensemble part-of-speech tagging approach for source\ncode identifiers. Ensemble tagging is a technique that uses machine-learning\nand the output from multiple part-of-speech taggers to annotate natural\nlanguage text at a higher quality than the part-of-speech taggers are able to\nobtain independently. Our ensemble uses three state-of-the-art part-of-speech\ntaggers: SWUM, POSSE, and Stanford. We study the quality of the ensemble's\nannotations on five different types of identifier names: function, class,\nattribute, parameter, and declaration statement at the level of both individual\nwords and full identifier names. We also study and discuss the weaknesses of\nour tagger to promote the future amelioration of these problems through further\nresearch. Our results show that the ensemble achieves 75\\% accuracy at the\nidentifier level and 84-86\\% accuracy at the word level. This is an increase of\n+17\\% points at the identifier level from the closest independent\npart-of-speech tagger.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Newman_C/0/1/0/all/0/1\">Christian D. Newman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Decker_M/0/1/0/all/0/1\">Michael J. Decker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+AlSuhaibani_R/0/1/0/all/0/1\">Reem S. AlSuhaibani</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peruma_A/0/1/0/all/0/1\">Anthony Peruma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mohapatra_S/0/1/0/all/0/1\">Satyajit Mohapatra</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vishnoi_T/0/1/0/all/0/1\">Tejal Vishnoi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1\">Marcos Zampieri</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mkaouer_M/0/1/0/all/0/1\">Mohamed W. Mkaouer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sheldon_T/0/1/0/all/0/1\">Timothy J. Sheldon</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hill_E/0/1/0/all/0/1\">Emily Hill</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The VoicePrivacy 2020 Challenge: Results and findings. (arXiv:2109.00648v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00648","description":"<p>This paper presents the results and analyses stemming from the first\nVoicePrivacy 2020 Challenge which focuses on developing anonymization solutions\nfor speech technology. We provide a systematic overview of the challenge design\nwith an analysis of submitted systems and evaluation results. In particular, we\ndescribe the voice anonymization task and datasets used for system development\nand evaluation. Also, we present different attack models and the associated\nobjective and subjective evaluation metrics. We introduce two anonymization\nbaselines and provide a summary description of the anonymization systems\ndeveloped by the challenge participants. We report objective and subjective\nevaluation results for baseline and submitted systems. In addition, we present\nexperimental results for alternative privacy metrics and attack models\ndeveloped as a part of the post-evaluation analysis. Finally, we summarize our\ninsights and observations that will influence the design of the next\nVoicePrivacy challenge edition and some directions for future voice\nanonymization research.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tomashenko_N/0/1/0/all/0/1\">Natalia Tomashenko</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1\">Xin Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Vincent_E/0/1/0/all/0/1\">Emmanuel Vincent</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Patino_J/0/1/0/all/0/1\">Jose Patino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1\">Brij Mohan Lal Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Noe_P/0/1/0/all/0/1\">Paul-Gauthier No&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nautsch_A/0/1/0/all/0/1\">Andreas Nautsch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Evans_N/0/1/0/all/0/1\">Nicholas Evans</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yamagishi_J/0/1/0/all/0/1\">Junichi Yamagishi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+OBrien_B/0/1/0/all/0/1\">Benjamin O&#x27;Brien</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chanclu_A/0/1/0/all/0/1\">Ana&#xef;s Chanclu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bonastre_J/0/1/0/all/0/1\">Jean-Fran&#xe7;ois Bonastre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Todisco_M/0/1/0/all/0/1\">Massimiliano Todisco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Maouche_M/0/1/0/all/0/1\">Mohamed Maouche</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Making the Most of Dialogue Characteristics for Neural Chat Translation. (arXiv:2109.00668v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00668","description":"<p>Neural Chat Translation (NCT) aims to translate conversational text between\nspeakers of different languages. Despite the promising performance of\nsentence-level and context-aware neural machine translation models, there still\nremain limitations in current NCT models because the inherent dialogue\ncharacteristics of chat, such as dialogue coherence and speaker personality,\nare neglected. In this paper, we propose to promote the chat translation by\nintroducing the modeling of dialogue characteristics into the NCT model. To\nthis end, we design four auxiliary tasks including monolingual response\ngeneration, cross-lingual response generation, next utterance discrimination,\nand speaker identification. Together with the main chat translation task, we\noptimize the NCT model through the training objectives of all these tasks. By\nthis means, the NCT model can be enhanced by capturing the inherent dialogue\ncharacteristics, thus generating more coherent and speaker-relevant\ntranslations. Comprehensive experiments on four language directions\n(English-German and English-Chinese) verify the effectiveness and superiority\nof the proposed approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1\">Chulun Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1\">Jinsong Su</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Empirical Exploration in Quality Filtering of Text Data. (arXiv:2109.00698v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00698","description":"<p>While conventional wisdom suggests that more aggressively filtering data from\nlow-quality sources like Common Crawl always monotonically improves the quality\nof training data, we find that aggressive filtering can in fact lead to a\ndecrease in model quality on a wide array of downstream tasks for a GPT-like\nlanguage model. We speculate that this is because optimizing sufficiently\nstrongly for a proxy metric harms performance on the true objective, suggesting\na need for more robust filtering objectives when attempting to filter more\naggressively. We hope this work leads to detailed analysis of the effects of\ndataset filtering design choices on downstream model performance in future\nwork.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1\">Leo Gao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ShopTalk: A System for Conversational Faceted Search. (arXiv:2109.00702v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00702","description":"<p>We present ShopTalk, a multi-turn conversational faceted search system for\nshopping that is designed to handle large and complex schemas that are beyond\nthe scope of state of the art slot-filling systems. ShopTalk decouples dialog\nmanagement from fulfillment, thereby allowing the dialog understanding system\nto be domain-agnostic and not tied to the particular shopping application. The\ndialog understanding system consists of a deep-learned Contextual Language\nUnderstanding module, which interprets user utterances, and a primarily\nrules-based Dialog-State Tracker (DST), which updates the dialog state and\nformulates search requests intended for the fulfillment engine. The interface\nbetween the two modules consists of a minimal set of domain-agnostic \"intent\noperators,\" which instruct the DST on how to update the dialog state. ShopTalk\nwas deployed in 2020 on the Google Assistant for Shopping searches.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Manku_G/0/1/0/all/0/1\">Gurmeet Manku</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_Thorp_J/0/1/0/all/0/1\">James Lee-Thorp</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kanagal_B/0/1/0/all/0/1\">Bhargav Kanagal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ainslie_J/0/1/0/all/0/1\">Joshua Ainslie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1\">Jingchen Feng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pearson_Z/0/1/0/all/0/1\">Zach Pearson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Anjorin_E/0/1/0/all/0/1\">Ebenezer Anjorin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gandhe_S/0/1/0/all/0/1\">Sudeep Gandhe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eckstein_I/0/1/0/all/0/1\">Ilya Eckstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rosswog_J/0/1/0/all/0/1\">Jim Rosswog</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sanghai_S/0/1/0/all/0/1\">Sumit Sanghai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pohl_M/0/1/0/all/0/1\">Michael Pohl</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adams_L/0/1/0/all/0/1\">Larry Adams</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sivakumar_D/0/1/0/all/0/1\">D. Sivakumar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LightNER: A Lightweight Generative Framework with Prompt-guided Attention for Low-resource NER. (arXiv:2109.00720v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00720","description":"<p>NER in low-resource languages or domains suffers from inadequate training\ndata. Existing transfer learning approaches for low-resource NER usually have\nthe challenge that the target domain has different label sets compared with a\nresource-rich source domain, which can be concluded as class transfer and\ndomain transfer problems. In this paper, we propose a lightweight generative\nframework with prompt-guided attention for low-resource NER (LightNER) to\naddress these issues. Concretely, instead of tackling the problem by training\nlabel-specific discriminative classifiers, we convert sequence labeling to\ngenerate the entity pointer index sequence and entity categories without any\nlabel-specific classifiers, which can address the class transfer issue. We\nfurther propose prompt-guided attention by incorporating continuous prompts\ninto the self-attention layer to re-modulate the attention and adapt\npre-trained weights. Note that we only tune those continuous prompts with the\nwhole parameter of the pre-trained language model fixed, thus, making our\napproach lightweight and flexible for low-resource scenarios and can better\ntransfer knowledge across domains. Experimental results show that by tuning\nonly 0.16% of the parameters, LightNER can obtain comparable performance in the\nstandard setting and outperform standard sequence labeling and prototype-based\nmethods in low-resource settings.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Lei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xin Xie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1\">Shumin Deng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1\">Chuanqi Tan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1\">Fei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1\">Luo Si</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond. (arXiv:2109.00725v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00725","description":"<p>A fundamental goal of scientific research is to learn about causal\nrelationships. However, despite its critical role in the life and social\nsciences, causality has not had the same importance in Natural Language\nProcessing (NLP), which has traditionally placed more emphasis on predictive\ntasks. This distinction is beginning to fade, with an emerging area of\ninterdisciplinary research at the convergence of causal inference and language\nprocessing. Still, research on causality in NLP remains scattered across\ndomains without unified definitions, benchmark datasets and clear articulations\nof the remaining challenges. In this survey, we consolidate research across\nacademic areas and situate it in the broader NLP landscape. We introduce the\nstatistical challenge of estimating causal effects, encompassing settings where\ntext is used as an outcome, treatment, or as a means to address confounding. In\naddition, we explore potential uses of causal inference to improve the\nperformance, robustness, fairness, and interpretability of NLP models. We thus\nprovide a unified overview of causal inference for the computational\nlinguistics community.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1\">Katherine A. Keith</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manzoor_E/0/1/0/all/0/1\">Emaad Manzoor</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pryzant_R/0/1/0/all/0/1\">Reid Pryzant</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sridhar_D/0/1/0/all/0/1\">Dhanya Sridhar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wood_Doughty_Z/0/1/0/all/0/1\">Zach Wood-Doughty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Eisenstein_J/0/1/0/all/0/1\">Jacob Eisenstein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Grimmer_J/0/1/0/all/0/1\">Justin Grimmer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Roberts_M/0/1/0/all/0/1\">Margaret E. Roberts</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stewart_B/0/1/0/all/0/1\">Brandon M. Stewart</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veitch_V/0/1/0/all/0/1\">Victor Veitch</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1\">Diyi Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ConQX: Semantic Expansion of Spoken Queries for Intent Detection based on Conditioned Text Generation. (arXiv:2109.00729v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00729","description":"<p>Intent detection of spoken queries is a challenging task due to their noisy\nstructure and short length. To provide additional information regarding the\nquery and enhance the performance of intent detection, we propose a method for\nsemantic expansion of spoken queries, called ConQX, which utilizes the text\ngeneration ability of an auto-regressive language model, GPT-2. To avoid\noff-topic text generation, we condition the input query to a structured context\nwith prompt mining. We then apply zero-shot, one-shot, and few-shot learning.\nWe lastly use the expanded queries to fine-tune BERT and RoBERTa for intent\ndetection. The experimental results show that the performance of intent\ndetection can be improved by our semantic expansion method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yilmaz_E/0/1/0/all/0/1\">Eyup Halit Yilmaz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Toraman_C/0/1/0/all/0/1\">Cagri Toraman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural News Recommendation with Collaborative News Encoding and Structural User Encoding. (arXiv:2109.00750v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00750","description":"<p>Automatic news recommendation has gained much attention from the academic\ncommunity and industry. Recent studies reveal that the key to this task lies\nwithin the effective representation learning of both news and users. Existing\nworks typically encode news title and content separately while neglecting their\nsemantic interaction, which is inadequate for news text comprehension. Besides,\nprevious models encode user browsing history without leveraging the structural\ncorrelation of user browsed news to reflect user interests explicitly. In this\nwork, we propose a news recommendation framework consisting of collaborative\nnews encoding (CNE) and structural user encoding (SUE) to enhance news and user\nrepresentation learning. CNE equipped with bidirectional LSTMs encodes news\ntitle and content collaboratively with cross-selection and cross-attention\nmodules to learn semantic-interactive news representations. SUE utilizes graph\nconvolutional networks to extract cluster-structural features of user history,\nfollowed by intra-cluster and inter-cluster attention modules to learn\nhierarchical user interest representations. Experiment results on the MIND\ndataset validate the effectiveness of our model to improve the performance of\nnews recommendation. Our code is released at\nhttps://github.com/Veason-silverbullet/NNR.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1\">Zhiming Mao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1\">Xingshan Zeng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1\">Kam-Fai Wong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MWPToolkit: An Open-Source Framework for Deep Learning-Based Math Word Problem Solvers. (arXiv:2109.00799v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00799","description":"<p>Developing automatic Math Word Problem (MWP) solvers has been an interest of\nNLP researchers since the 1960s. Over the last few years, there are a growing\nnumber of datasets and deep learning-based methods proposed for effectively\nsolving MWPs. However, most existing methods are benchmarked soly on one or two\ndatasets, varying in different configurations, which leads to a lack of\nunified, standardized, fair, and comprehensive comparison between methods. This\npaper presents MWPToolkit, the first open-source framework for solving MWPs. In\nMWPToolkit, we decompose the procedure of existing MWP solvers into multiple\ncore components and decouple their models into highly reusable modules. We also\nprovide a hyper-parameter search function to boost the performance. In total,\nwe implement and compare 17 MWP solvers on 4 widely-used single equation\ngeneration benchmarks and 2 multiple equations generation benchmarks. These\nfeatures enable our MWPToolkit to be suitable for researchers to reproduce\nadvanced baseline models and develop new MWP solvers quickly. Code and\ndocuments are available at https://github.com/LYH-YF/MWPToolkit.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yihuai Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1\">Lei Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qiyuan Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1\">Yunshi Lan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1\">Bing Tian Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dongxiang Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1\">Ee-Peng Lim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Imposing Relation Structure in Language-Model Embeddings Using Contrastive Learning. (arXiv:2109.00840v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00840","description":"<p>Though language model text embeddings have revolutionized NLP research, their\nability to capture high-level semantic information, such as relations between\nentities in text, is limited. In this paper, we propose a novel contrastive\nlearning framework that trains sentence embeddings to encode the relations in a\ngraph structure. Given a sentence (unstructured text) and its graph, we use\ncontrastive learning to impose relation-related structure on the token-level\nrepresentations of the sentence obtained with a CharacterBERT (El Boukkouri et\nal.,2020) model. The resulting relation-aware sentence embeddings achieve\nstate-of-the-art results on the relation extraction task using only a simple\nKNN classifier, thereby demonstrating the success of the proposed method.\nAdditional visualization by a tSNE analysis shows the effectiveness of the\nlearned representation space compared to baselines. Furthermore, we show that\nwe can learn a different space for named entity recognition, again using a\ncontrastive learning objective, and demonstrate how to successfully combine\nboth representation spaces in an entity-relation task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Theodoropoulos_C/0/1/0/all/0/1\">Christos Theodoropoulos</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Henderson_J/0/1/0/all/0/1\">James Henderson</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Coman_A/0/1/0/all/0/1\">Andrei C. Coman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Moens_M/0/1/0/all/0/1\">Marie-Francine Moens</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models for Code Understanding and Generation. (arXiv:2109.00859v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00859","description":"<p>Pre-trained models for Natural Languages (NL) like BERT and GPT have been\nrecently shown to transfer well to Programming Languages (PL) and largely\nbenefit a broad set of code-related tasks. Despite their success, most current\nmethods either rely on an encoder-only (or decoder-only) pre-training that is\nsuboptimal for generation (resp. understanding) tasks or process the code\nsnippet in the same way as NL, neglecting the special characteristics of PL\nsuch as token types. We present CodeT5, a unified pre-trained encoder-decoder\nTransformer model that better leverages the code semantics conveyed from the\ndeveloper-assigned identifiers. Our model employs a unified framework to\nseamlessly support both code understanding and generation tasks and allows for\nmulti-task learning. Besides, we propose a novel identifier-aware pre-training\ntask that enables the model to distinguish which code tokens are identifiers\nand to recover them when they are masked. Furthermore, we propose to exploit\nthe user-written code comments with a bimodal dual generation task for better\nNL-PL alignment. Comprehensive experiments show that CodeT5 significantly\noutperforms prior methods on understanding tasks such as code defect detection\nand clone detection, and generation tasks across various directions including\nPL-NL, NL-PL, and PL-PL. Further analysis reveals that our model can better\ncapture semantic information from code. Our code and pre-trained models are\nreleased at https: //github.com/salesforce/CodeT5 .\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yue Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1\">Weishi Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1\">Shafiq Joty</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1\">Steven C.H. Hoi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge Perceived Multi-modal Pretraining in E-commerce. (arXiv:2109.00895v1 [cs.CV])","link":"http://arxiv.org/abs/2109.00895","description":"<p>In this paper, we address multi-modal pretraining of product data in the\nfield of E-commerce. Current multi-modal pretraining methods proposed for image\nand text modalities lack robustness in the face of modality-missing and\nmodality-noise, which are two pervasive problems of multi-modal product data in\nreal E-commerce scenarios. To this end, we propose a novel method, K3M, which\nintroduces knowledge modality in multi-modal pretraining to correct the noise\nand supplement the missing of image and text modalities. The modal-encoding\nlayer extracts the features of each modality. The modal-interaction layer is\ncapable of effectively modeling the interaction of multiple modalities, where\nan initial-interactive feature fusion model is designed to maintain the\nindependence of image modality and text modality, and a structure aggregation\nmodule is designed to fuse the information of image, text, and knowledge\nmodalities. We pretrain K3M with three pretraining tasks, including masked\nobject modeling (MOM), masked language modeling (MLM), and link prediction\nmodeling (LPM). Experimental results on a real-world E-commerce dataset and a\nseries of product-based downstream tasks demonstrate that K3M achieves\nsignificant improvements in performances than the baseline and state-of-the-art\nmethods when modality-noise or modality-missing exists.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yushan Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tou_H/0/1/0/all/0/1\">Huaixiao Tou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1\">Wen Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1\">Ganqiang Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Hui Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1\">Huajun Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"MultiEURLEX -- A multi-lingual and multi-label legal document classification dataset for zero-shot cross-lingual transfer. (arXiv:2109.00904v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00904","description":"<p>We introduce MULTI-EURLEX, a new multilingual dataset for topic\nclassification of legal documents. The dataset comprises 65k European Union\n(EU) laws, officially translated in 23 languages, annotated with multiple\nlabels from the EUROVOC taxonomy. We highlight the effect of temporal concept\ndrift and the importance of chronological, instead of random splits. We use the\ndataset as a testbed for zero-shot cross-lingual transfer, where we exploit\nannotated training documents in one language (source) to classify documents in\nanother language (target). We find that fine-tuning a multilingually pretrained\nmodel (XLM-ROBERTA, MT5) in a single source language leads to catastrophic\nforgetting of multilingual knowledge and, consequently, poor zero-shot transfer\nto other languages. Adaptation strategies, namely partial fine-tuning,\nadapters, BITFIT, LNFIT, originally proposed to accelerate fine-tuning for new\nend-tasks, help retain multilingual knowledge from pretraining, substantially\nimproving zero-shot cross-lingual transfer, but their impact also depends on\nthe pretrained model used and the size of the label set.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chalkidis_I/0/1/0/all/0/1\">Ilias Chalkidis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fergadiotis_M/0/1/0/all/0/1\">Manos Fergadiotis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Androutsopoulos_I/0/1/0/all/0/1\">Ion Androutsopoulos</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coarse-To-Fine And Cross-Lingual ASR Transfer. (arXiv:2109.00916v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00916","description":"<p>End-to-end neural automatic speech recognition systems achieved recently\nstate-of-the-art results, but they require large datasets and extensive\ncomputing resources. Transfer learning has been proposed to overcome these\ndifficulties even across languages, e.g., German ASR trained from an English\nmodel. We experiment with much less related languages, reusing an English model\nfor Czech ASR. To simplify the transfer, we propose to use an intermediate\nalphabet, Czech without accents, and document that it is a highly effective\nstrategy. The technique is also useful on Czech data alone, in the style of\ncoarse-to-fine training. We achieve substantial eductions in training time as\nwell as word error rate (WER).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Polak_P/0/1/0/all/0/1\">Peter Pol&#xe1;k</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bojar_O/0/1/0/all/0/1\">Ond&#x159;ej Bojar</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Improving Multimodal fusion via Mutual Dependency Maximisation. (arXiv:2109.00922v1 [cs.LG])","link":"http://arxiv.org/abs/2109.00922","description":"<p>Multimodal sentiment analysis is a trending area of research, and the\nmultimodal fusion is one of its most active topic. Acknowledging humans\ncommunicate through a variety of channels (i.e visual, acoustic, linguistic),\nmultimodal systems aim at integrating different unimodal representations into a\nsynthetic one. So far, a consequent effort has been made on developing complex\narchitectures allowing the fusion of these modalities. However, such systems\nare mainly trained by minimising simple losses such as $L_1$ or cross-entropy.\nIn this work, we investigate unexplored penalties and propose a set of new\nobjectives that measure the dependency between modalities. We demonstrate that\nour new penalties lead to a consistent improvement (up to $4.3$ on accuracy)\nacross a large variety of state-of-the-art models on two well-known sentiment\nanalysis datasets: \\texttt{CMU-MOSI} and \\texttt{CMU-MOSEI}. Our method not\nonly achieves a new SOTA on both datasets but also produces representations\nthat are more robust to modality drops. Finally, a by-product of our methods\nincludes a statistical network which can be used to interpret the high\ndimensional representations learnt by the model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Colombo_P/0/1/0/all/0/1\">Pierre Colombo</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chapuis_E/0/1/0/all/0/1\">Emile Chapuis</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labeau_M/0/1/0/all/0/1\">Matthieu Labeau</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Clavel_C/0/1/0/all/0/1\">Chloe Clavel</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Speaker-Conditioned Hierarchical Modeling for Automated Speech Scoring. (arXiv:2109.00928v1 [eess.AS])","link":"http://arxiv.org/abs/2109.00928","description":"<p>Automatic Speech Scoring (ASS) is the computer-assisted evaluation of a\ncandidate's speaking proficiency in a language. ASS systems face many\nchallenges like open grammar, variable pronunciations, and unstructured or\nsemi-structured content. Recent deep learning approaches have shown some\npromise in this domain. However, most of these approaches focus on extracting\nfeatures from a single audio, making them suffer from the lack of\nspeaker-specific context required to model such a complex task. We propose a\nnovel deep learning technique for non-native ASS, called speaker-conditioned\nhierarchical modeling. In our technique, we take advantage of the fact that\noral proficiency tests rate multiple responses for a candidate. We extract\ncontext vectors from these responses and feed them as additional\nspeaker-specific context to our network to score a particular response. We\ncompare our technique with strong baselines and find that such modeling\nimproves the model's average performance by 6.92% (maximum = 12.86%, minimum =\n4.51%). We further show both quantitative and qualitative insights into the\nimportance of this additional context in solving the problem of ASS.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Singla_Y/0/1/0/all/0/1\">Yaman Kumar Singla</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Gupta_A/0/1/0/all/0/1\">Avykat Gupta</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Bagga_S/0/1/0/all/0/1\">Shaurya Bagga</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1\">Changyou Chen</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Krishnamurthy_B/0/1/0/all/0/1\">Balaji Krishnamurthy</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Shah_R/0/1/0/all/0/1\">Rajiv Ratn Shah</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Coordinating Narratives and the Capitol Riots on Parler. (arXiv:2109.00945v1 [cs.SI])","link":"http://arxiv.org/abs/2109.00945","description":"<p>Coordinated disinformation campaigns are used to influence social media\nusers, potentially leading to offline violence. In this study, we introduce a\ngeneral methodology to uncover coordinated messaging through analysis of user\nparleys on Parler. The proposed method constructs a user-to-user coordination\nnetwork graph induced by a user-to-text graph and a text-to-text similarity\ngraph. The text-to-text graph is constructed based on the textual similarity of\nParler posts. We study three influential groups of users in the 6 January 2020\nCapitol riots and detect networks of coordinated user clusters that are all\nposting similar textual content in support of different disinformation\nnarratives related to the U.S. 2020 elections.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ng_L/0/1/0/all/0/1\">Lynnette Hui Xian Ng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cruickshank_I/0/1/0/all/0/1\">Iain Cruickshank</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Carley_K/0/1/0/all/0/1\">Kathleen M. Carley</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"LegaLMFiT: Efficient Short Legal Text Classification with LSTM Language Model Pre-Training. (arXiv:2109.00993v1 [cs.CL])","link":"http://arxiv.org/abs/2109.00993","description":"<p>Large Transformer-based language models such as BERT have led to broad\nperformance improvements on many NLP tasks. Domain-specific variants of these\nmodels have demonstrated excellent performance on a variety of specialised\ntasks. In legal NLP, BERT-based models have led to new state-of-the-art results\non multiple tasks. The exploration of these models has demonstrated the\nimportance of capturing the specificity of the legal language and its\nvocabulary. However, such approaches suffer from high computational costs,\nleading to a higher ecological impact and lower accessibility. Our findings,\nfocusing on English language legal text, show that lightweight LSTM-based\nLanguage Models are able to capture enough information from a small legal text\npretraining corpus and achieve excellent performance on short legal text\nclassification tasks. This is achieved with a significantly reduced\ncomputational overhead compared to BERT-based models. However, our method also\nshows degraded performance on a more complex task, multi-label classification\nof longer documents, highlighting the limitations of this lightweight approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Clavie_B/0/1/0/all/0/1\">Benjamin Clavi&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gheewala_A/0/1/0/all/0/1\">Akshita Gheewala</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Briton_P/0/1/0/all/0/1\">Paul Briton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Alphonsus_M/0/1/0/all/0/1\">Marc Alphonsus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Labiyaad_R/0/1/0/all/0/1\">Rym Labiyaad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piccoli_F/0/1/0/all/0/1\">Francesco Piccoli</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"TravelBERT: Pre-training Language Model Incorporating Domain-specific Heterogeneous Knowledge into A Unified Representation. (arXiv:2109.01048v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01048","description":"<p>Existing technologies expand BERT from different perspectives, e.g. designing\ndifferent pre-training tasks, different semantic granularities and different\nmodel architectures. Few models consider expanding BERT from different text\nformats. In this paper, we propose a heterogeneous knowledge language model\n(HKLM), a unified pre-trained language model (PLM) for all forms of text,\nincluding unstructured text, semi-structured text and well-structured text. To\ncapture the corresponding relations among these multi-format knowledge, our\napproach uses masked language model objective to learn word knowledge, uses\ntriple classification objective and title matching objective to learn entity\nknowledge and topic knowledge respectively. To obtain the aforementioned\nmulti-format text, we construct a corpus in the tourism domain and conduct\nexperiments on 5 tourism NLP datasets. The results show that our approach\noutperforms the pre-training of plain text using only 1/4 of the data. The\ncode, datasets, corpus and knowledge graph will be released.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1\">Hongyin Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1\">Hao Peng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1\">Zhiheng Lv</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1\">Lei Hou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Juanzi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1\">Jinghui Xiao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Skim-Attention: Learning to Focus via Document Layout. (arXiv:2109.01078v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01078","description":"<p>Transformer-based pre-training techniques of text and layout have proven\neffective in a number of document understanding tasks. Despite this success,\nmultimodal pre-training models suffer from very high computational and memory\ncosts. Motivated by human reading strategies, this paper presents\nSkim-Attention, a new attention mechanism that takes advantage of the structure\nof the document and its layout. Skim-Attention only attends to the\n2-dimensional position of the words in a document. Our experiments show that\nSkim-Attention obtains a lower perplexity than prior works, while being more\ncomputationally efficient. Skim-Attention can be further combined with\nlong-range Transformers to efficiently process long documents. We also show how\nSkim-Attention can be used off-the-shelf as a mask for any Pre-trained Language\nModel, allowing to improve their performance while restricting attention.\nFinally, we show the emergence of a document structure representation in\nSkim-Attention.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1\">Laura Nguyen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scialom_T/0/1/0/all/0/1\">Thomas Scialom</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1\">Jacopo Staiano</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Piwowarski_B/0/1/0/all/0/1\">Benjamin Piwowarski</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"How Suitable Are Subword Segmentation Strategies for Translating Non-Concatenative Morphology?. (arXiv:2109.01100v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01100","description":"<p>Data-driven subword segmentation has become the default strategy for\nopen-vocabulary machine translation and other NLP tasks, but may not be\nsufficiently generic for optimal learning of non-concatenative morphology. We\ndesign a test suite to evaluate segmentation strategies on different types of\nmorphological phenomena in a controlled, semi-synthetic setting. In our\nexperiments, we compare how well machine translation models trained on subword-\nand character-level can translate these morphological phenomena. We find that\nlearning to analyse and generate morphologically complex surface\nrepresentations is still challenging, especially for non-concatenative\nmorphological phenomena like reduplication or vowel harmony and for rare word\nstems. Based on our results, we recommend that novel text representation\nstrategies be tested on a range of typologically diverse languages to minimise\nthe risk of adopting a strategy that inadvertently disadvantages certain\nlanguages.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Amrhein_C/0/1/0/all/0/1\">Chantal Amrhein</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1\">Rico Sennrich</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Sequence-to-Sequence Learning with Latent Neural Grammars. (arXiv:2109.01135v1 [cs.CL])","link":"http://arxiv.org/abs/2109.01135","description":"<p>Sequence-to-sequence learning with neural networks has become the de facto\nstandard for sequence prediction tasks. This approach typically models the\nlocal distribution over the next word with a powerful neural network that can\ncondition on arbitrary context. While flexible and performant, these models\noften require large datasets for training and can fail spectacularly on\nbenchmarks designed to test for compositional generalization. This work\nexplores an alternative, hierarchical approach to sequence-to-sequence learning\nwith quasi-synchronous grammars, where each node in the target tree is\ntransduced by a node in the source tree. Both the source and target trees are\ntreated as latent and induced during training. We develop a neural\nparameterization of the grammar which enables parameter sharing over the\ncombinatorial space of derivation rules without the need for manual feature\nengineering. We apply this latent neural grammar to various domains -- a\ndiagnostic language navigation task designed to test for compositional\ngeneralization (SCAN), style transfer, and small-scale machine translation --\nand find that it performs respectably compared to standard baselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1\">Yoon Kim</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"An Iterative Multi-Knowledge Transfer Network for Aspect-Based Sentiment Analysis. (arXiv:2004.01935v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.01935","description":"<p>Aspect-based sentiment analysis (ABSA) mainly involves three subtasks: aspect\nterm extraction, opinion term extraction, and aspect-level sentiment\nclassification, which are typically handled in a separate or joint manner.\nHowever, previous approaches do not well exploit the interactive relations\namong three subtasks and do not pertinently leverage the easily available\ndocument-level labeled domain/sentiment knowledge, which restricts their\nperformances. To address these issues, we propose a novel Iterative\nMulti-Knowledge Transfer Network (IMKTN) for end-to-end ABSA. For one thing,\nthrough the interactive correlations between the ABSA subtasks, our IMKTN\ntransfers the task-specific knowledge from any two of the three subtasks to\nanother one at the token level by utilizing a well-designed routing algorithm,\nthat is, any two of the three subtasks will help the third one. For another,\nour IMKTN pertinently transfers the document-level knowledge, i.e.,\ndomain-specific and sentiment-related knowledge, to the aspect-level subtasks\nto further enhance the corresponding performance. Experimental results on three\nbenchmark datasets demonstrate the effectiveness and superiority of our\napproach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1\">Yunlong Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1\">Jinchao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1\">Yufeng Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1\">Jinan Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Accelerating Real-Time Question Answering via Question Generation. (arXiv:2009.05167v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2009.05167","description":"<p>Although deep neural networks have achieved tremendous success for question\nanswering (QA), they are still suffering from heavy computational and energy\ncost for real product deployment. Further, existing QA systems are bottlenecked\nby the encoding time of real-time questions with neural networks, thus\nsuffering from detectable latency in deployment for large-volume traffic. To\nreduce the computational cost and accelerate real-time question answering\n(RTQA) for practical usage, we propose to remove all the neural networks from\nonline QA systems, and present Ocean-Q (an Ocean of Questions), which\nintroduces a new question generation (QG) model to generate a large pool of QA\npairs offline, then in real time matches an input question with the candidate\nQA pool to predict the answer without question encoding. Ocean-Q can be readily\ndeployed in existing distributed database systems or search engine for\nlarge-scale query usage, and much greener with no additional cost for\nmaintaining large neural networks. Experiments on SQuAD(-open) and HotpotQA\nbenchmarks demonstrate that Ocean-Q is able to accelerate the fastest\nstate-of-the-art RTQA system by 4X times, with only a 3+% accuracy drop.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1\">Yuwei Fang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohang Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gan_Z/0/1/0/all/0/1\">Zhe Gan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Siqi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jingjing Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1\">Chenguang Zhu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GMH: A General Multi-hop Reasoning Model for KG Completion. (arXiv:2010.07620v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2010.07620","description":"<p>Knowledge graphs are essential for numerous downstream natural language\nprocessing applications, but are typically incomplete with many facts missing.\nThis results in research efforts on multi-hop reasoning task, which can be\nformulated as a search process and current models typically perform short\ndistance reasoning. However, the long-distance reasoning is also vital with the\nability to connect the superficially unrelated entities. To the best of our\nknowledge, there lacks a general framework that approaches multi-hop reasoning\nin mixed long-short distance reasoning scenarios. We argue that there are two\nkey issues for a general multi-hop reasoning model: i) where to go, and ii)\nwhen to stop. Therefore, we propose a general model which resolves the issues\nwith three modules: 1) the local-global knowledge module to estimate the\npossible paths, 2) the differentiated action dropout module to explore a\ndiverse set of paths, and 3) the adaptive stopping search module to avoid over\nsearching. The comprehensive results on three datasets demonstrate the\nsuperiority of our model with significant improvements against baselines in\nboth short and long distance reasoning scenarios.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1\">Yao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_H/0/1/0/all/0/1\">Hongru Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jatowt_A/0/1/0/all/0/1\">Adam Jatowt</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1\">Wenqiang Lei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1\">Xin Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1\">Ning Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1\">Zhenglu Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cross-lingual Transfer of Abstractive Summarizer to Less-resource Language. (arXiv:2012.04307v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.04307","description":"<p>Automatic text summarization extracts important information from texts and\npresents the information in the form of a summary. Abstractive summarization\napproaches progressed significantly by switching to deep neural networks, but\nresults are not yet satisfactory, especially for languages where large training\nsets do not exist. In several natural language processing tasks, a\ncross-lingual model transfer is successfully applied in less-resource\nlanguages. For summarization, the cross-lingual model transfer was not\nattempted due to a non-reusable decoder side of neural models that cannot\ncorrect target language generation. In our work, we use a pre-trained English\nsummarization model based on deep neural networks and sequence-to-sequence\narchitecture to summarize Slovene news articles. We address the problem of\ninadequate decoder by using an additional language model for the evaluation of\nthe generated text in target language. We test several cross-lingual\nsummarization models with different amounts of target data for fine-tuning. We\nassess the models with automatic evaluation measures and conduct a small-scale\nhuman evaluation. Automatic evaluation shows that the summaries of our best\ncross-lingual model are useful and of quality similar to the model trained only\nin the target language. Human evaluation shows that our best model generates\nsummaries with high accuracy and acceptable readability. However, similar to\nother abstractive models, our models are not perfect and may occasionally\nproduce misleading or absurd content.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zagar_A/0/1/0/all/0/1\">Ale&#x161; &#x17d;agar</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Robnik_Sikonja_M/0/1/0/all/0/1\">Marko Robnik-&#x160;ikonja</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Topic Coverage Approach to Evaluation of Topic Models. (arXiv:2012.06274v3 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2012.06274","description":"<p>Topic models are widely used unsupervised models capable of learning topics -\nweighted lists of words and documents - from large collections of text\ndocuments. When topic models are used for discovery of topics in text\ncollections, a question that arises naturally is how well the model-induced\ntopics correspond to topics of interest to the analyst. In this paper we\nrevisit and extend a so far neglected approach to topic model evaluation based\non measuring topic coverage - computationally matching model topics with a set\nof reference topics that models are expected to uncover. The approach is well\nsuited for analyzing models' performance in topic discovery and for large-scale\nanalysis of both topic models and measures of model quality. We propose new\nmeasures of coverage and evaluate, in a series of experiments, different types\nof topic models on two distinct text domains for which interest for topic\ndiscovery exists. The experiments include evaluation of model quality, analysis\nof coverage of distinct topic categories, and the analysis of the relationship\nbetween coverage and other methods of topic model evaluation. The paper\ncontributes a new supervised measure of coverage, and the first unsupervised\nmeasure of coverage. The supervised measure achieves topic matching accuracy\nclose to human agreement. The unsupervised measure correlates highly with the\nsupervised one (Spearman's $\\rho \\geq 0.95$). Other contributions include\ninsights into both topic models and different methods of model evaluation, and\nthe datasets and code for facilitating future research on topic coverage.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Korencic_D/0/1/0/all/0/1\">Damir Koren&#x10d;i&#x107;</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Ristov_S/0/1/0/all/0/1\">Strahil Ristov</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Repar_J/0/1/0/all/0/1\">Jelena Repar</a> (1), <a href=\"http://arxiv.org/find/cs/1/au:+Snajder_J/0/1/0/all/0/1\">Jan &#x160;najder</a> (2) ((1) Rudjer Bo&#x161;kovi&#x107; Institute, Croatia, (2) University of Zagreb, Faculty of Electrical Engineering and Computing, Croatia)"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"NewsBERT: Distilling Pre-trained Language Model for Intelligent News Application. (arXiv:2102.04887v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2102.04887","description":"<p>Pre-trained language models (PLMs) like BERT have made great progress in NLP.\nNews articles usually contain rich textual information, and PLMs have the\npotentials to enhance news text modeling for various intelligent news\napplications like news recommendation and retrieval. However, most existing\nPLMs are in huge size with hundreds of millions of parameters. Many online news\napplications need to serve millions of users with low latency tolerance, which\nposes huge challenges to incorporating PLMs in these scenarios. Knowledge\ndistillation techniques can compress a large PLM into a much smaller one and\nmeanwhile keeps good performance. However, existing language models are\npre-trained and distilled on general corpus like Wikipedia, which has some gaps\nwith the news domain and may be suboptimal for news intelligence. In this\npaper, we propose NewsBERT, which can distill PLMs for efficient and effective\nnews intelligence. In our approach, we design a teacher-student joint learning\nand distillation framework to collaboratively learn both teacher and student\nmodels, where the student model can learn from the learning experience of the\nteacher model. In addition, we propose a momentum distillation method by\nincorporating the gradients of teacher model into the update of student model\nto better transfer useful knowledge learned by the teacher model. Extensive\nexperiments on two real-world datasets with three tasks show that NewsBERT can\neffectively improve the model performance in various intelligent news\napplications with much smaller models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1\">Yang Yu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1\">Qi Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Data-Centric Framework for Composable NLP Workflows. (arXiv:2103.01834v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.01834","description":"<p>Empirical natural language processing (NLP) systems in application domains\n(e.g., healthcare, finance, education) involve interoperation among multiple\ncomponents, ranging from data ingestion, human annotation, to text retrieval,\nanalysis, generation, and visualization. We establish a unified open-source\nframework to support fast development of such sophisticated NLP workflows in a\ncomposable manner. The framework introduces a uniform data representation to\nencode heterogeneous results by a wide range of NLP tasks. It offers a large\nrepository of processors for NLP tasks, visualization, and annotation, which\ncan be easily assembled with full interoperability under the unified\nrepresentation. The highly extensible framework allows plugging in custom\nprocessors from external off-the-shelf NLP and deep learning libraries. The\nwhole framework is delivered through two modularized yet integratable\nopen-source projects, namely Forte (for workflow infrastructure and NLP\nfunction processors) and Stave (for user interaction, visualization, and\nannotation).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhengzhong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ding_G/0/1/0/all/0/1\">Guanxiong Ding</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bukkittu_A/0/1/0/all/0/1\">Avinash Bukkittu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1\">Mansi Gupta</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1\">Pengzhi Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1\">Atif Ahmed</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1\">Shikun Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1\">Xin Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Singhavi_S/0/1/0/all/0/1\">Swapnil Singhavi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1\">Linwei Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1\">Wei Wei</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zecong Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoran Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1\">Haoying Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1\">Xiaodan Liang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitamura_T/0/1/0/all/0/1\">Teruko Mitamura</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1\">Eric P. Xing</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1\">Zhiting Hu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Gradual Fine-Tuning for Low-Resource Domain Adaptation. (arXiv:2103.02205v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.02205","description":"<p>Fine-tuning is known to improve NLP models by adapting an initial model\ntrained on more plentiful but less domain-salient examples to data in a target\ndomain. Such domain adaptation is typically done using one stage of\nfine-tuning. We demonstrate that gradually fine-tuning in a multi-stage process\ncan yield substantial further gains and can be applied without modifying the\nmodel or learning objective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1\">Haoran Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ebner_S/0/1/0/all/0/1\">Seth Ebner</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yarmohammadi_M/0/1/0/all/0/1\">Mahsa Yarmohammadi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1\">Aaron Steven White</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1\">Benjamin Van Durme</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1\">Kenton Murray</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conceptual similarity and communicative need shape colexification: an experimental study. (arXiv:2103.11024v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2103.11024","description":"<p>Colexification refers to the phenomenon of multiple meanings sharing one word\nin a language. Cross-linguistic lexification patterns have been shown to be\nlargely predictable, as similar concepts are often colexified. We test a recent\nclaim that, beyond this general tendency, communicative needs play an important\nrole in shaping colexification patterns. We approach this question by means of\na series of human experiments, using an artificial language communication game\nparadigm. Our results across four experiments match the previous\ncross-linguistic findings: all other things being equal, speakers do prefer to\ncolexify similar concepts. However, we also find evidence supporting the\ncommunicative need hypothesis: when faced with a frequent need to distinguish\nsimilar pairs of meanings, speakers adjust their colexification preferences to\nmaintain communicative efficiency, and avoid colexifying those similar meanings\nwhich need to be distinguished in communication. This research provides further\nevidence to support the argument that languages are shaped by the needs and\npreferences of their speakers.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Karjus_A/0/1/0/all/0/1\">Andres Karjus</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Blythe_R/0/1/0/all/0/1\">Richard A. Blythe</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirby_S/0/1/0/all/0/1\">Simon Kirby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1\">Tianyu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1\">Kenny Smith</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Knowledge-Aware Graph-Enhanced GPT-2 for Dialogue State Tracking. (arXiv:2104.04466v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.04466","description":"<p>Dialogue State Tracking is central to multi-domain task-oriented dialogue\nsystems, responsible for extracting information from user utterances. We\npresent a novel hybrid architecture that augments GPT-2 with representations\nderived from Graph Attention Networks in such a way to allow causal, sequential\nprediction of slot values. The model architecture captures inter-slot\nrelationships and dependencies across domains that otherwise can be lost in\nsequential prediction. We report improvements in state tracking performance in\nMultiWOZ 2.0 against a strong GPT-2 baseline and investigate a simplified\nsparse training scenario in which DST models are trained only on session-level\nannotations but evaluated at the turn level. We further report detailed\nanalyses to demonstrate the effectiveness of graph models in DST by showing\nthat the proposed graph modules capture inter-slot dependencies and improve the\npredictions of values that are common to multiple domains.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1\">Weizhe Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tseng_B/0/1/0/all/0/1\">Bo-Hsian Tseng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1\">Bill Byrne</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The Power of Scale for Parameter-Efficient Prompt Tuning. (arXiv:2104.08691v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.08691","description":"<p>In this work, we explore \"prompt tuning\", a simple yet effective mechanism\nfor learning \"soft prompts\" to condition frozen language models to perform\nspecific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft\nprompts are learned through backpropagation and can be tuned to incorporate\nsignal from any number of labeled examples. Our end-to-end learned approach\noutperforms GPT-3's \"few-shot\" learning by a large margin. More remarkably,\nthrough ablations on model size using T5, we show that prompt tuning becomes\nmore competitive with scale: as models exceed billions of parameters, our\nmethod \"closes the gap\" and matches the strong performance of model tuning\n(where all model weights are tuned). This finding is especially relevant in\nthat large models are costly to share and serve, and the ability to reuse one\nfrozen model for multiple downstream tasks can ease this burden. Our method can\nbe seen as a simplification of the recently proposed \"prefix tuning\" of Li and\nLiang (2021), and we provide a comparison to this and other similar approaches.\nFinally, we show that conditioning a frozen model with soft prompts confers\nbenefits in robustness to domain transfer, as compared to full model tuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lester_B/0/1/0/all/0/1\">Brian Lester</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Al_Rfou_R/0/1/0/all/0/1\">Rami Al-Rfou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1\">Noah Constant</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Probing Commonsense Explanation in Dialogue Response Generation. (arXiv:2104.09574v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.09574","description":"<p>Humans use commonsense reasoning (CSR) implicitly to produce natural and\ncoherent responses in conversations. Aiming to close the gap between current\nresponse generation (RG) models and human communication abilities, we want to\nunderstand why RG models respond as they do by probing RG model's understanding\nof commonsense reasoning that elicits proper responses. We formalize the\nproblem by framing commonsense as a latent variable in the RG task and using\nexplanations for responses as textual form of commonsense. We collect 6k\nannotated explanations justifying responses from four dialogue datasets and ask\nhumans to verify them and propose two probing settings to evaluate RG models'\nCSR capabilities. Probing results show that models fail to capture the logical\nrelations between commonsense explanations and responses and fine-tuning on\nin-domain data and increasing model sizes do not lead to understanding of CSR\nfor RG. We hope our study motivates more research in making RG models emulate\nthe human reasoning process in pursuit of smooth human-AI communication.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1\">Pei Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jandaghi_P/0/1/0/all/0/1\">Pegah Jandaghi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1\">Bill Yuchen Lin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1\">Justin Cho</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pujara_J/0/1/0/all/0/1\">Jay Pujara</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"AMMU : A Survey of Transformer-based Biomedical Pretrained Language Models. (arXiv:2105.00827v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2105.00827","description":"<p>Transformer-based pretrained language models (PLMs) have started a new era in\nmodern natural language processing (NLP). These models combine the power of\ntransformers, transfer learning, and self-supervised learning (SSL). Following\nthe success of these models in the general domain, the biomedical research\ncommunity has developed various in-domain PLMs starting from BioBERT to the\nlatest BioELECTRA and BioALBERT models. We strongly believe there is a need for\na survey paper that can provide a comprehensive survey of various\ntransformer-based biomedical pretrained language models (BPLMs). In this\nsurvey, we start with a brief overview of foundational concepts like\nself-supervised learning, embedding layer and transformer encoder layers. We\ndiscuss core concepts of transformer-based PLMs like pretraining methods,\npretraining tasks, fine-tuning methods, and various embedding types specific to\nbiomedical domain. We introduce a taxonomy for transformer-based BPLMs and then\ndiscuss all the models. We discuss various challenges and present possible\nsolutions. We conclude by highlighting some of the open issues which will drive\nthe research community to further improve transformer-based BPLMs.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kalyan_K/0/1/0/all/0/1\">Katikapalli Subramanyam Kalyan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rajasekharan_A/0/1/0/all/0/1\">Ajit Rajasekharan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sangeetha_S/0/1/0/all/0/1\">Sivanesan Sangeetha</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fastformer: Additive Attention Can Be All You Need. (arXiv:2108.09084v5 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09084","description":"<p>Transformer is a powerful model for text understanding. However, it is\ninefficient due to its quadratic complexity to input sequence length. Although\nthere are many methods on Transformer acceleration, they are still either\ninefficient on long sequences or not effective enough. In this paper, we\npropose Fastformer, which is an efficient Transformer model based on additive\nattention. In Fastformer, instead of modeling the pair-wise interactions\nbetween tokens, we first use additive attention mechanism to model global\ncontexts, and then further transform each token representation based on its\ninteraction with global context representations. In this way, Fastformer can\nachieve effective context modeling with linear complexity. Extensive\nexperiments on five datasets show that Fastformer is much more efficient than\nmany existing Transformer models and can meanwhile achieve comparable or even\nbetter long text modeling performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smart Bird: Learnable Sparse Attention for Efficient and Effective Transformer. (arXiv:2108.09193v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09193","description":"<p>Transformer has achieved great success in NLP. However, the quadratic\ncomplexity of the self-attention mechanism in Transformer makes it inefficient\nin handling long sequences. Many existing works explore to accelerate\nTransformers by computing sparse self-attention instead of a dense one, which\nusually attends to tokens at certain positions or randomly selected tokens.\nHowever, manually selected or random tokens may be uninformative for context\nmodeling. In this paper, we propose Smart Bird, which is an efficient and\neffective Transformer with learnable sparse attention. In Smart Bird, we first\ncompute a sketched attention matrix with a single-head low-dimensional\nTransformer, which aims to find potential important interactions between\ntokens. We then sample token pairs based on their probability scores derived\nfrom the sketched attention matrix to generate different sparse attention index\nmatrices for different attention heads. Finally, we select token embeddings\naccording to the index matrices to form the input of sparse attention networks.\nExtensive experiments on six benchmark datasets for different tasks validate\nthe efficiency and effectiveness of Smart Bird in text modeling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1\">Chuhan Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1\">Fangzhao Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qi_T/0/1/0/all/0/1\">Tao Qi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiao_B/0/1/0/all/0/1\">Binxing Jiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1\">Daxin Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1\">Yongfeng Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1\">Xing Xie</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"One Chatbot Per Person: Creating Personalized Chatbots based on Implicit User Profiles. (arXiv:2108.09355v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.09355","description":"<p>Personalized chatbots focus on endowing chatbots with a consistent\npersonality to behave like real users, give more informative responses, and\nfurther act as personal assistants. Existing personalized approaches tried to\nincorporate several text descriptions as explicit user profiles. However, the\nacquisition of such explicit profiles is expensive and time-consuming, thus\nbeing impractical for large-scale real-world applications. Moreover, the\nrestricted predefined profile neglects the language behavior of a real user and\ncannot be automatically updated together with the change of user interests. In\nthis paper, we propose to learn implicit user profiles automatically from\nlarge-scale user dialogue history for building personalized chatbots.\nSpecifically, leveraging the benefits of Transformer on language understanding,\nwe train a personalized language model to construct a general user profile from\nthe user's historical responses. To highlight the relevant historical responses\nto the input post, we further establish a key-value memory network of\nhistorical post-response pairs, and build a dynamic post-aware user profile.\nThe dynamic profile mainly describes what and how the user has responded to\nsimilar posts in history. To explicitly utilize users' frequently used words,\nwe design a personalized decoder to fuse two decoding strategies, including\ngenerating a word from the generic vocabulary and copying one word from the\nuser's personalized vocabulary. Experiments on two real-world datasets show the\nsignificant improvement of our model compared with existing methods. Our code\nis available at https://github.com/zhengyima/DHAP\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1\">Zhengyi Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1\">Zhicheng Dou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1\">Yutao Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_H/0/1/0/all/0/1\">Hanxun Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1\">Ji-Rong Wen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Partition Filter Network for Joint Entity and Relation Extraction. (arXiv:2108.12202v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12202","description":"<p>In joint entity and relation extraction, existing work either sequentially\nencode task-specific features, leading to an imbalance in inter-task feature\ninteraction where features extracted later have no direct contact with those\nthat come first. Or they encode entity features and relation features in a\nparallel manner, meaning that feature representation learning for each task is\nlargely independent of each other except for input sharing. We propose a\npartition filter network to model two-way interaction between tasks properly,\nwhere feature encoding is decomposed into two steps: partition and filter. In\nour encoder, we leverage two gates: entity and relation gate, to segment\nneurons into two task partitions and one shared partition. The shared partition\nrepresents inter-task information valuable to both tasks and is evenly shared\nacross two tasks to ensure proper two-way interaction. The task partitions\nrepresent intra-task information and are formed through concerted efforts of\nboth gates, making sure that encoding of task-specific features is dependent\nupon each other. Experiment results on five public datasets show that our model\nperforms significantly better than previous approaches. In addition, contrary\nto what previous work claims, our auxiliary experiments suggest that relation\nprediction is contributory to named entity prediction in a non-negligible way.\nThe source code can be found at https://github.com/Coopercoppers/PFN.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1\">Zhiheng Yan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chong Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1\">Jinlan Fu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1\">Qi Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1\">Zhongyu Wei</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree Decomposition Attention for AMR-to-Text Generation. (arXiv:2108.12300v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12300","description":"<p>Text generation from AMR requires mapping a semantic graph to a string that\nit annotates. Transformer-based graph encoders, however, poorly capture vertex\ndependencies that may benefit sequence prediction. To impose order on an\nencoder, we locally constrain vertex self-attention using a graph's tree\ndecomposition. Instead of forming a full query-key bipartite graph, we restrict\nattention to vertices in parent, subtree, and same-depth bags of a vertex. This\nhierarchical context lends both sparsity and structure to vertex state updates.\nWe apply dynamic programming to derive a forest of tree decompositions,\nchoosing the most structurally similar tree to the AMR. Our system outperforms\na self-attentive baseline by 1.6 BLEU and 1.8 chrF++.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lisa Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Latent Tree Decomposition Parsers for AMR-to-Text Generation. (arXiv:2108.12304v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12304","description":"<p>Graph encoders in AMR-to-text generation models often rely on neighborhood\nconvolutions or global vertex attention. While these approaches apply to\ngeneral graphs, AMRs may be amenable to encoders that target their tree-like\nstructure. By clustering edges into a hierarchy, a tree decomposition\nsummarizes graph structure. Our model encodes a derivation forest of tree\ndecompositions and extracts an expected tree. From tree node embeddings, it\nbuilds graph edge features used in vertex attention of the graph encoder.\nEncoding TD forests instead of shortest-pairwise paths in a self-attentive\nbaseline raises BLEU by 0.7 and chrF++ by 0.3. The forest encoder also\nsurpasses a convolutional baseline for molecular property prediction by 1.92%\nROC-AUC.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1\">Lisa Jin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gildea_D/0/1/0/all/0/1\">Daniel Gildea</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Smoothing Dialogue States for Open Conversational Machine Reading. (arXiv:2108.12599v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2108.12599","description":"<p>Conversational machine reading (CMR) requires machines to communicate with\nhumans through multi-turn interactions between two salient dialogue states of\ndecision making and question generation processes. In open CMR settings, as the\nmore realistic scenario, the retrieved background knowledge would be noisy,\nwhich results in severe challenges in the information transmission. Existing\nstudies commonly train independent or pipeline systems for the two subtasks.\nHowever, those methods are trivial by using hard-label decisions to activate\nquestion generation, which eventually hinders the model performance. In this\nwork, we propose an effective gating strategy by smoothing the two dialogue\nstates in only one decoder and bridge decision making and question generation\nto provide a richer dialogue state reference. Experiments on the OR-ShARC\ndataset show the effectiveness of our method, which achieves new\nstate-of-the-art results.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhuosheng Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1\">Siru Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1\">Hai Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Utiyama_M/0/1/0/all/0/1\">Masao Utiyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sumita_E/0/1/0/all/0/1\">Eiichiro Sumita</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-02T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"taxo":"http://purl.org/rss/1.0/modules/taxonomy/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","dc":"http://purl.org/dc/elements/1.1/","admin":"http://webns.net/mvcb/","content":"http://purl.org/rss/1.0/modules/content/","syn":"http://purl.org/rss/1.0/modules/syndication/"}}]}]}