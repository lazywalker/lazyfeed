{"site_title":"ArxivDaily","project_name":"notfeed","project_version":"0.2.3","project_homepage":"https://github.com/NotCraft/NotFeed","days":[{"datetime":"2021-09-20T04:21:24.527273082Z","channels":[{"title":"Rust.cc","link":"https://rustcc.cn/rss","description":"This Is Rust Crustacean Community RSS feed.","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":null,"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Rust match总是不能按预期执行","link":"https://rustcc.cn/article?id=ab509fd4-cf1f-408b-b86f-c043c6193d42","description":"<p>我在db.txt中存放了如下user name数据：\nalice\nbob\ndavid</p>\n<p>下面的函数是从标准输入读取user name, 然后与存放在db.txt中的数据对比，如果存在，则打印exist，如果不存在，则写入到db.txt.\n但是下面的函数check_user_exist中，match第一个条件总是匹配的，这是为什么呢？</p>\n<pre><code>use std::fs::File;\nuse std::io::{self, prelude::*, BufReader};\nuse std::path::Path;\nuse std::fs::OpenOptions;\nuse std::io::Write;\n\npub fn login()-&gt;io::Result&lt;()&gt;{\n    db_init();\n\n    let mut name = String::new();\n    io::stdin().read_line(&amp;mut name)?;\n    match check_user_exist(name.as_str()) {\n        Ok(exist) =&gt; println!(\"{:?}\",exist),\n        Err(_) =&gt; println!(\"register new name.\"),\n    }\n    Ok(())\n}\n\nfn check_user_exist(name:&amp;str) -&gt; io::Result&lt;()&gt; {\n\n    let file = File::open(\"db.txt\")?;\n    let reader = BufReader::new(file);\n    for line in reader.lines().map(|l| l.unwrap()) {\n        match line.as_str(){\n            name  =&gt; println!(\"found str {}\",name),   // **always match, why ?? **\n            _ =&gt; register(name),\n        }\n    }\n    Ok(())\n}  \n\nfn register(name:&amp;str){\n\n    let mut file = OpenOptions::new().append(true).open(\"db.txt\").expect(\n        \"cannot open file\");\n     file.write_all(name.as_bytes()).expect(\"write file faile\");\n}\n\nfn db_init(){\n    if Path::new(\"db.txt\").exists() {\n        println!(\"db already inited!\");\n    }\n    else{\n        println!(\"db not initialized. create db file!\");\n        let file = File::create(\"db.txt\").expect(\"create file fail\");\n    }\n}\n\nfn main(){\n    println!(\"start main\\n\");\n\n    login();\n}\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-19 07:37:46","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【已解决】axum路由导致项目编译时间变长","link":"https://rustcc.cn/article?id=cc58b708-43af-477a-80e5-4df42b4a4025","description":"<p>axum的路由设计感觉有点问题，Router::new().route(...)，调用route方法越多，编译时间越长。\n如果有和我遇到同样问题的，可以使用此crate： https://github.com/programatik29/axum-router</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-19 05:21:26","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-09-18","link":"https://rustcc.cn/article?id=8812da66-04a5-4d69-908e-ac0c148cbd57","description":"<h3>Goscript现在已经完成了语言功能。</h3>\n<p>Go 规范作为脚本语言实现。</p>\n<p>尽管距离正式投入生产还有很长的路要走，但这对我来说是一个重要的里程碑。它现在支持 goroutine/channel/defer。</p>\n<p>如果您觉得它很有趣，<code>test</code>文件夹会显示它可以做什么。</p>\n<p><a href=\"https://github.com/oxfeeefeee/goscript\" rel=\"noopener noreferrer\">Gitlab 链接</a>，https://github.com/oxfeeefeee/goscript</p>\n<h3>GitHub - seed-rs/seed: 用于创建 Web 应用程序的 Rust 框架</h3>\n<p>Seed 是一个 Rust 前端框架，用于创建具有类似 Elm 架构的快速可靠的 Web 应用程序。</p>\n<ul>\n<li>完全用 Rust 编写，包括模板系统（例如 div！宏）</li>\n<li>基于 Elm 架构的内置状态管理。</li>\n<li>...</li>\n</ul>\n<p><a href=\"https://github.com/seed-rs/seed\" rel=\"noopener noreferrer\">Gitlab 链接</a>，https://github.com/seed-rs/seed</p>\n<h3>Rust 编程语言用于游戏工具（GDC 2021 幻灯片）</h3>\n<p>Rust 编程语言已悄然席卷科技界，但游戏工作室的采用速度较慢。\n自 2018 年以来，Treyarch 一直在逐步将 Rust 集成到我们的工具和管道中。 本次会议利用这一经验探索 Rust 可以给游戏工具程序员带来的机遇和挑战，并研究 Rust 可以成为工具的强有力的补充。</p>\n<p><a href=\"https://research.activision.com/publications/2021/09/the-rust-programming-language-for-game-tooling\" rel=\"noopener noreferrer\">文章链接</a>，https://research.activision.com/publications/2021/09/the-rust-programming-language-for-game-tooling</p>\n<hr>\n<p>From 日报小组 <a href=\"https://rustcc.cn/blog_with_author?author_id=dd4a77ca-2042-459e-901a-b8f9bfeb7db0\" rel=\"noopener noreferrer\">TOM</a></p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-18 14:44:29","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"交叉编译求助","link":"https://rustcc.cn/article?id=a61f8f7b-1bc7-47bc-a228-43f57f2b957e","description":"<p>环境：\ninfo: This is the version for the rustup toolchain manager, not the rustc compiler.\ninfo: The currently active <code>rustc</code> version is <code>rustc 1.52.1 (9bc8c42bb 2021-05-09)</code></p>\n<p>项目依赖:</p>\n<p>[dependencies]\nactix-web = \"3.3.2\"\nquickxml_to_serde = \"0.4.3\"\nserde_json = \"1.0.68\"</p>\n<p>错误：\ncargo build --release --target=x86_64-unknown-linux-musl --verbose\nFresh unicode-xid v0.2.2\nCompiling cfg-if v1.0.0\nRunning <code>rustc --crate-name cfg_if --edition=2018 /Users/lgy/.cargo/registry/src/mirrors.ustc.edu.cn-12df342d903acd47/cfg-if-1.0.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -Cembed-bitcode=no -C metadata=3659865723c6761c -C extra-filename=-3659865723c6761c --out-dir /Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps --target x86_64-unknown-linux-musl -C linker=x86_64-linux-musl-gcc -L dependency=/Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps -L dependency=/Users/lgy/Documents/xmlassint/target/release/deps --cap-lints allow</code>\nFresh autocfg v1.0.1\nFresh version_check v0.9.3\nCompiling slab v0.4.4\nCompiling lazy_static v1.4.0\nCompiling futures-sink v0.3.17\nRunning <code>rustc --crate-name slab --edition=2018 /Users/lgy/.cargo/registry/src/mirrors.ustc.edu.cn-12df342d903acd47/slab-0.4.4/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -Cembed-bitcode=no --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' -C metadata=48837676366b2903 -C extra-filename=-48837676366b2903 --out-dir /Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps --target x86_64-unknown-linux-musl -C linker=x86_64-linux-musl-gcc -L dependency=/Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps -L dependency=/Users/lgy/Documents/xmlassint/target/release/deps --cap-lints allow</code>\nRunning <code>rustc --crate-name lazy_static /Users/lgy/.cargo/registry/src/mirrors.ustc.edu.cn-12df342d903acd47/lazy_static-1.4.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -Cembed-bitcode=no -C metadata=dbe72fc09fa72228 -C extra-filename=-dbe72fc09fa72228 --out-dir /Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps --target x86_64-unknown-linux-musl -C linker=x86_64-linux-musl-gcc -L dependency=/Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps -L dependency=/Users/lgy/Documents/xmlassint/target/release/deps --cap-lints allow</code>\nRunning <code>rustc --crate-name futures_sink --edition=2018 /Users/lgy/.cargo/registry/src/mirrors.ustc.edu.cn-12df342d903acd47/futures-sink-0.3.17/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -Cembed-bitcode=no --cfg 'feature=\"alloc\"' --cfg 'feature=\"default\"' --cfg 'feature=\"std\"' -C metadata=2dc84c3d7f760b38 -C extra-filename=-2dc84c3d7f760b38 --out-dir /Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps --target x86_64-unknown-linux-musl -C linker=x86_64-linux-musl-gcc -L dependency=/Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps -L dependency=/Users/lgy/Documents/xmlassint/target/release/deps --cap-lints allow</code>\nerror[E0463]: can't find crate for <code>core</code>\n|\n= note: the <code>x86_64-unknown-linux-musl</code> target may not be installed</p>\n<p>error[E0463]: can't find crate for <code>core</code>\n|\n= note: the <code>x86_64-unknown-linux-musl</code> target may not be installed</p>\n<p>error: aborting due to previous error</p>\n<p>For more information about this error, try <code>rustc --explain E0463</code>.\nerror: aborting due to previous error</p>\n<p>For more information about this error, try <code>rustc --explain E0463</code>.\nerror: could not compile <code>lazy_static</code>.</p>\n<p>Caused by:\nprocess didn't exit successfully: <code>rustc --crate-name lazy_static /Users/lgy/.cargo/registry/src/mirrors.ustc.edu.cn-12df342d903acd47/lazy_static-1.4.0/src/lib.rs --error-format=json --json=diagnostic-rendered-ansi,artifacts --crate-type lib --emit=dep-info,metadata,link -C opt-level=3 -Cembed-bitcode=no -C metadata=dbe72fc09fa72228 -C extra-filename=-dbe72fc09fa72228 --out-dir /Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps --target x86_64-unknown-linux-musl -C linker=x86_64-linux-musl-gcc -L dependency=/Users/lgy/Documents/xmlassint/target/x86_64-unknown-linux-musl/release/deps -L dependency=/Users/lgy/Documents/xmlassint/target/release/deps --cap-lints allow</code> (exit code: 1)\nwarning: build failed, waiting for other jobs to finish...\nerror[E0463]: can't find crate for <code>std</code>\n|\n= note: the <code>x86_64-unknown-linux-musl</code> target may not be installed</p>\n<p>error[E0463]: can't find crate for <code>std</code>\n|\n= note: the <code>x86_64-unknown-linux-musl</code> target may not be installed</p>\n<p>error: aborting due to previous error</p>\n<p>For more information about this error, try <code>rustc --explain E0463</code>.\nerror: aborting due to previous error</p>\n<p>For more information about this error, try <code>rustc --explain E0463</code>.\nerror: build failed</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-18 09:00:34","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"宏直接生成多个参数 有没办法实现？","link":"https://rustcc.cn/article?id=dc136abf-037a-440f-8a6f-e7a9b938a3f3","description":"<p>//</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-17 15:25:48","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust 日报】2021-09-17 - GCC 代码生成后端现已加入 rust-lang 大家庭","link":"https://rustcc.cn/article?id=2755d737-97bd-48bc-8237-512c1d8e8903","description":"<h3>事件 - GCC 代码生成后端现已加入 rust-lang 大家庭</h3>\n<p>rustc_codegen_gcc 是为 Rustc 设计的 GCC 代码生成后端，目前已经加入 rust-lang 官方大家庭。</p>\n<p>它不仅可以复用现有的 Rustc 前端，还能够获取来自 GCC 的增益，比如支持更多架构以及应用 GCC 的独门优化。</p>\n<p>尽管该项目采用 libgccjit 进行实现，但其实并不是利用 JIT 技术，而是采用 AOT 方案，可不要被名字迷惑哦。</p>\n<p><a href=\"https://github.com/rust-lang/rustc_codegen_gcc\" rel=\"noopener noreferrer\">GitHub - rust-lang/rustc_codegen_gcc</a>: https://github.com/rust-lang/rustc_codegen_gcc</p>\n<h3>文章 - Kraken 使用 Rust 改进基础设施</h3>\n<p>Kraken 一家总部位于旧金山的公司，是世界上最大的基于欧元交易量和流动性的全球数字资产交易所。</p>\n<p>Kraken 的后端服务早期是使用 PHP 实现的，在最近两年时间，使用 Rust 改造原有的 PHP 服务并开发新的产品和功能，并使用 Rust 支撑了不断扩大的加密货币交易业务。</p>\n<p>Kraken 发布了关于使用 Rust 改进基础建设的一篇文章，并希望对于考虑使用 Rust 构建产品的公司和想要投入时间学习该语言的开发人员来说是一个有用的资源。同时，Kraken 还表示，为了感谢 RustAnalyzer 的出色工作，将会向该项目捐赠 50K EUR 。</p>\n<p><a href=\"https://blog.kraken.com/post/7964/oxidizing-kraken-improving-kraken-infrastructure-using-rust/\" rel=\"noopener noreferrer\">文章 - Oxidizing Kraken: Improving Kraken Infrastructure Using Rust</a>: https://blog.kraken.com/post/7964/oxidizing-kraken-improving-kraken-infrastructure-using-rust/</p>\n<h3>事件 - WGPU 与 Deno 在 CTS 上的成功合作</h3>\n<p>三月份的时候，Deno 1.8 就已经使用 wgpu 来提供了初始的 WebGPU 支持。Deno 团队花了更多的时间将一致性测试套件（CTS）连接到 Deno WebGPU 运行，并报告了 wgpu 上第一个 CTS 结果/问题。</p>\n<p>现在该工作已经与 wgpu CI 集成，作为 wgpu 基础设施和生态系统的一部分。</p>\n<p><a href=\"https://gfx-rs.github.io/2021/09/16/deno-webgpu.html\" rel=\"noopener noreferrer\">文章 - https://gfx-rs.github.io/2021/09/16/deno-webgpu.html</a>: https://gfx-rs.github.io/2021/09/16/deno-webgpu.html</p>\n<hr>\n<p>From 日报小组 <a href=\"https://github.com/PsiACE\" rel=\"noopener noreferrer\">PsiACE</a></p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rust.cc 论坛: 支持 rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust 语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-17 15:10:02","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【已解决】编译巨慢，要等2-3小时才能编译完","link":"https://rustcc.cn/article?id=c1509426-0a1e-458c-87a4-49463183df6a","description":"<p>最近试着用rust写一个小项目，原先编译还好，5分钟内差不多能编译完，突然今天，整个项目编译就变得巨慢，rustc进程cpu达到100%， 内存占了8g多。</p>\n<p>使用的电脑是MacBook\n现在用vscode打开整个项目，rls也会占8g多内存。</p>\n<p>新手求教，这是什么原因造成的？</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-17 09:52:20","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【活动】Rust China Conf 2021 开放报名","link":"https://rustcc.cn/article?id=3f9908dd-2c8f-44ef-af97-0fb0a39fe810","description":"<p>链接在 <a href=\"https://mp.weixin.qq.com/s?__biz=MzI1MjAzNDI1MA==&amp;mid=2648214472&amp;idx=1&amp;sn=5959c81f42c0d60eaedef0e1379b9051&amp;chksm=f1c5c66dc6b24f7ba908f89db46c57e1353a88a95c31c6d34d37710950693168107ee388f69f&amp;token=2083353868&amp;lang=zh_CN#rd\" rel=\"noopener noreferrer\">这里</a></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-17 05:43:00","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"tokio vs golang","link":"https://rustcc.cn/article?id=5985dfe8-e6f8-46c6-9172-3e05d3ee91f7","description":"<p>用 go 写的类似的代码，平均只需要1毫秒，但用 rust+tokio 却平均要3毫秒，如何优化能达到 go的水平？</p>\n<pre><code>use std::{thread, time::Duration};\nuse std::time::SystemTime;\nuse std::sync::atomic::{AtomicI32, Ordering};\nuse std::ops::{Div, AddAssign};\nuse std::sync::{Arc};\n\nuse tokio::runtime::{Builder};\nuse tokio::sync::mpsc;\nuse tokio::sync::{Mutex};\n\n\n#[derive(Debug)]\nstruct Event {\n    _type: String,\n    time: SystemTime,\n}\n\nfn main() {\n    let (tx, mut rx) = mpsc::unbounded_channel::&lt;Event&gt;();\n\n    let rt = Builder::new_multi_thread().worker_threads(8)\n        .build()\n        .unwrap();\n\n    let count: Arc&lt;AtomicI32&gt; = Arc::new(AtomicI32::new(0));\n    let total: Arc&lt;Mutex&lt;Duration&gt;&gt; = Arc::new(Mutex::new(Duration::new(0, 0)));\n\n    let count2 = count.clone();\n    let total2 = total.clone();\n    rt.spawn(async move {\n        while let Some(res) = rx.recv().await {\n            let elapsed = res.time.elapsed().ok().unwrap();\n            count2.fetch_add(1, Ordering::Relaxed);\n            total2.lock().await.add_assign(elapsed);\n        }\n    });\n\n    let max_loop = 100000;\n\n    for _ in 0..max_loop {\n        let event = Event {\n            _type: \"hello\".to_string(),\n            time: SystemTime::now(),\n        };\n        let _ = tx.send(event);\n\n        thread::sleep(Duration::from_micros(1));\n    }\n\n    thread::sleep(Duration::from_millis(4000));\n\n    println!(\"{:?}\", count.load(Ordering::Relaxed));\n    rt.block_on(async move {\n        let total2 = total.lock().await;\n        println!(\n            \"total: {:?}, average: {:?}, {:?}/s\",\n            total2,\n            total2.div(count.load(Ordering::Relaxed) as u32),\n            count.load(Ordering::Relaxed) as f64 / total2.as_secs_f64()\n        );\n    });\n}\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-17 03:25:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"通过rusoto-s3连接minio服务","link":"https://rustcc.cn/article?id=de64869a-ebca-44f3-ae94-d2988cf5fb4a","description":"<h3>近期调研rusoto, 想通过它实现调用minio, 但是目前出现问题</h3>\n<ul>\n<li>rusoto-s3的接口new没有参数可传入minio access-key以及secret-key</li>\n<li>rusoto-s3的接口new-withh中的参数<strong>request_dispatcher</strong>, 不知该如何使用</li>\n</ul>\n<p>请教诸位大佬, 能不能提供个示例, 如何使用rusoto调用minio</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-17 02:55:26","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 培养提高计划 Vol. 7 - 8 | Rust 项目工程来了","link":"https://rustcc.cn/article?id=9dec6eeb-38d8-4ec4-b75e-783bd11bf24b","description":"<p>我们的 Rust 公开课进行了 6 期了，带大家了解了 ：</p>\n<ol>\n<li>认识面向基础架构语言</li>\n<li>理解 Rust 所有权</li>\n<li>通过实战理解 Rust 宏</li>\n<li>通过 Datafuse 理解全链路跟踪</li>\n<li>Rust 异步编程入门 Future Part 1</li>\n<li>Rust 异步编程入门 Future Part 2</li>\n</ol>\n<p>目前视频回放传到 B 站收获许多好评，赞，也给我们很大的鼓励。希望我们的 Rust 培养提高计划 | Datafuse 可以帮助更多的朋友快速的使用上 Rust 。\n本周给大家排两个公开课：周四晚上，周日晚上。我们 Rust 培养提高计划邀请到第二位分享嘉宾 董泽润老师， 另外 Rust 培养提高计划 的内容上也做了一些调整。</p>\n<hr>\n<p>分享主题：《深入了解rust 闭包》 | Vol. 7</p>\n<p>分享时间： 周四晚上2021-09-09 20:00-21:00</p>\n<p>分享讲师： 董泽润</p>\n<p>内容介绍： 深入浅出了解 rust 闭包工作原理，让大家了解底层实现\n讲师介绍：\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/07-%E8%91%A3%E6%B3%BD%E6%B6%A6.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<hr>\n<p>分享主题：《利用 Tokio 实现一个高性能 Mini Http server》 | Vol. 8</p>\n<p>分享时间：  周日晚上2021-09-12 20:00-21:00</p>\n<p>分享讲师： 苏林</p>\n<p>首先感谢苏林老师的坚持付出， 带我们学习 Rust 的重点知识。 经过和苏琳老师沟通，我们后续的课程，会更加往实战方向转变。接下是一个系列的内容：</p>\n<ol>\n<li>利用 Tokio 实现一个 Mini Http server</li>\n<li>基于 Http server提供内容动态的 API 网关</li>\n<li>利用 Redis 实现对 API 网关加速</li>\n<li>学习 Rust RPC 调用，实现微服务调用</li>\n</ol>\n<p>这个内容可能需要4次左右的公开课，目的是带着大家做一些小项目，带大家熟悉一下 Rust 工程，让大家可以快速把 Rust 用到后端开发中。</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8Ev2.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<p>Rust 异步编程入门 Future Part 1   | Vol. 5\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<p>Rust 异步编程入门 Future Part 2  | Vol. 6\nhttps://www.bilibili.com/video/bv1oy4y1G7jC</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-07 02:23:16","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"rust 学习随笔","link":"https://rustcc.cn/article?id=aea829f0-61d7-413a-a030-8ddd413f26d8","description":"<h1>切换镜像源</h1>\n<p>crm =&gt; https://github.com/wtklbm/crm</p>\n<p>常用命令就是 <code>crm best</code></p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 14:35:49","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"pretree 补全文档发布了,再次谢谢大神的指点终于入门了。","link":"https://rustcc.cn/article?id=49d6f015-c98a-4415-95eb-1554cf80d827","description":"<h1>Pretree</h1>\n<p>pretree is a package for storing and querying routing rules with prefix tree .</p>\n<p>pretree 是一个用于存储和查询路由规则的包。它用前缀树存储路由规则，支持包含变量的路由。</p>\n<p>pretree is a package for storing and querying routing rules. It uses prefix tree to store routing rules and supports routing with variables.</p>\n<p>Inspired by <a href=\"https://github.com/obity/pretree\" rel=\"noopener noreferrer\">obity/pretree</a> (golang)</p>\n<h1>Doc</h1>\n<p>See this document at <a href=\"https://docs.rs/pretree\" rel=\"noopener noreferrer\">API documentation</a></p>\n<h1>Install</h1>\n<p>Add the following line to your Cargo.toml file:</p>\n<pre><code>pretree = \"1.0.0\"\n</code></pre>\n<h1>Example</h1>\n<pre><code>use pretree::Pretree;\nlet mut p = Pretree::new();\np.store(\"GET\",\"account/{id}/info/:name\");\np.store(\"GET\",\"account/:id/login\");\np.store(\"GET\",\"account/{id}\");\np.store(\"GET\",\"bacteria/count_number_by_month\");\nlet (ok,rule,vars) = p.query(\"GET\",\"account/929239\");\nprintln!(\"ok:{} rule:{} vars:{:#?}\",ok,rule,vars);\n\n</code></pre>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-06 09:37:30","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 异步编程二: Tokio 入门运行时介绍 | Rust 培养提高计划 Vol. 6","link":"https://rustcc.cn/article?id=dfff3602-cc0c-4423-b48b-e200b624db1a","description":"<h3>本周公开课：《 Rust 异步编程二: Tokio 入门运行时介绍》|Vol. 6</h3>\n<p><strong>课程时间:</strong>  2021年9月5日 20:00-21:00</p>\n<p><strong>课程介绍:</strong>  上周公开课我们讲解了 Rust 异步编程模型（ 属于一个非常经典的内容，建议观看 ）, 大家对 Rust 异步编程模型有了一个初步认识,  Rust 异步编程模型里需要 Executor、Reactor、Future 等, 本周公开课将以 Tokio 框架为基础, 和大家一起聊聊 Tokio 里的 Executor、Reactor、Future 是什么?</p>\n<h3>课程大纲</h3>\n<p>1、回顾 Rust 异步编程模型.</p>\n<p>2、谈谈对 Rust 异步框架的认识 ( futures-rs、async-std、tokio ) .</p>\n<p>3、Tokio 介绍.</p>\n<p>4、Tokio 里的 Executor、Reactor、Future 如何使用.</p>\n<p>5、使用 Tokio 实现一个简单的服务端与客户端程序.</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/\nRust 异步编程入门 Future Part 1  回放地址：\nhttps://www.bilibili.com/video/BV1mf4y1N7MJ/</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n<p>Rust 异步编程教材：https://rust-lang.github.io/async-book/</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-09-02 08:40:15","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"公开课：《 Rust 异步编程入门 Future 》|Vol. 5","link":"https://rustcc.cn/article?id=d2927245-abd1-4ce4-bab2-0870ff229e70","description":"<h3>本周公开课：《 Rust 异步编程入门 Future 》|Vol. 5</h3>\n<p><strong>课程时间:</strong> 2021年8月29日 20:00-21:00</p>\n<p><strong>课程介绍:</strong>  讲到 Rust 使用 Future 异步编程，就不得不说 futures 和 tokio 这两个 crate，其实标准库中的 future，以及 async/await 就是从 futures 库中整合进标准库的, Tokio 拥有极快的性能，是大部分系统异步处理的选择，其构建于 future 之上。Future 是  Rust 异步编程的核心基础。</p>\n<h3>课程大纲</h3>\n<p>1、为什么需要异步.</p>\n<p>2、理解异步编程模型.</p>\n<p>3、Future 编程模型讲解.</p>\n<p>4、带领大家实现一个简化版的 future , 再次帮忙大家理解</p>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<p>通过 Datafuse 理解全链路跟踪 | Vol. 4  https://www.bilibili.com/video/BV1YA411c7ia/</p>\n<h3>课程中推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-23 03:14:21","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"【Rust日报】2021-08-19 -- Rust Edition 2021 可能会出现在 Rust 1.56中","link":"https://rustcc.cn/article?id=7a3f7b1a-836a-4eab-a014-e5f354640f8c","description":"<h3>Rust Edition 2021 可能会出现在 Rust 1.56中</h3>\n<p>已经在下载次数最多的前 10000 个crate 上测试了版本迁移,并且将测试所有公共的 crate。</p>\n<p>ReadMore:<a href=\"https://twitter.com/m_ou_se/status/1427666611977297924\" rel=\"noopener noreferrer\">https://twitter.com/m_ou_se/status/1427666611977297924</a></p>\n<h3>异步引擎 C++20, Rust &amp; Zig</h3>\n<p>ReadMore:<a href=\"https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/\" rel=\"noopener noreferrer\">https://www.reddit.com/r/rust/comments/p63o4g/async_engines_in_c20_rust_zig/</a></p>\n<h3>RG3D -- Rust 3D 游戏引擎</h3>\n<ul>\n<li><strong>PC（Windows、Linux、macOS）和 Web (WebAssembly)</strong> 支持。</li>\n<li><strong>延迟着色</strong></li>\n<li><strong>内置保存/加载</strong></li>\n<li><strong>独立场景编辑器</strong></li>\n<li><strong>高级物理模型</strong></li>\n<li><strong>分层模型资源</strong></li>\n<li><strong>几何实例化</strong></li>\n</ul>\n<p>ReadMore:<a href=\"https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/\" rel=\"noopener noreferrer\">https://gamefromscratch.com/rg3d-open-source-rust-3d-game-engine/</a></p>\n<p>ReadMore:<a href=\"https://github.com/rg3dengine/rg3d\" rel=\"noopener noreferrer\">https://github.com/rg3dengine/rg3d</a></p>\n<hr>\n<p>From 日报小组 冰山上的 mook &amp;&amp; 挺肥</p>\n<p>社区学习交流平台订阅：</p>\n<ul>\n<li><a href=\"https://rustcc.cn/\" rel=\"noopener noreferrer\">Rustcc论坛: 支持rss</a></li>\n<li><a href=\"https://rustcc.cn/article?id=ed7c9379-d681-47cb-9532-0db97d883f62\" rel=\"noopener noreferrer\">微信公众号：Rust语言中文社区</a></li>\n</ul>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-18 16:31:44","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"公开课: 通过 Datafuse 理解全链路跟踪 | Vol. 4","link":"https://rustcc.cn/article?id=d07779e9-c748-4179-b365-4990a09c55e8","description":"<p><strong>本周公开课：《通过Datafuse理解全链路跟踪》| Vol. 4</strong></p>\n<p><strong>课程时间：</strong>  2021年8月22日 20:30-21:30</p>\n<p><strong>课程介绍：</strong> 数据库系统也是一个非常复杂，庞大的系统。特别是在调试和观察SQL执行，多线程任务切换，因为没有内存调用或堆栈跟踪，这也是分布式追踪的由来。这里面涉及到多进行分布式追踪为描述和分析跨进程事务提供了一种解决方案。Google Dapper(Dapper: 大规模分布式系统链路追踪基础设施)论文(各tracer的基础)中描述了分布式追踪的一些使用案例包括异常检测、诊断稳态问题、分布式分析、资源属性和微服务的工作负载建模。</p>\n<p>本次公开课通 Google 的 OpenTraceing 介绍，结合Rust的 tokio-rs/tracing 使用，最终结合 Datafuse 项目给大家展示一下大型应用的全链路跟踪分析过程。</p>\n<p>关于Datafuse : https://github.com/datafuselabs/datafuse</p>\n<h3>课程大纲</h3>\n<ol>\n<li>\n<p>什么是分布式追踪系统OpenTracing及应用场景</p>\n</li>\n<li>\n<p>介绍 tokio-rs/tracing 及在程序开发中的作用</p>\n</li>\n<li>\n<p>为什么需要tokio-rs/tracing库</p>\n</li>\n<li>\n<p>演示Datafuse项目中tokio-rs/tracing的使用</p>\n</li>\n</ol>\n<h3><strong>讲师介绍</strong></h3>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：Datafuse项目、Rust语言中文社区、知数堂 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>获取 T-Shirt 的方法：</h3>\n<ol>\n<li>给 https://github.com/datafuselabs/datafuse 提 issue/pr</li>\n<li>进行 Rust，大数据，数据库方面的公开课分享</li>\n<li>社区里分享 datafuse 相关文章</li>\n<li>datafuse.rs 上面文档翻译工作</li>\n</ol>\n<h3>往期课程回放</h3>\n<p>认识面向基础架构语言 Rust | Vol. 1  https://www.bilibili.com/video/BV1mg411778g</p>\n<p>理解 Rust 的所有权 | Vol. 2    https://www.bilibili.com/video/BV1264y1i7U9</p>\n<p>通过实战理解 Rust 宏 | Vol. 3 (https://www.bilibili.com/video/BV1Yb4y1U7r1</p>\n<h3>课程中苏林老师推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n<p>Rust宏的练习项目：   https://github.com/dtolnay/proc-macro-workshop</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-16 03:14:03","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"论坛github账户无法登录解决笔记","link":"https://rustcc.cn/article?id=8be810c8-be92-4ca5-96ed-a5b638952190","description":"<p>有反映这两天github账户无法登录了。</p>\n<p>报这个错：</p>\n<pre><code>get github user info err\n</code></pre>\n<p>查了几个地方：</p>\n<ol>\n<li>代码是否运行正常：Ok</li>\n<li>https代理是否正常：Ok</li>\n<li>检查了github返回日志，发现是：</li>\n</ol>\n<pre><code>get_github_user_info: response body: \"{\\\"message\\\":\\\"Must specify access token via Authorization header. https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param\\\",\\\"documentation_url\\\":\\\"https://docs.github.com/v3/#oauth2-token-sent-in-a-header\\\"}\"\nget_github_user_info: Got: Err(Custom(\"read json login error\"))\n</code></pre>\n<p>进入这个地址一看：<a href=\"https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/\" rel=\"noopener noreferrer\">https://developer.github.com/changes/2020-02-10-deprecating-auth-through-query-param/</a></p>\n<p>原来2020年2月就已经说了，要改要改。不过我确实没留意到这个信息。：（</p>\n<p>意思就是说access_token不要放在query参数中，而是要放在header里面。照它说的，改了后就好了。</p>\n<p>特此记录。</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-13 07:03:09","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust 的 Future 与 Javascript 的 Promise 功能对照参考","link":"https://rustcc.cn/article?id=2d0a7629-2740-435f-9ef7-98735bf4f095","description":"<h1><code>Rust</code>的<code>Future</code>与<code>Javascript</code>的<code>Promise</code>功能对照参考</h1>\n<p>学习新鲜技术时，我总是会习惯性向曾经熟悉的内容上靠，甚至套用现有的认知模型。这次也不例外，对照<code>Javascript - Promise/A+ API</code>来记忆一部分<code>Rust Future</code>常用<code>API</code>。</p>\n<blockquote>\n<p>注意：所有的<code>Rust - Future</code>操作都是以<code>.await</code>结尾的。这是因为，不同于<code>Javascript - Promise/A+</code>，<code>Rust - Future</code>是惰性的。只有被<code>.await</code>指令激活后，在<code>Rust - Future</code>内封装的操作才会被真正地执行。</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>javascript</th>\n<th align=\"center\">rust</th>\n<th align=\"center\">描述</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>Promise.resolve(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Ok(...))</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>Promise.reject(...)</td>\n<td align=\"center\">use ::async_std::future;future::ready(Err(...))</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>Promise.catch(err =&gt; err)</td>\n<td align=\"center\">use ::async_std::future;future::ready(...)</td>\n<td align=\"center\">在 rust 中，Future 自身不区分异步成功，还是异步失败。需要给异步计算结果套上 Result&lt;T, E&gt; 马甲，来做 resolve 与 reject 的差别处理。</td>\n</tr>\n<tr>\n<td>new Promise(() =&gt; {/* 什么都不做 */})</td>\n<td align=\"center\">use ::async_std::future;future::pending()</td>\n<td align=\"center\"></td>\n</tr>\n<tr>\n<td>new Promise((resolve, reject) =&gt; setTimeout(() =&gt; {  if (Math.random() &gt; .5) {    resolve(1);  } else {    reject(new Error('1'));  }}, 500))</td>\n<td align=\"center\">use ::async_std::task;use ::std::{thread, time::Duration};use ::rand::prelude::*;task::spawn_blocking(|| {    thread::sleep(Duration::from_millis(500));    let mut rng = rand::thread_rng();    if rng.gen() &gt; 0.5f64 {       Ok(1)    } else {       Err('1')    }}).await;</td>\n<td align=\"center\">1. future::poll_fn&lt;F, T&gt;(f: F) -&gt; T where F: FnMut(&amp;mut Context&lt;'_&gt;) -&gt; Poll 不能被用来构造包含了异步操作的 Future 实例，因为【回调闭包】内的【可修改引用】&amp;mut Context&lt;'_&gt; 不能被  （1）跨线程传递  （2）传递出闭包作用域2. task::spawn_blocking() 【回调闭包】输入参数内的 thread::sleep() 不是阻塞运行 task::spawn_blocking() 的主线程，而是阻塞从【阻塞任务线程池】中分配来运行阻塞任务的【工作线程】。</td>\n</tr>\n<tr>\n<td>Promise.all([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_join(future2).try_join(future3).await</td>\n<td align=\"center\">1. 有一个 promise/future 失败就整体性地失败。2. try_join 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;(T1, T2, T3), E&gt;</td>\n</tr>\n<tr>\n<td>Promise.all([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.join(future2).join(future3).await</td>\n<td align=\"center\">1. promise/future 的成功与失败结果都收集2. 返回结果：(T1, T2, T3)</td>\n</tr>\n<tr>\n<td>Promise.race([promise1, promise2, promise3])</td>\n<td align=\"center\">future1.try_race(future2).try_race(future3).await</td>\n<td align=\"center\">1. 仅只收集第一个成功的 promise/future2. try_race 成员方法要求其 Self 为 Future&lt;Output = Result&lt;T, E&gt;&gt;3. 返回结果：Result&lt;T, E&gt;</td>\n</tr>\n<tr>\n<td>Promise.race([  promise1.catch(err =&gt; err),  promise2.catch(err =&gt; err)  promise3.catch(err =&gt; err)])</td>\n<td align=\"center\">future1.race(future2).race(future3).await</td>\n<td align=\"center\">1. 收集第一个结束的 promise/future，无论它是成功结束还是失败收场。2. 返回结果：T</td>\n</tr>\n</tbody>\n</table>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-11 23:36:19","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null},{"title":"Rust公开课：《通过实战理解 Rust 宏》| Vol. 3","link":"https://rustcc.cn/article?id=dfb80624-2266-448f-87b1-d10f1e8d7c21","description":"<p><strong>课程主题：</strong>《通过实战理解 Rust 宏》</p>\n<p><strong>课程时间：</strong>  2021年8月15日 20:30-21:30</p>\n<p><strong>课程介绍：</strong></p>\n<p>如果想用 Rust 开发大型目，或者学习大型项目代码，特别是框架级别的项目，那么 Rust 的宏机制肯定是一个必须掌握的技能。 例如 datafuse 中的一些配置管理：\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/3/rust-macro-1628478411126.jpg\" alt=\"\"></p>\n<p>这就是通过宏实现配置的统一行为，代码参考：\nhttps://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/settings.rs#L19</p>\n<p>https://github.com/datafuselabs/datafuse/blob/master/fusequery/query/src/sessions/macros.rs</p>\n<p>Rust 语言强大的一个特点就是可以创建和利用宏，不过创建宏看起来挺复杂，常常令刚接触 Rust 的开发者生畏惧。 在本次公开课中帮助你理解 Rust Macro 的基本原理，学习如何创自已的 Rust 宏，以及查看源码学习宏的实现。</p>\n<h3>课程大纲</h3>\n<ul>\n<li>什么是 Rust 宏</li>\n<li>什么是宏运行原理</li>\n<li>如何创建 Rust 宏过程</li>\n<li>阅读 datafuse 项目源码， 学习项目中宏的实现</li>\n</ul>\n<p><strong>讲师介绍</strong>\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E8%8B%8F%E6%9E%97%E4%BB%8B%E7%BB%8D.png\" alt=\"\"></p>\n<p><img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/%E6%89%AB%E7%A0%81%E5%8F%82%E4%B8%8E.png\" alt=\"\"></p>\n<p>本次活动由：知数堂、Datafuse项目、Rust语言中文社区 共同发起。后期也欢迎Rust爱好者，Rust优秀项目， Data Cloud 项目来分享，公开课分享合作联系微信：82565387 备注：Rust 。 公开课嘉宾 &amp; Datafuse contributor都可以获取Datafuse纪念T恤。\n<img src=\"https://datafuse-1255499614.cos.ap-beijing.myqcloud.com/pbc/T-shirt.png\" alt=\"\"></p>\n<h3>课程中苏林老师推荐入门资料：</h3>\n<p>Rust在线编辑器:                     https://play.rust-lang.org/</p>\n<p>《Rust语言程序设计》:            https://kaisery.github.io/trpl-zh-cn/</p>\n<p>打怪通关学习方式Rustlings:   https://github.com/rust-lang/rustlings</p>\n<p>Rust优秀项目Datafuse：        https://github.com/datafuselabs/datafuse</p>\n","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":"2021-08-09 05:46:45","source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":null}],"extensions":{},"itunes_ext":null,"dublin_core_ext":null,"syndication_ext":null,"namespaces":{}}]},{"datetime":"2021-09-20T01:30:00Z","channels":[{"title":"cs.CL updates on arXiv.org","link":"http://export.arxiv.org/rss/cs.CL","description":"Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive","language":null,"copyright":null,"managing_editor":null,"webmaster":null,"pub_date":null,"last_build_date":null,"categories":[],"generator":null,"docs":null,"cloud":null,"rating":null,"ttl":null,"image":{"url":"http://arxiv.org/icons/sfx.gif","title":"arXiv.org","link":"http://arxiv.org/","width":null,"height":null,"description":null},"text_input":null,"skip_hours":[],"skip_days":[],"items":[{"title":"Fast-Slow Transformer for Visually Grounding Speech. (arXiv:2109.08186v1 [eess.AS])","link":"http://arxiv.org/abs/2109.08186","description":"<p>We present Fast-Slow Transformer for Visually Grounding Speech, or FaST-VGS.\nFaST-VGS is a Transformer-based model for learning the associations between raw\nspeech waveforms and visual images. The model unifies dual-encoder and\ncross-attention architectures into a single model, reaping the superior\nretrieval speed of the former along with the accuracy of the latter. FaST-VGS\nachieves state-of-the-art speech-image retrieval accuracy on benchmark\ndatasets, and its learned representations exhibit strong performance on the\nZeroSpeech 2021 phonetic and semantic tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/eess/1/au:+Peng_P/0/1/0/all/0/1\">Puyuan Peng</a>, <a href=\"http://arxiv.org/find/eess/1/au:+Harwath_D/0/1/0/all/0/1\">David Harwath</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Numerical reasoning in machine reading comprehension tasks: are we there yet?. (arXiv:2109.08207v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08207","description":"<p>Numerical reasoning based machine reading comprehension is a task that\ninvolves reading comprehension along with using arithmetic operations such as\naddition, subtraction, sorting, and counting. The DROP benchmark (Dua et al.,\n2019) is a recent dataset that has inspired the design of NLP models aimed at\nsolving this task. The current standings of these models in the DROP\nleaderboard, over standard metrics, suggest that the models have achieved\nnear-human performance. However, does this mean that these models have learned\nto reason? In this paper, we present a controlled study on some of the\ntop-performing model architectures for the task of numerical reasoning. Our\nobservations suggest that the standard metrics are incapable of measuring\nprogress towards such tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Al_Negheimish_H/0/1/0/all/0/1\">Hadeel Al-Negheimish</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1\">Pranava Madhyastha</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1\">Alessandra Russo</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Control of Situated Agents through Natural Language. (arXiv:2109.08214v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08214","description":"<p>When humans conceive how to perform a particular task, they do so\nhierarchically: splitting higher-level tasks into smaller sub-tasks. However,\nin the literature on natural language (NL) command of situated agents, most\nworks have treated the procedures to be executed as flat sequences of simple\nactions, or any hierarchies of procedures have been shallow at best. In this\npaper, we propose a formalism of procedures as programs, a powerful yet\nintuitive method of representing hierarchical procedural knowledge for agent\ncommand and control. We further propose a modeling paradigm of hierarchical\nmodular networks, which consist of a planner and reactors that convert NL\nintents to predictions of executable programs and probe the environment for\ninformation necessary to complete the program execution. We instantiate this\nframework on the IQA and ALFRED datasets for NL instruction following. Our\nmodel outperforms reactive baselines by a large margin on both datasets. We\nalso demonstrate that our framework is more data-efficient, and that it allows\nfor fast iterative development.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1\">Shuyan Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yin_P/0/1/0/all/0/1\">Pengcheng Yin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Neubig_G/0/1/0/all/0/1\">Graham Neubig</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Bag of Tricks for Dialogue Summarization. (arXiv:2109.08232v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08232","description":"<p>Dialogue summarization comes with its own peculiar challenges as opposed to\nnews or scientific articles summarization. In this work, we explore four\ndifferent challenges of the task: handling and differentiating parts of the\ndialogue belonging to multiple speakers, negation understanding, reasoning\nabout the situation, and informal language understanding. Using a pretrained\nsequence-to-sequence language model, we explore speaker name substitution,\nnegation scope highlighting, multi-task learning with relevant tasks, and\npretraining on in-domain data. Our experiments show that our proposed\ntechniques indeed improve summarization performance, outperforming strong\nbaselines.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Khalifa_M/0/1/0/all/0/1\">Muhammad Khalifa</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ballesteros_M/0/1/0/all/0/1\">Miguel Ballesteros</a>, <a href=\"http://arxiv.org/find/cs/1/au:+McKeown_K/0/1/0/all/0/1\">Kathleen McKeown</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Regularized Training of Nearest Neighbor Language Models. (arXiv:2109.08249v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08249","description":"<p>Including memory banks in a natural language processing architecture\nincreases model capacity by equipping it with additional data at inference\ntime. In this paper, we build upon $k$NN-LM \\citep{khandelwal20generalization},\nwhich uses a pre-trained language model together with an exhaustive $k$NN\nsearch through the training data (memory bank) to achieve state-of-the-art\nresults. We investigate whether we can improve the $k$NN-LM performance by\ninstead training a LM with the knowledge that we will be using a $k$NN\npost-hoc. We achieved significant improvement using our method on language\nmodeling tasks on \\texttt{WIKI-2} and \\texttt{WIKI-103}. The main phenomenon\nthat we encounter is that adding a simple L2 regularization on the activations\n(not weights) of the model, a transformer, improves the post-hoc $k$NN\nclassification performance. We explore some possible reasons for this\nimprovement. In particular, we find that the added L2 regularization seems to\nimprove the performance for high-frequency words without deteriorating the\nperformance for low frequency ones.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ton_J/0/1/0/all/0/1\">Jean-Francois Ton</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Talbott_W/0/1/0/all/0/1\">Walter Talbott</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1\">Shuangfei Zhai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1\">Josh Susskind</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Balancing out Bias: Achieving Fairness Through Training Reweighting. (arXiv:2109.08253v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08253","description":"<p>Bias in natural language processing arises primarily from models learning\ncharacteristics of the author such as gender and race when modelling tasks such\nas sentiment and syntactic parsing. This problem manifests as disparities in\nerror rates across author demographics, typically disadvantaging minority\ngroups. Existing methods for mitigating and measuring bias do not directly\naccount for correlations between author demographics and linguistic variables.\nMoreover, evaluation of bias has been inconsistent in previous work, in terms\nof dataset balance and evaluation methods. This paper introduces a very simple\nbut highly effective method for countering bias using instance reweighting,\nbased on the frequency of both task labels and author demographics. We extend\nthe method in the form of a gated model which incorporates the author\ndemographic as an input, and show that while it is highly vulnerable to input\ndata bias, it provides debiased predictions through demographic input\nperturbation, and outperforms all other bias mitigation techniques when\ncombined with instance reweighting.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1\">Xudong Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1\">Timothy Baldwin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cohn_T/0/1/0/all/0/1\">Trevor Cohn</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics Sheet for Automatic Emotion Recognition and Sentiment Analysis. (arXiv:2109.08256v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08256","description":"<p>The importance and pervasiveness of emotions in our lives makes affective\ncomputing a tremendously important and vibrant line of work. Systems for\nautomatic emotion recognition (AER) and sentiment analysis can be facilitators\nof enormous progress (e.g., in improving public health and commerce) but also\nenablers of great harm (e.g., for suppressing dissidents and manipulating\nvoters). Thus, it is imperative that the affective computing community actively\nengage with the ethical ramifications of their creations. In this paper, I have\nsynthesized and organized information from AI Ethics and Emotion Recognition\nliterature to present fifty ethical considerations relevant to AER. Notably,\nthe sheet fleshes out assumptions hidden in how AER is commonly framed, and in\nthe choices often made regarding the data, method, and evaluation. Special\nattention is paid to the implications of AER on privacy and social groups. The\nobjective of the sheet is to facilitate and encourage more thoughtfulness on\nwhy to automate, how to automate, and how to judge success well before the\nbuilding of AER systems. Additionally, the sheet acts as a useful introductory\ndocument on emotion recognition (complementing survey articles).\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-training with Few-shot Rationalization: Teacher Explanations Aid Student in Few-shot NLU. (arXiv:2109.08259v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08259","description":"<p>While pre-trained language models have obtained state-of-the-art performance\nfor several natural language understanding tasks, they are quite opaque in\nterms of their decision-making process. While some recent works focus on\nrationalizing neural predictions by highlighting salient concepts in the text\nas justifications or rationales, they rely on thousands of labeled training\nexamples for both task labels as well as an-notated rationales for every\ninstance. Such extensive large-scale annotations are infeasible to obtain for\nmany tasks. To this end, we develop a multi-task teacher-student framework\nbased on self-training language models with limited task-specific labels and\nrationales, and judicious sample selection to learn from informative\npseudo-labeled examples1. We study several characteristics of what constitutes\na good rationale and demonstrate that the neural model performance can be\nsignificantly improved by making it aware of its rationalized predictions,\nparticularly in low-resource settings. Extensive experiments in several\nbench-mark datasets demonstrate the effectiveness of our approach.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1\">Meghana Moorthy Bhat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sordoni_A/0/1/0/all/0/1\">Alessandro Sordoni</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1\">Subhabrata Mukherjee</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Language Models as a Knowledge Source for Cognitive Agents. (arXiv:2109.08270v1 [cs.AI])","link":"http://arxiv.org/abs/2109.08270","description":"<p>Language models (LMs) are sentence-completion engines trained on massive\ncorpora. LMs have emerged as a significant breakthrough in natural-language\nprocessing, providing capabilities that go far beyond sentence completion\nincluding question answering, summarization, and natural-language inference.\nWhile many of these capabilities have potential application to cognitive\nsystems, exploiting language models as a source of task knowledge, especially\nfor task learning, offers significant, near-term benefits. We introduce\nlanguage models and the various tasks to which they have been applied and then\nreview methods of knowledge extraction from language models. The resulting\nanalysis outlines both the challenges and opportunities for using language\nmodels as a new knowledge source for cognitive systems. It also identifies\npossible ways to improve knowledge extraction from language models using the\ncapabilities provided by cognitive systems. Central to success will be the\nability of a cognitive agent to itself learn an abstract model of the knowledge\nimplicit in the LM as well as methods to extract high-quality knowledge\neffectively and efficiently. To illustrate, we introduce a hypothetical robot\nagent and describe how language models could extend its task knowledge and\nimprove its performance and the kinds of knowledge and methods the agent can\nuse to exploit the knowledge within a language model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wray_R/0/1/0/all/0/1\">Robert E. Wray, III</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kirk_J/0/1/0/all/0/1\">James R. Kirk</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Laird_J/0/1/0/all/0/1\">John E. Laird</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"SentiPrompt: Sentiment Knowledge Enhanced Prompt-Tuning for Aspect-Based Sentiment Analysis. (arXiv:2109.08306v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08306","description":"<p>Aspect-based sentiment analysis (ABSA) is an emerging fine-grained sentiment\nanalysis task that aims to extract aspects, classify corresponding sentiment\npolarities and find opinions as the causes of sentiment. The latest research\ntends to solve the ABSA task in a unified way with end-to-end frameworks. Yet,\nthese frameworks get fine-tuned from downstream tasks without any task-adaptive\nmodification. Specifically, they do not use task-related knowledge well or\nexplicitly model relations between aspect and opinion terms, hindering them\nfrom better performance. In this paper, we propose SentiPrompt to use sentiment\nknowledge enhanced prompts to tune the language model in the unified framework.\nWe inject sentiment knowledge regarding aspects, opinions, and polarities into\nprompt and explicitly model term relations via constructing consistency and\npolarity judgment templates from the ground truth triplets. Experimental\nresults demonstrate that our approach can outperform strong baselines on\nTriplet Extraction, Pair Extraction, and Aspect Term Extraction with Sentiment\nClassification by a notable margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Chengxi Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1\">Feiyu Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1\">Jiajun Bu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1\">Lu Xu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiang Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1\">Zirui Shao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1\">Qi Zheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1\">Ningyu Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yongpan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1\">Zhi Yu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Multimodal Sentiment Dataset for Video Recommendation. (arXiv:2109.08333v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08333","description":"<p>Recently, multimodal sentiment analysis has seen remarkable advance and a lot\nof datasets are proposed for its development. In general, current multimodal\nsentiment analysis datasets usually follow the traditional system of\nsentiment/emotion, such as positive, negative and so on. However, when applied\nin the scenario of video recommendation, the traditional sentiment/emotion\nsystem is hard to be leveraged to represent different contents of videos in the\nperspective of visual senses and language understanding. Based on this, we\npropose a multimodal sentiment analysis dataset, named baiDu Video Sentiment\ndataset (DuVideoSenti), and introduce a new sentiment system which is designed\nto describe the sentimental style of a video on recommendation scenery.\nSpecifically, DuVideoSenti consists of 5,630 videos which displayed on Baidu,\neach video is manually annotated with a sentimental style label which describes\nthe user's real feeling of a video. Furthermore, we propose UNIMO as our\nbaseline for DuVideoSenti. Experimental results show that DuVideoSenti brings\nnew challenges to multimodal sentiment analysis, and could be used as a new\nbenchmark for evaluating approaches designed for video understanding and\nmultimodal fusion. We also expect our proposed DuVideoSenti could further\nimprove the development of multimodal sentiment analysis and its application to\nvideo recommendations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1\">Hongxuan Tang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1\">Xinyan Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Task-adaptive Pre-training of Language Models with Word Embedding Regularization. (arXiv:2109.08354v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08354","description":"<p>Pre-trained language models (PTLMs) acquire domain-independent linguistic\nknowledge through pre-training with massive textual resources. Additional\npre-training is effective in adapting PTLMs to domains that are not well\ncovered by the pre-training corpora. Here, we focus on the static word\nembeddings of PTLMs for domain adaptation to teach PTLMs domain-specific\nmeanings of words. We propose a novel fine-tuning process: task-adaptive\npre-training with word embedding regularization (TAPTER). TAPTER runs\nadditional pre-training by making the static word embeddings of a PTLM close to\nthe word embeddings obtained in the target domain with fastText. TAPTER\nrequires no additional corpus except for the training data of the downstream\ntask. We confirmed that TAPTER improves the performance of the standard\nfine-tuning and the task-adaptive pre-training on BioASQ (question answering in\nthe biomedical domain) and on SQuAD (the Wikipedia domain) when their\npre-training corpora were not dominated by in-domain data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nishida_K/0/1/0/all/0/1\">Kyosuke Nishida</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yoshida_S/0/1/0/all/0/1\">Sen Yoshida</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Distilling Linguistic Context for Language Model Compression. (arXiv:2109.08359v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08359","description":"<p>A computationally expensive and memory intensive neural network lies behind\nthe recent success of language representation learning. Knowledge distillation,\na major technique for deploying such a vast language model in resource-scarce\nenvironments, transfers the knowledge on individual word representations\nlearned without restrictions. In this paper, inspired by the recent\nobservations that language representations are relatively positioned and have\nmore semantic knowledge as a whole, we present a new knowledge distillation\nobjective for language representation learning that transfers the contextual\nknowledge via two types of relationships across representations: Word Relation\nand Layer Transforming Relation. Unlike other recent distillation techniques\nfor the language models, our contextual distillation does not have any\nrestrictions on architectural changes between teacher and student. We validate\nthe effectiveness of our method on challenging benchmarks of language\nunderstanding tasks, not only in architectures of various sizes, but also in\ncombination with DynaBERT, the recently proposed adaptive size pruning method.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1\">Geondo Park</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1\">Gyeongman Kim</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1\">Eunho Yang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CodeQA: A Question Answering Dataset for Source Code Comprehension. (arXiv:2109.08365v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08365","description":"<p>We propose CodeQA, a free-form question answering dataset for the purpose of\nsource code comprehension: given a code snippet and a question, a textual\nanswer is required to be generated. CodeQA contains a Java dataset with 119,778\nquestion-answer pairs and a Python dataset with 70,085 question-answer pairs.\nTo obtain natural and faithful questions and answers, we implement syntactic\nrules and semantic analysis to transform code comments into question-answer\npairs. We present the construction process and conduct systematic analysis of\nour dataset. Experiment results achieved by several neural baselines on our\ndataset are shown and discussed. While research on question-answering and\nmachine reading comprehension develops rapidly, few prior work has drawn\nattention to code question answering. This new dataset can serve as a useful\nresearch benchmark for source code comprehension.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chenxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1\">Xiaojun Wan</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To be Closer: Learning to Link up Aspects with Opinions. (arXiv:2109.08382v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08382","description":"<p>Dependency parse trees are helpful for discovering the opinion words in\naspect-based sentiment analysis (ABSA). However, the trees obtained from\noff-the-shelf dependency parsers are static, and could be sub-optimal in ABSA.\nThis is because the syntactic trees are not designed for capturing the\ninteractions between opinion words and aspect words. In this work, we aim to\nshorten the distance between aspects and corresponding opinion words by\nlearning an aspect-centric tree structure. The aspect and opinion words are\nexpected to be closer along such tree structure compared to the standard\ndependency parse tree. The learning process allows the tree structure to\nadaptively correlate the aspect and opinion words, enabling us to better\nidentify the polarity in the ABSA task. We conduct experiments on five\naspect-based sentiment datasets, and the proposed model significantly\noutperforms recent strong baselines. Furthermore, our thorough analysis\ndemonstrates the average distance between aspect and opinion words are\nshortened by at least 19% on the standard SemEval Restaurant14 dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yuxiang Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liao_L/0/1/0/all/0/1\">Lejian Liao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1\">Yang Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jie_Z/0/1/0/all/0/1\">Zhanming Jie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"reproducing \"ner and pos when nothing is capitalized\". (arXiv:2109.08396v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08396","description":"<p>Capitalization is an important feature in many NLP tasks such as Named Entity\nRecognition (NER) or Part of Speech Tagging (POS). We are trying to reproduce\nresults of paper which shows how to mitigate a significant performance drop\nwhen casing is mismatched between training and testing data. In particular we\nshow that lowercasing 50% of the dataset provides the best performance,\nmatching the claims of the original paper. We also show that we got slightly\nlower performance in almost all experiments we have tried to reproduce,\nsuggesting that there might be some hidden factors impacting our performance.\nLastly, we make all of our work available in a public github repository.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Kuster_A/0/1/0/all/0/1\">Andreas Kuster</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Filipek_J/0/1/0/all/0/1\">Jakub Filipek</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Muppirala_V/0/1/0/all/0/1\">Viswa Virinchi Muppirala</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers. (arXiv:2109.08406v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08406","description":"<p>Despite the success of fine-tuning pretrained language encoders like BERT for\ndownstream natural language understanding (NLU) tasks, it is still poorly\nunderstood how neural networks change after fine-tuning. In this work, we use\ncentered kernel alignment (CKA), a method for comparing learned\nrepresentations, to measure the similarity of representations in task-tuned\nmodels across layers. In experiments across twelve NLU tasks, we discover a\nconsistent block diagonal structure in the similarity of representations within\nfine-tuned RoBERTa and ALBERT models, with strong similarity within clusters of\nearlier and later layers, but not between them. The similarity of later layer\nrepresentations implies that later layers only marginally contribute to task\nperformance, and we verify in experiments that the top few layers of fine-tuned\nTransformers can be discarded without hurting performance, even with no further\ntuning.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Phang_J/0/1/0/all/0/1\">Jason Phang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Haokun Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1\">Samuel R. Bowman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"A Role-Selected Sharing Network for Joint Machine-Human Chatting Handoff and Service Satisfaction Analysis. (arXiv:2109.08412v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08412","description":"<p>Chatbot is increasingly thriving in different domains, however, because of\nunexpected discourse complexity and training data sparseness, its potential\ndistrust hatches vital apprehension. Recently, Machine-Human Chatting Handoff\n(MHCH), predicting chatbot failure and enabling human-algorithm collaboration\nto enhance chatbot quality, has attracted increasing attention from industry\nand academia. In this study, we propose a novel model, Role-Selected Sharing\nNetwork (RSSN), which integrates both dialogue satisfaction estimation and\nhandoff prediction in one multi-task learning framework. Unlike prior efforts\nin dialog mining, by utilizing local user satisfaction as a bridge, global\nsatisfaction detector and handoff predictor can effectively exchange critical\ninformation. Specifically, we decouple the relation and interaction between the\ntwo tasks by the role information after the shared encoder. Extensive\nexperiments on two public datasets demonstrate the effectiveness of our model.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1\">Jiawei Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1\">Kaisong Song</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1\">Yangyang Kang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1\">Guoxiu He</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1\">Zhuoren Jiang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1\">Changlong Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1\">Wei Lu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaozhong Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"New Students on Sesame Street: What Order-Aware Matrix Embeddings Can Learn from BERT. (arXiv:2109.08449v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08449","description":"<p>Large-scale pretrained language models (PreLMs) are revolutionizing natural\nlanguage processing across all benchmarks. However, their sheer size is\nprohibitive in low-resource or large-scale applications. While common\napproaches reduce the size of PreLMs via same-architecture distillation or\npruning, we explore distilling PreLMs into more efficient order-aware embedding\nmodels. Our results on the GLUE benchmark show that embedding-centric students,\nwhich have learned from BERT, yield scores comparable to DistilBERT on QQP and\nRTE, often match or exceed the scores of ELMo, and only fall behind on\ndetecting linguistic acceptability.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Galke_L/0/1/0/all/0/1\">Lukas Galke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cuber_I/0/1/0/all/0/1\">Isabelle Cuber</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meyer_C/0/1/0/all/0/1\">Christoph Meyer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nolscher_H/0/1/0/all/0/1\">Henrik Ferdinand N&#xf6;lscher</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sonderecker_A/0/1/0/all/0/1\">Angelina Sonderecker</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Scherp_A/0/1/0/all/0/1\">Ansgar Scherp</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Neural Unification for Logic Reasoning over Natural Language. (arXiv:2109.08460v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08460","description":"<p>Automated Theorem Proving (ATP) deals with the development of computer\nprograms being able to show that some conjectures (queries) are a logical\nconsequence of a set of axioms (facts and rules). There exists several\nsuccessful ATPs where conjectures and axioms are formally provided (e.g.\nformalised as First Order Logic formulas). Recent approaches, such as (Clark et\nal., 2020), have proposed transformer-based architectures for deriving\nconjectures given axioms expressed in natural language (English). The\nconjecture is verified through a binary text classifier, where the transformers\nmodel is trained to predict the truth value of a conjecture given the axioms.\nThe RuleTaker approach of (Clark et al., 2020) achieves appealing results both\nin terms of accuracy and in the ability to generalize, showing that when the\nmodel is trained with deep enough queries (at least 3 inference steps), the\ntransformers are able to correctly answer the majority of queries (97.6%) that\nrequire up to 5 inference steps. In this work we propose a new architecture,\nnamely the Neural Unifier, and a relative training procedure, which achieves\nstate-of-the-art results in term of generalisation, showing that mimicking a\nwell-known inference procedure, the backward chaining, it is possible to answer\ndeep queries even when the model is trained only on shallow ones. The approach\nis demonstrated in experiments using a diverse set of benchmark data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Picco_G/0/1/0/all/0/1\">Gabriele Picco</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lam_H/0/1/0/all/0/1\">Hoang Thanh Lam</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sbodio_M/0/1/0/all/0/1\">Marco Luca Sbodio</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Garcia_V/0/1/0/all/0/1\">Vanessa Lopez Garcia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"GoG: Relation-aware Graph-over-Graph Network for Visual Dialog. (arXiv:2109.08475v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08475","description":"<p>Visual dialog, which aims to hold a meaningful conversation with humans about\na given image, is a challenging task that requires models to reason the complex\ndependencies among visual content, dialog history, and current questions. Graph\nneural networks are recently applied to model the implicit relations between\nobjects in an image or dialog. However, they neglect the importance of 1)\ncoreference relations among dialog history and dependency relations between\nwords for the question representation; and 2) the representation of the image\nbased on the fully represented question. Therefore, we propose a novel\nrelation-aware graph-over-graph network (GoG) for visual dialog. Specifically,\nGoG consists of three sequential graphs: 1) H-Graph, which aims to capture\ncoreference relations among dialog history; 2) History-aware Q-Graph, which\naims to fully understand the question through capturing dependency relations\nbetween words based on coreference resolution on the dialog history; and 3)\nQuestion-aware I-Graph, which aims to capture the relations between objects in\nan image based on fully question representation. As an additional feature\nrepresentation module, we add GoG to the existing visual dialogue model.\nExperimental results show that our model outperforms the strong baseline in\nboth generative and discriminative settings by a significant margin.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Multimodal Incremental Transformer with Visual Grounding for Visual Dialogue Generation. (arXiv:2109.08478v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08478","description":"<p>Visual dialogue is a challenging task since it needs to answer a series of\ncoherent questions on the basis of understanding the visual environment.\nPrevious studies focus on the implicit exploration of multimodal co-reference\nby implicitly attending to spatial image features or object-level image\nfeatures but neglect the importance of locating the objects explicitly in the\nvisual content, which is associated with entities in the textual content.\nTherefore, in this paper we propose a {\\bf M}ultimodal {\\bf I}ncremental {\\bf\nT}ransformer with {\\bf V}isual {\\bf G}rounding, named MITVG, which consists of\ntwo key parts: visual grounding and multimodal incremental transformer. Visual\ngrounding aims to explicitly locate related objects in the image guided by\ntextual entities, which helps the model exclude the visual content that does\nnot need attention. On the basis of visual grounding, the multimodal\nincremental transformer encodes the multi-turn dialogue history combined with\nvisual scene step by step according to the order of the dialogue and then\ngenerates a contextually and visually coherent response. Experimental results\non the VisDial v0.9 and v1.0 datasets demonstrate the superiority of the\nproposed model, which achieves comparable performance.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1\">Feilong Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1\">Fandong Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1\">Xiuyi Chen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1\">Peng Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1\">Jie Zhou</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Simple Entity-Centric Questions Challenge Dense Retrievers. (arXiv:2109.08535v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08535","description":"<p>Open-domain question answering has exploded in popularity recently due to the\nsuccess of dense retrieval models, which have surpassed sparse models using\nonly a few supervised training examples. However, in this paper, we demonstrate\ncurrent dense models are not yet the holy grail of retrieval. We first\nconstruct EntityQuestions, a set of simple, entity-rich questions based on\nfacts from Wikidata (e.g., \"Where was Arve Furset born?\"), and observe that\ndense retrievers drastically underperform sparse methods. We investigate this\nissue and uncover that dense retrievers can only generalize to common entities\nunless the question pattern is explicitly observed during training. We discuss\ntwo simple solutions towards addressing this critical problem. First, we\ndemonstrate that data augmentation is unable to fix the generalization problem.\nSecond, we argue a more robust passage encoder helps facilitate better question\nadaptation using specialized question encoders. We hope our work can shed light\non the challenges in creating a robust, universal dense retriever that works\nwell across different input distributions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sciavolino_C/0/1/0/all/0/1\">Christopher Sciavolino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1\">Zexuan Zhong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jinhyuk Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1\">Danqi Chen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Conversational Multi-Hop Reasoning with Neural Commonsense Knowledge and Symbolic Logic Rules. (arXiv:2109.08544v1 [cs.AI])","link":"http://arxiv.org/abs/2109.08544","description":"<p>One of the challenges faced by conversational agents is their inability to\nidentify unstated presumptions of their users' commands, a task trivial for\nhumans due to their common sense. In this paper, we propose a zero-shot\ncommonsense reasoning system for conversational agents in an attempt to achieve\nthis. Our reasoner uncovers unstated presumptions from user commands satisfying\na general template of if-(state), then-(action), because-(goal). Our reasoner\nuses a state-of-the-art transformer-based generative commonsense knowledge base\n(KB) as its source of background knowledge for reasoning. We propose a novel\nand iterative knowledge query mechanism to extract multi-hop reasoning chains\nfrom the neural KB which uses symbolic logic rules to significantly reduce the\nsearch space. Similar to any KBs gathered to date, our commonsense KB is prone\nto missing knowledge. Therefore, we propose to conversationally elicit the\nmissing knowledge from human users with our novel dynamic question generation\nstrategy, which generates and presents contextualized queries to human users.\nWe evaluate the model with a user study with human users that achieves a 35%\nhigher success rate compared to SOTA.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1\">Forough Arabshahi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1\">Jennifer Lee</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1\">Antoine Bosselut</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1\">Yejin Choi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1\">Tom Mitchell</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Slot Filling for Biomedical Information Extraction. (arXiv:2109.08564v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08564","description":"<p>Information Extraction (IE) from text refers to the task of extracting\nstructured knowledge from unstructured text. The task typically consists of a\nseries of sub-tasks such as Named Entity Recognition and Relation Extraction.\nSourcing entity and relation type specific training data is a major bottleneck\nin the above sub-tasks.In this work we present a slot filling approach to the\ntask of biomedical IE, effectively replacing the need for entity and\nrelation-specific training data, allowing to deal with zero-shot settings. We\nfollow the recently proposed paradigm of coupling a Tranformer-based\nbi-encoder, Dense Passage Retrieval, with a Transformer-based reader model to\nextract relations from biomedical text. We assemble a biomedical slot filling\ndataset for both retrieval and reading comprehension and conduct a series of\nexperiments demonstrating that our approach outperforms a number of simpler\nbaselines. We also evaluate our approach end-to-end for standard as well as\nzero-shot settings. Our work provides a fresh perspective on how to solve\nbiomedical IE tasks, in the absence of relevant training data. Our code, models\nand pretrained data are available at\nhttps://github.com/healx/biomed-slot-filling.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Papanikolaou_Y/0/1/0/all/0/1\">Yannis Papanikolaou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_F/0/1/0/all/0/1\">Francine Bennett</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Exploring Multitask Learning for Low-Resource AbstractiveSummarization. (arXiv:2109.08565v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08565","description":"<p>This paper explores the effect of using multitask learning for abstractive\nsummarization in the context of small training corpora. In particular, we\nincorporate four different tasks (extractive summarization, language modeling,\nconcept detection, and paraphrase detection) both individually and in\ncombination, with the goal of enhancing the target task of abstractive\nsummarization via multitask learning. We show that for many task combinations,\na model trained in a multitask setting outperforms a model trained only for\nabstractive summarization, with no additional summarization data introduced.\nAdditionally, we do a comprehensive search and find that certain tasks (e.g.\nparaphrase detection) consistently benefit abstractive summarization, not only\nwhen combined with other tasks but also when using different architectures and\ntraining corpora.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magooda_A/0/1/0/all/0/1\">Ahmed Magooda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Elaraby_M/0/1/0/all/0/1\">Mohamed Elaraby</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Mitigating Data Scarceness through Data Synthesis, Augmentation and Curriculum for Abstractive Summarization. (arXiv:2109.08569v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08569","description":"<p>This paper explores three simple data manipulation techniques (synthesis,\naugmentation, curriculum) for improving abstractive summarization models\nwithout the need for any additional data. We introduce a method of data\nsynthesis with paraphrasing, a data augmentation technique with sample mixing,\nand curriculum learning with two new difficulty metrics based on specificity\nand abstractiveness. We conduct experiments to show that these three techniques\ncan help improve abstractive summarization across two summarization models and\ntwo different small datasets. Furthermore, we show that these techniques can\nimprove performance when applied in isolation and when combined.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Magooda_A/0/1/0/all/0/1\">Ahmed Magooda</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Litman_D/0/1/0/all/0/1\">Diane Litman</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchy-Aware T5 with Path-Adaptive Mask Mechanism for Hierarchical Text Classification. (arXiv:2109.08585v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08585","description":"<p>Hierarchical Text Classification (HTC), which aims to predict text labels\norganized in hierarchical space, is a significant task lacking in investigation\nin natural language processing. Existing methods usually encode the entire\nhierarchical structure and fail to construct a robust label-dependent model,\nmaking it hard to make accurate predictions on sparse lower-level labels and\nachieving low Macro-F1. In this paper, we propose a novel PAMM-HiA-T5 model for\nHTC: a hierarchy-aware T5 model with path-adaptive mask mechanism that not only\nbuilds the knowledge of upper-level labels into low-level ones but also\nintroduces path dependency information in label prediction. Specifically, we\ngenerate a multi-level sequential label structure to exploit hierarchical\ndependency across different levels with Breadth-First Search (BFS) and T5\nmodel. To further improve label dependency prediction within each path, we then\npropose an original path-adaptive mask mechanism (PAMM) to identify the label's\npath information, eliminating sources of noises from other paths. Comprehensive\nexperiments on three benchmark datasets show that our novel PAMM-HiA-T5 model\ngreatly outperforms all state-of-the-art HTC approaches especially in Macro-F1.\nThe ablation studies show that the improvements mainly come from our innovative\napproach instead of T5.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1\">Wei Huang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1\">Chen Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1\">Yihua Zhao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1\">Xinyun Yang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1\">Zhaoming Pan</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1\">Zhimin Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1\">Guiquan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Does Commonsense help in detecting Sarcasm?. (arXiv:2109.08588v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08588","description":"<p>Sarcasm detection is important for several NLP tasks such as sentiment\nidentification in product reviews, user feedback, and online forums. It is a\nchallenging task requiring a deep understanding of language, context, and world\nknowledge. In this paper, we investigate whether incorporating commonsense\nknowledge helps in sarcasm detection. For this, we incorporate commonsense\nknowledge into the prediction process using a graph convolution network with\npre-trained language model embeddings as input. Our experiments with three\nsarcasm detection datasets indicate that the approach does not outperform the\nbaseline model. We perform an exhaustive set of experiments to analyze where\ncommonsense support adds value and where it hurts classification. Our\nimplementation is publicly available at:\nhttps://github.com/brcsomnath/commonsense-sarcasm.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Boosting Transformers for Job Expression Extraction and Classification in a Low-Resource Setting. (arXiv:2109.08597v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08597","description":"<p>In this paper, we explore possible improvements of transformer models in a\nlow-resource setting. In particular, we present our approaches to tackle the\nfirst two of three subtasks of the MEDDOPROF competition, i.e., the extraction\nand classification of job expressions in Spanish clinical texts. As neither\nlanguage nor domain experts, we experiment with the multilingual XLM-R\ntransformer model and tackle these low-resource information extraction tasks as\nsequence-labeling problems. We explore domain- and language-adaptive\npretraining, transfer learning and strategic datasplits to boost the\ntransformer model. Our results show strong improvements using these methods by\nup to 5.3 F1 points compared to a fine-tuned XLM-R model. Our best models\nachieve 83.2 and 79.3 F1 for the first two tasks, respectively.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Lange_L/0/1/0/all/0/1\">Lukas Lange</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Adel_H/0/1/0/all/0/1\">Heike Adel</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Strotgen_J/0/1/0/all/0/1\">Jannik Str&#xf6;tgen</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"The futility of STILTs for the classification of lexical borrowings in Spanish. (arXiv:2109.08607v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08607","description":"<p>The first edition of the IberLEF 2021 shared task on automatic detection of\nborrowings (ADoBo) focused on detecting lexical borrowings that appeared in the\nSpanish press and that have recently been imported into the Spanish language.\nIn this work, we tested supplementary training on intermediate labeled-data\ntasks (STILTs) from part of speech (POS), named entity recognition (NER),\ncode-switching, and language identification approaches to the classification of\nborrowings at the token level using existing pre-trained transformer-based\nlanguage models. Our extensive experimental results suggest that STILTs do not\nprovide any improvement over direct fine-tuning of multilingual models.\nHowever, multilingual models trained on small subsets of languages perform\nreasonably better than multilingual BERT but not as good as multilingual\nRoBERTa for the given dataset.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosa_J/0/1/0/all/0/1\">Javier de la Rosa</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Adversarial Scrubbing of Demographic Information for Text Classification. (arXiv:2109.08613v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08613","description":"<p>Contextual representations learned by language models can often encode\nundesirable attributes, like demographic associations of the users, while being\ntrained for an unrelated target task. We aim to scrub such undesirable\nattributes and learn fair representations while maintaining performance on the\ntarget task. In this paper, we present an adversarial learning framework\n\"Adversarial Scrubber\" (ADS), to debias contextual representations. We perform\ntheoretical analysis to show that our framework converges without leaking\ndemographic information under certain conditions. We extend previous evaluation\ntechniques by evaluating debiasing performance using Minimum Description Length\n(MDL) probing. Experimental evaluations on 8 datasets show that ADS generates\nrepresentations with minimal information about demographic attributes while\nbeing maximally informative about the target task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1\">Sayan Ghosh</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yiyuan Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Oliva_J/0/1/0/all/0/1\">Junier B. Oliva</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1\">Shashank Srivastava</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"CKMorph: A Comprehensive Morphological Analyzer for Central Kurdish. (arXiv:2109.08615v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08615","description":"<p>A morphological analyzer, which is a significant component of many natural\nlanguage processing applications especially for morphologically rich languages,\ndivides an input word into all its composing morphemes and identifies their\nmorphological roles. In this paper, we introduce a comprehensive morphological\nanalyzer for Central Kurdish (CK), a low-resourced language with a rich\nmorphology. Building upon the limited existing literature, we first assembled\nand systematically categorized a comprehensive collection of the morphological\nand morphophonological rules of the language. Additionally, we collected and\nmanually labeled a generative lexicon containing nearly 10,000 verb, noun and\nadjective stems, named entities, and other types of word stems. We used these\nrule sets and resources to implement CKMorph Analyzer based on finite-state\ntransducers. In order to provide a benchmark for future research, we collected,\nmanually labeled, and publicly shared test sets for evaluating accuracy and\ncoverage of the analyzer. CKMorph was able to correctly analyze 95.9% of the\naccuracy test set, containing 1,000 CK words morphologically analyzed according\nto the context. Moreover, CKMorph gave at least one analysis for 95.5% of 4.22M\nCK tokens of the coverage test set. The demonstration of the application and\nresources including CK verb database and test sets are openly accessible at\nhttps://github.com/CKMorph.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Naserzade_M/0/1/0/all/0/1\">Morteza Naserzade</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Mahmudi_A/0/1/0/all/0/1\">Aso Mahmudi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Veisi_H/0/1/0/all/0/1\">Hadi Veisi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hosseini_H/0/1/0/all/0/1\">Hawre Hosseini</a>, <a href=\"http://arxiv.org/find/cs/1/au:+MohammadAmini_M/0/1/0/all/0/1\">Mohammad MohammadAmini</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Classification-based Quality Estimation: Small and Efficient Models for Real-world Applications. (arXiv:2109.08627v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08627","description":"<p>Sentence-level Quality estimation (QE) of machine translation is\ntraditionally formulated as a regression task, and the performance of QE models\nis typically measured by Pearson correlation with human labels. Recent QE\nmodels have achieved previously-unseen levels of correlation with human\njudgments, but they rely on large multilingual contextualized language models\nthat are computationally expensive and make them infeasible for real-world\napplications. In this work, we evaluate several model compression techniques\nfor QE and find that, despite their popularity in other NLP tasks, they lead to\npoor performance in this regression setting. We observe that a full model\nparameterization is required to achieve SoTA results in a regression task.\nHowever, we argue that the level of expressiveness of a model in a continuous\nrange is unnecessary given the downstream applications of QE, and show that\nreframing QE as a classification problem and evaluating QE models using\nclassification metrics would better reflect their actual performance in\nreal-world applications.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Shuo Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+El_Kishky_A/0/1/0/all/0/1\">Ahmed El-Kishky</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1\">Vishrav Chaudhary</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cross_J/0/1/0/all/0/1\">James Cross</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Guzman_F/0/1/0/all/0/1\">Francisco Guzm&#xe1;n</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Specia_L/0/1/0/all/0/1\">Lucia Specia</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Grounding Natural Language Instructions: Can Large Language Models Capture Spatial Information?. (arXiv:2109.08634v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08634","description":"<p>Models designed for intelligent process automation are required to be capable\nof grounding user interface elements. This task of interface element grounding\nis centred on linking instructions in natural language to their target\nreferents. Even though BERT and similar pre-trained language models have\nexcelled in several NLP tasks, their use has not been widely explored for the\nUI grounding domain. This work concentrates on testing and probing the\ngrounding abilities of three different transformer-based models: BERT, RoBERTa\nand LayoutLM. Our primary focus is on these models' spatial reasoning skills,\ngiven their importance in this domain. We observe that LayoutLM has a promising\nadvantage for applications in this domain, even though it was created for a\ndifferent original purpose (representing scanned documents): the learned\nspatial features appear to be transferable to the UI grounding setting,\nespecially as they demonstrate the ability to discriminate between target\ndirections in natural language instructions.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rozanova_J/0/1/0/all/0/1\">Julia Rozanova</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ferreira_D/0/1/0/all/0/1\">Deborah Ferreira</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dubba_K/0/1/0/all/0/1\">Krishna Dubba</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1\">Weiwei Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1\">Dell Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Freitas_A/0/1/0/all/0/1\">Andre Freitas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Efficient Measuring of Readability to Improve Documents Accessibility for Arabic Language Learners. (arXiv:2109.08648v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08648","description":"<p>This paper presents an approach based on supervised machine learning methods\nto build a classifier that can identify text complexity in order to present\nArabic language learners with texts suitable to their levels. The approach is\nbased on machine learning classification methods to discriminate between the\ndifferent levels of difficulty in reading and understanding a text. Several\nmodels were trained on a large corpus mined from online Arabic websites and\nmanually annotated. The model uses both Count and TF-IDF representations and\napplies five machine learning algorithms; Multinomial Naive Bayes, Bernoulli\nNaive Bayes, Logistic Regression, Support Vector Machine and Random Forest,\nusing unigrams and bigrams features. With the goal of extracting the text\ncomplexity, the problem is usually addressed by formulating the level\nidentification as a classification task. Experimental results showed that\nn-gram features could be indicative of the reading level of a text and could\nsubstantially improve performance, and showed that SVM and Multinomial Naive\nBayes are the most accurate in predicting the complexity level. Best results\nwere achieved using TF-IDF Vectors trained by a combination of word-based\nunigrams and bigrams with an overall accuracy of 87.14% over four classes of\ncomplexity.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Bessou_S/0/1/0/all/0/1\">Sadik Bessou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chenni_G/0/1/0/all/0/1\">Ghozlane Chenni</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Towards Handling Unconstrained User Preferences in Dialogue. (arXiv:2109.08650v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08650","description":"<p>A user input to a schema-driven dialogue information navigation system, such\nas venue search, is typically constrained by the underlying database which\nrestricts the user to specify a predefined set of preferences, or slots,\ncorresponding to the database fields. We envision a more natural information\nnavigation dialogue interface where a user has flexibility to specify\nunconstrained preferences that may not match a predefined schema. We propose to\nuse information retrieval from unstructured knowledge to identify entities\nrelevant to a user request. We update the Cambridge restaurants database with\nunstructured knowledge snippets (reviews and information from the web) for each\nof the restaurants and annotate a set of query-snippet pairs with a relevance\nlabel. We use the annotated dataset to train and evaluate snippet relevance\nclassifiers, as a proxy to evaluating recommendation accuracy. We show that\nwith a pretrained transformer model as an encoder, an unsupervised/supervised\nclassifier achieves a weighted F1 of .661/.856.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pandey_S/0/1/0/all/0/1\">Suraj Pandey</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Stoyanchev_S/0/1/0/all/0/1\">Svetlana Stoyanchev</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Doddipatla_R/0/1/0/all/0/1\">Rama Doddipatla</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Primer: Searching for Efficient Transformers for Language Modeling. (arXiv:2109.08668v1 [cs.LG])","link":"http://arxiv.org/abs/2109.08668","description":"<p>Large Transformer models have been central to recent advances in natural\nlanguage processing. The training and inference costs of these models, however,\nhave grown rapidly and become prohibitively expensive. Here we aim to reduce\nthe costs of Transformers by searching for a more efficient variant. Compared\nto previous approaches, our search is performed at a lower level, over the\nprimitives that define a Transformer TensorFlow program. We identify an\narchitecture, named Primer, that has a smaller training cost than the original\nTransformer and other variants for auto-regressive language modeling. Primer's\nimprovements can be mostly attributed to two simple modifications: squaring\nReLU activations and adding a depthwise convolution layer after each Q, K, and\nV projection in self-attention.\n</p>\n<p>Experiments show Primer's gains over Transformer increase as compute scale\ngrows and follow a power law with respect to quality at optimal model sizes. We\nalso verify empirically that Primer can be dropped into different codebases to\nsignificantly speed up training without additional tuning. For example, at a\n500M parameter size, Primer improves the original T5 architecture on C4\nauto-regressive language modeling, reducing the training cost by 4X.\nFurthermore, the reduced training cost means Primer needs much less compute to\nreach a target one-shot performance. For instance, in a 1.9B parameter\nconfiguration similar to GPT-3 XL, Primer uses 1/3 of the training compute to\nachieve the same one-shot performance as Transformer. We open source our models\nand several comparisons in T5 to help with reproducibility.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+So_D/0/1/0/all/0/1\">David R. So</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Manke_W/0/1/0/all/0/1\">Wojciech Ma&#x144;ke</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1\">Hanxiao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1\">Zihang Dai</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shazeer_N/0/1/0/all/0/1\">Noam Shazeer</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1\">Quoc V. Le</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RnG-KBQA: Generation Augmented Iterative Ranking for Knowledge Base Question Answering. (arXiv:2109.08678v1 [cs.CL])","link":"http://arxiv.org/abs/2109.08678","description":"<p>Existing KBQA approaches, despite achieving strong performance on i.i.d. test\ndata, often struggle in generalizing to questions involving unseen KB schema\nitems. Prior ranking-based approaches have shown some success in\ngeneralization, but suffer from the coverage issue. We present RnG-KBQA, a\nRank-and-Generate approach for KBQA, which remedies the coverage issue with a\ngeneration model while preserving a strong generalization capability. Our\napproach first uses a contrastive ranker to rank a set of candidate logical\nforms obtained by searching over the knowledge graph. It then introduces a\ntailored generation model conditioned on the question and the top-ranked\ncandidates to compose the final logical form. We achieve new state-of-the-art\nresults on GrailQA and WebQSP datasets. In particular, our method surpasses the\nprior state-of-the-art by a large margin on the GrailQA leaderboard. In\naddition, RnG-KBQA outperforms all prior approaches on the popular WebQSP\nbenchmark, even including the ones that use the oracle entity linking. The\nexperimental results demonstrate the effectiveness of the interplay between\nranking and generation, which leads to the superior performance of our proposed\napproach across all settings with especially strong improvements in zero-shot\ngeneralization.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1\">Xi Ye</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1\">Semih Yavuz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Hashimoto_K/0/1/0/all/0/1\">Kazuma Hashimoto</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1\">Yingbo Zhou</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Caiming Xiong</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Capturing Global Informativeness in Open Domain Keyphrase Extraction. (arXiv:2004.13639v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2004.13639","description":"<p>Open-domain KeyPhrase Extraction (KPE) aims to extract keyphrases from\ndocuments without domain or quality restrictions, e.g., web pages with variant\ndomains and qualities. Recently, neural methods have shown promising results in\nmany KPE tasks due to their powerful capacity for modeling contextual semantics\nof the given documents. However, we empirically show that most neural KPE\nmethods prefer to extract keyphrases with good phraseness, such as short and\nentity-style n-grams, instead of globally informative keyphrases from\nopen-domain documents. This paper presents JointKPE, an open-domain KPE\narchitecture built on pre-trained language models, which can capture both local\nphraseness and global informativeness when extracting keyphrases. JointKPE\nlearns to rank keyphrases by estimating their informativeness in the entire\ndocument and is jointly trained on the keyphrase chunking task to guarantee the\nphraseness of keyphrase candidates. Experiments on two large KPE datasets with\ndiverse domains, OpenKP and KP20k, demonstrate the effectiveness of JointKPE on\ndifferent pre-trained variants in open-domain scenarios. Further analyses\nreveal the significant advantages of JointKPE in predicting long and non-entity\nkeyphrases, which are challenging for previous neural KPE methods. Our code is\npublicly available at https://github.com/thunlp/BERT-KPE.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1\">Si Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhenghao Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1\">Zhiyuan Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bao_J/0/1/0/all/0/1\">Jie Bao</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. (arXiv:2007.15779v6 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2007.15779","description":"<p>Pretraining large neural language models, such as BERT, has led to impressive\ngains on many natural language processing (NLP) tasks. However, most\npretraining efforts focus on general domain corpora, such as newswire and Web.\nA prevailing assumption is that even domain-specific pretraining can benefit by\nstarting from general-domain language models. In this paper, we challenge this\nassumption by showing that for domains with abundant unlabeled text, such as\nbiomedicine, pretraining language models from scratch results in substantial\ngains over continual pretraining of general-domain language models. To\nfacilitate this investigation, we compile a comprehensive biomedical NLP\nbenchmark from publicly-available datasets. Our experiments show that\ndomain-specific pretraining serves as a solid foundation for a wide range of\nbiomedical NLP tasks, leading to new state-of-the-art results across the board.\nFurther, in conducting a thorough evaluation of modeling choices, both for\npretraining and task-specific fine-tuning, we discover that some common\npractices are unnecessary with BERT models, such as using complex tagging\nschemes in named entity recognition (NER). To help accelerate research in\nbiomedical NLP, we have released our state-of-the-art pretrained and\ntask-specific models for the community, and created a leaderboard featuring our\nBLURB benchmark (short for Biomedical Language Understanding &amp; Reasoning\nBenchmark) at https://aka.ms/BLURB.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1\">Yu Gu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Lucas_M/0/1/0/all/0/1\">Michael Lucas</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1\">Xiaodong Liu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Self-supervised Document Clustering Based on BERT with Data Augment. (arXiv:2011.08523v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2011.08523","description":"<p>Contrastive learning is a promising approach to unsupervised learning, as it\ninherits the advantages of well-studied deep models without a dedicated and\ncomplex model design. In this paper, based on bidirectional encoder\nrepresentations from transformers, we propose self-supervised contrastive\nlearning (SCL) as well as few-shot contrastive learning (FCL) with unsupervised\ndata augmentation (UDA) for text clustering. SCL outperforms state-of-the-art\nunsupervised clustering approaches for short texts and those for long texts in\nterms of several clustering evaluation measures. FCL achieves performance close\nto supervised learning, and FCL with UDA further improves the performance for\nshort texts.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1\">Haoxiang Shi</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Cen Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"To what extent do human explanations of model behavior align with actual model behavior?. (arXiv:2012.13354v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.13354","description":"<p>Given the increasingly prominent role NLP models (will) play in our lives, it\nis important for human expectations of model behavior to align with actual\nmodel behavior. Using Natural Language Inference (NLI) as a case study, we\ninvestigate the extent to which human-generated explanations of models'\ninference decisions align with how models actually make these decisions. More\nspecifically, we define three alignment metrics that quantify how well natural\nlanguage explanations align with model sensitivity to input words, as measured\nby integrated gradients. Then, we evaluate eight different models (the base and\nlarge versions of BERT, RoBERTa and ELECTRA, as well as anRNN and bag-of-words\nmodel), and find that the BERT-base model has the highest alignment with\nhuman-generated explanations, for all alignment metrics. Focusing in on\ntransformers, we find that the base versions tend to have higher alignment with\nhuman-generated explanations than their larger counterparts, suggesting that\nincreasing the number of model parameters leads, in some cases, to worse\nalignment with human explanations. Finally, we find that a model's alignment\nwith human explanations is not predicted by the model's accuracy, suggesting\nthat accuracy and alignment are complementary ways to evaluate models.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Prasad_G/0/1/0/all/0/1\">Grusha Prasad</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1\">Yixin Nie</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1\">Mohit Bansal</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1\">Robin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Kiela_D/0/1/0/all/0/1\">Douwe Kiela</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1\">Adina Williams</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ECONET: Effective Continual Pretraining of Language Models for Event Temporal Reasoning. (arXiv:2012.15283v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15283","description":"<p>While pre-trained language models (PTLMs) have achieved noticeable success on\nmany NLP tasks, they still struggle for tasks that require event temporal\nreasoning, which is essential for event-centric applications. We present a\ncontinual pre-training approach that equips PTLMs with targeted knowledge about\nevent temporal relations. We design self-supervised learning objectives to\nrecover masked-out event and temporal indicators and to discriminate sentences\nfrom their corrupted counterparts (where event or temporal indicators got\nreplaced). By further pre-training a PTLM with these objectives jointly, we\nreinforce its attention to event and temporal information, yielding enhanced\ncapability on event temporal reasoning. This effective continual pre-training\nframework for event temporal reasoning (ECONET) improves the PTLMs' fine-tuning\nperformances across five relation extraction and question answering tasks and\nachieves new or on-par state-of-the-art performances in most of our downstream\ntasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Han_R/0/1/0/all/0/1\">Rujun Han</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1\">Xiang Ren</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1\">Nanyun Peng</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"ERNIE-M: Enhanced Multilingual Representation by Aligning Cross-lingual Semantics with Monolingual Corpora. (arXiv:2012.15674v4 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15674","description":"<p>Recent studies have demonstrated that pre-trained cross-lingual models\nachieve impressive performance in downstream cross-lingual tasks. This\nimprovement benefits from learning a large amount of monolingual and parallel\ncorpora. Although it is generally acknowledged that parallel corpora are\ncritical for improving the model performance, existing methods are often\nconstrained by the size of parallel corpora, especially for low-resource\nlanguages. In this paper, we propose ERNIE-M, a new training method that\nencourages the model to align the representation of multiple languages with\nmonolingual corpora, to overcome the constraint that the parallel corpus size\nplaces on the model performance. Our key insight is to integrate\nback-translation into the pre-training process. We generate pseudo-parallel\nsentence pairs on a monolingual corpus to enable the learning of semantic\nalignments between different languages, thereby enhancing the semantic modeling\nof cross-lingual models. Experimental results show that ERNIE-M outperforms\nexisting cross-lingual models and delivers new state-of-the-art results in\nvarious cross-lingual downstream tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Ouyang_X/0/1/0/all/0/1\">Xuan Ouyang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1\">Shuohuan Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1\">Chao Pang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1\">Yu Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1\">Hao Tian</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1\">Hua Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1\">Haifeng Wang</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Understanding Politics via Contextualized Discourse Processing. (arXiv:2012.15784v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2012.15784","description":"<p>Politicians often have underlying agendas when reacting to events. Arguments\nin contexts of various events reflect a fairly consistent set of agendas for a\ngiven entity. In spite of recent advances in Pretrained Language Models (PLMs),\nthose text representations are not designed to capture such nuanced patterns.\nIn this paper, we propose a Compositional Reader model consisting of encoder\nand composer modules, that attempts to capture and leverage such information to\ngenerate more effective representations for entities, issues, and events. These\nrepresentations are contextualized by tweets, press releases, issues, news\narticles, and participating entities. Our model can process several documents\nat once and generate composed representations for multiple entities over\nseveral issues or events. Via qualitative and quantitative empirical analysis,\nwe show that these representations are meaningful and effective.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Pujari_R/0/1/0/all/0/1\">Rajkumar Pujari</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1\">Dan Goldwasser</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Is Everything in Order? A Simple Way to Order Sentences. (arXiv:2104.07064v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07064","description":"<p>The task of organizing a shuffled set of sentences into a coherent text has\nbeen used to evaluate a machine's understanding of causal and temporal\nrelations. We formulate the sentence ordering task as a conditional\ntext-to-marker generation problem. We present Reorder-BART (Re-BART) that\nleverages a pre-trained Transformer-based model to identify a coherent order\nfor a given set of shuffled sentences. The model takes a set of shuffled\nsentences with sentence-specific markers as input and generates a sequence of\nposition markers of the sentences in the ordered text. Re-BART achieves the\nstate-of-the-art performance across 7 datasets in Perfect Match Ratio (PMR) and\nKendall's tau ($\\tau$). We perform evaluations in a zero-shot setting,\nshowcasing that our model is able to generalize well across other datasets. We\nadditionally perform several experiments to understand the functioning and\nlimitations of our framework.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1\">Somnath Basu Roy Chowdhury</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1\">Faeze Brahman</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Chaturvedi_S/0/1/0/all/0/1\">Snigdha Chaturvedi</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Hierarchical Learning for Generation with Long Source Sequences. (arXiv:2104.07545v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2104.07545","description":"<p>One of the challenges for current sequence to sequence (seq2seq) models is\nprocessing long sequences, such as those in summarization and document level\nmachine translation tasks. These tasks require the model to reason at the token\nlevel as well as the sentence and paragraph level. We design and study a new\nHierarchical Attention Transformer-based architecture (HAT) that outperforms\nstandard Transformers on several sequence to sequence tasks. Furthermore, our\nmodel achieves state-of-the-art ROUGE scores on four summarization tasks,\nincluding PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms\ndocument-level machine translation baseline on the WMT20 English to German\ntranslation task. We investigate what the hierarchical layers learn by\nvisualizing the hierarchical encoder-decoder attention. Finally, we study\nhierarchical learning on encoder-only pre-training and analyze its performance\non classification tasks.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rohde_T/0/1/0/all/0/1\">Tobias Rohde</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1\">Xiaoxia Wu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1\">Yinhan Liu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Are VQA Systems RAD? Measuring Robustness to Augmented Data with Focused Interventions. (arXiv:2106.04484v2 [cs.CV] UPDATED)","link":"http://arxiv.org/abs/2106.04484","description":"<p>Deep learning algorithms have shown promising results in visual question\nanswering (VQA) tasks, but a more careful look reveals that they often do not\nunderstand the rich signal they are being fed with. To understand and better\nmeasure the generalization capabilities of VQA systems, we look at their\nrobustness to counterfactually augmented data. Our proposed augmentations are\ndesigned to make a focused intervention on a specific property of the question\nsuch that the answer changes. Using these augmentations, we propose a new\nrobustness measure, Robustness to Augmented Data (RAD), which measures the\nconsistency of model predictions between original and augmented examples.\nThrough extensive experimentation, we show that RAD, unlike classical accuracy\nmeasures, can quantify when state-of-the-art systems are not robust to\ncounterfactuals. We find substantial failure cases which reveal that current\nVQA systems are still brittle. Finally, we connect between robustness and\ngeneralization, demonstrating the predictive power of RAD for performance on\nunseen augmentations.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Rosenberg_D/0/1/0/all/0/1\">Daniel Rosenberg</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gat_I/0/1/0/all/0/1\">Itai Gat</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Feder_A/0/1/0/all/0/1\">Amir Feder</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1\">Roi Reichart</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Domain-Specific Pretraining for Vertical Search: Case Study on Biomedical Literature. (arXiv:2106.13375v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2106.13375","description":"<p>Information overload is a prevalent challenge in many high-value domains. A\nprominent case in point is the explosion of the biomedical literature on\nCOVID-19, which swelled to hundreds of thousands of papers in a matter of\nmonths. In general, biomedical literature expands by two papers every minute,\ntotalling over a million new papers every year. Search in the biomedical realm,\nand many other vertical domains is challenging due to the scarcity of direct\nsupervision from click logs. Self-supervised learning has emerged as a\npromising direction to overcome the annotation bottleneck. We propose a general\napproach for vertical search based on domain-specific pretraining and present a\ncase study for the biomedical domain. Despite being substantially simpler and\nnot using any relevance labels for training or development, our method performs\ncomparably or better than the best systems in the official TREC-COVID\nevaluation, a COVID-related biomedical search competition. Using distributed\ncomputing in modern cloud infrastructure, our system can scale to tens of\nmillions of articles on PubMed and has been deployed as Microsoft Biomedical\nSearch, a new search experience for biomedical literature:\nhttps://aka.ms/biomedsearch.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1\">Yu Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1\">Jinchao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1\">Tristan Naumann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1\">Chenyan Xiong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1\">Hao Cheng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Tinn_R/0/1/0/all/0/1\">Robert Tinn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1\">Cliff Wong</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1\">Naoto Usuyama</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Rogahn_R/0/1/0/all/0/1\">Richard Rogahn</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1\">Zhihong Shen</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1\">Yang Qin</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1\">Eric Horvitz</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bennett_P/0/1/0/all/0/1\">Paul N. Bennett</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1\">Hoifung Poon</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Ethics Sheets for AI Tasks. (arXiv:2107.01183v3 [cs.AI] UPDATED)","link":"http://arxiv.org/abs/2107.01183","description":"<p>Recent innovations such as Datasheets for Datasets and Model Cards for Model\nReporting have made useful contributions to furthering ethical research. Yet,\nseveral high-profile events, such as the mass testing of emotion recognition\nsystems on vulnerable sub-populations, have highlighted how technology will\noften lead to more adverse outcomes for those that are already marginalized. In\nthis paper, I will make a case for thinking about ethical considerations not\njust at the level of individual models and datasets, but also at the level of\nAI tasks. I will present a new form of such an effort, Ethics Sheets for AI\nTasks, dedicated to fleshing out the assumptions and ethical considerations\nhidden in how a task is commonly framed and in the choices we make regarding\nthe data, method, and evaluation. Finally, I will provide an example ethics\nsheet for automatic emotion recognition. Ethics sheets are a mechanism to\ndocument ethical considerations \\textit{before} building datasets and systems.\nSuch pre-production activities (e.g., ethics analyses) and associated artifacts\n(e.g., accessible documentation) are crucial for responsible AI: for\ncommunicating risks to all stakeholders, to help decision and policy making,\nand for developing more effective post-production documents such as Data Sheets\nand Model Cards.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Mohammad_S/0/1/0/all/0/1\">Saif M. Mohammad</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Pattern-based Acquisition of Scientific Entities from Scholarly Article Titles. (arXiv:2109.00199v2 [cs.IR] UPDATED)","link":"http://arxiv.org/abs/2109.00199","description":"<p>We describe a rule-based approach for the automatic acquisition of salient\nscientific entities from Computational Linguistics (CL) scholarly article\ntitles. Two observations motivated the approach: (i) noting salient aspects of\nan article's contribution in its title; and (ii) pattern regularities capturing\nthe salient terms that could be expressed in a set of rules. Only those\nlexico-syntactic patterns were selected that were easily recognizable, occurred\nfrequently, and positionally indicated a scientific entity type. The rules were\ndeveloped on a collection of 50,237 CL titles covering all articles in the ACL\nAnthology. In total, 19,799 research problems, 18,111 solutions, 20,033\nresources, 1,059 languages, 6,878 tools, and 21,687 methods were extracted at\nan average precision of 75%.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1\">Jennifer D&#x27;Souza</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1\">Soeren Auer</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"WebQA: Multihop and Multimodal QA. (arXiv:2109.00590v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00590","description":"<p>Web search is fundamentally multimodal and multihop. Often, even before\nasking a question we choose to go directly to image search to find our answers.\nFurther, rarely do we find an answer from a single source but aggregate\ninformation and reason through implications. Despite the frequency of this\neveryday occurrence, at present, there is no unified question answering\nbenchmark that requires a single model to answer long-form natural language\nquestions from text and open-ended visual sources -- akin to a human's\nexperience. We propose to bridge this gap between the natural language and\ncomputer vision communities with WebQA. We show that A. our multihop text\nqueries are difficult for a large-scale transformer model, and B. existing\nmulti-modal transformers and visual representations do not perform well on\nopen-domain visual queries. Our challenge for the community is to create a\nunified multimodal reasoning model that seamlessly transitions and reasons\nregardless of the source modality.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1\">Yingshan Chang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Narang_M/0/1/0/all/0/1\">Mridu Narang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Suzuki_H/0/1/0/all/0/1\">Hisami Suzuki</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1\">Guihong Cao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1\">Jianfeng Gao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1\">Yonatan Bisk</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Tree-constrained Pointer Generator for End-to-end Contextual Speech Recognition. (arXiv:2109.00627v3 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.00627","description":"<p>Contextual knowledge is important for real-world automatic speech recognition\n(ASR) applications. In this paper, a novel tree-constrained pointer generator\n(TCPGen) component is proposed that incorporates such knowledge as a list of\nbiasing words into both attention-based encoder-decoder and transducer\nend-to-end ASR models in a neural-symbolic way. TCPGen structures the biasing\nwords into an efficient prefix tree to serve as its symbolic input and creates\na neural shortcut between the tree and the final ASR output distribution to\nfacilitate recognising biasing words during decoding. Systems were trained and\nevaluated on the Librispeech corpus where biasing words were extracted at the\nscales of an utterance, a chapter, or a book to simulate different application\nscenarios. Experimental results showed that TCPGen consistently improved word\nerror rates (WERs) compared to the baselines, and in particular, achieved\nsignificant WER reductions on the biasing words. TCPGen is highly efficient: it\ncan handle 5,000 biasing words and distractors and only add a small overhead to\nmemory use and computation cost.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1\">Guangzhi Sun</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1\">Chao Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1\">Philip C. Woodland</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Biomedical and Clinical Language Models for Spanish: On the Benefits of Domain-Specific Pretraining in a Mid-Resource Scenario. (arXiv:2109.03570v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.03570","description":"<p>This work presents biomedical and clinical language models for Spanish by\nexperimenting with different pretraining choices, such as masking at word and\nsubword level, varying the vocabulary size and testing with domain data,\nlooking for better language representations. Interestingly, in the absence of\nenough clinical data to train a model from scratch, we applied mixed-domain\npretraining and cross-domain transfer approaches to generate a performant\nbio-clinical model suitable for real-world clinical data. We evaluated our\nmodels on Named Entity Recognition (NER) tasks for biomedical documents and\nchallenging hospital discharge reports. When compared against the competitive\nmBERT and BETO models, we outperform them in all NER tasks by a significant\nmargin. Finally, we studied the impact of the model's vocabulary on the NER\nperformances by offering an interesting vocabulary-centric analysis. The\nresults confirm that domain-specific pretraining is fundamental to achieving\nhigher performances in downstream NER tasks, even within a mid-resource\nscenario. To the best of our knowledge, we provide the first biomedical and\nclinical transformer-based pretrained language models for Spanish, intending to\nboost native Spanish NLP applications in biomedicine. Our best models are\nfreely available in the HuggingFace hub: https://huggingface.co/BSC-TeMU.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Carrino_C/0/1/0/all/0/1\">Casimiro Pio Carrino</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Armengol_Estape_J/0/1/0/all/0/1\">Jordi Armengol-Estap&#xe9;</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gutierrez_Fandino_A/0/1/0/all/0/1\">Asier Guti&#xe9;rrez-Fandi&#xf1;o</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Llop_Palao_J/0/1/0/all/0/1\">Joan Llop-Palao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Pamies_M/0/1/0/all/0/1\">Marc P&#xe0;mies</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Gonzalez_Agirre_A/0/1/0/all/0/1\">Aitor Gonzalez-Agirre</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Villegas_M/0/1/0/all/0/1\">Marta Villegas</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Fixing exposure bias with imitation learning needs powerful oracles. (arXiv:2109.04114v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04114","description":"<p>We apply imitation learning (IL) to tackle the NMT exposure bias problem with\nerror-correcting oracles, and evaluate an SMT lattice-based oracle which,\ndespite its excellent performance in an unconstrained oracle translation task,\nturned out to be too pruned and idiosyncratic to serve as the oracle for IL.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hormann_L/0/1/0/all/0/1\">Luca Hormann</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Sokolov_A/0/1/0/all/0/1\">Artem Sokolov</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Cartography Active Learning. (arXiv:2109.04282v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.04282","description":"<p>We propose Cartography Active Learning (CAL), a novel Active Learning (AL)\nalgorithm that exploits the behavior of the model on individual instances\nduring training as a proxy to find the most informative instances for labeling.\nCAL is inspired by data maps, which were recently proposed to derive insights\ninto dataset quality (Swayamdipta et al., 2020). We compare our method on\npopular text classification tasks to commonly used AL strategies, which instead\nrely on post-training behavior. We demonstrate that CAL is competitive to other\ncommon AL methods, showing that training dynamics derived from small seed data\ncan be successfully used for AL. We provide insights into our new AL method by\nanalyzing batch-level statistics utilizing the data maps. Our results further\nshow that CAL results in a more data-efficient learning strategy, achieving\ncomparable or better results with considerably less training data.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1\">Mike Zhang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Plank_B/0/1/0/all/0/1\">Barbara Plank</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"Asking Questions Like Educational Experts: Automatically Generating Question-Answer Pairs on Real-World Examination Data. (arXiv:2109.05179v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.05179","description":"<p>Generating high quality question-answer pairs is a hard but meaningful task.\nAlthough previous works have achieved great results on answer-aware question\ngeneration, it is difficult to apply them into practical application in the\neducation field. This paper for the first time addresses the question-answer\npair generation task on the real-world examination data, and proposes a new\nunified framework on RACE. To capture the important information of the input\npassage we first automatically generate(rather than extracting) keyphrases,\nthus this task is reduced to keyphrase-question-answer triplet joint\ngeneration. Accordingly, we propose a multi-agent communication model to\ngenerate and optimize the question and keyphrases iteratively, and then apply\nthe generated question and keyphrases to guide the generation of answers. To\nestablish a solid benchmark, we build our model on the strong generative\npre-training model. Experimental results show that our model makes great\nbreakthroughs in the question-answer pair generation task. Moreover, we make a\ncomprehensive analysis on our model, suggesting new directions for this\nchallenging task.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Qu_F/0/1/0/all/0/1\">Fanyi Qu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1\">Xin Jia</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1\">Yunfang Wu</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}},{"title":"RankNAS: Efficient Neural Architecture Search by Pairwise Ranking. (arXiv:2109.07383v2 [cs.CL] UPDATED)","link":"http://arxiv.org/abs/2109.07383","description":"<p>This paper addresses the efficiency challenge of Neural Architecture Search\n(NAS) by formulating the task as a ranking problem. Previous methods require\nnumerous training examples to estimate the accurate performance of\narchitectures, although the actual goal is to find the distinction between\n\"good\" and \"bad\" candidates. Here we do not resort to performance predictors.\nInstead, we propose a performance ranking method (RankNAS) via pairwise\nranking. It enables efficient architecture search using much fewer training\nexamples. Moreover, we develop an architecture selection method to prune the\nsearch space and concentrate on more promising candidates. Extensive\nexperiments on machine translation and language modeling tasks show that\nRankNAS can design high-performance architectures while being orders of\nmagnitude faster than state-of-the-art NAS systems.\n</p>","author":null,"categories":[],"comments":null,"enclosure":null,"guid":null,"pub_date":null,"source":null,"content":null,"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":["<a href=\"http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1\">Chi Hu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1\">Chenglong Wang</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1\">Xiangnan Ma</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1\">Xia Meng</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1\">Yinqiao Li</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1\">Tong Xiao</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1\">Jingbo Zhu</a>, <a href=\"http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1\">Changliang Li</a>"],"dates":[],"descriptions":[],"formats":[],"identifiers":[],"languages":[],"publishers":[],"relations":[],"rights":[],"sources":[],"subjects":[],"titles":[],"types":[]}}],"extensions":{},"itunes_ext":null,"dublin_core_ext":{"contributors":[],"coverages":[],"creators":[],"dates":["2021-09-19T20:30:00-05:00"],"descriptions":[],"formats":[],"identifiers":[],"languages":["en-us"],"publishers":["help@arxiv.org"],"relations":[],"rights":[],"sources":[],"subjects":["Computer Science -- Computation and Language"],"titles":[],"types":[]},"syndication_ext":{"period":"DAILY","frequency":1,"base":"1901-01-01T00:00+00:00"},"namespaces":{"dc":"http://purl.org/dc/elements/1.1/","taxo":"http://purl.org/rss/1.0/modules/taxonomy/","syn":"http://purl.org/rss/1.0/modules/syndication/","admin":"http://webns.net/mvcb/","rdf":"http://www.w3.org/1999/02/22-rdf-syntax-ns#","content":"http://purl.org/rss/1.0/modules/content/"}}]}]}